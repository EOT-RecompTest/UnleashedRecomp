#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_823253D8"))) PPC_WEAK_FUNC(sub_823253D8);
PPC_FUNC_IMPL(__imp__sub_823253D8) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r4,5384(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5384, ctx.r4.u32);
	// stw r5,5388(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5388, ctx.r5.u32);
	// stw r11,5392(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5392, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823253EC"))) PPC_WEAK_FUNC(sub_823253EC);
PPC_FUNC_IMPL(__imp__sub_823253EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823253F0"))) PPC_WEAK_FUNC(sub_823253F0);
PPC_FUNC_IMPL(__imp__sub_823253F0) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x82325400
	if (ctx.cr6.eq) goto loc_82325400;
	// lwz r11,5384(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5384);
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
loc_82325400:
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// lwz r11,5392(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5392);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82325414"))) PPC_WEAK_FUNC(sub_82325414);
PPC_FUNC_IMPL(__imp__sub_82325414) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82325418"))) PPC_WEAK_FUNC(sub_82325418);
PPC_FUNC_IMPL(__imp__sub_82325418) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r5,32767
	ctx.r5.s64 = 2147418112;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// ori r5,r5,65535
	ctx.r5.u64 = ctx.r5.u64 | 65535;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82325454;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r8,28(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x82325470;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmpwi cr6,r7,4
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 4, ctx.xer);
	// bge cr6,0x82325484
	if (!ctx.cr6.lt) goto loc_82325484;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8232548c
	goto loc_8232548C;
loc_82325484:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// bl 0x82327538
	ctx.lr = 0x8232548C;
	sub_82327538(ctx, base);
loc_8232548C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823254A4"))) PPC_WEAK_FUNC(sub_823254A4);
PPC_FUNC_IMPL(__imp__sub_823254A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823254A8"))) PPC_WEAK_FUNC(sub_823254A8);
PPC_FUNC_IMPL(__imp__sub_823254A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lwz r3,5240(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5240);
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x82325538
	if (!ctx.cr6.eq) goto loc_82325538;
	// li r5,64
	ctx.r5.s64 = 64;
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r4,4(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// bl 0x82327578
	ctx.lr = 0x823254E0;
	sub_82327578(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82325534
	if (ctx.cr6.eq) goto loc_82325534;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// li r5,-1
	ctx.r5.s64 = -1;
	// subf r11,r3,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r3.s64;
	// add r4,r11,r10
	ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
	// bl 0x82327578
	ctx.lr = 0x82325504;
	sub_82327578(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82325534
	if (ctx.cr6.eq) goto loc_82325534;
	// bl 0x82327538
	ctx.lr = 0x82325510;
	sub_82327538(ctx, base);
	// rlwinm r11,r3,0,27,27
	ctx.r11.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x10;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x82325524
	if (ctx.cr6.eq) goto loc_82325524;
	// li r11,2
	ctx.r11.s64 = 2;
	// b 0x82325530
	goto loc_82325530;
loc_82325524:
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x82325534
	if (ctx.cr6.eq) goto loc_82325534;
	// li r11,1
	ctx.r11.s64 = 1;
loc_82325530:
	// stw r11,5240(r30)
	PPC_STORE_U32(ctx.r30.u32 + 5240, ctx.r11.u32);
loc_82325534:
	// lwz r3,5240(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 5240);
loc_82325538:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82325550"))) PPC_WEAK_FUNC(sub_82325550);
PPC_FUNC_IMPL(__imp__sub_82325550) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// addi r10,r11,11264
	ctx.r10.s64 = ctx.r11.s64 + 11264;
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r11,4396(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4396, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82325564"))) PPC_WEAK_FUNC(sub_82325564);
PPC_FUNC_IMPL(__imp__sub_82325564) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82325568"))) PPC_WEAK_FUNC(sub_82325568);
PPC_FUNC_IMPL(__imp__sub_82325568) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// li r5,64
	ctx.r5.s64 = 64;
	// addi r10,r11,11280
	ctx.r10.s64 = ctx.r11.s64 + 11280;
	// addi r3,r3,2304
	ctx.r3.s64 = ctx.r3.s64 + 2304;
	// lwz r4,4(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// b 0x82329ab8
	sub_82329AB8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82325580"))) PPC_WEAK_FUNC(sub_82325580);
PPC_FUNC_IMPL(__imp__sub_82325580) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// li r10,64
	ctx.r10.s64 = 64;
	// addi r11,r3,2560
	ctx.r11.s64 = ctx.r3.s64 + 2560;
	// lis r9,16256
	ctx.r9.s64 = 1065353216;
	// addi r11,r11,-4
	ctx.r11.s64 = ctx.r11.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_82325594:
	// stwu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r11.u32 = ea;
	// bdnz 0x82325594
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82325594;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823255A0"))) PPC_WEAK_FUNC(sub_823255A0);
PPC_FUNC_IMPL(__imp__sub_823255A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x823255A8;
	__restfpr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,2
	ctx.r11.s64 = 2;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// stw r11,5332(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5332, ctx.r11.u32);
	// li r28,0
	ctx.r28.s64 = 0;
	// lwz r11,4876(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4876);
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// stw r10,4876(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4876, ctx.r10.u32);
	// addi r29,r3,5288
	ctx.r29.s64 = ctx.r3.s64 + 5288;
	// stw r28,5400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5400, ctx.r28.u32);
	// lis r5,32767
	ctx.r5.s64 = 2147418112;
	// stw r28,5404(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5404, ctx.r28.u32);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// stw r28,4932(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4932, ctx.r28.u32);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// ori r5,r5,65535
	ctx.r5.u64 = ctx.r5.u64 | 65535;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r8,24(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x82325600;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r7,5288(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5288);
	// rlwinm r11,r7,0,0,29
	ctx.r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFC;
	// subf r6,r11,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r11.s64;
	// rlwinm r10,r6,3,0,28
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzu r8,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r8.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// beq cr6,0x82325624
	if (ctx.cr6.eq) goto loc_82325624;
	// slw r8,r8,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
loc_82325624:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r4,r11,4
	ctx.r4.s64 = ctx.r11.s64 + 4;
	// cmpwi cr6,r10,7
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 7, ctx.xer);
	// blt cr6,0x82325670
	if (ctx.cr6.lt) goto loc_82325670;
	// addic. r11,r10,-7
	ctx.xer.ca = ctx.r10.u32 > 6;
	ctx.r11.s64 = ctx.r10.s64 + -7;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x8232565c
	if (ctx.cr0.eq) goto loc_8232565C;
	// subfic r9,r11,25
	ctx.xer.ca = ctx.r11.u32 <= 25;
	ctx.r9.s64 = 25 - ctx.r11.s64;
	// slw r10,r7,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// srw r7,r7,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r9.u8 & 0x3F));
	// or r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 | ctx.r8.u64;
	// lwz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// rlwinm r9,r6,25,7,31
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// b 0x8232567c
	goto loc_8232567C;
loc_8232565C:
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// lwz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// rlwinm r9,r8,25,7,31
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// b 0x8232567c
	goto loc_8232567C;
loc_82325670:
	// addi r11,r10,25
	ctx.r11.s64 = ctx.r10.s64 + 25;
	// rlwinm r9,r8,25,7,31
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// rlwinm r10,r8,25,0,6
	ctx.r10.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0xFE000000;
loc_8232567C:
	// rlwinm r8,r9,26,6,31
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 26) & 0x3FFFFFF;
	// clrlwi r6,r9,26
	ctx.r6.u64 = ctx.r9.u32 & 0x3F;
	// rlwinm r5,r8,25,7,31
	ctx.r5.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// clrlwi r3,r8,26
	ctx.r3.u64 = ctx.r8.u32 & 0x3F;
	// stw r6,4872(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4872, ctx.r6.u32);
	// rlwinm r9,r5,26,6,31
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 26) & 0x3FFFFFF;
	// clrlwi r8,r5,26
	ctx.r8.u64 = ctx.r5.u32 & 0x3F;
	// stw r3,4868(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4868, ctx.r3.u32);
	// clrlwi r6,r9,27
	ctx.r6.u64 = ctx.r9.u32 & 0x1F;
	// rlwinm r5,r9,27,5,31
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x7FFFFFF;
	// stw r8,4864(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4864, ctx.r8.u32);
	// rlwinm r3,r10,1,31,31
	ctx.r3.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// stw r6,4860(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4860, ctx.r6.u32);
	// stw r5,4856(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4856, ctx.r5.u32);
	// cmpwi cr6,r11,31
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 31, ctx.xer);
	// stw r3,5072(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5072, ctx.r3.u32);
	// bne cr6,0x823256cc
	if (!ctx.cr6.eq) goto loc_823256CC;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// b 0x823256d4
	goto loc_823256D4;
loc_823256CC:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
loc_823256D4:
	// rlwinm r10,r7,1,31,31
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// cmpwi cr6,r11,31
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 31, ctx.xer);
	// stw r10,5076(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5076, ctx.r10.u32);
	// bne cr6,0x823256f0
	if (!ctx.cr6.eq) goto loc_823256F0;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// b 0x823256f4
	goto loc_823256F4;
loc_823256F0:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
loc_823256F4:
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// srawi r9,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 3;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// subf r11,r10,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// addi r4,r11,-8
	ctx.r4.s64 = ctx.r11.s64 + -8;
	// bl 0x8232b788
	ctx.lr = 0x8232571C;
	sub_8232B788(ctx, base);
	// lwz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r7,32(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 32);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x82325738;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r6,0(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r11,28(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82325754;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82325760"))) PPC_WEAK_FUNC(sub_82325760);
PPC_FUNC_IMPL(__imp__sub_82325760) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e460
	ctx.lr = 0x82325768;
	__restfpr_26(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,3
	ctx.r11.s64 = 3;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// stw r11,5332(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5332, ctx.r11.u32);
	// addi r28,r3,5288
	ctx.r28.s64 = ctx.r3.s64 + 5288;
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lis r5,32767
	ctx.r5.s64 = 2147418112;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// ori r5,r5,65535
	ctx.r5.u64 = ctx.r5.u64 | 65535;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// addi r30,r31,4828
	ctx.r30.s64 = ctx.r31.s64 + 4828;
	// lwz r9,24(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x823257A8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r8,5288(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5288);
	// rlwinm r11,r8,0,0,29
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFC;
	// subf r7,r11,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r10,r7,3,0,28
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// addi r8,r11,4
	ctx.r8.s64 = ctx.r11.s64 + 4;
	// slw r11,r9,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// bne cr6,0x823257d0
	if (!ctx.cr6.eq) goto loc_823257D0;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_823257D0:
	// lwz r5,0(r8)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r29,r8,4
	ctx.r29.s64 = ctx.r8.s64 + 4;
	// cmpwi cr6,r10,22
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 22, ctx.xer);
	// blt cr6,0x82325824
	if (ctx.cr6.lt) goto loc_82325824;
	// addic. r10,r10,-22
	ctx.xer.ca = ctx.r10.u32 > 21;
	ctx.r10.s64 = ctx.r10.s64 + -22;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x8232580c
	if (ctx.cr0.eq) goto loc_8232580C;
	// subfic r9,r10,10
	ctx.xer.ca = ctx.r10.u32 <= 10;
	ctx.r9.s64 = 10 - ctx.r10.s64;
	// srw r8,r5,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 | ctx.r11.u64;
	// slw r11,r5,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r6,r7,10,22,31
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 10) & 0x3FF;
	// stw r6,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r6.u32);
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x82325834
	goto loc_82325834;
loc_8232580C:
	// rlwinm r9,r11,10,22,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 10) & 0x3FF;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// stw r9,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r9.u32);
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x82325834
	goto loc_82325834;
loc_82325824:
	// rlwinm r9,r11,10,22,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 10) & 0x3FF;
	// addi r10,r10,10
	ctx.r10.s64 = ctx.r10.s64 + 10;
	// stw r9,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r9.u32);
	// rlwinm r11,r11,10,0,21
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 10) & 0xFFFFFC00;
loc_82325834:
	// cmpwi cr6,r10,29
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 29, ctx.xer);
	// blt cr6,0x82325878
	if (ctx.cr6.lt) goto loc_82325878;
	// addic. r10,r10,-29
	ctx.xer.ca = ctx.r10.u32 > 28;
	ctx.r10.s64 = ctx.r10.s64 + -29;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x82325864
	if (ctx.cr0.eq) goto loc_82325864;
	// subfic r9,r10,3
	ctx.xer.ca = ctx.r10.u32 <= 3;
	ctx.r9.s64 = 3 - ctx.r10.s64;
	// srw r8,r5,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 | ctx.r11.u64;
	// slw r11,r5,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// rlwinm r9,r7,3,29,31
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0x7;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x82325884
	goto loc_82325884;
loc_82325864:
	// rlwinm r9,r11,3,29,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0x7;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x82325884
	goto loc_82325884;
loc_82325878:
	// rlwinm r9,r11,3,29,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0x7;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
loc_82325884:
	// cmpwi cr6,r10,16
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 16, ctx.xer);
	// stw r9,24(r30)
	PPC_STORE_U32(ctx.r30.u32 + 24, ctx.r9.u32);
	// blt cr6,0x823258d4
	if (ctx.cr6.lt) goto loc_823258D4;
	// addic. r10,r10,-16
	ctx.xer.ca = ctx.r10.u32 > 15;
	ctx.r10.s64 = ctx.r10.s64 + -16;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x823258bc
	if (ctx.cr0.eq) goto loc_823258BC;
	// subfic r9,r10,16
	ctx.xer.ca = ctx.r10.u32 <= 16;
	ctx.r9.s64 = 16 - ctx.r10.s64;
	// slw r8,r5,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r10.u8 & 0x3F));
	// srw r7,r5,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r9.u8 & 0x3F));
	// or r6,r7,r11
	ctx.r6.u64 = ctx.r7.u64 | ctx.r11.u64;
	// rlwinm r5,r6,16,16,31
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 16) & 0xFFFF;
	// stw r5,5080(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5080, ctx.r5.u32);
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x823258e4
	goto loc_823258E4;
loc_823258BC:
	// rlwinm r11,r11,16,16,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 16) & 0xFFFF;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// stw r11,5080(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5080, ctx.r11.u32);
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x823258e4
	goto loc_823258E4;
loc_823258D4:
	// rlwinm r9,r11,16,16,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 16) & 0xFFFF;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r9,5080(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5080, ctx.r9.u32);
	// rlwinm r8,r11,16,0,15
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 16) & 0xFFFF0000;
loc_823258E4:
	// lwz r4,24(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	// cmpwi cr6,r4,1
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 1, ctx.xer);
	// beq cr6,0x8232591c
	if (ctx.cr6.eq) goto loc_8232591C;
	// cmpwi cr6,r4,2
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 2, ctx.xer);
	// beq cr6,0x8232591c
	if (ctx.cr6.eq) goto loc_8232591C;
	// lwz r9,5400(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5400);
	// lwz r11,5404(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5404);
	// rlwinm r7,r9,16,0,15
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 16) & 0xFFFF0000;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addis r6,r7,-1
	ctx.r6.s64 = ctx.r7.s64 + -65536;
	// stw r11,5404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5404, ctx.r11.u32);
	// or r3,r6,r11
	ctx.r3.u64 = ctx.r6.u64 | ctx.r11.u64;
	// stw r3,104(r30)
	PPC_STORE_U32(ctx.r30.u32 + 104, ctx.r3.u32);
	// b 0x82325938
	goto loc_82325938;
loc_8232591C:
	// lwz r11,5400(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5400);
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r9,5404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5404, ctx.r9.u32);
	// rlwinm r7,r11,16,0,15
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 16) & 0xFFFF0000;
	// stw r11,5400(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5400, ctx.r11.u32);
	// stw r7,104(r30)
	PPC_STORE_U32(ctx.r30.u32 + 104, ctx.r7.u32);
loc_82325938:
	// cmpwi cr6,r4,2
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 2, ctx.xer);
	// beq cr6,0x82325948
	if (ctx.cr6.eq) goto loc_82325948;
	// cmpwi cr6,r4,3
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 3, ctx.xer);
	// bne cr6,0x823259dc
	if (!ctx.cr6.eq) goto loc_823259DC;
loc_82325948:
	// rlwinm r11,r8,1,31,31
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1;
	// cmpwi cr6,r10,31
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 31, ctx.xer);
	// stw r11,5132(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5132, ctx.r11.u32);
	// bne cr6,0x82325ab0
	if (!ctx.cr6.eq) goto loc_82325AB0;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
loc_82325968:
	// rlwinm r9,r11,3,29,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0x7;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// rlwinm r8,r11,3,0,28
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
loc_82325974:
	// addi r11,r9,-1
	ctx.r11.s64 = ctx.r9.s64 + -1;
	// li r7,1
	ctx.r7.s64 = 1;
	// subfic r9,r11,27
	ctx.xer.ca = ctx.r11.u32 <= 27;
	ctx.r9.s64 = 27 - ctx.r11.s64;
	// stw r11,5136(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5136, ctx.r11.u32);
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// stw r9,5140(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5140, ctx.r9.u32);
	// cmpwi cr6,r4,3
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 3, ctx.xer);
	// stw r6,5144(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5144, ctx.r6.u32);
	// bne cr6,0x823259dc
	if (!ctx.cr6.eq) goto loc_823259DC;
	// rlwinm r11,r8,1,31,31
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1;
	// cmpwi cr6,r10,31
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 31, ctx.xer);
	// stw r11,5168(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5168, ctx.r11.u32);
	// bne cr6,0x82325afc
	if (!ctx.cr6.eq) goto loc_82325AFC;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
loc_823259B8:
	// rlwinm r9,r11,3,29,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0x7;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// rlwinm r8,r11,3,0,28
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
loc_823259C4:
	// addi r11,r9,-1
	ctx.r11.s64 = ctx.r9.s64 + -1;
	// subfic r9,r11,27
	ctx.xer.ca = ctx.r11.u32 <= 27;
	ctx.r9.s64 = 27 - ctx.r11.s64;
	// stw r11,5172(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5172, ctx.r11.u32);
	// slw r7,r7,r11
	ctx.r7.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// stw r9,5176(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5176, ctx.r9.u32);
	// stw r7,5180(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5180, ctx.r7.u32);
loc_823259DC:
	// lwz r11,4776(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4776);
	// lis r7,-32253
	ctx.r7.s64 = -2113732608;
	// lwz r6,4768(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4768);
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,-3
	ctx.r11.s64 = ctx.r11.s64 + -3;
	// rlwinm r9,r6,1,0,30
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// addic r6,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r6.s64 = ctx.r11.s64 + -1;
	// addi r30,r7,-1444
	ctx.r30.s64 = ctx.r7.s64 + -1444;
	// subfe r11,r6,r11
	temp.u8 = (~ctx.r6.u32 + ctx.r11.u32 < ~ctx.r6.u32) | (~ctx.r6.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r6.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// lis r26,-32183
	ctx.r26.s64 = -2109145088;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r3,r30
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r30.u32);
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r11,r7
	ctx.r7.u64 = ctx.r11.u64 + ctx.r7.u64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// addi r11,r26,14824
	ctx.r11.s64 = ctx.r26.s64 + 14824;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// stw r3,5084(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5084, ctx.r3.u32);
	// add r6,r9,r4
	ctx.r6.u64 = ctx.r9.u64 + ctx.r4.u64;
	// rlwinm r4,r7,2,0,29
	ctx.r4.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r3,r11,164
	ctx.r3.s64 = ctx.r11.s64 + 164;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r11,124
	ctx.r7.s64 = ctx.r11.s64 + 124;
	// addi r6,r11,80
	ctx.r6.s64 = ctx.r11.s64 + 80;
	// addi r30,r11,204
	ctx.r30.s64 = ctx.r11.s64 + 204;
	// lwzx r3,r4,r3
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r3.u32);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// stw r3,5088(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5088, ctx.r3.u32);
	// lwzx r11,r9,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// stw r11,5100(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5100, ctx.r11.u32);
	// lwzx r9,r4,r7
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r7.u32);
	// stw r9,5108(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5108, ctx.r9.u32);
	// lwzx r7,r4,r6
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r6.u32);
	// rotlwi r6,r7,0
	ctx.r6.u64 = rotl32(ctx.r7.u32, 0);
	// stw r7,5112(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5112, ctx.r7.u32);
	// lwzx r4,r4,r30
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r30.u32);
	// stw r4,5116(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5116, ctx.r4.u32);
	// stw r6,5104(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5104, ctx.r6.u32);
	// bge cr6,0x82325b6c
	if (!ctx.cr6.lt) goto loc_82325B6C;
	// lwz r7,0(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// addi r11,r10,7
	ctx.r11.s64 = ctx.r10.s64 + 7;
	// lwz r6,5292(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5292);
loc_82325A88:
	// addi r10,r10,9
	ctx.r10.s64 = ctx.r10.s64 + 9;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// cmpwi cr6,r10,32
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 32, ctx.xer);
	// blt cr6,0x82325b48
	if (ctx.cr6.lt) goto loc_82325B48;
	// addi r10,r10,-32
	ctx.r10.s64 = ctx.r10.s64 + -32;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r8,r5,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x82325b4c
	goto loc_82325B4C;
loc_82325AB0:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r11,r8,1,0,30
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r10,29
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 29, ctx.xer);
	// blt cr6,0x82325968
	if (ctx.cr6.lt) goto loc_82325968;
	// addic. r10,r10,-29
	ctx.xer.ca = ctx.r10.u32 > 28;
	ctx.r10.s64 = ctx.r10.s64 + -29;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x82325ae8
	if (ctx.cr0.eq) goto loc_82325AE8;
	// subfic r9,r10,3
	ctx.xer.ca = ctx.r10.u32 <= 3;
	ctx.r9.s64 = 3 - ctx.r10.s64;
	// slw r8,r5,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r10.u8 & 0x3F));
	// srw r7,r5,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r9.u8 & 0x3F));
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// or r6,r7,r11
	ctx.r6.u64 = ctx.r7.u64 | ctx.r11.u64;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// rlwinm r9,r6,3,29,31
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0x7;
	// b 0x82325974
	goto loc_82325974;
loc_82325AE8:
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// rlwinm r9,r11,3,29,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0x7;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x82325974
	goto loc_82325974;
loc_82325AFC:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r11,r8,1,0,30
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r10,29
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 29, ctx.xer);
	// blt cr6,0x823259b8
	if (ctx.cr6.lt) goto loc_823259B8;
	// addic. r10,r10,-29
	ctx.xer.ca = ctx.r10.u32 > 28;
	ctx.r10.s64 = ctx.r10.s64 + -29;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x82325b34
	if (ctx.cr0.eq) goto loc_82325B34;
	// subfic r9,r10,3
	ctx.xer.ca = ctx.r10.u32 <= 3;
	ctx.r9.s64 = 3 - ctx.r10.s64;
	// slw r8,r5,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r10.u8 & 0x3F));
	// srw r6,r5,r9
	ctx.r6.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r9.u8 & 0x3F));
	// or r5,r6,r11
	ctx.r5.u64 = ctx.r6.u64 | ctx.r11.u64;
	// rlwinm r9,r5,3,29,31
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0x7;
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x823259c4
	goto loc_823259C4;
loc_82325B34:
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// rlwinm r9,r11,3,29,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0x7;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x823259c4
	goto loc_823259C4;
loc_82325B48:
	// rlwinm r8,r8,9,0,22
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 9) & 0xFFFFFE00;
loc_82325B4C:
	// srawi r9,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 3;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + ctx.r29.u64;
	// addi r4,r9,-8
	ctx.r4.s64 = ctx.r9.s64 + -8;
	// cmpw cr6,r6,r4
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r4.s32, ctx.xer);
	// ble cr6,0x82325bec
	if (!ctx.cr6.gt) goto loc_82325BEC;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// blt cr6,0x82325a88
	if (ctx.cr6.lt) goto loc_82325A88;
loc_82325B6C:
	// addi r11,r10,1
	ctx.r11.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x82325b80
	if (ctx.cr6.lt) goto loc_82325B80;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
loc_82325B80:
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// srawi r9,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 3;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// subf r11,r10,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// add r11,r11,r29
	ctx.r11.u64 = ctx.r11.u64 + ctx.r29.u64;
	// addi r4,r11,-8
	ctx.r4.s64 = ctx.r11.s64 + -8;
	// bl 0x8232b788
	ctx.lr = 0x82325BA8;
	sub_8232B788(ctx, base);
	// lwz r8,0(r27)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// lwz r7,32(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 32);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x82325BC4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r6,0(r27)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// lwz r11,28(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82325BE0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
loc_82325BEC:
	// li r3,-3
	ctx.r3.s64 = -3;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82325BF8"))) PPC_WEAK_FUNC(sub_82325BF8);
PPC_FUNC_IMPL(__imp__sub_82325BF8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e454
	ctx.lr = 0x82325C00;
	__restfpr_23(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r23,0
	ctx.r23.s64 = 0;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// addic. r26,r5,-4
	ctx.xer.ca = ctx.r5.u32 > 3;
	ctx.r26.s64 = ctx.r5.s64 + -4;
	ctx.cr0.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// addi r30,r4,4
	ctx.r30.s64 = ctx.r4.s64 + 4;
	// mr r31,r23
	ctx.r31.u64 = ctx.r23.u64;
	// ble 0x82325cd0
	if (!ctx.cr0.gt) goto loc_82325CD0;
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// li r24,3
	ctx.r24.s64 = 3;
	// addi r25,r10,-1416
	ctx.r25.s64 = ctx.r10.s64 + -1416;
	// addi r27,r11,-1424
	ctx.r27.s64 = ctx.r11.s64 + -1424;
loc_82325C30:
	// add r28,r31,r30
	ctx.r28.u64 = ctx.r31.u64 + ctx.r30.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// li r5,7
	ctx.r5.s64 = 7;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233dc60
	ctx.lr = 0x82325C44;
	sub_8233DC60(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x82325c6c
	if (!ctx.cr6.eq) goto loc_82325C6C;
	// add r11,r31,r30
	ctx.r11.u64 = ctx.r31.u64 + ctx.r30.u64;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// bl 0x8233d168
	ctx.lr = 0x82325C58;
	sub_8233D168(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x82325c68
	if (!ctx.cr6.eq) goto loc_82325C68;
	// stw r23,5300(r29)
	PPC_STORE_U32(ctx.r29.u32 + 5300, ctx.r23.u32);
	// b 0x82325c6c
	goto loc_82325C6C;
loc_82325C68:
	// stw r24,5300(r29)
	PPC_STORE_U32(ctx.r29.u32 + 5300, ctx.r24.u32);
loc_82325C6C:
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// li r5,7
	ctx.r5.s64 = 7;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233dc60
	ctx.lr = 0x82325C7C;
	sub_8233DC60(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x82325cb4
	if (!ctx.cr6.eq) goto loc_82325CB4;
	// add r11,r31,r30
	ctx.r11.u64 = ctx.r31.u64 + ctx.r30.u64;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// bl 0x8233d168
	ctx.lr = 0x82325C90;
	sub_8233D168(ctx, base);
	// stw r3,5244(r29)
	PPC_STORE_U32(ctx.r29.u32 + 5244, ctx.r3.u32);
	// add r11,r31,r30
	ctx.r11.u64 = ctx.r31.u64 + ctx.r30.u64;
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// bl 0x8233d168
	ctx.lr = 0x82325CA0;
	sub_8233D168(ctx, base);
	// stw r3,5248(r29)
	PPC_STORE_U32(ctx.r29.u32 + 5248, ctx.r3.u32);
	// add r11,r31,r30
	ctx.r11.u64 = ctx.r31.u64 + ctx.r30.u64;
	// addi r3,r11,32
	ctx.r3.s64 = ctx.r11.s64 + 32;
	// bl 0x8233d168
	ctx.lr = 0x82325CB0;
	sub_8233D168(ctx, base);
	// stw r3,5252(r29)
	PPC_STORE_U32(ctx.r29.u32 + 5252, ctx.r3.u32);
loc_82325CB4:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82327538
	ctx.lr = 0x82325CBC;
	sub_82327538(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x82325cd0
	if (!ctx.cr6.eq) goto loc_82325CD0;
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// cmpw cr6,r31,r26
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r26.s32, ctx.xer);
	// blt cr6,0x82325c30
	if (ctx.cr6.lt) goto loc_82325C30;
loc_82325CD0:
	// lwz r11,5244(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 5244);
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// bne cr6,0x82325ce0
	if (!ctx.cr6.eq) goto loc_82325CE0;
	// li r23,-1
	ctx.r23.s64 = -1;
loc_82325CE0:
	// cmpwi cr6,r11,9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 9, ctx.xer);
	// li r3,-1
	ctx.r3.s64 = -1;
	// beq cr6,0x82325cf0
	if (ctx.cr6.eq) goto loc_82325CF0;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
loc_82325CF0:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8233e4a4
	__restgprlr_23(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82325CF8"))) PPC_WEAK_FUNC(sub_82325CF8);
PPC_FUNC_IMPL(__imp__sub_82325CF8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x82325D00;
	__restfpr_29(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r10,32767
	ctx.r10.s64 = 2147418112;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// ori r29,r10,65535
	ctx.r29.u64 = ctx.r10.u64 | 65535;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// lwz r9,24(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x82325D2C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r4,4
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 4, ctx.xer);
	// blt cr6,0x82325dcc
	if (ctx.cr6.lt) goto loc_82325DCC;
loc_82325D38:
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x82327578
	ctx.lr = 0x82325D44;
	sub_82327578(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x82325df4
	if (!ctx.cr6.eq) goto loc_82325DF4;
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,-3
	ctx.r4.s64 = ctx.r11.s64 + -3;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8232b788
	ctx.lr = 0x82325D68;
	sub_8232B788(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x82325D84;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,28(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x82325DA0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82325DC0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r4,4
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 4, ctx.xer);
	// bge cr6,0x82325d38
	if (!ctx.cr6.lt) goto loc_82325D38;
loc_82325DCC:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82325DE8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_82325DF4:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82327538
	ctx.lr = 0x82325DFC;
	sub_82327538(ctx, base);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// subf r4,r11,r30
	ctx.r4.s64 = ctx.r30.s64 - ctx.r11.s64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8232b788
	ctx.lr = 0x82325E18;
	sub_8232B788(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x82325E34;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,28(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x82325E50;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82325E5C"))) PPC_WEAK_FUNC(sub_82325E5C);
PPC_FUNC_IMPL(__imp__sub_82325E5C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82325E60"))) PPC_WEAK_FUNC(sub_82325E60);
PPC_FUNC_IMPL(__imp__sub_82325E60) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x82325E68;
	__restfpr_28(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r10,32767
	ctx.r10.s64 = 2147418112;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// ori r29,r10,65535
	ctx.r29.u64 = ctx.r10.u64 | 65535;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r9,24(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x82325E98;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r4,4
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 4, ctx.xer);
	// blt cr6,0x82325f38
	if (ctx.cr6.lt) goto loc_82325F38;
loc_82325EA4:
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x82327578
	ctx.lr = 0x82325EB0;
	sub_82327578(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x82325f68
	if (!ctx.cr6.eq) goto loc_82325F68;
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,-3
	ctx.r4.s64 = ctx.r11.s64 + -3;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8232b788
	ctx.lr = 0x82325ED4;
	sub_8232B788(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x82325EF0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,28(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x82325F0C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82325F2C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r4,4
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 4, ctx.xer);
	// bge cr6,0x82325ea4
	if (!ctx.cr6.lt) goto loc_82325EA4;
loc_82325F38:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82325F54;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r9,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_82325F68:
	// lbz r11,3(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 3);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// ori r10,r11,256
	ctx.r10.u64 = ctx.r11.u64 | 256;
	// stw r10,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r10.u32);
	// bl 0x82327538
	ctx.lr = 0x82325F7C;
	sub_82327538(ctx, base);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// subf r4,r9,r30
	ctx.r4.s64 = ctx.r30.s64 - ctx.r9.s64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8232b788
	ctx.lr = 0x82325F98;
	sub_8232B788(ctx, base);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,32(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 32);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x82325FB4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r11,28(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82325FD0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82325FDC"))) PPC_WEAK_FUNC(sub_82325FDC);
PPC_FUNC_IMPL(__imp__sub_82325FDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82325FE0"))) PPC_WEAK_FUNC(sub_82325FE0);
PPC_FUNC_IMPL(__imp__sub_82325FE0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82326010;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cntlzw r9,r30
	ctx.r9.u64 = ctx.r30.u32 == 0 ? 32 : __builtin_clz(ctx.r30.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// rlwinm r4,r9,27,31,31
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,32(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 32);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x82326030;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232604C"))) PPC_WEAK_FUNC(sub_8232604C);
PPC_FUNC_IMPL(__imp__sub_8232604C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82326050"))) PPC_WEAK_FUNC(sub_82326050);
PPC_FUNC_IMPL(__imp__sub_82326050) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x82326058;
	__restfpr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// bl 0x82325fe0
	ctx.lr = 0x8232606C;
	sub_82325FE0(ctx, base);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmpw cr6,r3,r30
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r30.s32, ctx.xer);
	// blt cr6,0x82326080
	if (ctx.cr6.lt) goto loc_82326080;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_82326080:
	// subf r5,r31,r30
	ctx.r5.s64 = ctx.r30.s64 - ctx.r31.s64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82325fe0
	ctx.lr = 0x82326090;
	sub_82325FE0(ctx, base);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + ctx.r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232609C"))) PPC_WEAK_FUNC(sub_8232609C);
PPC_FUNC_IMPL(__imp__sub_8232609C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823260A0"))) PPC_WEAK_FUNC(sub_823260A0);
PPC_FUNC_IMPL(__imp__sub_823260A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e454
	ctx.lr = 0x823260A8;
	__restfpr_23(ctx, base);
	// stfd f31,-88(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.f31.u64);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r27,0
	ctx.r27.s64 = 0;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r27,5244(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5244, ctx.r27.u32);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// stw r11,5332(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5332, ctx.r11.u32);
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// stw r27,5300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5300, ctx.r27.u32);
	// bl 0x82325550
	ctx.lr = 0x823260D0;
	sub_82325550(ctx, base);
	// lwz r11,4880(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4880);
	// addi r23,r3,5288
	ctx.r23.s64 = ctx.r3.s64 + 5288;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// lis r5,32767
	ctx.r5.s64 = 2147418112;
	// stw r10,4880(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4880, ctx.r10.u32);
	// addi r25,r3,4828
	ctx.r25.s64 = ctx.r3.s64 + 4828;
	// stw r27,5400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5400, ctx.r27.u32);
	// mr r6,r23
	ctx.r6.u64 = ctx.r23.u64;
	// stw r27,5404(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5404, ctx.r27.u32);
	// ori r5,r5,65535
	ctx.r5.u64 = ctx.r5.u64 | 65535;
	// stw r27,4932(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4932, ctx.r27.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// lwz r9,0(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// lwz r8,24(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x82326114;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r7,5288(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 5288);
	// rlwinm r11,r7,0,0,29
	ctx.r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFC;
	// subf r6,r11,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r11.s64;
	// rlwinm r10,r6,3,0,28
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// addi r8,r11,4
	ctx.r8.s64 = ctx.r11.s64 + 4;
	// slw r11,r9,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// bne cr6,0x8232613c
	if (!ctx.cr6.eq) goto loc_8232613C;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8232613C:
	// lwz r31,0(r8)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r26,r8,4
	ctx.r26.s64 = ctx.r8.s64 + 4;
	// cmpwi cr6,r10,20
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 20, ctx.xer);
	// blt cr6,0x82326190
	if (ctx.cr6.lt) goto loc_82326190;
	// addic. r10,r10,-20
	ctx.xer.ca = ctx.r10.u32 > 19;
	ctx.r10.s64 = ctx.r10.s64 + -20;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x82326178
	if (ctx.cr0.eq) goto loc_82326178;
	// subfic r9,r10,12
	ctx.xer.ca = ctx.r10.u32 <= 12;
	ctx.r9.s64 = 12 - ctx.r10.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 | ctx.r11.u64;
	// slw r11,r31,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r6,r7,12,20,31
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stw r6,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r6.u32);
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823261a0
	goto loc_823261A0;
loc_82326178:
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 12) & 0xFFF;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// stw r9,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r9.u32);
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823261a0
	goto loc_823261A0;
loc_82326190:
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 12) & 0xFFF;
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// stw r9,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r9.u32);
	// rlwinm r11,r11,12,0,19
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 12) & 0xFFFFF000;
loc_823261A0:
	// cmpwi cr6,r10,20
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 20, ctx.xer);
	// blt cr6,0x823261ec
	if (ctx.cr6.lt) goto loc_823261EC;
	// addic. r10,r10,-20
	ctx.xer.ca = ctx.r10.u32 > 19;
	ctx.r10.s64 = ctx.r10.s64 + -20;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x823261d4
	if (ctx.cr0.eq) goto loc_823261D4;
	// subfic r9,r10,12
	ctx.xer.ca = ctx.r10.u32 <= 12;
	ctx.r9.s64 = 12 - ctx.r10.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 | ctx.r11.u64;
	// slw r11,r31,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r6,r7,12,20,31
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stw r6,4(r25)
	PPC_STORE_U32(ctx.r25.u32 + 4, ctx.r6.u32);
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823261fc
	goto loc_823261FC;
loc_823261D4:
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 12) & 0xFFF;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// stw r9,4(r25)
	PPC_STORE_U32(ctx.r25.u32 + 4, ctx.r9.u32);
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823261fc
	goto loc_823261FC;
loc_823261EC:
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 12) & 0xFFF;
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// rlwinm r11,r11,12,0,19
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 12) & 0xFFFFF000;
	// stw r9,4(r25)
	PPC_STORE_U32(ctx.r25.u32 + 4, ctx.r9.u32);
loc_823261FC:
	// cmpwi cr6,r10,28
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 28, ctx.xer);
	// blt cr6,0x82326248
	if (ctx.cr6.lt) goto loc_82326248;
	// addic. r10,r10,-28
	ctx.xer.ca = ctx.r10.u32 > 27;
	ctx.r10.s64 = ctx.r10.s64 + -28;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x82326230
	if (ctx.cr0.eq) goto loc_82326230;
	// subfic r9,r10,4
	ctx.xer.ca = ctx.r10.u32 <= 4;
	ctx.r9.s64 = 4 - ctx.r10.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 | ctx.r11.u64;
	// slw r11,r31,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r6,r7,4,28,31
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xF;
	// stw r6,5056(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5056, ctx.r6.u32);
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326258
	goto loc_82326258;
loc_82326230:
	// rlwinm r9,r11,4,28,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xF;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// stw r9,5056(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5056, ctx.r9.u32);
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326258
	goto loc_82326258;
loc_82326248:
	// rlwinm r9,r11,4,28,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xF;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r9,5056(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5056, ctx.r9.u32);
loc_82326258:
	// cmpwi cr6,r10,28
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 28, ctx.xer);
	// blt cr6,0x8232629c
	if (ctx.cr6.lt) goto loc_8232629C;
	// addic. r10,r10,-28
	ctx.xer.ca = ctx.r10.u32 > 27;
	ctx.r10.s64 = ctx.r10.s64 + -28;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x82326288
	if (ctx.cr0.eq) goto loc_82326288;
	// subfic r9,r10,4
	ctx.xer.ca = ctx.r10.u32 <= 4;
	ctx.r9.s64 = 4 - ctx.r10.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 | ctx.r11.u64;
	// slw r11,r31,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r7,4,28,31
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823262a8
	goto loc_823262A8;
loc_82326288:
	// rlwinm r9,r11,4,28,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xF;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823262a8
	goto loc_823262A8;
loc_8232629C:
	// rlwinm r9,r11,4,28,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xF;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
loc_823262A8:
	// stw r9,16(r25)
	PPC_STORE_U32(ctx.r25.u32 + 16, ctx.r9.u32);
	// cmpwi cr6,r10,14
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 14, ctx.xer);
	// blt cr6,0x823262f8
	if (ctx.cr6.lt) goto loc_823262F8;
	// addic. r10,r10,-14
	ctx.xer.ca = ctx.r10.u32 > 13;
	ctx.r10.s64 = ctx.r10.s64 + -14;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x823262e0
	if (ctx.cr0.eq) goto loc_823262E0;
	// subfic r9,r10,18
	ctx.xer.ca = ctx.r10.u32 <= 18;
	ctx.r9.s64 = 18 - ctx.r10.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 | ctx.r11.u64;
	// slw r11,r31,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r6,r7,18,14,31
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 18) & 0x3FFFF;
	// stw r6,5060(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5060, ctx.r6.u32);
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326308
	goto loc_82326308;
loc_823262E0:
	// rlwinm r9,r11,18,14,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 18) & 0x3FFFF;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// stw r9,5060(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5060, ctx.r9.u32);
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326308
	goto loc_82326308;
loc_823262F8:
	// rlwinm r9,r11,18,14,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 18) & 0x3FFFF;
	// addi r10,r10,18
	ctx.r10.s64 = ctx.r10.s64 + 18;
	// rlwinm r11,r11,18,0,13
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 18) & 0xFFFC0000;
	// stw r9,5060(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5060, ctx.r9.u32);
loc_82326308:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r10,32
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 32, ctx.xer);
	// blt cr6,0x82326328
	if (ctx.cr6.lt) goto loc_82326328;
	// addi r10,r10,-32
	ctx.r10.s64 = ctx.r10.s64 + -32;
	// slw r11,r31,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x8232632c
	goto loc_8232632C;
loc_82326328:
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_8232632C:
	// cmpwi cr6,r10,22
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 22, ctx.xer);
	// blt cr6,0x82326378
	if (ctx.cr6.lt) goto loc_82326378;
	// addic. r10,r10,-22
	ctx.xer.ca = ctx.r10.u32 > 21;
	ctx.r10.s64 = ctx.r10.s64 + -22;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x82326360
	if (ctx.cr0.eq) goto loc_82326360;
	// subfic r9,r10,10
	ctx.xer.ca = ctx.r10.u32 <= 10;
	ctx.r9.s64 = 10 - ctx.r10.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 | ctx.r11.u64;
	// slw r11,r31,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r6,r7,10,22,31
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 10) & 0x3FF;
	// stw r6,5064(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5064, ctx.r6.u32);
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326388
	goto loc_82326388;
loc_82326360:
	// rlwinm r9,r11,10,22,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 10) & 0x3FF;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// stw r9,5064(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5064, ctx.r9.u32);
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326388
	goto loc_82326388;
loc_82326378:
	// rlwinm r9,r11,10,22,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 10) & 0x3FF;
	// addi r10,r10,10
	ctx.r10.s64 = ctx.r10.s64 + 10;
	// rlwinm r11,r11,10,0,21
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 10) & 0xFFFFFC00;
	// stw r9,5064(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5064, ctx.r9.u32);
loc_82326388:
	// rlwinm r9,r11,1,31,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0x1;
	// cmpwi cr6,r10,31
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 31, ctx.xer);
	// stw r9,5068(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5068, ctx.r9.u32);
	// bne cr6,0x823263ac
	if (!ctx.cr6.eq) goto loc_823263AC;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// mr r10,r27
	ctx.r10.u64 = ctx.r27.u64;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823263b4
	goto loc_823263B4;
loc_823263AC:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_823263B4:
	// rlwinm r9,r11,1,31,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0x1;
	// cmpwi cr6,r10,31
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 31, ctx.xer);
	// bne cr6,0x823263d4
	if (!ctx.cr6.eq) goto loc_823263D4;
	// mr r30,r31
	ctx.r30.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// mr r29,r27
	ctx.r29.u64 = ctx.r27.u64;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823263dc
	goto loc_823263DC;
loc_823263D4:
	// addi r29,r10,1
	ctx.r29.s64 = ctx.r10.s64 + 1;
	// rlwinm r30,r11,1,0,30
	ctx.r30.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
loc_823263DC:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lfs f31,11124(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 11124);
	ctx.f31.f64 = double(temp.f32);
	// beq cr6,0x82326820
	if (ctx.cr6.eq) goto loc_82326820;
	// li r11,8
	ctx.r11.s64 = 8;
	// mr r8,r27
	ctx.r8.u64 = ctx.r27.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
loc_823263F8:
	// cmpwi cr6,r29,24
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 24, ctx.xer);
	// blt cr6,0x8232643c
	if (ctx.cr6.lt) goto loc_8232643C;
	// addic. r11,r29,-24
	ctx.xer.ca = ctx.r29.u32 > 23;
	ctx.r11.s64 = ctx.r29.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82326428
	if (ctx.cr0.eq) goto loc_82326428;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// srw r7,r31,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// or r6,r7,r30
	ctx.r6.u64 = ctx.r7.u64 | ctx.r30.u64;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// rlwinm r9,r6,8,24,31
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFF;
	// b 0x82326448
	goto loc_82326448;
loc_82326428:
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r30,8,24,31
	ctx.r9.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326448
	goto loc_82326448;
loc_8232643C:
	// rlwinm r9,r30,8,24,31
	ctx.r9.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 8) & 0xFF;
	// addi r11,r29,8
	ctx.r11.s64 = ctx.r29.s64 + 8;
	// rlwinm r10,r30,8,0,23
	ctx.r10.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 8) & 0xFFFFFF00;
loc_82326448:
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r7,4396(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// lbzx r6,r7,r8
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r8.u32);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// extsb r9,r6
	ctx.r9.s64 = ctx.r6.s8;
	// addi r5,r9,576
	ctx.r5.s64 = ctx.r9.s64 + 576;
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r4,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x823264bc
	if (ctx.cr6.lt) goto loc_823264BC;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x823264a8
	if (ctx.cr0.eq) goto loc_823264A8;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r7,r31,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r6,r7,r10
	ctx.r6.u64 = ctx.r7.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r6,8,24,31
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823264c8
	goto loc_823264C8;
loc_823264A8:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823264c8
	goto loc_823264C8;
loc_823264BC:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_823264C8:
	// clrldi r7,r9,32
	ctx.r7.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// add r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r5,1(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,576
	ctx.r4.s64 = ctx.r9.s64 + 576;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x82326540
	if (ctx.cr6.lt) goto loc_82326540;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x8232652c
	if (ctx.cr0.eq) goto loc_8232652C;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r7,r31,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r6,r7,r10
	ctx.r6.u64 = ctx.r7.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r6,8,24,31
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x8232654c
	goto loc_8232654C;
loc_8232652C:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x8232654c
	goto loc_8232654C;
loc_82326540:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_8232654C:
	// clrldi r7,r9,32
	ctx.r7.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r7,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r7.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// add r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r5,2(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,576
	ctx.r4.s64 = ctx.r9.s64 + 576;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x823265c4
	if (ctx.cr6.lt) goto loc_823265C4;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x823265b0
	if (ctx.cr0.eq) goto loc_823265B0;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r7,r31,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r6,r7,r10
	ctx.r6.u64 = ctx.r7.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r6,8,24,31
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823265d0
	goto loc_823265D0;
loc_823265B0:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823265d0
	goto loc_823265D0;
loc_823265C4:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_823265D0:
	// clrldi r7,r9,32
	ctx.r7.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r7,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r7.u64);
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// add r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r5,3(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,576
	ctx.r4.s64 = ctx.r9.s64 + 576;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x82326648
	if (ctx.cr6.lt) goto loc_82326648;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82326634
	if (ctx.cr0.eq) goto loc_82326634;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r7,r31,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r6,r7,r10
	ctx.r6.u64 = ctx.r7.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r6,8,24,31
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326654
	goto loc_82326654;
loc_82326634:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326654
	goto loc_82326654;
loc_82326648:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_82326654:
	// clrldi r7,r9,32
	ctx.r7.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r7,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r7.u64);
	// add r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r5,4(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,576
	ctx.r4.s64 = ctx.r9.s64 + 576;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x823266cc
	if (ctx.cr6.lt) goto loc_823266CC;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x823266b8
	if (ctx.cr0.eq) goto loc_823266B8;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r7,r31,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r6,r7,r10
	ctx.r6.u64 = ctx.r7.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r6,8,24,31
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823266d8
	goto loc_823266D8;
loc_823266B8:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823266d8
	goto loc_823266D8;
loc_823266CC:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_823266D8:
	// clrldi r7,r9,32
	ctx.r7.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r7,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r7.u64);
	// add r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r5,5(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,576
	ctx.r4.s64 = ctx.r9.s64 + 576;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,120(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x82326750
	if (ctx.cr6.lt) goto loc_82326750;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x8232673c
	if (ctx.cr0.eq) goto loc_8232673C;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r7,r31,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r6,r7,r10
	ctx.r6.u64 = ctx.r7.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r6,8,24,31
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x8232675c
	goto loc_8232675C;
loc_8232673C:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x8232675c
	goto loc_8232675C;
loc_82326750:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_8232675C:
	// clrldi r7,r9,32
	ctx.r7.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r7,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r7.u64);
	// add r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r5,6(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,576
	ctx.r4.s64 = ctx.r9.s64 + 576;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,128(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x823267d4
	if (ctx.cr6.lt) goto loc_823267D4;
	// addic. r29,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r29.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq 0x823267c0
	if (ctx.cr0.eq) goto loc_823267C0;
	// subfic r11,r29,8
	ctx.xer.ca = ctx.r29.u32 <= 8;
	ctx.r11.s64 = 8 - ctx.r29.s64;
	// slw r30,r31,r29
	ctx.r30.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r29.u8 & 0x3F));
	// srw r9,r31,r11
	ctx.r9.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// or r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 | ctx.r10.u64;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// rlwinm r9,r7,8,24,31
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 8) & 0xFF;
	// b 0x823267e0
	goto loc_823267E0;
loc_823267C0:
	// mr r30,r31
	ctx.r30.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823267e0
	goto loc_823267E0;
loc_823267D4:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r29,r11,8
	ctx.r29.s64 = ctx.r11.s64 + 8;
	// rlwinm r30,r10,8,0,23
	ctx.r30.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_823267E0:
	// clrldi r10,r9,32
	ctx.r10.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r11,4396(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// std r10,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r10.u64);
	// add r9,r11,r8
	ctx.r9.u64 = ctx.r11.u64 + ctx.r8.u64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// lbz r7,7(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 7);
	// extsb r11,r7
	ctx.r11.s64 = ctx.r7.s8;
	// addi r6,r11,576
	ctx.r6.s64 = ctx.r11.s64 + 576;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r5,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r28.u32, temp.u32);
	// bdnz 0x823263f8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823263F8;
	// b 0x82326828
	goto loc_82326828;
loc_82326820:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82325568
	ctx.lr = 0x82326828;
	sub_82325568(ctx, base);
loc_82326828:
	// rlwinm r10,r30,1,31,31
	ctx.r10.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0x1;
	// cmpwi cr6,r29,31
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 31, ctx.xer);
	// bne cr6,0x82326848
	if (!ctx.cr6.eq) goto loc_82326848;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// mr r8,r27
	ctx.r8.u64 = ctx.r27.u64;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326850
	goto loc_82326850;
loc_82326848:
	// addi r8,r29,1
	ctx.r8.s64 = ctx.r29.s64 + 1;
	// rlwinm r11,r30,1,0,30
	ctx.r11.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
loc_82326850:
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82326c8c
	if (ctx.cr6.eq) goto loc_82326C8C;
	// li r10,8
	ctx.r10.s64 = 8;
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_82326864:
	// cmpwi cr6,r8,24
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 24, ctx.xer);
	// blt cr6,0x823268a8
	if (ctx.cr6.lt) goto loc_823268A8;
	// addic. r10,r8,-24
	ctx.xer.ca = ctx.r8.u32 > 23;
	ctx.r10.s64 = ctx.r8.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x82326894
	if (ctx.cr0.eq) goto loc_82326894;
	// subfic r8,r10,8
	ctx.xer.ca = ctx.r10.u32 <= 8;
	ctx.r8.s64 = 8 - ctx.r10.s64;
	// slw r9,r31,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r10.u8 & 0x3F));
	// srw r7,r31,r8
	ctx.r7.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r8.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// or r5,r7,r11
	ctx.r5.u64 = ctx.r7.u64 | ctx.r11.u64;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// rlwinm r7,r5,8,24,31
	ctx.r7.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFF;
	// b 0x823268b4
	goto loc_823268B4;
loc_82326894:
	// mr r9,r31
	ctx.r9.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r7,r11,8,24,31
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823268b4
	goto loc_823268B4;
loc_823268A8:
	// rlwinm r7,r11,8,24,31
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFF;
	// addi r10,r8,8
	ctx.r10.s64 = ctx.r8.s64 + 8;
	// rlwinm r9,r11,8,0,23
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFFFFFF00;
loc_823268B4:
	// clrldi r11,r7,32
	ctx.r11.u64 = ctx.r7.u64 & 0xFFFFFFFF;
	// lwz r8,4396(r28)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r10,24
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 24, ctx.xer);
	// std r11,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r11.u64);
	// lbzx r7,r8,r6
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r6.u32);
	// extsb r11,r7
	ctx.r11.s64 = ctx.r7.s8;
	// addi r5,r11,640
	ctx.r5.s64 = ctx.r11.s64 + 640;
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r4,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x82326928
	if (ctx.cr6.lt) goto loc_82326928;
	// addic. r11,r10,-24
	ctx.xer.ca = ctx.r10.u32 > 23;
	ctx.r11.s64 = ctx.r10.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82326914
	if (ctx.cr0.eq) goto loc_82326914;
	// subfic r8,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r8.s64 = 8 - ctx.r11.s64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// srw r7,r31,r8
	ctx.r7.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r8.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// or r5,r7,r9
	ctx.r5.u64 = ctx.r7.u64 | ctx.r9.u64;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// rlwinm r8,r5,8,24,31
	ctx.r8.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFF;
	// b 0x82326934
	goto loc_82326934;
loc_82326914:
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r8,r9,8,24,31
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326934
	goto loc_82326934;
loc_82326928:
	// addi r11,r10,8
	ctx.r11.s64 = ctx.r10.s64 + 8;
	// rlwinm r8,r9,8,24,31
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFF;
	// rlwinm r10,r9,8,0,23
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
loc_82326934:
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r8,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r8.u64);
	// add r7,r9,r6
	ctx.r7.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbz r5,1(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,640
	ctx.r4.s64 = ctx.r9.s64 + 640;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,128(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x823269ac
	if (ctx.cr6.lt) goto loc_823269AC;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82326998
	if (ctx.cr0.eq) goto loc_82326998;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r10
	ctx.r7.u64 = ctx.r8.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r7,8,24,31
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823269b8
	goto loc_823269B8;
loc_82326998:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x823269b8
	goto loc_823269B8;
loc_823269AC:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_823269B8:
	// clrldi r8,r9,32
	ctx.r8.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r8,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r8.u64);
	// add r7,r9,r6
	ctx.r7.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbz r5,2(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,640
	ctx.r4.s64 = ctx.r9.s64 + 640;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,120(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x82326a30
	if (ctx.cr6.lt) goto loc_82326A30;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82326a1c
	if (ctx.cr0.eq) goto loc_82326A1C;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r10
	ctx.r7.u64 = ctx.r8.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r7,8,24,31
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326a3c
	goto loc_82326A3C;
loc_82326A1C:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326a3c
	goto loc_82326A3C;
loc_82326A30:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_82326A3C:
	// clrldi r8,r9,32
	ctx.r8.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f0,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// add r7,r9,r6
	ctx.r7.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbz r5,3(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 3);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,640
	ctx.r4.s64 = ctx.r9.s64 + 640;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x82326ab4
	if (ctx.cr6.lt) goto loc_82326AB4;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82326aa0
	if (ctx.cr0.eq) goto loc_82326AA0;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r10
	ctx.r7.u64 = ctx.r8.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r7,8,24,31
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326ac0
	goto loc_82326AC0;
loc_82326AA0:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326ac0
	goto loc_82326AC0;
loc_82326AB4:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_82326AC0:
	// clrldi r8,r9,32
	ctx.r8.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// add r7,r9,r6
	ctx.r7.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbz r5,4(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,640
	ctx.r4.s64 = ctx.r9.s64 + 640;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x82326b38
	if (ctx.cr6.lt) goto loc_82326B38;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82326b24
	if (ctx.cr0.eq) goto loc_82326B24;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r10
	ctx.r7.u64 = ctx.r8.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r7,8,24,31
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326b44
	goto loc_82326B44;
loc_82326B24:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326b44
	goto loc_82326B44;
loc_82326B38:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_82326B44:
	// clrldi r8,r9,32
	ctx.r8.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// add r7,r9,r6
	ctx.r7.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbz r5,5(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 5);
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,640
	ctx.r4.s64 = ctx.r9.s64 + 640;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x82326bbc
	if (ctx.cr6.lt) goto loc_82326BBC;
	// addic. r11,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r11.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82326ba8
	if (ctx.cr0.eq) goto loc_82326BA8;
	// subfic r9,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r11.s64;
	// srw r8,r31,r9
	ctx.r8.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// or r7,r8,r10
	ctx.r7.u64 = ctx.r8.u64 | ctx.r10.u64;
	// slw r10,r31,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r7,8,24,31
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326bc8
	goto loc_82326BC8;
loc_82326BA8:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326bc8
	goto loc_82326BC8;
loc_82326BBC:
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_82326BC8:
	// clrldi r8,r9,32
	ctx.r8.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r9,4396(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// add r7,r9,r6
	ctx.r7.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbz r5,6(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 6);
	// extsb r9,r5
	ctx.r9.s64 = ctx.r5.s8;
	// addi r4,r9,640
	ctx.r4.s64 = ctx.r9.s64 + 640;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// blt cr6,0x82326c40
	if (ctx.cr6.lt) goto loc_82326C40;
	// addic. r8,r11,-24
	ctx.xer.ca = ctx.r11.u32 > 23;
	ctx.r8.s64 = ctx.r11.s64 + -24;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq 0x82326c2c
	if (ctx.cr0.eq) goto loc_82326C2C;
	// subfic r9,r8,8
	ctx.xer.ca = ctx.r8.u32 <= 8;
	ctx.r9.s64 = 8 - ctx.r8.s64;
	// slw r11,r31,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r8.u8 & 0x3F));
	// srw r7,r31,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r9.u8 & 0x3F));
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// or r5,r7,r10
	ctx.r5.u64 = ctx.r7.u64 | ctx.r10.u64;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// rlwinm r9,r5,8,24,31
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFF;
	// b 0x82326c4c
	goto loc_82326C4C;
loc_82326C2C:
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// lwz r31,0(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// b 0x82326c4c
	goto loc_82326C4C;
loc_82326C40:
	// addi r8,r11,8
	ctx.r8.s64 = ctx.r11.s64 + 8;
	// rlwinm r9,r10,8,24,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFF;
	// rlwinm r11,r10,8,0,23
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
loc_82326C4C:
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lwz r10,4396(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4396);
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// add r7,r10,r6
	ctx.r7.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// lbz r5,7(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 7);
	// extsb r10,r5
	ctx.r10.s64 = ctx.r5.s8;
	// addi r4,r10,640
	ctx.r4.s64 = ctx.r10.s64 + 640;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfsx f11,r3,r28
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r28.u32, temp.u32);
	// bdnz 0x82326864
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82326864;
	// b 0x82326c94
	goto loc_82326C94;
loc_82326C8C:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82325580
	ctx.lr = 0x82326C94;
	sub_82325580(ctx, base);
loc_82326C94:
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	// addi r9,r8,7
	ctx.r9.s64 = ctx.r8.s64 + 7;
	// lwz r11,4(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 4);
	// addi r6,r1,136
	ctx.r6.s64 = ctx.r1.s64 + 136;
	// addi r8,r10,15
	ctx.r8.s64 = ctx.r10.s64 + 15;
	// addi r7,r11,15
	ctx.r7.s64 = ctx.r11.s64 + 15;
	// srawi r11,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r8.s32 >> 4;
	// srawi r10,r7,4
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 4;
	// stw r11,8(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8, ctx.r11.u32);
	// srawi r4,r9,3
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r4.s64 = ctx.r9.s32 >> 3;
	// mullw r11,r10,r11
	ctx.r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// stw r10,12(r25)
	PPC_STORE_U32(ctx.r25.u32 + 12, ctx.r10.u32);
	// lwz r10,5060(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 5060);
	// addi r9,r11,-1
	ctx.r9.s64 = ctx.r11.s64 + -1;
	// mr r5,r23
	ctx.r5.u64 = ctx.r23.u64;
	// stw r9,5216(r28)
	PPC_STORE_U32(ctx.r28.u32 + 5216, ctx.r9.u32);
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// stw r10,72(r25)
	PPC_STORE_U32(ctx.r25.u32 + 72, ctx.r10.u32);
	// lwz r8,5064(r28)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r28.u32 + 5064);
	// stw r8,76(r25)
	PPC_STORE_U32(ctx.r25.u32 + 76, ctx.r8.u32);
	// lwz r7,5056(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 5056);
	// stb r7,89(r25)
	PPC_STORE_U8(ctx.r25.u32 + 89, ctx.r7.u8);
	// lwz r10,5068(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 5068);
	// stb r10,90(r25)
	PPC_STORE_U8(ctx.r25.u32 + 90, ctx.r10.u8);
	// lwz r8,0(r23)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r23.u32 + 0);
	// subf r11,r8,r4
	ctx.r11.s64 = ctx.r4.s64 - ctx.r8.s64;
	// add r11,r11,r26
	ctx.r11.u64 = ctx.r11.u64 + ctx.r26.u64;
	// addi r4,r11,-8
	ctx.r4.s64 = ctx.r11.s64 + -8;
	// bl 0x8232b788
	ctx.lr = 0x82326D08;
	sub_8232B788(ctx, base);
	// lwz r7,0(r24)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// mr r5,r23
	ctx.r5.u64 = ctx.r23.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// lwz r6,32(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 32);
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
	// bctrl 
	ctx.lr = 0x82326D24;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,0(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// addi r5,r1,136
	ctx.r5.s64 = ctx.r1.s64 + 136;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82326D40;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f31,-88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x8233e4a4
	__restgprlr_23(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82326D50"))) PPC_WEAK_FUNC(sub_82326D50);
PPC_FUNC_IMPL(__imp__sub_82326D50) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x82326D58;
	__restfpr_29(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r31,r3,5288
	ctx.r31.s64 = ctx.r3.s64 + 5288;
	// lis r5,32767
	ctx.r5.s64 = 2147418112;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// ori r5,r5,65535
	ctx.r5.u64 = ctx.r5.u64 | 65535;
	// li r4,1
	ctx.r4.s64 = 1;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82326D8C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,5288(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 5288);
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rlwinm r11,r9,0,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFC;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// subf r7,r11,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r11.s64;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r10,r7,3,0,28
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// addi r4,r10,7
	ctx.r4.s64 = ctx.r10.s64 + 7;
	// srawi r10,r4,3
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r4.s32 >> 3;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r4,r11,-4
	ctx.r4.s64 = ctx.r11.s64 + -4;
	// bl 0x8232b788
	ctx.lr = 0x82326DC8;
	sub_8232B788(ctx, base);
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r8,32(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x82326DE4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r6,28(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
	// bctrl 
	ctx.lr = 0x82326E00;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82325cf8
	ctx.lr = 0x82326E08;
	sub_82325CF8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82326E14"))) PPC_WEAK_FUNC(sub_82326E14);
PPC_FUNC_IMPL(__imp__sub_82326E14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82326E18"))) PPC_WEAK_FUNC(sub_82326E18);
PPC_FUNC_IMPL(__imp__sub_82326E18) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e458
	ctx.lr = 0x82326E20;
	__restfpr_24(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r31,r5,-3
	ctx.r31.s64 = ctx.r5.s64 + -3;
	// lwz r28,5332(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5332);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// li r25,0
	ctx.r25.s64 = 0;
	// li r24,0
	ctx.r24.s64 = 0;
	// li r30,4
	ctx.r30.s64 = 4;
	// cmpwi cr6,r31,4
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 4, ctx.xer);
	// ble cr6,0x82326e64
	if (!ctx.cr6.gt) goto loc_82326E64;
loc_82326E48:
	// add r3,r30,r26
	ctx.r3.u64 = ctx.r30.u64 + ctx.r26.u64;
	// bl 0x82327538
	ctx.lr = 0x82326E50;
	sub_82327538(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x82326e64
	if (!ctx.cr6.eq) goto loc_82326E64;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// cmpw cr6,r30,r31
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r31.s32, ctx.xer);
	// blt cr6,0x82326e48
	if (ctx.cr6.lt) goto loc_82326E48;
loc_82326E64:
	// cmpw cr6,r30,r31
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r31.s32, ctx.xer);
	// bne cr6,0x82326e70
	if (!ctx.cr6.eq) goto loc_82326E70;
	// li r24,-1
	ctx.r24.s64 = -1;
loc_82326E70:
	// cmpwi cr6,r28,1
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 1, ctx.xer);
	// bne cr6,0x82326e8c
	if (!ctx.cr6.eq) goto loc_82326E8C;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82325bf8
	ctx.lr = 0x82326E88;
	sub_82325BF8(ctx, base);
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
loc_82326E8C:
	// rlwinm r11,r28,1,0,30
	ctx.r11.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r27,r11,r29
	ctx.r27.u64 = ctx.r11.u64 + ctx.r29.u64;
	// lwz r31,5336(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 5336);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x82326f7c
	if (ctx.cr6.eq) goto loc_82326F7C;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82326EC8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x8233e4e0
	ctx.lr = 0x82326ED8;
	sub_8233E4E0(ctx, base);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r8,32(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x82326EF4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// bge cr6,0x82326f50
	if (!ctx.cr6.lt) goto loc_82326F50;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// subf r5,r11,r30
	ctx.r5.s64 = ctx.r30.s64 - ctx.r11.s64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r9,24(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x82326F20;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r5,92(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// add r4,r11,r26
	ctx.r4.u64 = ctx.r11.u64 + ctx.r26.u64;
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x8233e4e0
	ctx.lr = 0x82326F34;
	sub_8233E4E0(ctx, base);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,32(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 32);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x82326F50;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82326F50:
	// addi r11,r28,445
	ctx.r11.s64 = ctx.r28.s64 + 445;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r29
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r29.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82326f7c
	if (ctx.cr6.eq) goto loc_82326F7C;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,5344(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 5344);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82326F7C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82326F7C:
	// cmpwi cr6,r28,3
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 3, ctx.xer);
	// bne cr6,0x82326fac
	if (!ctx.cr6.eq) goto loc_82326FAC;
	// lwz r3,5384(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 5384);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82326fac
	if (ctx.cr6.eq) goto loc_82326FAC;
	// lwz r5,5388(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 5388);
	// cmpw cr6,r30,r5
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r5.s32, ctx.xer);
	// bge cr6,0x82326fa0
	if (!ctx.cr6.lt) goto loc_82326FA0;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
loc_82326FA0:
	// stw r5,5392(r29)
	PPC_STORE_U32(ctx.r29.u32 + 5392, ctx.r5.u32);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x8233e4e0
	ctx.lr = 0x82326FAC;
	sub_8233E4E0(ctx, base);
loc_82326FAC:
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bne cr6,0x82326fbc
	if (!ctx.cr6.eq) goto loc_82326FBC;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
loc_82326FBC:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8233e4a8
	__restgprlr_24(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82326FC4"))) PPC_WEAK_FUNC(sub_82326FC4);
PPC_FUNC_IMPL(__imp__sub_82326FC4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82326FC8"))) PPC_WEAK_FUNC(sub_82326FC8);
PPC_FUNC_IMPL(__imp__sub_82326FC8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x82326FD0;
	__restfpr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r9,5280(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5280);
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// lwz r10,4756(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4756);
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// addi r11,r3,4956
	ctx.r11.s64 = ctx.r3.s64 + 4956;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x82327030
	if (ctx.cr6.eq) goto loc_82327030;
	// lwz r9,5284(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5284);
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// addi r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 1;
	// stw r8,5280(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5280, ctx.r8.u32);
	// stw r7,5284(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5284, ctx.r7.u32);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r6,r9,1
	ctx.r6.s64 = ctx.r9.s64 + 1;
	// stw r6,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r6.u32);
	// bne cr6,0x82327024
	if (!ctx.cr6.eq) goto loc_82327024;
	// li r3,-2
	ctx.r3.s64 = -2;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_82327024:
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r9.u32);
loc_82327030:
	// cntlzw r11,r10
	ctx.r11.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// addi r30,r11,-3
	ctx.r30.s64 = ctx.r11.s64 + -3;
	// bl 0x82325cf8
	ctx.lr = 0x82327044;
	sub_82325CF8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x82327090
	if (ctx.cr6.eq) goto loc_82327090;
loc_8232704C:
	// and r11,r3,r29
	ctx.r11.u64 = ctx.r3.u64 & ctx.r29.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232708c
	if (!ctx.cr6.eq) goto loc_8232708C;
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82325fe0
	ctx.lr = 0x82327068;
	sub_82325FE0(ctx, base);
	// cmpwi cr6,r3,4
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 4, ctx.xer);
	// bne cr6,0x82327090
	if (!ctx.cr6.eq) goto loc_82327090;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82325cf8
	ctx.lr = 0x82327078;
	sub_82325CF8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x8232704c
	if (!ctx.cr6.eq) goto loc_8232704C;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_8232708C:
	// li r30,0
	ctx.r30.s64 = 0;
loc_82327090:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232709C"))) PPC_WEAK_FUNC(sub_8232709C);
PPC_FUNC_IMPL(__imp__sub_8232709C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823270A0"))) PPC_WEAK_FUNC(sub_823270A0);
PPC_FUNC_IMPL(__imp__sub_823270A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x823270A8;
	__restfpr_27(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r30,r3,5288
	ctx.r30.s64 = ctx.r3.s64 + 5288;
	// lis r5,32767
	ctx.r5.s64 = 2147418112;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// ori r5,r5,65535
	ctx.r5.u64 = ctx.r5.u64 | 65535;
	// li r4,1
	ctx.r4.s64 = 1;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x823270DC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r4,5288(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5288);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,5292(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5292);
	// rlwinm r11,r4,0,0,29
	ctx.r11.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFC;
	// subf r9,r11,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r11.s64;
	// addi r29,r11,8
	ctx.r29.s64 = ctx.r11.s64 + 8;
	// rlwinm r28,r9,3,0,28
	ctx.r28.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82326e18
	ctx.lr = 0x823270FC;
	sub_82326E18(ctx, base);
	// addi r8,r28,7
	ctx.r8.s64 = ctx.r28.s64 + 7;
	// lwz r7,5288(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5288);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// srawi r6,r8,3
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 3;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// subf r11,r7,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r7.s64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// add r11,r11,r29
	ctx.r11.u64 = ctx.r11.u64 + ctx.r29.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,-4
	ctx.r4.s64 = ctx.r11.s64 + -4;
	// bl 0x8232b788
	ctx.lr = 0x82327128;
	sub_8232B788(ctx, base);
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82327144;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r8,28(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x82327160;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x82325cf8
	ctx.lr = 0x82327168;
	sub_82325CF8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82327174"))) PPC_WEAK_FUNC(sub_82327174);
PPC_FUNC_IMPL(__imp__sub_82327174) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327178"))) PPC_WEAK_FUNC(sub_82327178);
PPC_FUNC_IMPL(__imp__sub_82327178) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x82327180;
	__restfpr_29(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// bl 0x82324040
	ctx.lr = 0x82327190;
	sub_82324040(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x823271b0
	if (ctx.cr6.eq) goto loc_823271B0;
	// lis r4,-253
	ctx.r4.s64 = -16580608;
	// li r3,0
	ctx.r3.s64 = 0;
	// ori r4,r4,524
	ctx.r4.u64 = ctx.r4.u64 | 524;
	// bl 0x823251a0
	ctx.lr = 0x823271A8;
	sub_823251A0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_823271B0:
	// li r11,0
	ctx.r11.s64 = 0;
	// lis r5,32767
	ctx.r5.s64 = 2147418112;
	// stw r11,5392(r30)
	PPC_STORE_U32(ctx.r30.u32 + 5392, ctx.r11.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// ori r5,r5,65535
	ctx.r5.u64 = ctx.r5.u64 | 65535;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r9,24(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x823271DC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,28(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x823271F8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stw r6,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r6.u32);
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// bl 0x823254a8
	ctx.lr = 0x82327214;
	sub_823254A8(ctx, base);
	// cmpwi cr6,r3,2
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 2, ctx.xer);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bne cr6,0x82327230
	if (!ctx.cr6.eq) goto loc_82327230;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// bl 0x820b85b8
	ctx.lr = 0x82327228;
	sub_820B85B8(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_82327230:
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// li r4,-1
	ctx.r4.s64 = -1;
	// bl 0x82326fc8
	ctx.lr = 0x8232723C;
	sub_82326FC8(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x82327308
	if (!ctx.cr6.eq) goto loc_82327308;
loc_82327248:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82325418
	ctx.lr = 0x82327254;
	sub_82325418(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x82327320
	if (ctx.cr6.eq) goto loc_82327320;
	// clrlwi r11,r3,30
	ctx.r11.u64 = ctx.r3.u32 & 0x3;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x82327320
	if (!ctx.cr6.eq) goto loc_82327320;
	// rlwinm r11,r3,0,24,24
	ctx.r11.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x80;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232731c
	if (!ctx.cr6.eq) goto loc_8232731C;
	// cmpwi cr6,r3,16
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 16, ctx.xer);
	// bgt cr6,0x823272c0
	if (ctx.cr6.gt) goto loc_823272C0;
	// beq cr6,0x823272b0
	if (ctx.cr6.eq) goto loc_823272B0;
	// cmpwi cr6,r3,4
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 4, ctx.xer);
	// beq cr6,0x823272a0
	if (ctx.cr6.eq) goto loc_823272A0;
	// cmpwi cr6,r3,8
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 8, ctx.xer);
	// bne cr6,0x823272ec
	if (!ctx.cr6.eq) goto loc_823272EC;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x823255a0
	ctx.lr = 0x8232729C;
	sub_823255A0(ctx, base);
	// b 0x823272ec
	goto loc_823272EC;
loc_823272A0:
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82325760
	ctx.lr = 0x823272AC;
	sub_82325760(ctx, base);
	// b 0x823272ec
	goto loc_823272EC;
loc_823272B0:
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82326d50
	ctx.lr = 0x823272BC;
	sub_82326D50(ctx, base);
	// b 0x823272ec
	goto loc_823272EC;
loc_823272C0:
	// cmpwi cr6,r3,32
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 32, ctx.xer);
	// beq cr6,0x823272e0
	if (ctx.cr6.eq) goto loc_823272E0;
	// cmpwi cr6,r3,64
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 64, ctx.xer);
	// bne cr6,0x823272ec
	if (!ctx.cr6.eq) goto loc_823272EC;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x823260a0
	ctx.lr = 0x823272DC;
	sub_823260A0(ctx, base);
	// b 0x823272ec
	goto loc_823272EC;
loc_823272E0:
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x823270a0
	ctx.lr = 0x823272EC;
	sub_823270A0(ctx, base);
loc_823272EC:
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// li r4,-1
	ctx.r4.s64 = -1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82326fc8
	ctx.lr = 0x823272FC;
	sub_82326FC8(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x82327248
	if (ctx.cr6.eq) goto loc_82327248;
loc_82327308:
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x823251a0
	ctx.lr = 0x82327314;
	sub_823251A0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_8232731C:
	// li r29,-2
	ctx.r29.s64 = -2;
loc_82327320:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232732C"))) PPC_WEAK_FUNC(sub_8232732C);
PPC_FUNC_IMPL(__imp__sub_8232732C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327330"))) PPC_WEAK_FUNC(sub_82327330);
PPC_FUNC_IMPL(__imp__sub_82327330) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x82327338;
	__restfpr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lwz r4,4(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// bl 0x82323c48
	ctx.lr = 0x82327354;
	sub_82323C48(ctx, base);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x8232736c
	if (!ctx.cr6.eq) goto loc_8232736C;
	// li r3,-1
	ctx.r3.s64 = -1;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_8232736C:
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82327178
	ctx.lr = 0x82327378;
	sub_82327178(ctx, base);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82327394;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// subf r8,r3,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r3.s64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r8,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r8.u32);
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r6,12(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
	// bctrl 
	ctx.lr = 0x823273B4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823273C0"))) PPC_WEAK_FUNC(sub_823273C0);
PPC_FUNC_IMPL(__imp__sub_823273C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// bl 0x82324040
	ctx.lr = 0x823273E0;
	sub_82324040(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x823273fc
	if (ctx.cr6.eq) goto loc_823273FC;
	// lis r4,-253
	ctx.r4.s64 = -16580608;
	// li r3,0
	ctx.r3.s64 = 0;
	// ori r4,r4,524
	ctx.r4.u64 = ctx.r4.u64 | 524;
	// bl 0x823251a0
	ctx.lr = 0x823273F8;
	sub_823251A0(ctx, base);
	// b 0x82327410
	goto loc_82327410;
loc_823273FC:
	// addi r4,r31,4828
	ctx.r4.s64 = ctx.r31.s64 + 4828;
	// li r5,128
	ctx.r5.s64 = 128;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8233e4e0
	ctx.lr = 0x8232740C;
	sub_8233E4E0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_82327410:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327428"))) PPC_WEAK_FUNC(sub_82327428);
PPC_FUNC_IMPL(__imp__sub_82327428) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// li r11,256
	ctx.r11.s64 = 256;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
loc_82327438:
	// ori r11,r10,256
	ctx.r11.u64 = ctx.r10.u64 | 256;
	// cmplwi cr6,r11,256
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 256, ctx.xer);
	// bne cr6,0x8232744c
	if (!ctx.cr6.eq) goto loc_8232744C;
	// li r11,4
	ctx.r11.s64 = 4;
	// b 0x823274c0
	goto loc_823274C0;
loc_8232744C:
	// cmplwi cr6,r11,257
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 257, ctx.xer);
	// bne cr6,0x8232745c
	if (!ctx.cr6.eq) goto loc_8232745C;
	// li r11,3
	ctx.r11.s64 = 3;
	// b 0x823274c0
	goto loc_823274C0;
loc_8232745C:
	// ble cr6,0x82327470
	if (!ctx.cr6.gt) goto loc_82327470;
	// cmplwi cr6,r11,431
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 431, ctx.xer);
	// bgt cr6,0x82327470
	if (ctx.cr6.gt) goto loc_82327470;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x823274c0
	goto loc_823274C0;
loc_82327470:
	// cmplwi cr6,r11,434
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 434, ctx.xer);
	// bne cr6,0x82327480
	if (!ctx.cr6.eq) goto loc_82327480;
	// li r11,32
	ctx.r11.s64 = 32;
	// b 0x823274c0
	goto loc_823274C0;
loc_82327480:
	// cmplwi cr6,r11,435
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 435, ctx.xer);
	// bne cr6,0x82327490
	if (!ctx.cr6.eq) goto loc_82327490;
	// li r11,64
	ctx.r11.s64 = 64;
	// b 0x823274c0
	goto loc_823274C0;
loc_82327490:
	// cmplwi cr6,r11,437
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 437, ctx.xer);
	// bne cr6,0x823274a0
	if (!ctx.cr6.eq) goto loc_823274A0;
	// li r11,16
	ctx.r11.s64 = 16;
	// b 0x823274c0
	goto loc_823274C0;
loc_823274A0:
	// cmplwi cr6,r11,439
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 439, ctx.xer);
	// bne cr6,0x823274b0
	if (!ctx.cr6.eq) goto loc_823274B0;
	// li r11,128
	ctx.r11.s64 = 128;
	// b 0x823274c0
	goto loc_823274C0;
loc_823274B0:
	// addi r11,r11,-440
	ctx.r11.s64 = ctx.r11.s64 + -440;
	// addic r8,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r8.s64 = ctx.r11.s64 + -1;
	// subfe r6,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r6.u64 = ~ctx.r7.u64 + ctx.r7.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r6,r9
	ctx.r11.u64 = ctx.r6.u64 & ctx.r9.u64;
loc_823274C0:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// stbx r11,r10,r3
	PPC_STORE_U8(ctx.r10.u32 + ctx.r3.u32, ctx.r11.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82327438
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82327438;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823274D4"))) PPC_WEAK_FUNC(sub_823274D4);
PPC_FUNC_IMPL(__imp__sub_823274D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823274D8"))) PPC_WEAK_FUNC(sub_823274D8);
PPC_FUNC_IMPL(__imp__sub_823274D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82327428
	ctx.lr = 0x823274EC;
	sub_82327428(ctx, base);
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r10,-1408
	ctx.r10.s64 = ctx.r10.s64 + -1408;
	// addi r8,r11,256
	ctx.r8.s64 = ctx.r11.s64 + 256;
loc_823274FC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne 0x8232751c
	if (!ctx.cr0.eq) goto loc_8232751C;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bne cr6,0x823274fc
	if (!ctx.cr6.eq) goto loc_823274FC;
loc_8232751C:
	// subfic r11,r9,0
	ctx.xer.ca = ctx.r9.u32 <= 0;
	ctx.r11.s64 = 0 - ctx.r9.s64;
	// subfe r3,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r3.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327534"))) PPC_WEAK_FUNC(sub_82327534);
PPC_FUNC_IMPL(__imp__sub_82327534) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327538"))) PPC_WEAK_FUNC(sub_82327538);
PPC_FUNC_IMPL(__imp__sub_82327538) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r10,1(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// rotlwi r9,r11,8
	ctx.r9.u64 = rotl32(ctx.r11.u32, 8);
	// lbz r8,2(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// or r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 | ctx.r10.u64;
	// rlwinm r6,r7,8,0,23
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 8) & 0xFFFFFF00;
	// or r5,r6,r8
	ctx.r5.u64 = ctx.r6.u64 | ctx.r8.u64;
	// cmpwi cr6,r5,1
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 1, ctx.xer);
	// beq cr6,0x82327564
	if (ctx.cr6.eq) goto loc_82327564;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_82327564:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lbz r10,3(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 3);
	// addi r9,r11,-1408
	ctx.r9.s64 = ctx.r11.s64 + -1408;
	// lbzx r3,r10,r9
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327578"))) PPC_WEAK_FUNC(sub_82327578);
PPC_FUNC_IMPL(__imp__sub_82327578) {
	PPC_FUNC_PROLOGUE();
	// li r8,-256
	ctx.r8.s64 = -256;
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x823275cc
	if (!ctx.cr6.gt) goto loc_823275CC;
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// addi r7,r10,-1408
	ctx.r7.s64 = ctx.r10.s64 + -1408;
loc_82327590:
	// lbzx r9,r11,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r3.u32);
	// add r10,r11,r3
	ctx.r10.u64 = ctx.r11.u64 + ctx.r3.u64;
	// li r6,128
	ctx.r6.s64 = 128;
	// dcbt r6,r10
	// cmplwi cr6,r8,256
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 256, ctx.xer);
	// bne cr6,0x823275b8
	if (!ctx.cr6.eq) goto loc_823275B8;
	// lbzx r6,r9,r7
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// and r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 & ctx.r5.u64;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// bne cr6,0x823275d4
	if (!ctx.cr6.eq) goto loc_823275D4;
loc_823275B8:
	// or r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 | ctx.r8.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r8,r10,8,0,23
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// blt cr6,0x82327590
	if (ctx.cr6.lt) goto loc_82327590;
loc_823275CC:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_823275D4:
	// addi r3,r10,-3
	ctx.r3.s64 = ctx.r10.s64 + -3;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823275DC"))) PPC_WEAK_FUNC(sub_823275DC);
PPC_FUNC_IMPL(__imp__sub_823275DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823275E0"))) PPC_WEAK_FUNC(sub_823275E0);
PPC_FUNC_IMPL(__imp__sub_823275E0) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// addi r9,r11,-1080
	ctx.r9.s64 = ctx.r11.s64 + -1080;
	// stw r9,15068(r10)
	PPC_STORE_U32(ctx.r10.u32 + 15068, ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823275F4"))) PPC_WEAK_FUNC(sub_823275F4);
PPC_FUNC_IMPL(__imp__sub_823275F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823275F8"))) PPC_WEAK_FUNC(sub_823275F8);
PPC_FUNC_IMPL(__imp__sub_823275F8) {
	PPC_FUNC_PROLOGUE();
	// addi r11,r3,105
	ctx.r11.s64 = ctx.r3.s64 + 105;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327604"))) PPC_WEAK_FUNC(sub_82327604);
PPC_FUNC_IMPL(__imp__sub_82327604) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327608"))) PPC_WEAK_FUNC(sub_82327608);
PPC_FUNC_IMPL(__imp__sub_82327608) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8232764c
	if (ctx.cr6.eq) goto loc_8232764C;
	// lwz r4,20(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r30,0(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x82327640
	if (ctx.cr6.eq) goto loc_82327640;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82337d50
	ctx.lr = 0x82327640;
	sub_82337D50(ctx, base);
loc_82327640:
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82337d50
	ctx.lr = 0x8232764C;
	sub_82337D50(ctx, base);
loc_8232764C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327664"))) PPC_WEAK_FUNC(sub_82327664);
PPC_FUNC_IMPL(__imp__sub_82327664) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327668"))) PPC_WEAK_FUNC(sub_82327668);
PPC_FUNC_IMPL(__imp__sub_82327668) {
	PPC_FUNC_PROLOGUE();
	// stw r4,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r4.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327670"))) PPC_WEAK_FUNC(sub_82327670);
PPC_FUNC_IMPL(__imp__sub_82327670) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,20(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r5,r11,r10
	PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, ctx.r5.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327680"))) PPC_WEAK_FUNC(sub_82327680);
PPC_FUNC_IMPL(__imp__sub_82327680) {
	PPC_FUNC_PROLOGUE();
	// li r11,1
	ctx.r11.s64 = 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r11,12(r3)
	PPC_STORE_U8(ctx.r3.u32 + 12, ctx.r11.u8);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327694"))) PPC_WEAK_FUNC(sub_82327694);
PPC_FUNC_IMPL(__imp__sub_82327694) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327698"))) PPC_WEAK_FUNC(sub_82327698);
PPC_FUNC_IMPL(__imp__sub_82327698) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r11,12(r3)
	PPC_STORE_U8(ctx.r3.u32 + 12, ctx.r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823276A4"))) PPC_WEAK_FUNC(sub_823276A4);
PPC_FUNC_IMPL(__imp__sub_823276A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823276A8"))) PPC_WEAK_FUNC(sub_823276A8);
PPC_FUNC_IMPL(__imp__sub_823276A8) {
	PPC_FUNC_PROLOGUE();
	// lbz r3,12(r3)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + 12);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823276B0"))) PPC_WEAK_FUNC(sub_823276B0);
PPC_FUNC_IMPL(__imp__sub_823276B0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e460
	ctx.lr = 0x823276B8;
	__restfpr_26(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r26,16(r3)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// li r5,-1
	ctx.r5.s64 = -1;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// addi r30,r27,24
	ctx.r30.s64 = ctx.r27.s64 + 24;
	// addi r28,r27,140
	ctx.r28.s64 = ctx.r27.s64 + 140;
	// bl 0x82339958
	ctx.lr = 0x823276E0;
	sub_82339958(ctx, base);
	// lwz r4,92(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x823277d4
	if (ctx.cr6.eq) goto loc_823277D4;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x8233a580
	ctx.lr = 0x823276F8;
	sub_8233A580(ctx, base);
	// lbz r10,111(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 111);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8232771c
	if (ctx.cr6.eq) goto loc_8232771C;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x82339978
	ctx.lr = 0x82327714;
	sub_82339978(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
loc_8232771C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r4,92(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x8233a710
	ctx.lr = 0x8232772C;
	sub_8233A710(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x8233b5d0
	ctx.lr = 0x82327738;
	sub_8233B5D0(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233af28
	ctx.lr = 0x82327740;
	sub_8233AF28(ctx, base);
	// lbz r11,5(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 5);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// extsb r4,r11
	ctx.r4.s64 = ctx.r11.s8;
	// bl 0x8233af38
	ctx.lr = 0x82327750;
	sub_8233AF38(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233af40
	ctx.lr = 0x82327764;
	sub_8233AF40(ctx, base);
	// lhz r10,20(r30)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r30.u32 + 20);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lwz r5,8(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// extsh r4,r10
	ctx.r4.s64 = ctx.r10.s16;
	// bl 0x8233afc0
	ctx.lr = 0x82327778;
	sub_8233AFC0(ctx, base);
	// lbz r9,5(r30)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r30.u32 + 5);
	// li r31,0
	ctx.r31.s64 = 0;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x823277b4
	if (ctx.cr6.eq) goto loc_823277B4;
	// addi r29,r30,36
	ctx.r29.s64 = ctx.r30.s64 + 36;
loc_8232778C:
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lhz r5,-14(r29)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r29.u32 + -14);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lhzu r6,2(r29)
	ea = 2 + ctx.r29.u32;
	ctx.r6.u64 = PPC_LOAD_U16(ea);
	ctx.r29.u32 = ea;
	// bl 0x8233af50
	ctx.lr = 0x823277A0;
	sub_8233AF50(ctx, base);
	// lbz r11,5(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 5);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// extsb r10,r11
	ctx.r10.s64 = ctx.r11.s8;
	// cmplw cr6,r31,r10
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x8232778c
	if (ctx.cr6.lt) goto loc_8232778C;
loc_823277B4:
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x82339978
	ctx.lr = 0x823277C4;
	sub_82339978(ctx, base);
	// li r11,2
	ctx.r11.s64 = 2;
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r11,12(r27)
	PPC_STORE_U8(ctx.r27.u32 + 12, ctx.r11.u8);
	// stw r10,352(r27)
	PPC_STORE_U32(ctx.r27.u32 + 352, ctx.r10.u32);
loc_823277D4:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823277DC"))) PPC_WEAK_FUNC(sub_823277DC);
PPC_FUNC_IMPL(__imp__sub_823277DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823277E0"))) PPC_WEAK_FUNC(sub_823277E0);
PPC_FUNC_IMPL(__imp__sub_823277E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e444
	ctx.lr = 0x823277E8;
	__restfpr_19(ctx, base);
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r19,16(r3)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// mr r24,r3
	ctx.r24.u64 = ctx.r3.u64;
	// lbz r11,29(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 29);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lwz r20,20(r3)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// li r5,-1
	ctx.r5.s64 = -1;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// extsb r27,r11
	ctx.r27.s64 = ctx.r11.s8;
	// addi r22,r24,140
	ctx.r22.s64 = ctx.r24.s64 + 140;
	// li r26,0
	ctx.r26.s64 = 0;
	// bl 0x82339958
	ctx.lr = 0x8232781C;
	sub_82339958(ctx, base);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x82327a5c
	if (ctx.cr6.eq) goto loc_82327A5C;
	// addi r5,r1,176
	ctx.r5.s64 = ctx.r1.s64 + 176;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x8233a580
	ctx.lr = 0x82327834;
	sub_8233A580(ctx, base);
	// lbz r10,191(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 191);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82327a4c
	if (!ctx.cr6.eq) goto loc_82327A4C;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x8233a710
	ctx.lr = 0x82327850;
	sub_8233A710(ctx, base);
	// lbz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r10,1(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// mr r21,r3
	ctx.r21.u64 = ctx.r3.u64;
	// rotlwi r9,r11,8
	ctx.r9.u64 = rotl32(ctx.r11.u32, 8);
	// or r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 | ctx.r10.u64;
	// clrlwi r7,r8,16
	ctx.r7.u64 = ctx.r8.u32 & 0xFFFF;
	// cmplwi cr6,r7,32769
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 32769, ctx.xer);
	// bne cr6,0x82327890
	if (!ctx.cr6.eq) goto loc_82327890;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// bl 0x82339978
	ctx.lr = 0x82327880;
	sub_82339978(ctx, base);
	// li r11,1
	ctx.r11.s64 = 1;
	// stb r11,12(r24)
	PPC_STORE_U8(ctx.r24.u32 + 12, ctx.r11.u8);
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
loc_82327890:
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r10,18
	ctx.r10.s64 = 18;
	// lwz r9,348(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 348);
	// li r31,0
	ctx.r31.s64 = 0;
	// divwu r8,r11,r10
	ctx.r8.u32 = ctx.r11.u32 / ctx.r10.u32;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// divwu r7,r8,r27
	ctx.r7.u32 = ctx.r8.u32 / ctx.r27.u32;
	// twllei r27,0
	if (ctx.r27.u32 <= 0) __builtin_debugtrap();
	// mullw r6,r7,r9
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// rlwinm r25,r6,5,0,26
	ctx.r25.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// beq cr6,0x823278e4
	if (ctx.cr6.eq) goto loc_823278E4;
	// mr r30,r20
	ctx.r30.u64 = ctx.r20.u64;
loc_823278C0:
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// bl 0x82339988
	ctx.lr = 0x823278CC;
	sub_82339988(ctx, base);
	// cmplw cr6,r3,r25
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r25.u32, ctx.xer);
	// blt cr6,0x823279a8
	if (ctx.cr6.lt) goto loc_823279A8;
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// cmplw cr6,r31,r27
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r27.u32, ctx.xer);
	// blt cr6,0x823278c0
	if (ctx.cr6.lt) goto loc_823278C0;
loc_823278E4:
	// li r28,0
	ctx.r28.s64 = 0;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// beq cr6,0x82327958
	if (ctx.cr6.eq) goto loc_82327958;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// addi r31,r1,224
	ctx.r31.s64 = ctx.r1.s64 + 224;
	// subf r23,r11,r20
	ctx.r23.s64 = ctx.r20.s64 - ctx.r11.s64;
	// li r30,0
	ctx.r30.s64 = 0;
loc_82327900:
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// add r29,r30,r11
	ctx.r29.u64 = ctx.r30.u64 + ctx.r11.u64;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwzx r3,r23,r29
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r23.u32 + ctx.r29.u32);
	// bl 0x82339958
	ctx.lr = 0x8232791C;
	sub_82339958(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmplw cr6,r11,r25
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r25.u32, ctx.xer);
	// blt cr6,0x8232798c
	if (ctx.cr6.lt) goto loc_8232798C;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// lwz r8,348(r24)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r24.u32 + 348);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r31,r31,8
	ctx.r31.s64 = ctx.r31.s64 + 8;
	// divwu r26,r11,r8
	ctx.r26.u32 = ctx.r11.u32 / ctx.r8.u32;
	// twllei r8,0
	if (ctx.r8.u32 <= 0) __builtin_debugtrap();
	// stwx r10,r30,r9
	PPC_STORE_U32(ctx.r30.u32 + ctx.r9.u32, ctx.r10.u32);
	// cmplw cr6,r28,r27
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r27.u32, ctx.xer);
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// stw r10,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r10.u32);
	// blt cr6,0x82327900
	if (ctx.cr6.lt) goto loc_82327900;
loc_82327958:
	// lwz r11,344(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 344);
	// mr r10,r26
	ctx.r10.u64 = ctx.r26.u64;
	// lwz r6,88(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// mr r8,r27
	ctx.r8.u64 = ctx.r27.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// addi r7,r1,92
	ctx.r7.s64 = ctx.r1.s64 + 92;
	// mr r5,r21
	ctx.r5.u64 = ctx.r21.u64;
	// mr r3,r22
	ctx.r3.u64 = ctx.r22.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bne cr6,0x823279c0
	if (!ctx.cr6.eq) goto loc_823279C0;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// bl 0x8233aa68
	ctx.lr = 0x82327988;
	sub_8233AA68(ctx, base);
	// b 0x823279c8
	goto loc_823279C8;
loc_8232798C:
	// rlwinm r9,r28,2,0,29
	ctx.r9.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r28,3,0,28
	ctx.r10.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r1,224
	ctx.r11.s64 = ctx.r1.s64 + 224;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwzx r3,r9,r20
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r20.u32);
	// bl 0x82339968
	ctx.lr = 0x823279A8;
	sub_82339968(ctx, base);
loc_823279A8:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// bl 0x82339968
	ctx.lr = 0x823279B8;
	sub_82339968(ctx, base);
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
loc_823279C0:
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// bl 0x8233ac48
	ctx.lr = 0x823279C8;
	sub_8233AC48(ctx, base);
loc_823279C8:
	// lwz r10,36(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 36);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lwz r11,352(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 352);
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	// cmplw cr6,r3,r10
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x823279e4
	if (!ctx.cr6.gt) goto loc_823279E4;
	// mr r29,r10
	ctx.r29.u64 = ctx.r10.u64;
loc_823279E4:
	// add r11,r11,r29
	ctx.r11.u64 = ctx.r11.u64 + ctx.r29.u64;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// stw r11,352(r24)
	PPC_STORE_U32(ctx.r24.u32 + 352, ctx.r11.u32);
	// beq cr6,0x82327a4c
	if (ctx.cr6.eq) goto loc_82327A4C;
	// addi r31,r20,-4
	ctx.r31.s64 = ctx.r20.s64 + -4;
	// addi r30,r1,224
	ctx.r30.s64 = ctx.r1.s64 + 224;
loc_823279FC:
	// lwz r11,344(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 344);
	// rlwinm r4,r29,1,0,30
	ctx.r4.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x82327a10
	if (ctx.cr6.eq) goto loc_82327A10;
	// rlwinm r4,r29,2,0,29
	ctx.r4.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
loc_82327A10:
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,104
	ctx.r5.s64 = ctx.r1.s64 + 104;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82339998
	ctx.lr = 0x82327A20;
	sub_82339998(ctx, base);
	// addi r5,r1,104
	ctx.r5.s64 = ctx.r1.s64 + 104;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// bl 0x82339978
	ctx.lr = 0x82327A30;
	sub_82339978(ctx, base);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwzu r3,4(r31)
	ea = 4 + ctx.r31.u32;
	ctx.r3.u64 = PPC_LOAD_U32(ea);
	ctx.r31.u32 = ea;
	// bl 0x82339968
	ctx.lr = 0x82327A40;
	sub_82339968(ctx, base);
	// addic. r27,r27,-1
	ctx.xer.ca = ctx.r27.u32 > 0;
	ctx.r27.s64 = ctx.r27.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// addi r30,r30,8
	ctx.r30.s64 = ctx.r30.s64 + 8;
	// bne 0x823279fc
	if (!ctx.cr0.eq) goto loc_823279FC;
loc_82327A4C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// bl 0x82339978
	ctx.lr = 0x82327A5C;
	sub_82339978(ctx, base);
loc_82327A5C:
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82327A64"))) PPC_WEAK_FUNC(sub_82327A64);
PPC_FUNC_IMPL(__imp__sub_82327A64) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327A68"))) PPC_WEAK_FUNC(sub_82327A68);
PPC_FUNC_IMPL(__imp__sub_82327A68) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x82327aa4
	if (!ctx.cr6.eq) goto loc_82327AA4;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,16(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// bl 0x82339988
	ctx.lr = 0x82327A94;
	sub_82339988(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x82327aa4
	if (!ctx.cr6.eq) goto loc_82327AA4;
	// li r11,3
	ctx.r11.s64 = 3;
	// stb r11,12(r31)
	PPC_STORE_U8(ctx.r31.u32 + 12, ctx.r11.u8);
loc_82327AA4:
	// lbz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 12);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x82327ab8
	if (!ctx.cr6.eq) goto loc_82327AB8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x823276b0
	ctx.lr = 0x82327AB8;
	sub_823276B0(ctx, base);
loc_82327AB8:
	// lbz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 12);
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bne cr6,0x82327acc
	if (!ctx.cr6.eq) goto loc_82327ACC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x823277e0
	ctx.lr = 0x82327ACC;
	sub_823277E0(ctx, base);
loc_82327ACC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327AE4"))) PPC_WEAK_FUNC(sub_82327AE4);
PPC_FUNC_IMPL(__imp__sub_82327AE4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327AE8"))) PPC_WEAK_FUNC(sub_82327AE8);
PPC_FUNC_IMPL(__imp__sub_82327AE8) {
	PPC_FUNC_PROLOGUE();
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327AF4"))) PPC_WEAK_FUNC(sub_82327AF4);
PPC_FUNC_IMPL(__imp__sub_82327AF4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327AF8"))) PPC_WEAK_FUNC(sub_82327AF8);
PPC_FUNC_IMPL(__imp__sub_82327AF8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e45c
	ctx.lr = 0x82327B00;
	__restfpr_25(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lbz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 12);
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// addi r31,r3,24
	ctx.r31.s64 = ctx.r3.s64 + 24;
	// addi r27,r3,140
	ctx.r27.s64 = ctx.r3.s64 + 140;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x82327c3c
	if (!ctx.cr6.eq) goto loc_82327C3C;
	// li r5,116
	ctx.r5.s64 = 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x82327B38;
	sub_8233EAF0(ctx, base);
	// clrlwi r10,r30,24
	ctx.r10.u64 = ctx.r30.u32 & 0xFF;
	// li r11,4
	ctx.r11.s64 = 4;
	// stw r28,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r28.u32);
	// li r26,0
	ctx.r26.s64 = 0;
	// stb r10,5(r31)
	PPC_STORE_U8(ctx.r31.u32 + 5, ctx.r10.u8);
	// li r9,3
	ctx.r9.s64 = 3;
	// stb r11,0(r31)
	PPC_STORE_U8(ctx.r31.u32 + 0, ctx.r11.u8);
	// li r8,18
	ctx.r8.s64 = 18;
	// stb r11,3(r31)
	PPC_STORE_U8(ctx.r31.u32 + 3, ctx.r11.u8);
	// li r7,32
	ctx.r7.s64 = 32;
	// stb r9,2(r31)
	PPC_STORE_U8(ctx.r31.u32 + 2, ctx.r9.u8);
	// li r6,500
	ctx.r6.s64 = 500;
	// stb r8,4(r31)
	PPC_STORE_U8(ctx.r31.u32 + 4, ctx.r8.u8);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r29,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r29.u32);
	// mr r11,r26
	ctx.r11.u64 = ctx.r26.u64;
	// stw r7,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r7.u32);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// sth r6,20(r31)
	PPC_STORE_U16(ctx.r31.u32 + 20, ctx.r6.u16);
	// ble cr6,0x82327ba8
	if (!ctx.cr6.gt) goto loc_82327BA8;
	// addi r10,r31,36
	ctx.r10.s64 = ctx.r31.s64 + 36;
loc_82327B8C:
	// sth r26,-14(r10)
	PPC_STORE_U16(ctx.r10.u32 + -14, ctx.r26.u16);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// sthu r26,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r26.u16);
	ctx.r10.u32 = ea;
	// lbz r9,5(r31)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r31.u32 + 5);
	// extsb r8,r9
	ctx.r8.s64 = ctx.r9.s8;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x82327b8c
	if (ctx.cr6.lt) goto loc_82327B8C;
loc_82327BA8:
	// li r11,1
	ctx.r11.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// sth r11,60(r31)
	PPC_STORE_U16(ctx.r31.u32 + 60, ctx.r11.u16);
	// bl 0x8233af28
	ctx.lr = 0x82327BB8;
	sub_8233AF28(ctx, base);
	// lbz r10,5(r31)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r31.u32 + 5);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// extsb r4,r10
	ctx.r4.s64 = ctx.r10.s8;
	// bl 0x8233af38
	ctx.lr = 0x82327BC8;
	sub_8233AF38(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x8233af40
	ctx.lr = 0x82327BDC;
	sub_8233AF40(ctx, base);
	// lhz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 20);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// extsh r4,r9
	ctx.r4.s64 = ctx.r9.s16;
	// bl 0x8233afc0
	ctx.lr = 0x82327BF0;
	sub_8233AFC0(ctx, base);
	// lbz r8,5(r31)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r31.u32 + 5);
	// mr r30,r26
	ctx.r30.u64 = ctx.r26.u64;
	// extsb r7,r8
	ctx.r7.s64 = ctx.r8.s8;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// ble cr6,0x82327c30
	if (!ctx.cr6.gt) goto loc_82327C30;
	// addi r29,r31,36
	ctx.r29.s64 = ctx.r31.s64 + 36;
loc_82327C08:
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lhz r5,-14(r29)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r29.u32 + -14);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// lhzu r6,2(r29)
	ea = 2 + ctx.r29.u32;
	ctx.r6.u64 = PPC_LOAD_U16(ea);
	ctx.r29.u32 = ea;
	// bl 0x8233af50
	ctx.lr = 0x82327C1C;
	sub_8233AF50(ctx, base);
	// lbz r11,5(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 5);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// extsb r10,r11
	ctx.r10.s64 = ctx.r11.s8;
	// cmpw cr6,r30,r10
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x82327c08
	if (ctx.cr6.lt) goto loc_82327C08;
loc_82327C30:
	// li r11,2
	ctx.r11.s64 = 2;
	// stw r26,352(r25)
	PPC_STORE_U32(ctx.r25.u32 + 352, ctx.r26.u32);
	// stb r11,12(r25)
	PPC_STORE_U8(ctx.r25.u32 + 12, ctx.r11.u8);
loc_82327C3C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82327C44"))) PPC_WEAK_FUNC(sub_82327C44);
PPC_FUNC_IMPL(__imp__sub_82327C44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327C48"))) PPC_WEAK_FUNC(sub_82327C48);
PPC_FUNC_IMPL(__imp__sub_82327C48) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x82327C50;
	__restfpr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r11,-984
	ctx.r5.s64 = ctx.r11.s64 + -984;
	// li r4,356
	ctx.r4.s64 = 356;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// bl 0x82337c48
	ctx.lr = 0x82327C74;
	sub_82337C48(ctx, base);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82327cc0
	if (ctx.cr6.eq) goto loc_82327CC0;
	// li r5,356
	ctx.r5.s64 = 356;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8233eaf0
	ctx.lr = 0x82327C8C;
	sub_8233EAF0(ctx, base);
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// rlwinm r29,r29,2,0,29
	ctx.r29.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r11,-996
	ctx.r5.s64 = ctx.r11.s64 + -996;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82337c48
	ctx.lr = 0x82327CAC;
	sub_82337C48(ctx, base);
	// stw r3,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x82327ccc
	if (!ctx.cr6.eq) goto loc_82327CCC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82327608
	ctx.lr = 0x82327CC0;
	sub_82327608(ctx, base);
loc_82327CC0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_82327CCC:
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8233eaf0
	ctx.lr = 0x82327CD8;
	sub_8233EAF0(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r28,344(r31)
	PPC_STORE_U32(ctx.r31.u32 + 344, ctx.r28.u32);
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// li r11,2
	ctx.r11.s64 = 2;
	// beq cr6,0x82327cf4
	if (ctx.cr6.eq) goto loc_82327CF4;
	// li r11,4
	ctx.r11.s64 = 4;
loc_82327CF4:
	// stw r11,348(r31)
	PPC_STORE_U32(ctx.r31.u32 + 348, ctx.r11.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82327D04"))) PPC_WEAK_FUNC(sub_82327D04);
PPC_FUNC_IMPL(__imp__sub_82327D04) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327D08"))) PPC_WEAK_FUNC(sub_82327D08);
PPC_FUNC_IMPL(__imp__sub_82327D08) {
	PPC_FUNC_PROLOGUE();
	// b 0x8232b7e0
	sub_8232B7E0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82327D0C"))) PPC_WEAK_FUNC(sub_82327D0C);
PPC_FUNC_IMPL(__imp__sub_82327D0C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327D10"))) PPC_WEAK_FUNC(sub_82327D10);
PPC_FUNC_IMPL(__imp__sub_82327D10) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x8233e318
	ctx.lr = 0x82327D28;
	sub_8233E318(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327D40"))) PPC_WEAK_FUNC(sub_82327D40);
PPC_FUNC_IMPL(__imp__sub_82327D40) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x82342210
	ctx.lr = 0x82327D58;
	sub_82342210(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327D70"))) PPC_WEAK_FUNC(sub_82327D70);
PPC_FUNC_IMPL(__imp__sub_82327D70) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x82342358
	ctx.lr = 0x82327D88;
	sub_82342358(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327DA0"))) PPC_WEAK_FUNC(sub_82327DA0);
PPC_FUNC_IMPL(__imp__sub_82327DA0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// lwz r11,15120(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 15120);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,15120(r10)
	PPC_STORE_U32(ctx.r10.u32 + 15120, ctx.r11.u32);
	// lwz r11,15120(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 15120);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x82327e04
	if (!ctx.cr6.eq) goto loc_82327E04;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r10,0
	ctx.r10.s64 = 0;
	// lis r9,-32183
	ctx.r9.s64 = -2109145088;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r31,r9,15076
	ctx.r31.s64 = ctx.r9.s64 + 15076;
	// stw r10,15112(r11)
	PPC_STORE_U32(ctx.r11.u32 + 15112, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8232b4f0
	ctx.lr = 0x82327DEC;
	sub_8232B4F0(ctx, base);
	// stw r3,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r3.u32);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne 0x82327e04
	if (!ctx.cr0.eq) goto loc_82327E04;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,-748
	ctx.r3.s64 = ctx.r11.s64 + -748;
	// bl 0x8232b878
	ctx.lr = 0x82327E04;
	sub_8232B878(ctx, base);
loc_82327E04:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327E18"))) PPC_WEAK_FUNC(sub_82327E18);
PPC_FUNC_IMPL(__imp__sub_82327E18) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// addi r31,r11,15120
	ctx.r31.s64 = ctx.r11.s64 + 15120;
	// lwz r11,15120(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 15120);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x82327e6c
	if (!ctx.cr6.eq) goto loc_82327E6C;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// lwz r3,-12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + -12);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r10,15112(r11)
	PPC_STORE_U32(ctx.r11.u32 + 15112, ctx.r10.u32);
	// beq cr6,0x82327e6c
	if (ctx.cr6.eq) goto loc_82327E6C;
	// bl 0x8232b5d0
	ctx.lr = 0x82327E64;
	sub_8232B5D0(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,-12(r31)
	PPC_STORE_U32(ctx.r31.u32 + -12, ctx.r11.u32);
loc_82327E6C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327E80"))) PPC_WEAK_FUNC(sub_82327E80);
PPC_FUNC_IMPL(__imp__sub_82327E80) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// lwz r3,15108(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 15108);
	// bl 0x8232b660
	ctx.lr = 0x82327E98;
	sub_8232B660(ctx, base);
	// cmpwi r3,0
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bge 0x82327eac
	if (!ctx.cr0.lt) goto loc_82327EAC;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,-696
	ctx.r3.s64 = ctx.r11.s64 + -696;
	// bl 0x8232b878
	ctx.lr = 0x82327EAC;
	sub_8232B878(ctx, base);
loc_82327EAC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327EBC"))) PPC_WEAK_FUNC(sub_82327EBC);
PPC_FUNC_IMPL(__imp__sub_82327EBC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327EC0"))) PPC_WEAK_FUNC(sub_82327EC0);
PPC_FUNC_IMPL(__imp__sub_82327EC0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// lwz r3,15108(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 15108);
	// bl 0x8232b6f8
	ctx.lr = 0x82327ED8;
	sub_8232B6F8(ctx, base);
	// cmpwi r3,0
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bge 0x82327eec
	if (!ctx.cr0.lt) goto loc_82327EEC;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,-600
	ctx.r3.s64 = ctx.r11.s64 + -600;
	// bl 0x8232b878
	ctx.lr = 0x82327EEC;
	sub_8232B878(ctx, base);
loc_82327EEC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327EFC"))) PPC_WEAK_FUNC(sub_82327EFC);
PPC_FUNC_IMPL(__imp__sub_82327EFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82327F00"))) PPC_WEAK_FUNC(sub_82327F00);
PPC_FUNC_IMPL(__imp__sub_82327F00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r11,r11,-1520
	ctx.r11.s64 = ctx.r11.s64 + -1520;
loc_82327F0C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq cr6,0x82327f30
	if (ctx.cr6.eq) goto loc_82327F30;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x82327f0c
	if (ctx.cr6.eq) goto loc_82327F0C;
loc_82327F30:
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x82327f40
	if (ctx.cr6.eq) goto loc_82327F40;
loc_82327F38:
	// li r3,-1
	ctx.r3.s64 = -1;
	// blr 
	return;
loc_82327F40:
	// cmplwi cr6,r4,5420
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 5420, ctx.xer);
	// bne cr6,0x82327f38
	if (!ctx.cr6.eq) goto loc_82327F38;
	// addi r11,r5,-128
	ctx.r11.s64 = ctx.r5.s64 + -128;
	// subfic r11,r11,0
	ctx.xer.ca = ctx.r11.u32 <= 0;
	ctx.r11.s64 = 0 - ctx.r11.s64;
	// subfe r3,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r3.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82327F58"))) PPC_WEAK_FUNC(sub_82327F58);
PPC_FUNC_IMPL(__imp__sub_82327F58) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r5,r11,18392
	ctx.r5.s64 = ctx.r11.s64 + 18392;
	// li r4,576
	ctx.r4.s64 = 576;
	// addi r11,r5,1600
	ctx.r11.s64 = ctx.r5.s64 + 1600;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// addi r11,r11,-2
	ctx.r11.s64 = ctx.r11.s64 + -2;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_82327F78:
	// sthu r9,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82327f78
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82327F78;
	// li r10,12
	ctx.r10.s64 = 12;
	// li r8,571
	ctx.r8.s64 = 571;
	// addi r9,r5,1600
	ctx.r9.s64 = ctx.r5.s64 + 1600;
	// sth r8,1632(r5)
	PPC_STORE_U16(ctx.r5.u32 + 1632, ctx.r8.u16);
	// addi r11,r5,1600
	ctx.r11.s64 = ctx.r5.s64 + 1600;
	// sth r8,1634(r5)
	PPC_STORE_U16(ctx.r5.u32 + 1634, ctx.r8.u16);
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r10,r9,34
	ctx.r10.s64 = ctx.r9.s64 + 34;
loc_82327FA4:
	// sthu r7,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82327fa4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82327FA4;
	// li r10,16
	ctx.r10.s64 = 16;
	// li r8,555
	ctx.r8.s64 = 555;
	// addi r9,r5,1600
	ctx.r9.s64 = ctx.r5.s64 + 1600;
	// sth r8,1660(r5)
	PPC_STORE_U16(ctx.r5.u32 + 1660, ctx.r8.u16);
	// addi r11,r5,1600
	ctx.r11.s64 = ctx.r5.s64 + 1600;
	// sth r8,1662(r5)
	PPC_STORE_U16(ctx.r5.u32 + 1662, ctx.r8.u16);
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r10,r9,62
	ctx.r10.s64 = ctx.r9.s64 + 62;
loc_82327FD0:
	// sthu r7,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82327fd0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82327FD0;
	// addi r11,r5,1600
	ctx.r11.s64 = ctx.r5.s64 + 1600;
	// li r10,33
	ctx.r10.s64 = 33;
	// addi r11,r11,96
	ctx.r11.s64 = ctx.r11.s64 + 96;
loc_82327FE4:
	// rlwinm r9,r10,4,0,27
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// ori r8,r9,17421
	ctx.r8.u64 = ctx.r9.u64 | 17421;
	// ori r7,r9,1036
	ctx.r7.u64 = ctx.r9.u64 | 1036;
	// extsh r6,r8
	ctx.r6.s64 = ctx.r8.s16;
	// extsh r3,r7
	ctx.r3.s64 = ctx.r7.s16;
	// sth r6,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r6.u16);
	// cmpwi cr6,r10,22
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 22, ctx.xer);
	// sthu r3,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r11.u32 = ea;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// bge cr6,0x82327fe4
	if (!ctx.cr6.lt) goto loc_82327FE4;
	// li r10,21
	ctx.r10.s64 = 21;
loc_82328014:
	// rlwinm r9,r10,4,0,27
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// ori r8,r9,17420
	ctx.r8.u64 = ctx.r9.u64 | 17420;
	// ori r7,r9,1035
	ctx.r7.u64 = ctx.r9.u64 | 1035;
	// extsh r6,r8
	ctx.r6.s64 = ctx.r8.s16;
	// extsh r3,r7
	ctx.r3.s64 = ctx.r7.s16;
	// sth r6,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r6.u16);
	// cmpwi cr6,r10,16
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 16, ctx.xer);
	// sth r6,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r6.u16);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// sth r3,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r3.u16);
	// sth r3,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r3.u16);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bge cr6,0x82328014
	if (!ctx.cr6.lt) goto loc_82328014;
	// li r6,15
	ctx.r6.s64 = 15;
loc_82328050:
	// rlwinm r10,r6,4,0,27
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// li r8,8
	ctx.r8.s64 = 8;
	// ori r7,r10,17418
	ctx.r7.u64 = ctx.r10.u64 | 17418;
	// addi r9,r11,-2
	ctx.r9.s64 = ctx.r11.s64 + -2;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_82328068:
	// sthu r7,2(r9)
	ea = 2 + ctx.r9.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r9.u32 = ea;
	// bdnz 0x82328068
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328068;
	// ori r8,r10,1033
	ctx.r8.u64 = ctx.r10.u64 | 1033;
	// li r10,8
	ctx.r10.s64 = 8;
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// addi r11,r9,-2
	ctx.r11.s64 = ctx.r9.s64 + -2;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_82328088:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328088
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328088;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmpwi cr6,r6,10
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 10, ctx.xer);
	// bge cr6,0x82328050
	if (!ctx.cr6.lt) goto loc_82328050;
	// li r6,9
	ctx.r6.s64 = 9;
loc_823280A4:
	// rlwinm r10,r6,4,0,27
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// li r8,16
	ctx.r8.s64 = 16;
	// ori r7,r10,17417
	ctx.r7.u64 = ctx.r10.u64 | 17417;
	// addi r9,r11,-2
	ctx.r9.s64 = ctx.r11.s64 + -2;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_823280BC:
	// sthu r7,2(r9)
	ea = 2 + ctx.r9.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r9.u32 = ea;
	// bdnz 0x823280bc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823280BC;
	// ori r8,r10,1032
	ctx.r8.u64 = ctx.r10.u64 | 1032;
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r9,r11,32
	ctx.r9.s64 = ctx.r11.s64 + 32;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// addi r11,r9,-2
	ctx.r11.s64 = ctx.r9.s64 + -2;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_823280DC:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x823280dc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823280DC;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r11,r9,32
	ctx.r11.s64 = ctx.r9.s64 + 32;
	// cmpwi cr6,r6,8
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 8, ctx.xer);
	// bge cr6,0x823280a4
	if (!ctx.cr6.lt) goto loc_823280A4;
	// sth r4,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, ctx.r4.u16);
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// sth r4,2(r5)
	PPC_STORE_U16(ctx.r5.u32 + 2, ctx.r4.u16);
	// li r10,7
	ctx.r10.s64 = 7;
	// sth r4,4(r5)
	PPC_STORE_U16(ctx.r5.u32 + 4, ctx.r4.u16);
	// addi r11,r5,8
	ctx.r11.s64 = ctx.r5.s64 + 8;
	// sth r4,6(r5)
	PPC_STORE_U16(ctx.r5.u32 + 6, ctx.r4.u16);
loc_82328110:
	// rlwinm r9,r10,4,0,27
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// ori r8,r9,17415
	ctx.r8.u64 = ctx.r9.u64 | 17415;
	// ori r7,r9,1030
	ctx.r7.u64 = ctx.r9.u64 | 1030;
	// extsh r6,r8
	ctx.r6.s64 = ctx.r8.s16;
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// sth r6,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r6.u16);
	// cmpwi cr6,r10,6
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 6, ctx.xer);
	// sthu r5,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r5.u16);
	ctx.r11.u32 = ea;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// bge cr6,0x82328110
	if (!ctx.cr6.lt) goto loc_82328110;
	// li r10,5
	ctx.r10.s64 = 5;
loc_82328140:
	// rlwinm r9,r10,4,0,27
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// ori r8,r9,17414
	ctx.r8.u64 = ctx.r9.u64 | 17414;
	// ori r7,r9,1029
	ctx.r7.u64 = ctx.r9.u64 | 1029;
	// extsh r6,r8
	ctx.r6.s64 = ctx.r8.s16;
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// sth r6,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r6.u16);
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// sth r6,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r6.u16);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// sth r5,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r5.u16);
	// sth r5,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r5.u16);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bge cr6,0x82328140
	if (!ctx.cr6.lt) goto loc_82328140;
	// li r10,3
	ctx.r10.s64 = 3;
loc_8232817C:
	// rlwinm r9,r10,4,0,27
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// ori r8,r9,17413
	ctx.r8.u64 = ctx.r9.u64 | 17413;
	// ori r7,r9,1028
	ctx.r7.u64 = ctx.r9.u64 | 1028;
	// extsh r6,r8
	ctx.r6.s64 = ctx.r8.s16;
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// sth r6,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r6.u16);
	// cmpwi cr6,r10,2
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 2, ctx.xer);
	// sth r6,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r6.u16);
	// sth r6,4(r11)
	PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r6.u16);
	// sth r6,6(r11)
	PPC_STORE_U16(ctx.r11.u32 + 6, ctx.r6.u16);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// sth r5,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r5.u16);
	// sth r5,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r5.u16);
	// sth r5,4(r11)
	PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r5.u16);
	// sth r5,6(r11)
	PPC_STORE_U16(ctx.r11.u32 + 6, ctx.r5.u16);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bge cr6,0x8232817c
	if (!ctx.cr6.lt) goto loc_8232817C;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r11,-2
	ctx.r10.s64 = ctx.r11.s64 + -2;
	// li r8,17427
	ctx.r8.s64 = 17427;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_823281D4:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x823281d4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823281D4;
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// li r9,1042
	ctx.r9.s64 = 1042;
	// addi r11,r11,-2
	ctx.r11.s64 = ctx.r11.s64 + -2;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_823281F0:
	// sthu r9,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x823281f0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823281F0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823281FC"))) PPC_WEAK_FUNC(sub_823281FC);
PPC_FUNC_IMPL(__imp__sub_823281FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82328200"))) PPC_WEAK_FUNC(sub_82328200);
PPC_FUNC_IMPL(__imp__sub_82328200) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x82328208;
	__restfpr_27(ctx, base);
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r9,8
	ctx.r9.s64 = 8;
	// addi r11,r11,18328
	ctx.r11.s64 = ctx.r11.s64 + 18328;
	// li r6,576
	ctx.r6.s64 = 576;
	// addi r10,r11,2176
	ctx.r10.s64 = ctx.r11.s64 + 2176;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328228:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82328228
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328228;
	// li r9,6
	ctx.r9.s64 = 6;
	// li r8,571
	ctx.r8.s64 = 571;
	// addi r10,r11,2176
	ctx.r10.s64 = ctx.r11.s64 + 2176;
	// sth r8,2192(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2192, ctx.r8.u16);
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8232824C:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x8232824c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8232824C;
	// li r9,8
	ctx.r9.s64 = 8;
	// li r8,555
	ctx.r8.s64 = 555;
	// addi r10,r11,2176
	ctx.r10.s64 = ctx.r11.s64 + 2176;
	// sth r8,2206(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2206, ctx.r8.u16);
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// addi r10,r10,30
	ctx.r10.s64 = ctx.r10.s64 + 30;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328270:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82328270
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328270;
	// li r8,12
	ctx.r8.s64 = 12;
	// addi r10,r11,2176
	ctx.r10.s64 = ctx.r11.s64 + 2176;
	// li r9,33
	ctx.r9.s64 = 33;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_8232828C:
	// li r8,11
	ctx.r8.s64 = 11;
	// rlwimi r8,r9,4,0,27
	ctx.r8.u64 = (rotl32(ctx.r9.u32, 4) & 0xFFFFFFF0) | (ctx.r8.u64 & 0xFFFFFFFF0000000F);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// sth r7,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r7.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x8232828c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8232828C;
	// li r9,346
	ctx.r9.s64 = 346;
	// li r8,-22181
	ctx.r8.s64 = -22181;
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// li r7,330
	ctx.r7.s64 = 330;
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// li r5,-22197
	ctx.r5.s64 = -22197;
	// li r4,314
	ctx.r4.s64 = 314;
	// li r3,-22213
	ctx.r3.s64 = -22213;
	// li r31,298
	ctx.r31.s64 = 298;
	// li r30,-22229
	ctx.r30.s64 = -22229;
	// sthu r7,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r10.u32 = ea;
	// li r7,282
	ctx.r7.s64 = 282;
	// li r29,-22245
	ctx.r29.s64 = -22245;
	// li r28,266
	ctx.r28.s64 = 266;
	// li r8,6
	ctx.r8.s64 = 6;
	// li r27,-22261
	ctx.r27.s64 = -22261;
	// sthu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r5.u16);
	ctx.r10.u32 = ea;
	// li r9,15
	ctx.r9.s64 = 15;
	// li r5,-24565
	ctx.r5.s64 = -24565;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r8,-22519
	ctx.r8.s64 = -22519;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// sthu r31,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r31.u16);
	ctx.r10.u32 = ea;
	// sthu r30,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r30.u16);
	ctx.r10.u32 = ea;
	// sthu r7,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r10.u32 = ea;
	// li r7,-30710
	ctx.r7.s64 = -30710;
	// sthu r29,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r29.u16);
	ctx.r10.u32 = ea;
	// sthu r28,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r28.u16);
	ctx.r10.u32 = ea;
	// sthu r27,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r27.u16);
	ctx.r10.u32 = ea;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
loc_82328324:
	// rlwinm r4,r9,4,0,27
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// or r31,r4,r5
	ctx.r31.u64 = ctx.r4.u64 | ctx.r5.u64;
	// ori r3,r4,8
	ctx.r3.u64 = ctx.r4.u64 | 8;
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// sth r3,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r3.u16);
	// or r3,r4,r7
	ctx.r3.u64 = ctx.r4.u64 | ctx.r7.u64;
	// sthu r31,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r31.u16);
	ctx.r10.u32 = ea;
	// or r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 | ctx.r8.u64;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x82328324
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328324;
	// li r8,2
	ctx.r8.s64 = 2;
	// li r9,9
	ctx.r9.s64 = 9;
	// li r5,-24566
	ctx.r5.s64 = -24566;
	// li r7,-30711
	ctx.r7.s64 = -30711;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r8,-22520
	ctx.r8.s64 = -22520;
loc_82328388:
	// rlwinm r4,r9,4,0,27
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// ori r3,r4,7
	ctx.r3.u64 = ctx.r4.u64 | 7;
	// or r31,r4,r5
	ctx.r31.u64 = ctx.r4.u64 | ctx.r5.u64;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// sth r3,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r3.u16);
	// or r30,r4,r7
	ctx.r30.u64 = ctx.r4.u64 | ctx.r7.u64;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// or r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 | ctx.r8.u64;
	// extsh r3,r30
	ctx.r3.s64 = ctx.r30.s16;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// sthu r31,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r31.u16);
	ctx.r10.u32 = ea;
	// sthu r31,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r31.u16);
	ctx.r10.u32 = ea;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x82328388
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328388;
	// sth r6,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r6.u16);
	// li r10,117
	ctx.r10.s64 = 117;
	// sth r6,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r6.u16);
	// li r9,101
	ctx.r9.s64 = 101;
	// sth r10,4(r11)
	PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r10.u16);
	// sth r9,6(r11)
	PPC_STORE_U16(ctx.r11.u32 + 6, ctx.r9.u16);
	// li r8,84
	ctx.r8.s64 = 84;
	// li r7,-22443
	ctx.r7.s64 = -22443;
	// li r6,68
	ctx.r6.s64 = 68;
	// sth r8,8(r11)
	PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r8.u16);
	// li r10,-22459
	ctx.r10.s64 = -22459;
	// sth r7,10(r11)
	PPC_STORE_U16(ctx.r11.u32 + 10, ctx.r7.u16);
	// li r9,51
	ctx.r9.s64 = 51;
	// sth r6,12(r11)
	PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r6.u16);
	// sth r10,14(r11)
	PPC_STORE_U16(ctx.r11.u32 + 14, ctx.r10.u16);
	// li r8,-30667
	ctx.r8.s64 = -30667;
	// sth r9,16(r11)
	PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r9.u16);
	// li r7,-22476
	ctx.r7.s64 = -22476;
	// li r6,-22476
	ctx.r6.s64 = -22476;
	// sth r8,18(r11)
	PPC_STORE_U16(ctx.r11.u32 + 18, ctx.r8.u16);
	// li r10,35
	ctx.r10.s64 = 35;
	// sth r7,20(r11)
	PPC_STORE_U16(ctx.r11.u32 + 20, ctx.r7.u16);
	// li r9,-30683
	ctx.r9.s64 = -30683;
	// sth r6,22(r11)
	PPC_STORE_U16(ctx.r11.u32 + 22, ctx.r6.u16);
	// sth r10,24(r11)
	PPC_STORE_U16(ctx.r11.u32 + 24, ctx.r10.u16);
	// li r8,-22492
	ctx.r8.s64 = -22492;
	// sth r9,26(r11)
	PPC_STORE_U16(ctx.r11.u32 + 26, ctx.r9.u16);
	// li r7,-22492
	ctx.r7.s64 = -22492;
	// li r6,17
	ctx.r6.s64 = 17;
	// sth r8,28(r11)
	PPC_STORE_U16(ctx.r11.u32 + 28, ctx.r8.u16);
	// li r10,17
	ctx.r10.s64 = 17;
	// sth r7,30(r11)
	PPC_STORE_U16(ctx.r11.u32 + 30, ctx.r7.u16);
	// li r9,-24556
	ctx.r9.s64 = -24556;
	// sth r6,32(r11)
	PPC_STORE_U16(ctx.r11.u32 + 32, ctx.r6.u16);
	// sth r10,34(r11)
	PPC_STORE_U16(ctx.r11.u32 + 34, ctx.r10.u16);
	// li r8,-24556
	ctx.r8.s64 = -24556;
	// sth r9,36(r11)
	PPC_STORE_U16(ctx.r11.u32 + 36, ctx.r9.u16);
	// li r7,-30701
	ctx.r7.s64 = -30701;
	// li r6,-30701
	ctx.r6.s64 = -30701;
	// sth r8,38(r11)
	PPC_STORE_U16(ctx.r11.u32 + 38, ctx.r8.u16);
	// li r10,-30701
	ctx.r10.s64 = -30701;
	// sth r7,40(r11)
	PPC_STORE_U16(ctx.r11.u32 + 40, ctx.r7.u16);
	// li r9,-30701
	ctx.r9.s64 = -30701;
	// sth r6,42(r11)
	PPC_STORE_U16(ctx.r11.u32 + 42, ctx.r6.u16);
	// sth r10,44(r11)
	PPC_STORE_U16(ctx.r11.u32 + 44, ctx.r10.u16);
	// li r8,-22510
	ctx.r8.s64 = -22510;
	// sth r9,46(r11)
	PPC_STORE_U16(ctx.r11.u32 + 46, ctx.r9.u16);
	// li r7,-22510
	ctx.r7.s64 = -22510;
	// li r6,-22510
	ctx.r6.s64 = -22510;
	// sth r8,48(r11)
	PPC_STORE_U16(ctx.r11.u32 + 48, ctx.r8.u16);
	// li r10,-22510
	ctx.r10.s64 = -22510;
	// sth r7,50(r11)
	PPC_STORE_U16(ctx.r11.u32 + 50, ctx.r7.u16);
	// li r9,-22510
	ctx.r9.s64 = -22510;
	// sth r6,52(r11)
	PPC_STORE_U16(ctx.r11.u32 + 52, ctx.r6.u16);
	// mr r5,r11
	ctx.r5.u64 = ctx.r11.u64;
	// sth r10,54(r11)
	PPC_STORE_U16(ctx.r11.u32 + 54, ctx.r10.u16);
	// sth r9,56(r11)
	PPC_STORE_U16(ctx.r11.u32 + 56, ctx.r9.u16);
	// sth r8,58(r11)
	PPC_STORE_U16(ctx.r11.u32 + 58, ctx.r8.u16);
	// sth r7,60(r11)
	PPC_STORE_U16(ctx.r11.u32 + 60, ctx.r7.u16);
	// sth r6,62(r11)
	PPC_STORE_U16(ctx.r11.u32 + 62, ctx.r6.u16);
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823284EC"))) PPC_WEAK_FUNC(sub_823284EC);
PPC_FUNC_IMPL(__imp__sub_823284EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823284F0"))) PPC_WEAK_FUNC(sub_823284F0);
PPC_FUNC_IMPL(__imp__sub_823284F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e45c
	ctx.lr = 0x823284F8;
	__restfpr_25(ctx, base);
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r9,8
	ctx.r9.s64 = 8;
	// addi r11,r11,18776
	ctx.r11.s64 = ctx.r11.s64 + 18776;
	// li r31,576
	ctx.r31.s64 = 576;
	// addi r10,r11,-1088
	ctx.r10.s64 = ctx.r11.s64 + -1088;
	// mr r8,r31
	ctx.r8.u64 = ctx.r31.u64;
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328518:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82328518
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328518;
	// li r9,6
	ctx.r9.s64 = 6;
	// li r8,571
	ctx.r8.s64 = 571;
	// addi r10,r11,-1088
	ctx.r10.s64 = ctx.r11.s64 + -1088;
	// sth r8,-1072(r11)
	PPC_STORE_U16(ctx.r11.u32 + -1072, ctx.r8.u16);
	// mr r8,r31
	ctx.r8.u64 = ctx.r31.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8232853C:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x8232853c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8232853C;
	// li r9,8
	ctx.r9.s64 = 8;
	// li r8,555
	ctx.r8.s64 = 555;
	// addi r10,r11,-1088
	ctx.r10.s64 = ctx.r11.s64 + -1088;
	// sth r8,-1058(r11)
	PPC_STORE_U16(ctx.r11.u32 + -1058, ctx.r8.u16);
	// mr r8,r31
	ctx.r8.u64 = ctx.r31.u64;
	// addi r10,r10,30
	ctx.r10.s64 = ctx.r10.s64 + 30;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328560:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82328560
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328560;
	// li r8,12
	ctx.r8.s64 = 12;
	// addi r10,r11,-1088
	ctx.r10.s64 = ctx.r11.s64 + -1088;
	// li r9,33
	ctx.r9.s64 = 33;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_8232857C:
	// li r8,11
	ctx.r8.s64 = 11;
	// rlwimi r8,r9,4,0,27
	ctx.r8.u64 = (rotl32(ctx.r9.u32, 4) & 0xFFFFFFF0) | (ctx.r8.u64 & 0xFFFFFFFF0000000F);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// sth r7,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r7.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x8232857c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8232857C;
	// li r4,346
	ctx.r4.s64 = 346;
	// li r5,330
	ctx.r5.s64 = 330;
	// sth r4,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r4.u16);
	// li r6,314
	ctx.r6.s64 = 314;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// li r8,298
	ctx.r8.s64 = 298;
	// li r9,282
	ctx.r9.s64 = 282;
	// li r7,266
	ctx.r7.s64 = 266;
	// sthu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r5.u16);
	ctx.r10.u32 = ea;
	// sthu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r5.u16);
	ctx.r10.u32 = ea;
	// li r5,-28661
	ctx.r5.s64 = -28661;
	// sthu r6,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r6.u16);
	ctx.r10.u32 = ea;
	// sthu r6,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r6.u16);
	ctx.r10.u32 = ea;
	// li r6,-26613
	ctx.r6.s64 = -26613;
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// li r8,6
	ctx.r8.s64 = 6;
	// sthu r9,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r10.u32 = ea;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r8,-18422
	ctx.r8.s64 = -18422;
	// sthu r9,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r10.u32 = ea;
	// li r9,15
	ctx.r9.s64 = 15;
	// sthu r7,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r10.u32 = ea;
	// sthu r7,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r10.u32 = ea;
	// li r7,-20470
	ctx.r7.s64 = -20470;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
loc_82328600:
	// rlwinm r4,r9,4,0,27
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// ori r3,r4,8
	ctx.r3.u64 = ctx.r4.u64 | 8;
	// or r30,r4,r5
	ctx.r30.u64 = ctx.r4.u64 | ctx.r5.u64;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// extsh r30,r30
	ctx.r30.s64 = ctx.r30.s16;
	// sth r3,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r3.u16);
	// or r29,r4,r6
	ctx.r29.u64 = ctx.r4.u64 | ctx.r6.u64;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// or r3,r4,r7
	ctx.r3.u64 = ctx.r4.u64 | ctx.r7.u64;
	// extsh r29,r29
	ctx.r29.s64 = ctx.r29.s16;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// or r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 | ctx.r8.u64;
	// sthu r30,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r30.u16);
	ctx.r10.u32 = ea;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// sthu r29,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r29.u16);
	ctx.r10.u32 = ea;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// sthu r3,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r3.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// sthu r4,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r10.u32 = ea;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x82328600
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328600;
	// li r8,2
	ctx.r8.s64 = 2;
	// li r9,9
	ctx.r9.s64 = 9;
	// li r3,-24565
	ctx.r3.s64 = -24565;
	// li r4,-22517
	ctx.r4.s64 = -22517;
	// li r5,-28662
	ctx.r5.s64 = -28662;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r6,-26614
	ctx.r6.s64 = -26614;
	// li r7,-20471
	ctx.r7.s64 = -20471;
	// li r8,-18423
	ctx.r8.s64 = -18423;
loc_8232867C:
	// rlwinm r30,r9,4,0,27
	ctx.r30.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// ori r29,r30,7
	ctx.r29.u64 = ctx.r30.u64 | 7;
	// or r28,r30,r3
	ctx.r28.u64 = ctx.r30.u64 | ctx.r3.u64;
	// extsh r29,r29
	ctx.r29.s64 = ctx.r29.s16;
	// or r27,r30,r4
	ctx.r27.u64 = ctx.r30.u64 | ctx.r4.u64;
	// sth r29,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r29.u16);
	// or r26,r30,r6
	ctx.r26.u64 = ctx.r30.u64 | ctx.r6.u64;
	// sthu r29,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r29.u16);
	ctx.r10.u32 = ea;
	// extsh r27,r27
	ctx.r27.s64 = ctx.r27.s16;
	// or r29,r30,r5
	ctx.r29.u64 = ctx.r30.u64 | ctx.r5.u64;
	// or r25,r30,r7
	ctx.r25.u64 = ctx.r30.u64 | ctx.r7.u64;
	// extsh r29,r29
	ctx.r29.s64 = ctx.r29.s16;
	// or r30,r30,r8
	ctx.r30.u64 = ctx.r30.u64 | ctx.r8.u64;
	// sthu r28,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r28.u16);
	ctx.r10.u32 = ea;
	// extsh r28,r26
	ctx.r28.s64 = ctx.r26.s16;
	// extsh r26,r25
	ctx.r26.s64 = ctx.r25.s16;
	// extsh r30,r30
	ctx.r30.s64 = ctx.r30.s16;
	// sthu r27,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r27.u16);
	ctx.r10.u32 = ea;
	// sthu r29,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r29.u16);
	ctx.r10.u32 = ea;
	// sthu r29,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r29.u16);
	ctx.r10.u32 = ea;
	// sthu r28,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r28.u16);
	ctx.r10.u32 = ea;
	// sthu r28,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r28.u16);
	ctx.r10.u32 = ea;
	// sthu r26,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r26.u16);
	ctx.r10.u32 = ea;
	// sthu r26,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r26.u16);
	ctx.r10.u32 = ea;
	// sthu r26,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r26.u16);
	ctx.r10.u32 = ea;
	// sthu r26,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r26.u16);
	ctx.r10.u32 = ea;
	// sthu r30,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r30.u16);
	ctx.r10.u32 = ea;
	// sthu r30,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r30.u16);
	ctx.r10.u32 = ea;
	// sthu r30,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r30.u16);
	ctx.r10.u32 = ea;
	// sthu r30,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r30.u16);
	ctx.r10.u32 = ea;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x8232867c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8232867C;
	// sth r31,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r31.u16);
	// li r10,117
	ctx.r10.s64 = 117;
	// li r9,101
	ctx.r9.s64 = 101;
	// sth r31,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r31.u16);
	// li r8,84
	ctx.r8.s64 = 84;
	// li r7,84
	ctx.r7.s64 = 84;
	// sth r10,4(r11)
	PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r10.u16);
	// li r6,68
	ctx.r6.s64 = 68;
	// sth r9,6(r11)
	PPC_STORE_U16(ctx.r11.u32 + 6, ctx.r9.u16);
	// sth r8,8(r11)
	PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r8.u16);
	// li r10,68
	ctx.r10.s64 = 68;
	// sth r7,10(r11)
	PPC_STORE_U16(ctx.r11.u32 + 10, ctx.r7.u16);
	// li r9,51
	ctx.r9.s64 = 51;
	// sth r6,12(r11)
	PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r6.u16);
	// li r8,51
	ctx.r8.s64 = 51;
	// li r7,-20427
	ctx.r7.s64 = -20427;
	// sth r10,14(r11)
	PPC_STORE_U16(ctx.r11.u32 + 14, ctx.r10.u16);
	// li r6,-18379
	ctx.r6.s64 = -18379;
	// sth r9,16(r11)
	PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r9.u16);
	// sth r8,18(r11)
	PPC_STORE_U16(ctx.r11.u32 + 18, ctx.r8.u16);
	// li r10,35
	ctx.r10.s64 = 35;
	// sth r7,20(r11)
	PPC_STORE_U16(ctx.r11.u32 + 20, ctx.r7.u16);
	// li r9,35
	ctx.r9.s64 = 35;
	// sth r6,22(r11)
	PPC_STORE_U16(ctx.r11.u32 + 22, ctx.r6.u16);
	// li r8,-20443
	ctx.r8.s64 = -20443;
	// li r7,-18395
	ctx.r7.s64 = -18395;
	// sth r10,24(r11)
	PPC_STORE_U16(ctx.r11.u32 + 24, ctx.r10.u16);
	// li r6,17
	ctx.r6.s64 = 17;
	// sth r9,26(r11)
	PPC_STORE_U16(ctx.r11.u32 + 26, ctx.r9.u16);
	// sth r8,28(r11)
	PPC_STORE_U16(ctx.r11.u32 + 28, ctx.r8.u16);
	// li r10,17
	ctx.r10.s64 = 17;
	// sth r7,30(r11)
	PPC_STORE_U16(ctx.r11.u32 + 30, ctx.r7.u16);
	// li r9,-24555
	ctx.r9.s64 = -24555;
	// sth r6,32(r11)
	PPC_STORE_U16(ctx.r11.u32 + 32, ctx.r6.u16);
	// li r8,-22507
	ctx.r8.s64 = -22507;
	// li r7,-28652
	ctx.r7.s64 = -28652;
	// sth r10,34(r11)
	PPC_STORE_U16(ctx.r11.u32 + 34, ctx.r10.u16);
	// li r6,-28652
	ctx.r6.s64 = -28652;
	// sth r9,36(r11)
	PPC_STORE_U16(ctx.r11.u32 + 36, ctx.r9.u16);
	// sth r8,38(r11)
	PPC_STORE_U16(ctx.r11.u32 + 38, ctx.r8.u16);
	// li r10,-26604
	ctx.r10.s64 = -26604;
	// sth r7,40(r11)
	PPC_STORE_U16(ctx.r11.u32 + 40, ctx.r7.u16);
	// li r9,-26604
	ctx.r9.s64 = -26604;
	// sth r6,42(r11)
	PPC_STORE_U16(ctx.r11.u32 + 42, ctx.r6.u16);
	// li r8,-20461
	ctx.r8.s64 = -20461;
	// li r7,-20461
	ctx.r7.s64 = -20461;
	// sth r10,44(r11)
	PPC_STORE_U16(ctx.r11.u32 + 44, ctx.r10.u16);
	// li r6,-20461
	ctx.r6.s64 = -20461;
	// sth r9,46(r11)
	PPC_STORE_U16(ctx.r11.u32 + 46, ctx.r9.u16);
	// sth r8,48(r11)
	PPC_STORE_U16(ctx.r11.u32 + 48, ctx.r8.u16);
	// li r10,-20461
	ctx.r10.s64 = -20461;
	// sth r7,50(r11)
	PPC_STORE_U16(ctx.r11.u32 + 50, ctx.r7.u16);
	// li r9,-18413
	ctx.r9.s64 = -18413;
	// sth r6,52(r11)
	PPC_STORE_U16(ctx.r11.u32 + 52, ctx.r6.u16);
	// li r8,-18413
	ctx.r8.s64 = -18413;
	// li r7,-18413
	ctx.r7.s64 = -18413;
	// sth r10,54(r11)
	PPC_STORE_U16(ctx.r11.u32 + 54, ctx.r10.u16);
	// li r6,-18413
	ctx.r6.s64 = -18413;
	// sth r9,56(r11)
	PPC_STORE_U16(ctx.r11.u32 + 56, ctx.r9.u16);
	// mr r5,r11
	ctx.r5.u64 = ctx.r11.u64;
	// sth r8,58(r11)
	PPC_STORE_U16(ctx.r11.u32 + 58, ctx.r8.u16);
	// sth r7,60(r11)
	PPC_STORE_U16(ctx.r11.u32 + 60, ctx.r7.u16);
	// sth r6,62(r11)
	PPC_STORE_U16(ctx.r11.u32 + 62, ctx.r6.u16);
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82328800"))) PPC_WEAK_FUNC(sub_82328800);
PPC_FUNC_IMPL(__imp__sub_82328800) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r9,4358
	ctx.r9.s64 = 4358;
	// addi r11,r11,18584
	ctx.r11.s64 = ctx.r11.s64 + 18584;
	// li r7,4613
	ctx.r7.s64 = 4613;
	// li r6,6661
	ctx.r6.s64 = 6661;
	// li r5,261
	ctx.r5.s64 = 261;
	// li r10,2051
	ctx.r10.s64 = 2051;
	// li r8,8
	ctx.r8.s64 = 8;
	// sth r9,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r9.u16);
	// sth r7,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r7.u16);
	// addi r9,r11,8
	ctx.r9.s64 = ctx.r11.s64 + 8;
	// sth r6,4(r11)
	PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r6.u16);
	// addi r9,r11,14
	ctx.r9.s64 = ctx.r11.s64 + 14;
	// sth r5,6(r11)
	PPC_STORE_U16(ctx.r11.u32 + 6, ctx.r5.u16);
	// sth r10,8(r11)
	PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r10.u16);
	// sth r10,10(r11)
	PPC_STORE_U16(ctx.r11.u32 + 10, ctx.r10.u16);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// sth r10,12(r11)
	PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r10.u16);
	// sth r10,14(r11)
	PPC_STORE_U16(ctx.r11.u32 + 14, ctx.r10.u16);
	// li r10,514
	ctx.r10.s64 = 514;
loc_82328850:
	// sthu r10,2(r9)
	ea = 2 + ctx.r9.u32;
	PPC_STORE_U16(ea, ctx.r10.u16);
	ctx.r9.u32 = ea;
	// bdnz 0x82328850
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328850;
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,2561
	ctx.r9.s64 = 2561;
	// addi r11,r11,30
	ctx.r11.s64 = ctx.r11.s64 + 30;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_82328868:
	// sthu r9,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328868
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328868;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82328874"))) PPC_WEAK_FUNC(sub_82328874);
PPC_FUNC_IMPL(__imp__sub_82328874) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82328878"))) PPC_WEAK_FUNC(sub_82328878);
PPC_FUNC_IMPL(__imp__sub_82328878) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r8,7936
	ctx.r8.s64 = 7936;
	// addi r11,r11,17944
	ctx.r11.s64 = ctx.r11.s64 + 17944;
	// li r7,4358
	ctx.r7.s64 = 4358;
	// li r6,5638
	ctx.r6.s64 = 5638;
	// li r5,6662
	ctx.r5.s64 = 6662;
	// li r3,7685
	ctx.r3.s64 = 7685;
	// sth r8,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r8.u16);
	// li r4,261
	ctx.r4.s64 = 261;
	// sth r7,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r7.u16);
	// li r9,2052
	ctx.r9.s64 = 2052;
	// sth r6,4(r11)
	PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r6.u16);
	// li r10,2564
	ctx.r10.s64 = 2564;
	// sth r5,6(r11)
	PPC_STORE_U16(ctx.r11.u32 + 6, ctx.r5.u16);
	// li r8,8
	ctx.r8.s64 = 8;
	// sth r3,8(r11)
	PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r3.u16);
	// sth r3,10(r11)
	PPC_STORE_U16(ctx.r11.u32 + 10, ctx.r3.u16);
	// sth r4,12(r11)
	PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r4.u16);
	// sth r4,14(r11)
	PPC_STORE_U16(ctx.r11.u32 + 14, ctx.r4.u16);
	// sth r9,16(r11)
	PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r9.u16);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// sth r9,18(r11)
	PPC_STORE_U16(ctx.r11.u32 + 18, ctx.r9.u16);
	// sth r9,20(r11)
	PPC_STORE_U16(ctx.r11.u32 + 20, ctx.r9.u16);
	// sth r9,22(r11)
	PPC_STORE_U16(ctx.r11.u32 + 22, ctx.r9.u16);
	// sth r10,24(r11)
	PPC_STORE_U16(ctx.r11.u32 + 24, ctx.r10.u16);
	// addi r9,r11,8
	ctx.r9.s64 = ctx.r11.s64 + 8;
	// sth r10,26(r11)
	PPC_STORE_U16(ctx.r11.u32 + 26, ctx.r10.u16);
	// addi r9,r11,12
	ctx.r9.s64 = ctx.r11.s64 + 12;
	// sth r10,28(r11)
	PPC_STORE_U16(ctx.r11.u32 + 28, ctx.r10.u16);
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
	// sth r10,30(r11)
	PPC_STORE_U16(ctx.r11.u32 + 30, ctx.r10.u16);
	// addi r10,r11,24
	ctx.r10.s64 = ctx.r11.s64 + 24;
	// li r9,1027
	ctx.r9.s64 = 1027;
	// addi r10,r11,30
	ctx.r10.s64 = ctx.r11.s64 + 30;
loc_82328900:
	// sthu r9,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82328900
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328900;
	// li r9,8
	ctx.r9.s64 = 8;
	// li r8,1539
	ctx.r8.s64 = 1539;
	// addi r10,r11,46
	ctx.r10.s64 = ctx.r11.s64 + 46;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328918:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82328918
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328918;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,3074
	ctx.r8.s64 = 3074;
	// addi r10,r11,62
	ctx.r10.s64 = ctx.r11.s64 + 62;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328930:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82328930
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328930;
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,3586
	ctx.r9.s64 = 3586;
	// addi r11,r11,94
	ctx.r11.s64 = ctx.r11.s64 + 94;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_82328948:
	// sthu r9,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328948
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328948;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82328954"))) PPC_WEAK_FUNC(sub_82328954);
PPC_FUNC_IMPL(__imp__sub_82328954) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82328958"))) PPC_WEAK_FUNC(sub_82328958);
PPC_FUNC_IMPL(__imp__sub_82328958) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r9,24
	ctx.r9.s64 = 24;
	// addi r11,r11,18520
	ctx.r11.s64 = ctx.r11.s64 + 18520;
	// li r4,127
	ctx.r4.s64 = 127;
	// addi r10,r11,-448
	ctx.r10.s64 = ctx.r11.s64 + -448;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328978:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82328978
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328978;
	// addi r10,r11,-448
	ctx.r10.s64 = ctx.r11.s64 + -448;
	// li r8,16
	ctx.r8.s64 = 16;
	// li r9,-16
	ctx.r9.s64 = -16;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// li r7,11
	ctx.r7.s64 = 11;
loc_82328994:
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// rlwimi r6,r7,8,0,23
	ctx.r6.u64 = (rotl32(ctx.r7.u32, 8) & 0xFFFFFF00) | (ctx.r6.u64 & 0xFFFFFFFF000000FF);
	// rlwimi r5,r7,8,0,23
	ctx.r5.u64 = (rotl32(ctx.r7.u32, 8) & 0xFFFFFF00) | (ctx.r5.u64 & 0xFFFFFFFF000000FF);
	// sth r6,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r6.u16);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// sthu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r5.u16);
	ctx.r10.u32 = ea;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmpwi cr6,r9,-11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, -11, ctx.xer);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// ble cr6,0x82328994
	if (!ctx.cr6.gt) goto loc_82328994;
	// li r8,10
	ctx.r8.s64 = 10;
	// li r9,-10
	ctx.r9.s64 = -10;
loc_823289C8:
	// clrlwi r7,r8,24
	ctx.r7.u64 = ctx.r8.u32 & 0xFF;
	// clrlwi r6,r9,24
	ctx.r6.u64 = ctx.r9.u32 & 0xFF;
	// ori r5,r7,2560
	ctx.r5.u64 = ctx.r7.u64 | 2560;
	// ori r3,r6,2560
	ctx.r3.u64 = ctx.r6.u64 | 2560;
	// sth r5,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r5.u16);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// sth r5,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r5.u16);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmpwi cr6,r9,-8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, -8, ctx.xer);
	// sth r3,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r3.u16);
	// sth r3,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r3.u16);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// ble cr6,0x823289c8
	if (!ctx.cr6.gt) goto loc_823289C8;
	// li r5,7
	ctx.r5.s64 = 7;
	// li r6,-7
	ctx.r6.s64 = -7;
loc_82328A08:
	// li r8,8
	ctx.r8.s64 = 8;
	// clrlwi r7,r5,24
	ctx.r7.u64 = ctx.r5.u32 & 0xFF;
	// addi r9,r10,-2
	ctx.r9.s64 = ctx.r10.s64 + -2;
	// ori r7,r7,2048
	ctx.r7.u64 = ctx.r7.u64 | 2048;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_82328A1C:
	// sthu r7,2(r9)
	ea = 2 + ctx.r9.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r9.u32 = ea;
	// bdnz 0x82328a1c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328A1C;
	// li r9,8
	ctx.r9.s64 = 8;
	// clrlwi r7,r6,24
	ctx.r7.u64 = ctx.r6.u32 & 0xFF;
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// ori r7,r7,2048
	ctx.r7.u64 = ctx.r7.u64 | 2048;
	// addi r10,r8,-2
	ctx.r10.s64 = ctx.r8.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328A3C:
	// sthu r7,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82328a3c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328A3C;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r10,r8,16
	ctx.r10.s64 = ctx.r8.s64 + 16;
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// cmpwi cr6,r6,-5
	ctx.cr6.compare<int32_t>(ctx.r6.s32, -5, ctx.xer);
	// ble cr6,0x82328a08
	if (!ctx.cr6.gt) goto loc_82328A08;
	// li r8,16
	ctx.r8.s64 = 16;
	// addi r9,r10,-2
	ctx.r9.s64 = ctx.r10.s64 + -2;
	// li r7,1796
	ctx.r7.s64 = 1796;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_82328A68:
	// sthu r7,2(r9)
	ea = 2 + ctx.r9.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r9.u32 = ea;
	// bdnz 0x82328a68
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328A68;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,2044
	ctx.r8.s64 = 2044;
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328A84:
	// sthu r8,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x82328a84
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328A84;
	// sth r4,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r4.u16);
	// li r8,1283
	ctx.r8.s64 = 1283;
	// li r7,1533
	ctx.r7.s64 = 1533;
	// sth r4,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r4.u16);
	// li r5,1026
	ctx.r5.s64 = 1026;
	// sth r8,4(r11)
	PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r8.u16);
	// li r6,1278
	ctx.r6.s64 = 1278;
	// sth r7,6(r11)
	PPC_STORE_U16(ctx.r11.u32 + 6, ctx.r7.u16);
	// li r9,769
	ctx.r9.s64 = 769;
	// sth r5,8(r11)
	PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r5.u16);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// sth r5,10(r11)
	PPC_STORE_U16(ctx.r11.u32 + 10, ctx.r5.u16);
	// sth r6,12(r11)
	PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r6.u16);
	// li r10,1023
	ctx.r10.s64 = 1023;
	// sth r6,14(r11)
	PPC_STORE_U16(ctx.r11.u32 + 14, ctx.r6.u16);
	// sth r9,16(r11)
	PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r9.u16);
	// li r8,16
	ctx.r8.s64 = 16;
	// sth r9,18(r11)
	PPC_STORE_U16(ctx.r11.u32 + 18, ctx.r9.u16);
	// sth r9,20(r11)
	PPC_STORE_U16(ctx.r11.u32 + 20, ctx.r9.u16);
	// sth r9,22(r11)
	PPC_STORE_U16(ctx.r11.u32 + 22, ctx.r9.u16);
	// sth r10,24(r11)
	PPC_STORE_U16(ctx.r11.u32 + 24, ctx.r10.u16);
	// addi r9,r11,8
	ctx.r9.s64 = ctx.r11.s64 + 8;
	// sth r10,26(r11)
	PPC_STORE_U16(ctx.r11.u32 + 26, ctx.r10.u16);
	// addi r9,r11,12
	ctx.r9.s64 = ctx.r11.s64 + 12;
	// sth r10,28(r11)
	PPC_STORE_U16(ctx.r11.u32 + 28, ctx.r10.u16);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// sth r10,30(r11)
	PPC_STORE_U16(ctx.r11.u32 + 30, ctx.r10.u16);
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// addi r10,r11,24
	ctx.r10.s64 = ctx.r11.s64 + 24;
	// addi r11,r11,30
	ctx.r11.s64 = ctx.r11.s64 + 30;
	// li r10,256
	ctx.r10.s64 = 256;
loc_82328B08:
	// sthu r10,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r10.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328b08
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328B08;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82328B14"))) PPC_WEAK_FUNC(sub_82328B14);
PPC_FUNC_IMPL(__imp__sub_82328B14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82328B18"))) PPC_WEAK_FUNC(sub_82328B18);
PPC_FUNC_IMPL(__imp__sub_82328B18) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e458
	ctx.lr = 0x82328B20;
	__restfpr_24(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r10,-6391
	ctx.r10.s64 = -6391;
	// sth r11,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, ctx.r11.u16);
	// li r9,-9463
	ctx.r9.s64 = -9463;
	// sth r11,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r11.u16);
	// li r8,-1271
	ctx.r8.s64 = -1271;
	// sthu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r10.u16);
	ctx.r3.u32 = ea;
	// li r10,-2295
	ctx.r10.s64 = -2295;
	// li r25,-4343
	ctx.r25.s64 = -4343;
	// li r24,-8439
	ctx.r24.s64 = -8439;
	// li r26,-17912
	ctx.r26.s64 = -17912;
	// li r27,-18936
	ctx.r27.s64 = -18936;
	// sthu r9,2(r3)
	ea = 2 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r3.u32 = ea;
	// li r28,-20984
	ctx.r28.s64 = -20984;
	// li r29,-25080
	ctx.r29.s64 = -25080;
	// li r30,30984
	ctx.r30.s64 = 30984;
	// li r31,29960
	ctx.r31.s64 = 29960;
	// li r4,27912
	ctx.r4.s64 = 27912;
	// sthu r8,2(r3)
	ea = 2 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r3.u32 = ea;
	// li r5,23816
	ctx.r5.s64 = 23816;
	// li r6,-23032
	ctx.r6.s64 = -23032;
	// li r7,-26104
	ctx.r7.s64 = -26104;
	// li r8,25864
	ctx.r8.s64 = 25864;
	// li r11,22792
	ctx.r11.s64 = 22792;
	// sthu r10,2(r3)
	ea = 2 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r10.u16);
	ctx.r3.u32 = ea;
	// li r9,-5368
	ctx.r9.s64 = -5368;
	// li r10,-10488
	ctx.r10.s64 = -10488;
	// sthu r25,2(r3)
	ea = 2 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r25.u16);
	ctx.r3.u32 = ea;
	// sthu r24,2(r3)
	ea = 2 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r24.u16);
	ctx.r3.u32 = ea;
	// sthu r26,2(r3)
	ea = 2 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r26.u16);
	ctx.r3.u32 = ea;
	// sth r26,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r26.u16);
	// sthu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r27.u16);
	ctx.r3.u32 = ea;
	// sth r27,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r27.u16);
	// li r27,-12536
	ctx.r27.s64 = -12536;
	// sthu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r28.u16);
	ctx.r3.u32 = ea;
	// sth r28,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r28.u16);
	// li r28,-22008
	ctx.r28.s64 = -22008;
	// sthu r29,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r29.u16);
	ctx.r3.u32 = ea;
	// sth r29,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r29.u16);
	// li r29,-27128
	ctx.r29.s64 = -27128;
	// sthu r30,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r30.u16);
	ctx.r3.u32 = ea;
	// sth r30,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r30.u16);
	// li r30,-19960
	ctx.r30.s64 = -19960;
	// sthu r31,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r31.u16);
	ctx.r3.u32 = ea;
	// sth r31,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r31.u16);
	// li r31,-29176
	ctx.r31.s64 = -29176;
	// sthu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r3.u32 = ea;
	// sth r4,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r4.u16);
	// li r4,26888
	ctx.r4.s64 = 26888;
	// sthu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r5.u16);
	ctx.r3.u32 = ea;
	// sth r5,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r5.u16);
	// li r5,21768
	ctx.r5.s64 = 21768;
	// sthu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r6.u16);
	ctx.r3.u32 = ea;
	// sth r6,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r6.u16);
	// li r6,28936
	ctx.r6.s64 = 28936;
	// sthu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r3.u32 = ea;
	// sth r7,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r7.u16);
	// li r7,19720
	ctx.r7.s64 = 19720;
	// sthu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r3.u32 = ea;
	// sth r8,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r8.u16);
	// li r8,-7416
	ctx.r8.s64 = -7416;
	// sthu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r11.u16);
	ctx.r3.u32 = ea;
	// sth r11,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r11.u16);
	// li r11,-3320
	ctx.r11.s64 = -3320;
	// sthu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r3.u32 = ea;
	// sth r9,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r9.u16);
	// li r9,-11512
	ctx.r9.s64 = -11512;
	// sthu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r10.u16);
	ctx.r3.u32 = ea;
	// sth r10,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r10.u16);
	// li r10,-13560
	ctx.r10.s64 = -13560;
	// sthu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r11.u16);
	ctx.r3.u32 = ea;
	// sth r11,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r11.u16);
	// li r11,-14584
	ctx.r11.s64 = -14584;
	// sthu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r27.u16);
	ctx.r3.u32 = ea;
	// sth r27,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r27.u16);
	// sthu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r28.u16);
	ctx.r3.u32 = ea;
	// sth r28,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r28.u16);
	// sthu r29,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r29.u16);
	ctx.r3.u32 = ea;
	// sth r29,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r29.u16);
	// sthu r30,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r30.u16);
	ctx.r3.u32 = ea;
	// sth r30,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r30.u16);
	// sthu r31,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r31.u16);
	ctx.r3.u32 = ea;
	// sth r31,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r31.u16);
	// sthu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r3.u32 = ea;
	// sth r4,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r4.u16);
	// sthu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r5.u16);
	ctx.r3.u32 = ea;
	// sth r5,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r5.u16);
	// sthu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r6.u16);
	ctx.r3.u32 = ea;
	// sth r6,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r6.u16);
	// sthu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r3.u32 = ea;
	// sth r7,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r7.u16);
	// sthu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r3.u32 = ea;
	// sth r8,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r8.u16);
	// sthu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r3.u32 = ea;
	// sth r9,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r9.u16);
	// sthu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r10.u16);
	ctx.r3.u32 = ea;
	// sth r10,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r10.u16);
	// sthu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r11.u16);
	ctx.r3.u32 = ea;
	// sth r11,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r11.u16);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// b 0x8233e4a8
	__restgprlr_24(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82328CB4"))) PPC_WEAK_FUNC(sub_82328CB4);
PPC_FUNC_IMPL(__imp__sub_82328CB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82328CB8"))) PPC_WEAK_FUNC(sub_82328CB8);
PPC_FUNC_IMPL(__imp__sub_82328CB8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// li r31,-24057
	ctx.r31.s64 = -24057;
	// li r4,-28153
	ctx.r4.s64 = -28153;
	// sth r31,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, ctx.r31.u16);
	// li r5,-30201
	ctx.r5.s64 = -30201;
	// sth r31,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r31.u16);
	// li r6,-31225
	ctx.r6.s64 = -31225;
	// sth r31,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r31.u16);
	// li r7,24839
	ctx.r7.s64 = 24839;
	// sth r31,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r31.u16);
	// li r9,20743
	ctx.r9.s64 = 20743;
	// sthu r4,8(r3)
	ea = 8 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r4.u16);
	ctx.r3.u32 = ea;
	// li r10,18695
	ctx.r10.s64 = 18695;
	// li r11,17671
	ctx.r11.s64 = 17671;
	// li r8,-250
	ctx.r8.s64 = -250;
	// sth r4,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r4.u16);
	// sth r4,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r4.u16);
	// sth r4,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r4.u16);
	// sthu r5,8(r3)
	ea = 8 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r5.u16);
	ctx.r3.u32 = ea;
	// sth r5,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r5.u16);
	// sth r5,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r5.u16);
	// sth r5,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r5.u16);
	// sthu r6,8(r3)
	ea = 8 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r6.u16);
	ctx.r3.u32 = ea;
	// sth r6,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r6.u16);
	// sth r6,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r6.u16);
	// sth r6,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r6.u16);
	// sthu r7,8(r3)
	ea = 8 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r3.u32 = ea;
	// sth r7,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r7.u16);
	// sth r7,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r7.u16);
	// sth r7,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r7.u16);
	// sthu r9,8(r3)
	ea = 8 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r3.u32 = ea;
	// sth r9,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r9.u16);
	// sth r9,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r9.u16);
	// sth r9,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r9.u16);
	// li r9,-15610
	ctx.r9.s64 = -15610;
	// sthu r10,8(r3)
	ea = 8 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r10.u16);
	ctx.r3.u32 = ea;
	// sth r10,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r10.u16);
	// sth r10,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r10.u16);
	// sth r10,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r10.u16);
	// li r10,9222
	ctx.r10.s64 = 9222;
	// sthu r11,8(r3)
	ea = 8 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r11.u16);
	ctx.r3.u32 = ea;
	// sth r11,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r11.u16);
	// sth r11,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r11.u16);
	// sth r11,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r11.u16);
	// li r11,6150
	ctx.r11.s64 = 6150;
	// sthu r8,8(r3)
	ea = 8 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r3.u32 = ea;
	// sth r8,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r8.u16);
	// sth r8,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r8.u16);
	// sth r8,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r8.u16);
	// sth r8,8(r3)
	PPC_STORE_U16(ctx.r3.u32 + 8, ctx.r8.u16);
	// sth r8,10(r3)
	PPC_STORE_U16(ctx.r3.u32 + 10, ctx.r8.u16);
	// sth r8,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r8.u16);
	// sth r8,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r8.u16);
	// li r8,-16891
	ctx.r8.s64 = -16891;
	// sthu r9,16(r3)
	ea = 16 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r3.u32 = ea;
	// sth r9,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r9.u16);
	// sth r9,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r9.u16);
	// sth r9,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r9.u16);
	// sth r9,8(r3)
	PPC_STORE_U16(ctx.r3.u32 + 8, ctx.r9.u16);
	// sth r9,10(r3)
	PPC_STORE_U16(ctx.r3.u32 + 10, ctx.r9.u16);
	// sth r9,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r9.u16);
	// sth r9,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r9.u16);
	// sthu r10,16(r3)
	ea = 16 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r10.u16);
	ctx.r3.u32 = ea;
	// sth r10,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r10.u16);
	// sth r10,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r10.u16);
	// sth r10,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r10.u16);
	// sth r10,8(r3)
	PPC_STORE_U16(ctx.r3.u32 + 8, ctx.r10.u16);
	// sth r10,10(r3)
	PPC_STORE_U16(ctx.r3.u32 + 10, ctx.r10.u16);
	// sth r10,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r10.u16);
	// sth r10,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r10.u16);
	// li r10,16
	ctx.r10.s64 = 16;
	// sthu r11,16(r3)
	ea = 16 + ctx.r3.u32;
	PPC_STORE_U16(ea, ctx.r11.u16);
	ctx.r3.u32 = ea;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// sth r11,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r11.u16);
	// sth r11,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r11.u16);
	// sth r11,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r11.u16);
	// sth r11,8(r3)
	PPC_STORE_U16(ctx.r3.u32 + 8, ctx.r11.u16);
	// sth r11,10(r3)
	PPC_STORE_U16(ctx.r3.u32 + 10, ctx.r11.u16);
	// sth r11,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r11.u16);
	// sth r11,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r11.u16);
	// addi r11,r3,16
	ctx.r11.s64 = ctx.r3.s64 + 16;
	// addi r9,r11,-2
	ctx.r9.s64 = ctx.r11.s64 + -2;
loc_82328E00:
	// sthu r8,2(r9)
	ea = 2 + ctx.r9.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r9.u32 = ea;
	// bdnz 0x82328e00
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328E00;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r11,32
	ctx.r10.s64 = ctx.r11.s64 + 32;
	// li r8,-32251
	ctx.r8.s64 = -32251;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328E1C:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328e1c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328E1C;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,32005
	ctx.r8.s64 = 32005;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328E38:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328e38
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328E38;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,16645
	ctx.r8.s64 = 16645;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328E54:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328e54
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328E54;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,14341
	ctx.r8.s64 = 14341;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328E70:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328e70
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328E70;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,13317
	ctx.r8.s64 = 13317;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328E8C:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328e8c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328E8C;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,11269
	ctx.r8.s64 = 11269;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328EA8:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328ea8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328EA8;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,7173
	ctx.r8.s64 = 7173;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328EC4:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328ec4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328EC4;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,10245
	ctx.r8.s64 = 10245;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328EE0:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328ee0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328EE0;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,5125
	ctx.r8.s64 = 5125;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328EFC:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328efc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328EFC;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,12293
	ctx.r8.s64 = 12293;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328F18:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328f18
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328F18;
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,3077
	ctx.r8.s64 = 3077;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328F34:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328f34
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328F34;
	// li r9,32
	ctx.r9.s64 = 32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// li r8,8196
	ctx.r8.s64 = 8196;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328F50:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328f50
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328F50;
	// li r9,32
	ctx.r9.s64 = 32;
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// li r8,4100
	ctx.r8.s64 = 4100;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328F6C:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328f6c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328F6C;
	// li r9,32
	ctx.r9.s64 = 32;
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// li r8,2052
	ctx.r8.s64 = 2052;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328F88:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328f88
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328F88;
	// li r9,32
	ctx.r9.s64 = 32;
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// li r8,1028
	ctx.r8.s64 = 1028;
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328FA4:
	// sthu r8,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r8.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328fa4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328FA4;
	// li r9,64
	ctx.r9.s64 = 64;
	// addi r8,r10,64
	ctx.r8.s64 = ctx.r10.s64 + 64;
	// li r10,15363
	ctx.r10.s64 = 15363;
	// addi r11,r8,-2
	ctx.r11.s64 = ctx.r8.s64 + -2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328FC0:
	// sthu r10,2(r11)
	ea = 2 + ctx.r11.u32;
	PPC_STORE_U16(ea, ctx.r10.u16);
	ctx.r11.u32 = ea;
	// bdnz 0x82328fc0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328FC0;
	// addi r3,r8,128
	ctx.r3.s64 = ctx.r8.s64 + 128;
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82328FD4"))) PPC_WEAK_FUNC(sub_82328FD4);
PPC_FUNC_IMPL(__imp__sub_82328FD4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82328FD8"))) PPC_WEAK_FUNC(sub_82328FD8);
PPC_FUNC_IMPL(__imp__sub_82328FD8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r9,32
	ctx.r9.s64 = 32;
	// addi r11,r11,18648
	ctx.r11.s64 = ctx.r11.s64 + 18648;
	// li r8,18
	ctx.r8.s64 = 18;
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82328FF0:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82328ff0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82328FF0;
	// li r9,32
	ctx.r9.s64 = 32;
	// li r8,34
	ctx.r8.s64 = 34;
	// addi r10,r11,31
	ctx.r10.s64 = ctx.r11.s64 + 31;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329008:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329008
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329008;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,3
	ctx.r8.s64 = 3;
	// addi r10,r11,63
	ctx.r10.s64 = ctx.r11.s64 + 63;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329020:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329020
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329020;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,51
	ctx.r8.s64 = 51;
	// addi r10,r11,79
	ctx.r10.s64 = ctx.r11.s64 + 79;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329038:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329038
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329038;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,67
	ctx.r8.s64 = 67;
	// addi r10,r11,95
	ctx.r10.s64 = ctx.r11.s64 + 95;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329050:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329050
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329050;
	// li r10,84
	ctx.r10.s64 = 84;
	// li r9,84
	ctx.r9.s64 = 84;
	// stb r10,112(r11)
	PPC_STORE_U8(ctx.r11.u32 + 112, ctx.r10.u8);
	// li r8,84
	ctx.r8.s64 = 84;
	// stb r10,115(r11)
	PPC_STORE_U8(ctx.r11.u32 + 115, ctx.r10.u8);
	// stb r10,118(r11)
	PPC_STORE_U8(ctx.r11.u32 + 118, ctx.r10.u8);
	// li r10,101
	ctx.r10.s64 = 101;
	// stb r9,113(r11)
	PPC_STORE_U8(ctx.r11.u32 + 113, ctx.r9.u8);
	// stb r8,114(r11)
	PPC_STORE_U8(ctx.r11.u32 + 114, ctx.r8.u8);
	// stb r9,116(r11)
	PPC_STORE_U8(ctx.r11.u32 + 116, ctx.r9.u8);
	// stb r8,117(r11)
	PPC_STORE_U8(ctx.r11.u32 + 117, ctx.r8.u8);
	// li r8,101
	ctx.r8.s64 = 101;
	// stb r9,119(r11)
	PPC_STORE_U8(ctx.r11.u32 + 119, ctx.r9.u8);
	// li r9,101
	ctx.r9.s64 = 101;
	// stb r10,121(r11)
	PPC_STORE_U8(ctx.r11.u32 + 121, ctx.r10.u8);
	// li r10,118
	ctx.r10.s64 = 118;
	// stb r8,120(r11)
	PPC_STORE_U8(ctx.r11.u32 + 120, ctx.r8.u8);
	// stb r9,122(r11)
	PPC_STORE_U8(ctx.r11.u32 + 122, ctx.r9.u8);
	// li r9,118
	ctx.r9.s64 = 118;
	// stb r8,123(r11)
	PPC_STORE_U8(ctx.r11.u32 + 123, ctx.r8.u8);
	// li r8,-121
	ctx.r8.s64 = -121;
	// stb r10,124(r11)
	PPC_STORE_U8(ctx.r11.u32 + 124, ctx.r10.u8);
	// li r10,-121
	ctx.r10.s64 = -121;
	// stb r9,125(r11)
	PPC_STORE_U8(ctx.r11.u32 + 125, ctx.r9.u8);
	// stb r8,126(r11)
	PPC_STORE_U8(ctx.r11.u32 + 126, ctx.r8.u8);
	// stb r10,127(r11)
	PPC_STORE_U8(ctx.r11.u32 + 127, ctx.r10.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823290C4"))) PPC_WEAK_FUNC(sub_823290C4);
PPC_FUNC_IMPL(__imp__sub_823290C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823290C8"))) PPC_WEAK_FUNC(sub_823290C8);
PPC_FUNC_IMPL(__imp__sub_823290C8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r9,32
	ctx.r9.s64 = 32;
	// addi r11,r11,18840
	ctx.r11.s64 = ctx.r11.s64 + 18840;
	// li r8,2
	ctx.r8.s64 = 2;
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_823290E0:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x823290e0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823290E0;
	// li r9,32
	ctx.r9.s64 = 32;
	// li r8,18
	ctx.r8.s64 = 18;
	// addi r10,r11,31
	ctx.r10.s64 = ctx.r11.s64 + 31;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_823290F8:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x823290f8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823290F8;
	// li r9,32
	ctx.r9.s64 = 32;
	// li r8,34
	ctx.r8.s64 = 34;
	// addi r10,r11,63
	ctx.r10.s64 = ctx.r11.s64 + 63;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329110:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329110
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329110;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,51
	ctx.r8.s64 = 51;
	// addi r10,r11,95
	ctx.r10.s64 = ctx.r11.s64 + 95;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329128:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329128
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329128;
	// li r10,68
	ctx.r10.s64 = 68;
	// li r9,68
	ctx.r9.s64 = 68;
	// stb r10,112(r11)
	PPC_STORE_U8(ctx.r11.u32 + 112, ctx.r10.u8);
	// li r8,68
	ctx.r8.s64 = 68;
	// stb r10,115(r11)
	PPC_STORE_U8(ctx.r11.u32 + 115, ctx.r10.u8);
	// stb r10,118(r11)
	PPC_STORE_U8(ctx.r11.u32 + 118, ctx.r10.u8);
	// li r10,85
	ctx.r10.s64 = 85;
	// stb r9,113(r11)
	PPC_STORE_U8(ctx.r11.u32 + 113, ctx.r9.u8);
	// stb r8,114(r11)
	PPC_STORE_U8(ctx.r11.u32 + 114, ctx.r8.u8);
	// stb r9,116(r11)
	PPC_STORE_U8(ctx.r11.u32 + 116, ctx.r9.u8);
	// stb r8,117(r11)
	PPC_STORE_U8(ctx.r11.u32 + 117, ctx.r8.u8);
	// li r8,85
	ctx.r8.s64 = 85;
	// stb r9,119(r11)
	PPC_STORE_U8(ctx.r11.u32 + 119, ctx.r9.u8);
	// li r9,85
	ctx.r9.s64 = 85;
	// stb r10,121(r11)
	PPC_STORE_U8(ctx.r11.u32 + 121, ctx.r10.u8);
	// li r10,102
	ctx.r10.s64 = 102;
	// stb r8,120(r11)
	PPC_STORE_U8(ctx.r11.u32 + 120, ctx.r8.u8);
	// stb r9,122(r11)
	PPC_STORE_U8(ctx.r11.u32 + 122, ctx.r9.u8);
	// li r9,102
	ctx.r9.s64 = 102;
	// stb r8,123(r11)
	PPC_STORE_U8(ctx.r11.u32 + 123, ctx.r8.u8);
	// li r8,119
	ctx.r8.s64 = 119;
	// stb r10,124(r11)
	PPC_STORE_U8(ctx.r11.u32 + 124, ctx.r10.u8);
	// li r10,-120
	ctx.r10.s64 = -120;
	// stb r9,125(r11)
	PPC_STORE_U8(ctx.r11.u32 + 125, ctx.r9.u8);
	// stb r8,126(r11)
	PPC_STORE_U8(ctx.r11.u32 + 126, ctx.r8.u8);
	// stb r10,127(r11)
	PPC_STORE_U8(ctx.r11.u32 + 127, ctx.r10.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232919C"))) PPC_WEAK_FUNC(sub_8232919C);
PPC_FUNC_IMPL(__imp__sub_8232919C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823291A0"))) PPC_WEAK_FUNC(sub_823291A0);
PPC_FUNC_IMPL(__imp__sub_823291A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r9,256
	ctx.r9.s64 = 256;
	// addi r11,r11,18968
	ctx.r11.s64 = ctx.r11.s64 + 18968;
	// li r8,18
	ctx.r8.s64 = 18;
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_823291B8:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x823291b8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823291B8;
	// li r9,256
	ctx.r9.s64 = 256;
	// li r8,34
	ctx.r8.s64 = 34;
	// addi r10,r11,255
	ctx.r10.s64 = ctx.r11.s64 + 255;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_823291D0:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x823291d0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823291D0;
	// li r9,128
	ctx.r9.s64 = 128;
	// li r8,3
	ctx.r8.s64 = 3;
	// addi r10,r11,511
	ctx.r10.s64 = ctx.r11.s64 + 511;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_823291E8:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x823291e8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823291E8;
	// li r9,128
	ctx.r9.s64 = 128;
	// li r8,51
	ctx.r8.s64 = 51;
	// addi r10,r11,639
	ctx.r10.s64 = ctx.r11.s64 + 639;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329200:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329200
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329200;
	// li r9,128
	ctx.r9.s64 = 128;
	// li r8,67
	ctx.r8.s64 = 67;
	// addi r10,r11,767
	ctx.r10.s64 = ctx.r11.s64 + 767;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329218:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329218
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329218;
	// li r9,64
	ctx.r9.s64 = 64;
	// li r8,84
	ctx.r8.s64 = 84;
	// addi r10,r11,895
	ctx.r10.s64 = ctx.r11.s64 + 895;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329230:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329230
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329230;
	// li r9,32
	ctx.r9.s64 = 32;
	// li r8,101
	ctx.r8.s64 = 101;
	// addi r10,r11,959
	ctx.r10.s64 = ctx.r11.s64 + 959;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329248:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329248
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329248;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,118
	ctx.r8.s64 = 118;
	// addi r10,r11,991
	ctx.r10.s64 = ctx.r11.s64 + 991;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329260:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329260
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329260;
	// li r10,-121
	ctx.r10.s64 = -121;
	// li r9,-121
	ctx.r9.s64 = -121;
	// stb r10,1008(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1008, ctx.r10.u8);
	// li r8,-121
	ctx.r8.s64 = -121;
	// stb r10,1011(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1011, ctx.r10.u8);
	// stb r10,1014(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1014, ctx.r10.u8);
	// li r10,-104
	ctx.r10.s64 = -104;
	// stb r9,1009(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1009, ctx.r9.u8);
	// stb r8,1010(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1010, ctx.r8.u8);
	// stb r9,1012(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1012, ctx.r9.u8);
	// stb r8,1013(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1013, ctx.r8.u8);
	// li r8,-104
	ctx.r8.s64 = -104;
	// stb r9,1015(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1015, ctx.r9.u8);
	// li r9,-104
	ctx.r9.s64 = -104;
	// stb r10,1017(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1017, ctx.r10.u8);
	// li r10,-87
	ctx.r10.s64 = -87;
	// stb r8,1016(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1016, ctx.r8.u8);
	// stb r9,1018(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1018, ctx.r9.u8);
	// li r9,-87
	ctx.r9.s64 = -87;
	// stb r8,1019(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1019, ctx.r8.u8);
	// li r8,-71
	ctx.r8.s64 = -71;
	// stb r10,1020(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1020, ctx.r10.u8);
	// li r10,-71
	ctx.r10.s64 = -71;
	// stb r9,1021(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1021, ctx.r9.u8);
	// stb r8,1022(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1022, ctx.r8.u8);
	// stb r10,1023(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1023, ctx.r10.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823292D4"))) PPC_WEAK_FUNC(sub_823292D4);
PPC_FUNC_IMPL(__imp__sub_823292D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823292D8"))) PPC_WEAK_FUNC(sub_823292D8);
PPC_FUNC_IMPL(__imp__sub_823292D8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r9,256
	ctx.r9.s64 = 256;
	// addi r11,r11,15128
	ctx.r11.s64 = ctx.r11.s64 + 15128;
	// li r8,2
	ctx.r8.s64 = 2;
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_823292F0:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x823292f0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823292F0;
	// li r9,256
	ctx.r9.s64 = 256;
	// li r8,18
	ctx.r8.s64 = 18;
	// addi r10,r11,255
	ctx.r10.s64 = ctx.r11.s64 + 255;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329308:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329308
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329308;
	// li r9,256
	ctx.r9.s64 = 256;
	// li r8,34
	ctx.r8.s64 = 34;
	// addi r10,r11,511
	ctx.r10.s64 = ctx.r11.s64 + 511;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329320:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329320
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329320;
	// li r9,128
	ctx.r9.s64 = 128;
	// li r8,51
	ctx.r8.s64 = 51;
	// addi r10,r11,767
	ctx.r10.s64 = ctx.r11.s64 + 767;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329338:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329338
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329338;
	// li r9,64
	ctx.r9.s64 = 64;
	// li r8,68
	ctx.r8.s64 = 68;
	// addi r10,r11,895
	ctx.r10.s64 = ctx.r11.s64 + 895;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329350:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329350
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329350;
	// li r9,32
	ctx.r9.s64 = 32;
	// li r8,85
	ctx.r8.s64 = 85;
	// addi r10,r11,959
	ctx.r10.s64 = ctx.r11.s64 + 959;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329368:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329368
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329368;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,102
	ctx.r8.s64 = 102;
	// addi r10,r11,991
	ctx.r10.s64 = ctx.r11.s64 + 991;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329380:
	// stbu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	PPC_STORE_U8(ea, ctx.r8.u8);
	ctx.r10.u32 = ea;
	// bdnz 0x82329380
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329380;
	// li r10,119
	ctx.r10.s64 = 119;
	// li r9,119
	ctx.r9.s64 = 119;
	// stb r10,1008(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1008, ctx.r10.u8);
	// li r8,119
	ctx.r8.s64 = 119;
	// stb r10,1011(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1011, ctx.r10.u8);
	// stb r10,1014(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1014, ctx.r10.u8);
	// li r10,-120
	ctx.r10.s64 = -120;
	// stb r9,1009(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1009, ctx.r9.u8);
	// stb r8,1010(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1010, ctx.r8.u8);
	// stb r9,1012(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1012, ctx.r9.u8);
	// stb r8,1013(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1013, ctx.r8.u8);
	// li r8,-120
	ctx.r8.s64 = -120;
	// stb r9,1015(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1015, ctx.r9.u8);
	// li r9,-120
	ctx.r9.s64 = -120;
	// stb r10,1017(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1017, ctx.r10.u8);
	// li r10,-103
	ctx.r10.s64 = -103;
	// stb r8,1016(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1016, ctx.r8.u8);
	// stb r9,1018(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1018, ctx.r9.u8);
	// li r9,-103
	ctx.r9.s64 = -103;
	// stb r8,1019(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1019, ctx.r8.u8);
	// li r8,-86
	ctx.r8.s64 = -86;
	// stb r10,1020(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1020, ctx.r10.u8);
	// li r10,-70
	ctx.r10.s64 = -70;
	// stb r9,1021(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1021, ctx.r9.u8);
	// stb r8,1022(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1022, ctx.r8.u8);
	// stb r10,1023(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1023, ctx.r10.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823293F4"))) PPC_WEAK_FUNC(sub_823293F4);
PPC_FUNC_IMPL(__imp__sub_823293F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823293F8"))) PPC_WEAK_FUNC(sub_823293F8);
PPC_FUNC_IMPL(__imp__sub_823293F8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x82329400;
	__restfpr_27(ctx, base);
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r11,r11,17176
	ctx.r11.s64 = ctx.r11.s64 + 17176;
	// lis r10,6
	ctx.r10.s64 = 393216;
	// lis r5,8
	ctx.r5.s64 = 524288;
	// ori r10,r10,16448
	ctx.r10.u64 = ctx.r10.u64 | 16448;
	// ori r5,r5,514
	ctx.r5.u64 = ctx.r5.u64 | 514;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// lis r4,8
	ctx.r4.s64 = 524288;
	// stw r9,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r9.u32);
	// lis r3,8
	ctx.r3.s64 = 524288;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// ori r4,r4,265
	ctx.r4.u64 = ctx.r4.u64 | 265;
	// stw r9,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r9.u32);
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// ori r3,r3,1024
	ctx.r3.u64 = ctx.r3.u64 | 1024;
	// stw r10,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r10.u32);
	// lis r31,8
	ctx.r31.s64 = 524288;
	// stw r10,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r10.u32);
	// lis r9,7
	ctx.r9.s64 = 458752;
	// stw r10,28(r11)
	PPC_STORE_U32(ctx.r11.u32 + 28, ctx.r10.u32);
	// stw r5,32(r11)
	PPC_STORE_U32(ctx.r11.u32 + 32, ctx.r5.u32);
	// ori r31,r31,264
	ctx.r31.u64 = ctx.r31.u64 | 264;
	// stw r5,36(r11)
	PPC_STORE_U32(ctx.r11.u32 + 36, ctx.r5.u32);
	// stw r4,40(r11)
	PPC_STORE_U32(ctx.r11.u32 + 40, ctx.r4.u32);
	// ori r9,r9,263
	ctx.r9.u64 = ctx.r9.u64 | 263;
	// stw r4,44(r11)
	PPC_STORE_U32(ctx.r11.u32 + 44, ctx.r4.u32);
	// stw r3,48(r11)
	PPC_STORE_U32(ctx.r11.u32 + 48, ctx.r3.u32);
	// mr r8,r11
	ctx.r8.u64 = ctx.r11.u64;
	// stw r3,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r3.u32);
	// stw r31,56(r11)
	PPC_STORE_U32(ctx.r11.u32 + 56, ctx.r31.u32);
	// lis r8,7
	ctx.r8.s64 = 458752;
	// stw r31,60(r11)
	PPC_STORE_U32(ctx.r11.u32 + 60, ctx.r31.u32);
	// stw r9,64(r11)
	PPC_STORE_U32(ctx.r11.u32 + 64, ctx.r9.u32);
	// ori r8,r8,262
	ctx.r8.u64 = ctx.r8.u64 | 262;
	// stw r9,68(r11)
	PPC_STORE_U32(ctx.r11.u32 + 68, ctx.r9.u32);
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// stw r9,72(r11)
	PPC_STORE_U32(ctx.r11.u32 + 72, ctx.r9.u32);
	// addi r10,r11,32
	ctx.r10.s64 = ctx.r11.s64 + 32;
	// stw r9,76(r11)
	PPC_STORE_U32(ctx.r11.u32 + 76, ctx.r9.u32);
	// lis r7,7
	ctx.r7.s64 = 458752;
	// stw r8,80(r11)
	PPC_STORE_U32(ctx.r11.u32 + 80, ctx.r8.u32);
	// addi r10,r11,40
	ctx.r10.s64 = ctx.r11.s64 + 40;
	// stw r8,84(r11)
	PPC_STORE_U32(ctx.r11.u32 + 84, ctx.r8.u32);
	// addi r10,r11,48
	ctx.r10.s64 = ctx.r11.s64 + 48;
	// stw r8,88(r11)
	PPC_STORE_U32(ctx.r11.u32 + 88, ctx.r8.u32);
	// ori r7,r7,513
	ctx.r7.u64 = ctx.r7.u64 | 513;
	// stw r8,92(r11)
	PPC_STORE_U32(ctx.r11.u32 + 92, ctx.r8.u32);
	// addi r10,r11,56
	ctx.r10.s64 = ctx.r11.s64 + 56;
	// addi r10,r11,64
	ctx.r10.s64 = ctx.r11.s64 + 64;
	// stw r7,96(r11)
	PPC_STORE_U32(ctx.r11.u32 + 96, ctx.r7.u32);
	// addi r10,r11,80
	ctx.r10.s64 = ctx.r11.s64 + 80;
	// stw r7,100(r11)
	PPC_STORE_U32(ctx.r11.u32 + 100, ctx.r7.u32);
	// lis r6,7
	ctx.r6.s64 = 458752;
	// stw r7,104(r11)
	PPC_STORE_U32(ctx.r11.u32 + 104, ctx.r7.u32);
	// lis r29,9
	ctx.r29.s64 = 589824;
	// lis r5,9
	ctx.r5.s64 = 589824;
	// lis r28,9
	ctx.r28.s64 = 589824;
	// lis r4,9
	ctx.r4.s64 = 589824;
	// lis r27,9
	ctx.r27.s64 = 589824;
	// lis r9,9
	ctx.r9.s64 = 589824;
	// addi r10,r11,96
	ctx.r10.s64 = ctx.r11.s64 + 96;
	// addi r10,r11,112
	ctx.r10.s64 = ctx.r11.s64 + 112;
	// ori r6,r6,261
	ctx.r6.u64 = ctx.r6.u64 | 261;
	// li r30,8
	ctx.r30.s64 = 8;
	// ori r29,r29,269
	ctx.r29.u64 = ctx.r29.u64 | 269;
	// ori r5,r5,1536
	ctx.r5.u64 = ctx.r5.u64 | 1536;
	// ori r28,r28,268
	ctx.r28.u64 = ctx.r28.u64 | 268;
	// ori r4,r4,267
	ctx.r4.u64 = ctx.r4.u64 | 267;
	// ori r27,r27,515
	ctx.r27.u64 = ctx.r27.u64 | 515;
	// lis r10,9
	ctx.r10.s64 = 589824;
	// ori r9,r9,1280
	ctx.r9.u64 = ctx.r9.u64 | 1280;
	// lis r8,9
	ctx.r8.s64 = 589824;
	// stw r7,108(r11)
	PPC_STORE_U32(ctx.r11.u32 + 108, ctx.r7.u32);
	// stw r6,112(r11)
	PPC_STORE_U32(ctx.r11.u32 + 112, ctx.r6.u32);
	// ori r10,r10,769
	ctx.r10.u64 = ctx.r10.u64 | 769;
	// stw r6,116(r11)
	PPC_STORE_U32(ctx.r11.u32 + 116, ctx.r6.u32);
	// ori r8,r8,266
	ctx.r8.u64 = ctx.r8.u64 | 266;
	// stw r6,120(r11)
	PPC_STORE_U32(ctx.r11.u32 + 120, ctx.r6.u32);
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
	// stw r6,124(r11)
	PPC_STORE_U32(ctx.r11.u32 + 124, ctx.r6.u32);
	// stw r9,152(r11)
	PPC_STORE_U32(ctx.r11.u32 + 152, ctx.r9.u32);
	// lis r9,6
	ctx.r9.s64 = 393216;
	// stw r10,148(r11)
	PPC_STORE_U32(ctx.r11.u32 + 148, ctx.r10.u32);
	// addi r10,r11,156
	ctx.r10.s64 = ctx.r11.s64 + 156;
	// stw r29,128(r11)
	PPC_STORE_U32(ctx.r11.u32 + 128, ctx.r29.u32);
	// ori r9,r9,768
	ctx.r9.u64 = ctx.r9.u64 | 768;
	// stw r5,132(r11)
	PPC_STORE_U32(ctx.r11.u32 + 132, ctx.r5.u32);
	// stw r28,136(r11)
	PPC_STORE_U32(ctx.r11.u32 + 136, ctx.r28.u32);
	// stw r4,140(r11)
	PPC_STORE_U32(ctx.r11.u32 + 140, ctx.r4.u32);
	// stw r27,144(r11)
	PPC_STORE_U32(ctx.r11.u32 + 144, ctx.r27.u32);
	// stw r8,156(r11)
	PPC_STORE_U32(ctx.r11.u32 + 156, ctx.r8.u32);
loc_82329570:
	// stwu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x82329570
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329570;
	// li r9,8
	ctx.r9.s64 = 8;
	// lis r8,6
	ctx.r8.s64 = 393216;
	// addi r10,r11,188
	ctx.r10.s64 = ctx.r11.s64 + 188;
	// ori r8,r8,260
	ctx.r8.u64 = ctx.r8.u64 | 260;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8232958C:
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x8232958c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8232958C;
	// li r9,8
	ctx.r9.s64 = 8;
	// lis r8,6
	ctx.r8.s64 = 393216;
	// addi r10,r11,220
	ctx.r10.s64 = ctx.r11.s64 + 220;
	// ori r8,r8,259
	ctx.r8.u64 = ctx.r8.u64 | 259;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_823295A8:
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x823295a8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823295A8;
	// li r9,16
	ctx.r9.s64 = 16;
	// lis r8,5
	ctx.r8.s64 = 327680;
	// addi r10,r11,252
	ctx.r10.s64 = ctx.r11.s64 + 252;
	// ori r8,r8,512
	ctx.r8.u64 = ctx.r8.u64 | 512;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_823295C4:
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x823295c4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823295C4;
	// li r9,16
	ctx.r9.s64 = 16;
	// lis r8,5
	ctx.r8.s64 = 327680;
	// addi r10,r11,316
	ctx.r10.s64 = ctx.r11.s64 + 316;
	// ori r8,r8,258
	ctx.r8.u64 = ctx.r8.u64 | 258;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_823295E0:
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x823295e0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823295E0;
	// li r10,32
	ctx.r10.s64 = 32;
	// lis r9,4
	ctx.r9.s64 = 262144;
	// addi r11,r11,380
	ctx.r11.s64 = ctx.r11.s64 + 380;
	// ori r9,r9,257
	ctx.r9.u64 = ctx.r9.u64 | 257;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_823295FC:
	// stwu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r11.u32 = ea;
	// bdnz 0x823295fc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_823295FC;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82329608"))) PPC_WEAK_FUNC(sub_82329608);
PPC_FUNC_IMPL(__imp__sub_82329608) {
	PPC_FUNC_PROLOGUE();
	// b 0x823293f8
	sub_823293F8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232960C"))) PPC_WEAK_FUNC(sub_8232960C);
PPC_FUNC_IMPL(__imp__sub_8232960C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329610"))) PPC_WEAK_FUNC(sub_82329610);
PPC_FUNC_IMPL(__imp__sub_82329610) {
	PPC_FUNC_PROLOGUE();
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// lis r10,-32171
	ctx.r10.s64 = -2108358656;
	// addi r11,r11,15128
	ctx.r11.s64 = ctx.r11.s64 + 15128;
	// lis r7,-32171
	ctx.r7.s64 = -2108358656;
	// addi r9,r11,4864
	ctx.r9.s64 = ctx.r11.s64 + 4864;
	// lis r6,-32171
	ctx.r6.s64 = -2108358656;
	// stw r9,11360(r10)
	PPC_STORE_U32(ctx.r10.u32 + 11360, ctx.r9.u32);
	// addi r8,r11,3264
	ctx.r8.s64 = ctx.r11.s64 + 3264;
	// addi r9,r11,3200
	ctx.r9.s64 = ctx.r11.s64 + 3200;
	// lis r5,-32171
	ctx.r5.s64 = -2108358656;
	// stw r8,11328(r7)
	PPC_STORE_U32(ctx.r7.u32 + 11328, ctx.r8.u32);
	// lis r4,-32171
	ctx.r4.s64 = -2108358656;
	// stw r9,11332(r6)
	PPC_STORE_U32(ctx.r6.u32 + 11332, ctx.r9.u32);
	// addi r7,r11,5376
	ctx.r7.s64 = ctx.r11.s64 + 5376;
	// addi r8,r11,2560
	ctx.r8.s64 = ctx.r11.s64 + 2560;
	// lis r3,-32171
	ctx.r3.s64 = -2108358656;
	// lis r6,-32171
	ctx.r6.s64 = -2108358656;
	// stw r7,11336(r5)
	PPC_STORE_U32(ctx.r5.u32 + 11336, ctx.r7.u32);
	// stw r8,11320(r4)
	PPC_STORE_U32(ctx.r4.u32 + 11320, ctx.r8.u32);
	// addi r9,r11,3456
	ctx.r9.s64 = ctx.r11.s64 + 3456;
	// addi r8,r11,2816
	ctx.r8.s64 = ctx.r11.s64 + 2816;
	// lis r10,-32171
	ctx.r10.s64 = -2108358656;
	// lis r5,-32171
	ctx.r5.s64 = -2108358656;
	// stw r9,11312(r3)
	PPC_STORE_U32(ctx.r3.u32 + 11312, ctx.r9.u32);
	// stw r8,11340(r6)
	PPC_STORE_U32(ctx.r6.u32 + 11340, ctx.r8.u32);
	// addi r7,r11,3648
	ctx.r7.s64 = ctx.r11.s64 + 3648;
	// addi r9,r11,3392
	ctx.r9.s64 = ctx.r11.s64 + 3392;
	// lis r4,-32171
	ctx.r4.s64 = -2108358656;
	// lis r3,-32171
	ctx.r3.s64 = -2108358656;
	// stw r7,11316(r10)
	PPC_STORE_U32(ctx.r10.u32 + 11316, ctx.r7.u32);
	// lis r6,-32171
	ctx.r6.s64 = -2108358656;
	// stw r9,11308(r5)
	PPC_STORE_U32(ctx.r5.u32 + 11308, ctx.r9.u32);
	// addi r7,r11,2944
	ctx.r7.s64 = ctx.r11.s64 + 2944;
	// addi r8,r11,1024
	ctx.r8.s64 = ctx.r11.s64 + 1024;
	// addi r9,r11,3712
	ctx.r9.s64 = ctx.r11.s64 + 3712;
	// stw r7,11364(r4)
	PPC_STORE_U32(ctx.r4.u32 + 11364, ctx.r7.u32);
	// lis r5,-32253
	ctx.r5.s64 = -2113732608;
	// stw r8,11352(r3)
	PPC_STORE_U32(ctx.r3.u32 + 11352, ctx.r8.u32);
	// stw r9,11356(r6)
	PPC_STORE_U32(ctx.r6.u32 + 11356, ctx.r9.u32);
	// lis r4,-32171
	ctx.r4.s64 = -2108358656;
	// addi r10,r5,-508
	ctx.r10.s64 = ctx.r5.s64 + -508;
	// lis r3,-32171
	ctx.r3.s64 = -2108358656;
	// lis r6,-32171
	ctx.r6.s64 = -2108358656;
	// lis r5,-32171
	ctx.r5.s64 = -2108358656;
	// lis r31,-32171
	ctx.r31.s64 = -2108358656;
	// addi r7,r11,3520
	ctx.r7.s64 = ctx.r11.s64 + 3520;
	// addi r8,r11,3840
	ctx.r8.s64 = ctx.r11.s64 + 3840;
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// stw r7,11324(r4)
	PPC_STORE_U32(ctx.r4.u32 + 11324, ctx.r7.u32);
	// stw r8,11372(r3)
	PPC_STORE_U32(ctx.r3.u32 + 11372, ctx.r8.u32);
	// lis r4,-32171
	ctx.r4.s64 = -2108358656;
	// stw r9,11304(r6)
	PPC_STORE_U32(ctx.r6.u32 + 11304, ctx.r9.u32);
	// lis r3,-32171
	ctx.r3.s64 = -2108358656;
	// stw r11,11348(r5)
	PPC_STORE_U32(ctx.r5.u32 + 11348, ctx.r11.u32);
	// lis r6,-32171
	ctx.r6.s64 = -2108358656;
	// stw r10,11376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 11376, ctx.r10.u32);
	// lis r5,-32171
	ctx.r5.s64 = -2108358656;
	// lis r31,-32171
	ctx.r31.s64 = -2108358656;
	// addi r8,r10,64
	ctx.r8.s64 = ctx.r10.s64 + 64;
	// addi r7,r10,96
	ctx.r7.s64 = ctx.r10.s64 + 96;
	// addi r9,r10,128
	ctx.r9.s64 = ctx.r10.s64 + 128;
	// stw r8,11300(r4)
	PPC_STORE_U32(ctx.r4.u32 + 11300, ctx.r8.u32);
	// addi r10,r10,160
	ctx.r10.s64 = ctx.r10.s64 + 160;
	// stw r7,11380(r3)
	PPC_STORE_U32(ctx.r3.u32 + 11380, ctx.r7.u32);
	// addi r11,r11,2048
	ctx.r11.s64 = ctx.r11.s64 + 2048;
	// stw r9,11344(r6)
	PPC_STORE_U32(ctx.r6.u32 + 11344, ctx.r9.u32);
	// stw r10,11368(r5)
	PPC_STORE_U32(ctx.r5.u32 + 11368, ctx.r10.u32);
	// stw r11,11384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 11384, ctx.r11.u32);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232972C"))) PPC_WEAK_FUNC(sub_8232972C);
PPC_FUNC_IMPL(__imp__sub_8232972C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329730"))) PPC_WEAK_FUNC(sub_82329730);
PPC_FUNC_IMPL(__imp__sub_82329730) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// addi r31,r3,-512
	ctx.r31.s64 = ctx.r3.s64 + -512;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r4,r10,17176
	ctx.r4.s64 = ctx.r10.s64 + 17176;
	// stw r31,11384(r11)
	PPC_STORE_U32(ctx.r11.u32 + 11384, ctx.r31.u32);
	// li r5,128
	ctx.r5.s64 = 128;
	// bl 0x82329ab8
	ctx.lr = 0x82329764;
	sub_82329AB8(ctx, base);
	// lis r9,-32171
	ctx.r9.s64 = -2108358656;
	// addi r30,r31,-16
	ctx.r30.s64 = ctx.r31.s64 + -16;
	// lis r8,-32253
	ctx.r8.s64 = -2113732608;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r31,r8,-508
	ctx.r31.s64 = ctx.r8.s64 + -508;
	// stw r30,11368(r9)
	PPC_STORE_U32(ctx.r9.u32 + 11368, ctx.r30.u32);
	// li r5,4
	ctx.r5.s64 = 4;
	// addi r4,r31,160
	ctx.r4.s64 = ctx.r31.s64 + 160;
	// bl 0x82329ab8
	ctx.lr = 0x82329788;
	sub_82329AB8(ctx, base);
	// lis r7,-32171
	ctx.r7.s64 = -2108358656;
	// addi r30,r30,-32
	ctx.r30.s64 = ctx.r30.s64 + -32;
	// addi r4,r31,128
	ctx.r4.s64 = ctx.r31.s64 + 128;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// li r5,8
	ctx.r5.s64 = 8;
	// stw r30,11344(r7)
	PPC_STORE_U32(ctx.r7.u32 + 11344, ctx.r30.u32);
	// bl 0x82329ab8
	ctx.lr = 0x823297A4;
	sub_82329AB8(ctx, base);
	// lis r6,-32171
	ctx.r6.s64 = -2108358656;
	// addi r30,r30,-32
	ctx.r30.s64 = ctx.r30.s64 + -32;
	// addi r4,r31,96
	ctx.r4.s64 = ctx.r31.s64 + 96;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// li r5,8
	ctx.r5.s64 = 8;
	// stw r30,11380(r6)
	PPC_STORE_U32(ctx.r6.u32 + 11380, ctx.r30.u32);
	// bl 0x82329ab8
	ctx.lr = 0x823297C0;
	sub_82329AB8(ctx, base);
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// addi r30,r30,-32
	ctx.r30.s64 = ctx.r30.s64 + -32;
	// addi r4,r31,64
	ctx.r4.s64 = ctx.r31.s64 + 64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// li r5,8
	ctx.r5.s64 = 8;
	// stw r30,11300(r11)
	PPC_STORE_U32(ctx.r11.u32 + 11300, ctx.r30.u32);
	// bl 0x82329ab8
	ctx.lr = 0x823297DC;
	sub_82329AB8(ctx, base);
	// lis r10,-32171
	ctx.r10.s64 = -2108358656;
	// addi r30,r30,-32
	ctx.r30.s64 = ctx.r30.s64 + -32;
	// addi r4,r31,32
	ctx.r4.s64 = ctx.r31.s64 + 32;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// li r5,8
	ctx.r5.s64 = 8;
	// stw r30,11304(r10)
	PPC_STORE_U32(ctx.r10.u32 + 11304, ctx.r30.u32);
	// bl 0x82329ab8
	ctx.lr = 0x823297F8;
	sub_82329AB8(ctx, base);
	// lis r9,-32171
	ctx.r9.s64 = -2108358656;
	// addi r30,r30,-32
	ctx.r30.s64 = ctx.r30.s64 + -32;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// li r5,8
	ctx.r5.s64 = 8;
	// stw r30,11376(r9)
	PPC_STORE_U32(ctx.r9.u32 + 11376, ctx.r30.u32);
	// bl 0x82329ab8
	ctx.lr = 0x82329814;
	sub_82329AB8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329830"))) PPC_WEAK_FUNC(sub_82329830);
PPC_FUNC_IMPL(__imp__sub_82329830) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// addi r31,r3,-128
	ctx.r31.s64 = ctx.r3.s64 + -128;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r30,r10,18840
	ctx.r30.s64 = ctx.r10.s64 + 18840;
	// stw r31,11324(r11)
	PPC_STORE_U32(ctx.r11.u32 + 11324, ctx.r31.u32);
	// li r5,32
	ctx.r5.s64 = 32;
	// addi r4,r30,-192
	ctx.r4.s64 = ctx.r30.s64 + -192;
	// bl 0x82329ab8
	ctx.lr = 0x82329868;
	sub_82329AB8(ctx, base);
	// lis r9,-32171
	ctx.r9.s64 = -2108358656;
	// addi r31,r31,-128
	ctx.r31.s64 = ctx.r31.s64 + -128;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r5,32
	ctx.r5.s64 = 32;
	// stw r31,11356(r9)
	PPC_STORE_U32(ctx.r9.u32 + 11356, ctx.r31.u32);
	// bl 0x82329ab8
	ctx.lr = 0x82329884;
	sub_82329AB8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823298A0"))) PPC_WEAK_FUNC(sub_823298A0);
PPC_FUNC_IMPL(__imp__sub_823298A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// addi r31,r3,-256
	ctx.r31.s64 = ctx.r3.s64 + -256;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r30,r10,18520
	ctx.r30.s64 = ctx.r10.s64 + 18520;
	// stw r31,11364(r11)
	PPC_STORE_U32(ctx.r11.u32 + 11364, ctx.r31.u32);
	// li r5,64
	ctx.r5.s64 = 64;
	// addi r4,r30,-448
	ctx.r4.s64 = ctx.r30.s64 + -448;
	// bl 0x82329ab8
	ctx.lr = 0x823298D8;
	sub_82329AB8(ctx, base);
	// lis r9,-32171
	ctx.r9.s64 = -2108358656;
	// addi r31,r31,-64
	ctx.r31.s64 = ctx.r31.s64 + -64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// stw r31,11308(r9)
	PPC_STORE_U32(ctx.r9.u32 + 11308, ctx.r31.u32);
	// bl 0x82329ab8
	ctx.lr = 0x823298F4;
	sub_82329AB8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329910"))) PPC_WEAK_FUNC(sub_82329910);
PPC_FUNC_IMPL(__imp__sub_82329910) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// addi r31,r3,-64
	ctx.r31.s64 = ctx.r3.s64 + -64;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r30,r10,17944
	ctx.r30.s64 = ctx.r10.s64 + 17944;
	// stw r31,11312(r11)
	PPC_STORE_U32(ctx.r11.u32 + 11312, ctx.r31.u32);
	// li r5,16
	ctx.r5.s64 = 16;
	// addi r4,r30,640
	ctx.r4.s64 = ctx.r30.s64 + 640;
	// bl 0x82329ab8
	ctx.lr = 0x82329948;
	sub_82329AB8(ctx, base);
	// lis r9,-32171
	ctx.r9.s64 = -2108358656;
	// addi r31,r31,-128
	ctx.r31.s64 = ctx.r31.s64 + -128;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r5,32
	ctx.r5.s64 = 32;
	// stw r31,11340(r9)
	PPC_STORE_U32(ctx.r9.u32 + 11340, ctx.r31.u32);
	// bl 0x82329ab8
	ctx.lr = 0x82329964;
	sub_82329AB8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329980"))) PPC_WEAK_FUNC(sub_82329980);
PPC_FUNC_IMPL(__imp__sub_82329980) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x82327f58
	ctx.lr = 0x82329990;
	sub_82327F58(ctx, base);
	// bl 0x82328200
	ctx.lr = 0x82329994;
	sub_82328200(ctx, base);
	// bl 0x823284f0
	ctx.lr = 0x82329998;
	sub_823284F0(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823299A8"))) PPC_WEAK_FUNC(sub_823299A8);
PPC_FUNC_IMPL(__imp__sub_823299A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x82328800
	ctx.lr = 0x823299B8;
	sub_82328800(ctx, base);
	// bl 0x82328878
	ctx.lr = 0x823299BC;
	sub_82328878(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823299CC"))) PPC_WEAK_FUNC(sub_823299CC);
PPC_FUNC_IMPL(__imp__sub_823299CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823299D0"))) PPC_WEAK_FUNC(sub_823299D0);
PPC_FUNC_IMPL(__imp__sub_823299D0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// addi r3,r11,16152
	ctx.r3.s64 = ctx.r11.s64 + 16152;
	// bl 0x82328b18
	ctx.lr = 0x823299E8;
	sub_82328B18(ctx, base);
	// bl 0x82328cb8
	ctx.lr = 0x823299EC;
	sub_82328CB8(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823299FC"))) PPC_WEAK_FUNC(sub_823299FC);
PPC_FUNC_IMPL(__imp__sub_823299FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329A00"))) PPC_WEAK_FUNC(sub_82329A00);
PPC_FUNC_IMPL(__imp__sub_82329A00) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x82328fd8
	ctx.lr = 0x82329A10;
	sub_82328FD8(ctx, base);
	// bl 0x823290c8
	ctx.lr = 0x82329A14;
	sub_823290C8(ctx, base);
	// bl 0x823291a0
	ctx.lr = 0x82329A18;
	sub_823291A0(ctx, base);
	// bl 0x823292d8
	ctx.lr = 0x82329A1C;
	sub_823292D8(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329A2C"))) PPC_WEAK_FUNC(sub_82329A2C);
PPC_FUNC_IMPL(__imp__sub_82329A2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329A30"))) PPC_WEAK_FUNC(sub_82329A30);
PPC_FUNC_IMPL(__imp__sub_82329A30) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r3,1456
	ctx.r3.s64 = ctx.r3.s64 + 1456;
	// bl 0x82329730
	ctx.lr = 0x82329A44;
	sub_82329730(ctx, base);
	// bl 0x82329830
	ctx.lr = 0x82329A48;
	sub_82329830(ctx, base);
	// bl 0x823298a0
	ctx.lr = 0x82329A4C;
	sub_823298A0(ctx, base);
	// bl 0x82329910
	ctx.lr = 0x82329A50;
	sub_82329910(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329A60"))) PPC_WEAK_FUNC(sub_82329A60);
PPC_FUNC_IMPL(__imp__sub_82329A60) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x82329980
	ctx.lr = 0x82329A78;
	sub_82329980(ctx, base);
	// bl 0x823299a8
	ctx.lr = 0x82329A7C;
	sub_823299A8(ctx, base);
	// bl 0x82328958
	ctx.lr = 0x82329A80;
	sub_82328958(ctx, base);
	// bl 0x823299d0
	ctx.lr = 0x82329A84;
	sub_823299D0(ctx, base);
	// bl 0x82329a00
	ctx.lr = 0x82329A88;
	sub_82329A00(ctx, base);
	// bl 0x82329608
	ctx.lr = 0x82329A8C;
	sub_82329608(ctx, base);
	// bl 0x82329610
	ctx.lr = 0x82329A90;
	sub_82329610(ctx, base);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x82329aa0
	if (ctx.cr6.eq) goto loc_82329AA0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82329a30
	ctx.lr = 0x82329AA0;
	sub_82329A30(ctx, base);
loc_82329AA0:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329AB4"))) PPC_WEAK_FUNC(sub_82329AB4);
PPC_FUNC_IMPL(__imp__sub_82329AB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329AB8"))) PPC_WEAK_FUNC(sub_82329AB8);
PPC_FUNC_IMPL(__imp__sub_82329AB8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// clrlwi r9,r5,28
	ctx.r9.u64 = ctx.r5.u32 & 0xF;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82329ae4
	if (ctx.cr6.eq) goto loc_82329AE4;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329AD0:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82329ad0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329AD0;
loc_82329AE4:
	// rlwinm r9,r5,28,4,31
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 28) & 0xFFFFFFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82329AF8:
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwzu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r8,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r8.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r7,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r7.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// stw r6,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r6.u32);
	// stw r9,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r9.u32);
	// stw r8,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r8.u32);
	// stw r7,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, ctx.r7.u32);
	// lwzu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r8,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r8.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r7,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r7.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r6,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r6.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// stw r9,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, ctx.r9.u32);
	// stw r8,24(r10)
	PPC_STORE_U32(ctx.r10.u32 + 24, ctx.r8.u32);
	// stw r7,28(r10)
	PPC_STORE_U32(ctx.r10.u32 + 28, ctx.r7.u32);
	// stw r6,32(r10)
	PPC_STORE_U32(ctx.r10.u32 + 32, ctx.r6.u32);
	// lwzu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r8,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r8.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r7,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r7.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r6,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r6.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// stw r9,36(r10)
	PPC_STORE_U32(ctx.r10.u32 + 36, ctx.r9.u32);
	// stw r8,40(r10)
	PPC_STORE_U32(ctx.r10.u32 + 40, ctx.r8.u32);
	// stw r7,44(r10)
	PPC_STORE_U32(ctx.r10.u32 + 44, ctx.r7.u32);
	// stw r6,48(r10)
	PPC_STORE_U32(ctx.r10.u32 + 48, ctx.r6.u32);
	// lwzu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r8,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r8.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r7,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r7.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// lwzu r6,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r6.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,52(r10)
	PPC_STORE_U32(ctx.r10.u32 + 52, ctx.r9.u32);
	// stw r8,56(r10)
	PPC_STORE_U32(ctx.r10.u32 + 56, ctx.r8.u32);
	// stw r7,60(r10)
	PPC_STORE_U32(ctx.r10.u32 + 60, ctx.r7.u32);
	// stwu r6,64(r10)
	ea = 64 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x82329af8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329AF8;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329B84"))) PPC_WEAK_FUNC(sub_82329B84);
PPC_FUNC_IMPL(__imp__sub_82329B84) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329B88"))) PPC_WEAK_FUNC(sub_82329B88);
PPC_FUNC_IMPL(__imp__sub_82329B88) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// rlwinm r11,r5,2,0,29
	ctx.r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r10,r5,28
	ctx.r10.u64 = ctx.r5.u32 & 0xF;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82329ba8
	if (ctx.cr6.eq) goto loc_82329BA8;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_82329BA0:
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// bdnz 0x82329ba0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329BA0;
loc_82329BA8:
	// rlwinm r10,r5,28,4,31
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 28) & 0xFFFFFFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_82329BB8:
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// stwu r4,-4(r11)
	ea = -4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r11.u32 = ea;
	// bdnz 0x82329bb8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82329BB8;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329C00"))) PPC_WEAK_FUNC(sub_82329C00);
PPC_FUNC_IMPL(__imp__sub_82329C00) {
	PPC_FUNC_PROLOGUE();
	// b 0x82324938
	sub_82324938(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82329C04"))) PPC_WEAK_FUNC(sub_82329C04);
PPC_FUNC_IMPL(__imp__sub_82329C04) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329C08"))) PPC_WEAK_FUNC(sub_82329C08);
PPC_FUNC_IMPL(__imp__sub_82329C08) {
	PPC_FUNC_PROLOGUE();
	// b 0x82324a58
	sub_82324A58(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82329C0C"))) PPC_WEAK_FUNC(sub_82329C0C);
PPC_FUNC_IMPL(__imp__sub_82329C0C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329C10"))) PPC_WEAK_FUNC(sub_82329C10);
PPC_FUNC_IMPL(__imp__sub_82329C10) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x82324b00
	ctx.lr = 0x82329C28;
	sub_82324B00(ctx, base);
	// stw r3,5416(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5416, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82329c3c
	if (ctx.cr6.eq) goto loc_82329C3C;
	// lwz r4,4804(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4804);
	// bl 0x82324648
	ctx.lr = 0x82329C3C;
	sub_82324648(ctx, base);
loc_82329C3C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329C50"))) PPC_WEAK_FUNC(sub_82329C50);
PPC_FUNC_IMPL(__imp__sub_82329C50) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,5416(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5416);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82329c7c
	if (ctx.cr6.eq) goto loc_82329C7C;
	// bl 0x823247f0
	ctx.lr = 0x82329C74;
	sub_823247F0(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,5416(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5416, ctx.r11.u32);
loc_82329C7C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329C90"))) PPC_WEAK_FUNC(sub_82329C90);
PPC_FUNC_IMPL(__imp__sub_82329C90) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e450
	ctx.lr = 0x82329C98;
	__restfpr_22(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// li r30,-1
	ctx.r30.s64 = -1;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
	// stw r11,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r11.u32);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// stw r11,8(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8, ctx.r11.u32);
	// addi r25,r3,5288
	ctx.r25.s64 = ctx.r3.s64 + 5288;
	// stw r30,12(r28)
	PPC_STORE_U32(ctx.r28.u32 + 12, ctx.r30.u32);
	// lis r5,32767
	ctx.r5.s64 = 2147418112;
	// stw r30,16(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16, ctx.r30.u32);
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// stw r11,20(r28)
	PPC_STORE_U32(ctx.r28.u32 + 20, ctx.r11.u32);
	// ori r5,r5,65535
	ctx.r5.u64 = ctx.r5.u64 | 65535;
	// lwz r10,4816(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4816);
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r11,5408(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5408, ctx.r11.u32);
	// stw r10,5316(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5316, ctx.r10.u32);
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// lwz r9,0(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// lwz r23,4968(r31)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4968);
	// lwz r22,4972(r31)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4972);
	// lwz r8,24(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x82329D04;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r7,5288(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5288);
	// rlwinm r11,r7,0,0,29
	ctx.r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFC;
	// subf r6,r11,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r11.s64;
	// rlwinm r10,r6,3,0,28
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwzu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// addi r8,r11,4
	ctx.r8.s64 = ctx.r11.s64 + 4;
	// slw r11,r5,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r10.u8 & 0x3F));
	// beq cr6,0x82329d3c
	if (ctx.cr6.eq) goto loc_82329D3C;
	// subfic r7,r10,32
	ctx.xer.ca = ctx.r10.u32 <= 32;
	ctx.r7.s64 = 32 - ctx.r10.s64;
	// srw r6,r9,r7
	ctx.r6.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r7.u8 & 0x3F));
	// or r11,r6,r11
	ctx.r11.u64 = ctx.r6.u64 | ctx.r11.u64;
	// slw r9,r9,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
loc_82329D3C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// lwz r7,4836(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4836);
	// lwz r27,0(r8)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r29,r8,4
	ctx.r29.s64 = ctx.r8.s64 + 4;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r30,5212(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5212, ctx.r30.u32);
	// cmpwi cr6,r10,27
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 27, ctx.xer);
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
	// stw r11,5208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5208, ctx.r11.u32);
	// addi r6,r7,-1
	ctx.r6.s64 = ctx.r7.s64 + -1;
	// stw r6,5204(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5204, ctx.r6.u32);
	// blt cr6,0x82329db0
	if (ctx.cr6.lt) goto loc_82329DB0;
	// addic. r30,r10,-27
	ctx.xer.ca = ctx.r10.u32 > 26;
	ctx.r30.s64 = ctx.r10.s64 + -27;
	ctx.cr0.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq 0x82329d98
	if (ctx.cr0.eq) goto loc_82329D98;
	// subfic r11,r30,5
	ctx.xer.ca = ctx.r30.u32 <= 5;
	ctx.r11.s64 = 5 - ctx.r30.s64;
	// slw r26,r27,r30
	ctx.r26.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r27.u32 << (ctx.r30.u8 & 0x3F));
	// srw r10,r27,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r27.u32 >> (ctx.r11.u8 & 0x3F));
	// or r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 | ctx.r9.u64;
	// rlwinm r8,r9,5,27,31
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0x1F;
	// stw r8,5128(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5128, ctx.r8.u32);
	// lwz r27,0(r29)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x82329dc0
	goto loc_82329DC0;
loc_82329D98:
	// rlwinm r11,r9,5,27,31
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0x1F;
	// mr r26,r27
	ctx.r26.u64 = ctx.r27.u64;
	// stw r11,5128(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5128, ctx.r11.u32);
	// lwz r27,0(r29)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x82329dc0
	goto loc_82329DC0;
loc_82329DB0:
	// rlwinm r11,r9,5,27,31
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0x1F;
	// addi r30,r10,5
	ctx.r30.s64 = ctx.r10.s64 + 5;
	// stw r11,5128(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5128, ctx.r11.u32);
	// rlwinm r26,r9,5,0,26
	ctx.r26.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
loc_82329DC0:
	// addi r3,r31,5132
	ctx.r3.s64 = ctx.r31.s64 + 5132;
	// bl 0x823897e0
	ctx.lr = 0x82329DC8;
	sub_823897E0(ctx, base);
	// addi r3,r31,5168
	ctx.r3.s64 = ctx.r31.s64 + 5168;
	// bl 0x823897e0
	ctx.lr = 0x82329DD0;
	sub_823897E0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x823897f8
	ctx.lr = 0x82329DD8;
	sub_823897F8(ctx, base);
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// bge cr6,0x82329e38
	if (!ctx.cr6.lt) goto loc_82329E38;
	// lwz r9,0(r25)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	// addi r11,r30,7
	ctx.r11.s64 = ctx.r30.s64 + 7;
	// lwz r8,5292(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5292);
loc_82329DEC:
	// addi r30,r30,9
	ctx.r30.s64 = ctx.r30.s64 + 9;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// cmpwi cr6,r30,32
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 32, ctx.xer);
	// blt cr6,0x82329e14
	if (ctx.cr6.lt) goto loc_82329E14;
	// addi r30,r30,-32
	ctx.r30.s64 = ctx.r30.s64 + -32;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r26,r27,r30
	ctx.r26.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r27.u32 << (ctx.r30.u8 & 0x3F));
	// lwz r27,0(r29)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// b 0x82329e18
	goto loc_82329E18;
loc_82329E14:
	// rlwinm r26,r26,9,0,22
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 9) & 0xFFFFFE00;
loc_82329E18:
	// srawi r10,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 3;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + ctx.r29.u64;
	// addi r7,r10,-8
	ctx.r7.s64 = ctx.r10.s64 + -8;
	// cmpw cr6,r8,r7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, ctx.xer);
	// ble cr6,0x82329f04
	if (!ctx.cr6.gt) goto loc_82329F04;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// blt cr6,0x82329dec
	if (ctx.cr6.lt) goto loc_82329DEC;
loc_82329E38:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x82329e4c
	if (ctx.cr6.lt) goto loc_82329E4C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
loc_82329E4C:
	// clrlwi r10,r11,29
	ctx.r10.u64 = ctx.r11.u32 & 0x7;
	// lwz r9,0(r25)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// stw r10,5296(r31)
	PPC_STORE_U32(ctx.r31.u32 + 5296, ctx.r10.u32);
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// addi r8,r11,7
	ctx.r8.s64 = ctx.r11.s64 + 7;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// srawi r7,r8,3
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 3;
	// subf r11,r9,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r9.s64;
	// add r11,r11,r29
	ctx.r11.u64 = ctx.r11.u64 + ctx.r29.u64;
	// addi r4,r11,-8
	ctx.r4.s64 = ctx.r11.s64 + -8;
	// bl 0x8232b788
	ctx.lr = 0x82329E80;
	sub_8232B788(ctx, base);
	// lwz r6,0(r24)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// lwz r11,32(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 32);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82329E9C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// lwz r9,28(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x82329EB8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r8,5084(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5084);
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x82329ECC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r7,5204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5204);
	// lwz r6,5216(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5216);
	// cmpw cr6,r7,r6
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r6.s32, ctx.xer);
	// blt cr6,0x82329ee4
	if (ctx.cr6.lt) goto loc_82329EE4;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,20(r28)
	PPC_STORE_U32(ctx.r28.u32 + 20, ctx.r11.u32);
loc_82329EE4:
	// lwz r11,4968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4968);
	// subf r10,r23,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r23.s64;
	// stw r10,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r10.u32);
	// lwz r9,4972(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4972);
	// subf r8,r22,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r22.s64;
	// stw r8,4(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4, ctx.r8.u32);
	// lwz r7,5408(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5408);
	// stw r7,8(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8, ctx.r7.u32);
loc_82329F04:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8233e4a0
	__restgprlr_22(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82329F0C"))) PPC_WEAK_FUNC(sub_82329F0C);
PPC_FUNC_IMPL(__imp__sub_82329F0C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329F10"))) PPC_WEAK_FUNC(sub_82329F10);
PPC_FUNC_IMPL(__imp__sub_82329F10) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mulli r10,r4,5504
	ctx.r10.s64 = ctx.r4.s64 * 5504;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// add r31,r10,r3
	ctx.r31.u64 = ctx.r10.u64 + ctx.r3.u64;
	// li r5,5420
	ctx.r5.s64 = 5420;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// bl 0x8233e4e0
	ctx.lr = 0x82329F3C;
	sub_8233E4E0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82323e70
	ctx.lr = 0x82329F44;
	sub_82323E70(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82323fc8
	ctx.lr = 0x82329F4C;
	sub_82323FC8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329F64"))) PPC_WEAK_FUNC(sub_82329F64);
PPC_FUNC_IMPL(__imp__sub_82329F64) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329F68"))) PPC_WEAK_FUNC(sub_82329F68);
PPC_FUNC_IMPL(__imp__sub_82329F68) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,5412(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5412, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82329F7C"))) PPC_WEAK_FUNC(sub_82329F7C);
PPC_FUNC_IMPL(__imp__sub_82329F7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82329F80"))) PPC_WEAK_FUNC(sub_82329F80);
PPC_FUNC_IMPL(__imp__sub_82329F80) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x82329F88;
	__restfpr_28(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,5416(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5416);
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8232a020
	if (ctx.cr6.eq) goto loc_8232A020;
	// lwz r4,4804(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4804);
	// bl 0x82324648
	ctx.lr = 0x82329FA8;
	sub_82324648(ctx, base);
	// lwz r8,5216(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5216);
	// lis r11,-32198
	ctx.r11.s64 = -2110128128;
	// lwz r7,4832(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4832);
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r9,r11,-22276
	ctx.r9.s64 = ctx.r11.s64 + -22276;
	// stw r31,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r31.u32);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// addi r5,r31,4976
	ctx.r5.s64 = ctx.r31.s64 + 4976;
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// stw r7,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r7.u32);
	// lwz r3,5416(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5416);
	// bl 0x82324c20
	ctx.lr = 0x82329FE4;
	sub_82324C20(ctx, base);
	// lwz r6,116(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x8232a020
	if (ctx.cr6.eq) goto loc_8232A020;
	// lwz r11,4972(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4972);
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lhz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 128);
	// add r7,r11,r9
	ctx.r7.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwz r11,4968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4968);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r7,4972(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4972, ctx.r7.u32);
	// add r6,r11,r10
	ctx.r6.u64 = ctx.r11.u64 + ctx.r10.u64;
	// sth r8,5036(r31)
	PPC_STORE_U16(ctx.r31.u32 + 5036, ctx.r8.u16);
	// stw r6,4968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4968, ctx.r6.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_8232A020:
	// lwz r11,5412(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5412);
	// li r29,0
	ctx.r29.s64 = 0;
	// li r28,257
	ctx.r28.s64 = 257;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// sth r29,5036(r31)
	PPC_STORE_U16(ctx.r31.u32 + 5036, ctx.r29.u16);
	// bne cr6,0x8232a160
	if (!ctx.cr6.eq) goto loc_8232A160;
loc_8232A038:
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// li r4,-1
	ctx.r4.s64 = -1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82326fc8
	ctx.lr = 0x8232A048;
	sub_82326FC8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x8232a114
	if (!ctx.cr6.eq) goto loc_8232A114;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lis r5,32767
	ctx.r5.s64 = 2147418112;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// ori r5,r5,65535
	ctx.r5.u64 = ctx.r5.u64 | 65535;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8232A078;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r8,28(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x8232A094;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r7,4
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 4, ctx.xer);
	// blt cr6,0x8232a130
	if (ctx.cr6.lt) goto loc_8232A130;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x82327538
	ctx.lr = 0x8232A0A8;
	sub_82327538(ctx, base);
	// clrlwi r11,r3,31
	ctx.r11.u64 = ctx.r3.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x8232a130
	if (ctx.cr6.eq) goto loc_8232A130;
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lbz r10,3(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// ori r10,r10,256
	ctx.r10.u64 = ctx.r10.u64 | 256;
	// cmplw cr6,r28,r10
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r10.u32, ctx.xer);
	// bgt cr6,0x8232a124
	if (ctx.cr6.gt) goto loc_8232A124;
	// lhz r11,5036(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 5036);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// sth r9,5036(r31)
	PPC_STORE_U16(ctx.r31.u32 + 5036, ctx.r9.u16);
	// mr r28,r10
	ctx.r28.u64 = ctx.r10.u64;
	// bl 0x82329c90
	ctx.lr = 0x8232A0E8;
	sub_82329C90(ctx, base);
	// lwz r11,104(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// bne cr6,0x8232a130
	if (!ctx.cr6.eq) goto loc_8232A130;
	// lwz r11,5412(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5412);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x8232a038
	if (ctx.cr6.eq) goto loc_8232A038;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_8232A114:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x823251a0
	ctx.lr = 0x8232A11C;
	sub_823251A0(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_8232A124:
	// lwz r11,4968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4968);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4968, ctx.r11.u32);
loc_8232A130:
	// lwz r11,5204(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5204);
	// lwz r10,5216(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 5216);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// beq cr6,0x8232a14c
	if (ctx.cr6.eq) goto loc_8232A14C;
	// lwz r11,4968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4968);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4968, ctx.r11.u32);
loc_8232A14C:
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x8232a160
	if (ctx.cr6.eq) goto loc_8232A160;
	// lwz r11,4968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4968);
	// add r11,r11,r29
	ctx.r11.u64 = ctx.r11.u64 + ctx.r29.u64;
	// stw r11,4968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4968, ctx.r11.u32);
loc_8232A160:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232A16C"))) PPC_WEAK_FUNC(sub_8232A16C);
PPC_FUNC_IMPL(__imp__sub_8232A16C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232A170"))) PPC_WEAK_FUNC(sub_8232A170);
PPC_FUNC_IMPL(__imp__sub_8232A170) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x8232A178;
	__restfpr_28(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r10,-1
	ctx.r10.s64 = -1;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// stw r11,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r11.u32);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// stw r11,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, ctx.r11.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r10,12(r5)
	PPC_STORE_U32(ctx.r5.u32 + 12, ctx.r10.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stw r10,16(r5)
	PPC_STORE_U32(ctx.r5.u32 + 16, ctx.r10.u32);
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// stw r11,20(r5)
	PPC_STORE_U32(ctx.r5.u32 + 20, ctx.r11.u32);
	// bl 0x82325e60
	ctx.lr = 0x8232A1B4;
	sub_82325E60(ctx, base);
	// clrlwi r11,r3,31
	ctx.r11.u64 = ctx.r3.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x8232a28c
	if (ctx.cr6.eq) goto loc_8232A28C;
loc_8232A1C0:
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82329c90
	ctx.lr = 0x8232A1D0;
	sub_82329C90(ctx, base);
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add. r11,r10,r9
	ctx.r11.u64 = ctx.r10.u64 + ctx.r9.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// stw r6,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r6.u32);
	// add r5,r8,r10
	ctx.r5.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r5,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r5.u32);
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r28,116(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// add r4,r7,r9
	ctx.r4.u64 = ctx.r7.u64 + ctx.r9.u64;
	// stw r4,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r4.u32);
	// beq 0x8232a230
	if (ctx.cr0.eq) goto loc_8232A230;
	// lwz r10,4968(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4968);
	// lwz r9,4756(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4756);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r8,4968(r30)
	PPC_STORE_U32(ctx.r30.u32 + 4968, ctx.r8.u32);
	// beq cr6,0x8232a278
	if (ctx.cr6.eq) goto loc_8232A278;
	// lwz r10,4972(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4972);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,4972(r30)
	PPC_STORE_U32(ctx.r30.u32 + 4972, ctx.r11.u32);
loc_8232A230:
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// bne cr6,0x8232a278
	if (!ctx.cr6.eq) goto loc_8232A278;
	// lwz r11,5412(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 5412);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232a278
	if (!ctx.cr6.eq) goto loc_8232A278;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,-1
	ctx.r4.s64 = -1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82326fc8
	ctx.lr = 0x8232A254;
	sub_82326FC8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x8232a278
	if (!ctx.cr6.eq) goto loc_8232A278;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82325e60
	ctx.lr = 0x8232A268;
	sub_82325E60(ctx, base);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x8232a1c0
	if (ctx.cr6.eq) goto loc_8232A1C0;
loc_8232A278:
	// lwz r11,108(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r28,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r28.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
	// stw r10,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r10.u32);
loc_8232A28C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232A294"))) PPC_WEAK_FUNC(sub_8232A294);
PPC_FUNC_IMPL(__imp__sub_8232A294) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232A298"))) PPC_WEAK_FUNC(sub_8232A298);
PPC_FUNC_IMPL(__imp__sub_8232A298) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e458
	ctx.lr = 0x8232A2A0;
	__restfpr_24(ctx, base);
	// lis r8,-32183
	ctx.r8.s64 = -2109145088;
	// lis r11,-32199
	ctx.r11.s64 = -2110193664;
	// addi r24,r8,20760
	ctx.r24.s64 = ctx.r8.s64 + 20760;
	// addi r11,r11,-13792
	ctx.r11.s64 = ctx.r11.s64 + -13792;
	// lis r10,-32199
	ctx.r10.s64 = -2110193664;
	// lis r9,-32199
	ctx.r9.s64 = -2110193664;
	// stw r11,20760(r8)
	PPC_STORE_U32(ctx.r8.u32 + 20760, ctx.r11.u32);
	// lis r25,-32199
	ctx.r25.s64 = -2110193664;
	// addi r10,r10,-13048
	ctx.r10.s64 = ctx.r10.s64 + -13048;
	// addi r9,r9,-13456
	ctx.r9.s64 = ctx.r9.s64 + -13456;
	// addi r11,r25,-12640
	ctx.r11.s64 = ctx.r25.s64 + -12640;
	// stw r10,4(r24)
	PPC_STORE_U32(ctx.r24.u32 + 4, ctx.r10.u32);
	// lis r26,-32199
	ctx.r26.s64 = -2110193664;
	// stw r9,8(r24)
	PPC_STORE_U32(ctx.r24.u32 + 8, ctx.r9.u32);
	// lis r27,-32199
	ctx.r27.s64 = -2110193664;
	// stw r11,12(r24)
	PPC_STORE_U32(ctx.r24.u32 + 12, ctx.r11.u32);
	// lis r28,-32199
	ctx.r28.s64 = -2110193664;
	// addi r10,r26,-13792
	ctx.r10.s64 = ctx.r26.s64 + -13792;
	// addi r9,r27,-13048
	ctx.r9.s64 = ctx.r27.s64 + -13048;
	// addi r11,r28,-13456
	ctx.r11.s64 = ctx.r28.s64 + -13456;
	// stw r10,16(r24)
	PPC_STORE_U32(ctx.r24.u32 + 16, ctx.r10.u32);
	// lis r29,-32199
	ctx.r29.s64 = -2110193664;
	// stw r9,20(r24)
	PPC_STORE_U32(ctx.r24.u32 + 20, ctx.r9.u32);
	// lis r30,-32199
	ctx.r30.s64 = -2110193664;
	// stw r11,24(r24)
	PPC_STORE_U32(ctx.r24.u32 + 24, ctx.r11.u32);
	// lis r31,-32199
	ctx.r31.s64 = -2110193664;
	// addi r10,r29,-13456
	ctx.r10.s64 = ctx.r29.s64 + -13456;
	// addi r9,r30,-11960
	ctx.r9.s64 = ctx.r30.s64 + -11960;
	// addi r11,r31,-10328
	ctx.r11.s64 = ctx.r31.s64 + -10328;
	// stw r10,28(r24)
	PPC_STORE_U32(ctx.r24.u32 + 28, ctx.r10.u32);
	// lis r3,-32199
	ctx.r3.s64 = -2110193664;
	// stw r9,32(r24)
	PPC_STORE_U32(ctx.r24.u32 + 32, ctx.r9.u32);
	// lis r4,-32199
	ctx.r4.s64 = -2110193664;
	// stw r11,36(r24)
	PPC_STORE_U32(ctx.r24.u32 + 36, ctx.r11.u32);
	// lis r5,-32199
	ctx.r5.s64 = -2110193664;
	// addi r10,r3,-11192
	ctx.r10.s64 = ctx.r3.s64 + -11192;
	// addi r9,r4,-9088
	ctx.r9.s64 = ctx.r4.s64 + -9088;
	// addi r11,r5,-11960
	ctx.r11.s64 = ctx.r5.s64 + -11960;
	// stw r10,40(r24)
	PPC_STORE_U32(ctx.r24.u32 + 40, ctx.r10.u32);
	// lis r6,-32199
	ctx.r6.s64 = -2110193664;
	// stw r9,44(r24)
	PPC_STORE_U32(ctx.r24.u32 + 44, ctx.r9.u32);
	// lis r7,-32199
	ctx.r7.s64 = -2110193664;
	// stw r11,48(r24)
	PPC_STORE_U32(ctx.r24.u32 + 48, ctx.r11.u32);
	// lis r8,-32199
	ctx.r8.s64 = -2110193664;
	// addi r10,r6,-10328
	ctx.r10.s64 = ctx.r6.s64 + -10328;
	// addi r9,r7,-11192
	ctx.r9.s64 = ctx.r7.s64 + -11192;
	// addi r11,r8,-11192
	ctx.r11.s64 = ctx.r8.s64 + -11192;
	// stw r10,52(r24)
	PPC_STORE_U32(ctx.r24.u32 + 52, ctx.r10.u32);
	// stw r9,56(r24)
	PPC_STORE_U32(ctx.r24.u32 + 56, ctx.r9.u32);
	// stw r11,60(r24)
	PPC_STORE_U32(ctx.r24.u32 + 60, ctx.r11.u32);
	// b 0x8233e4a8
	__restgprlr_24(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232A36C"))) PPC_WEAK_FUNC(sub_8232A36C);
PPC_FUNC_IMPL(__imp__sub_8232A36C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232A370"))) PPC_WEAK_FUNC(sub_8232A370);
PPC_FUNC_IMPL(__imp__sub_8232A370) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,5008(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5008);
	// addi r10,r3,5008
	ctx.r10.s64 = ctx.r3.s64 + 5008;
	// lwz r9,5012(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5012);
	// lwz r8,5016(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5016);
	// lwz r7,5020(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5020);
	// stw r11,5040(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5040, ctx.r11.u32);
	// stw r9,5044(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5044, ctx.r9.u32);
	// stw r8,5048(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5048, ctx.r8.u32);
	// stw r7,5052(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5052, ctx.r7.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232A398"))) PPC_WEAK_FUNC(sub_8232A398);
PPC_FUNC_IMPL(__imp__sub_8232A398) {
	PPC_FUNC_PROLOGUE();
	// b 0x8232a298
	sub_8232A298(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232A39C"))) PPC_WEAK_FUNC(sub_8232A39C);
PPC_FUNC_IMPL(__imp__sub_8232A39C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232A3A0"))) PPC_WEAK_FUNC(sub_8232A3A0);
PPC_FUNC_IMPL(__imp__sub_8232A3A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// bl 0x8232b8a8
	ctx.lr = 0x8232A3BC;
	sub_8232B8A8(ctx, base);
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// addi r10,r11,20832
	ctx.r10.s64 = ctx.r11.s64 + 20832;
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// addi r11,r11,11296
	ctx.r11.s64 = ctx.r11.s64 + 11296;
	// stw r3,128(r10)
	PPC_STORE_U32(ctx.r10.u32 + 128, ctx.r3.u32);
	// beq cr6,0x8232a3f0
	if (ctx.cr6.eq) goto loc_8232A3F0;
	// lis r10,-32171
	ctx.r10.s64 = -2108358656;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r31,r10,11292
	ctx.r31.s64 = ctx.r10.s64 + 11292;
	// addi r10,r30,64
	ctx.r10.s64 = ctx.r30.s64 + 64;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// b 0x8232a404
	goto loc_8232A404;
loc_8232A3F0:
	// lis r9,-32171
	ctx.r9.s64 = -2108358656;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// addi r31,r9,11292
	ctx.r31.s64 = ctx.r9.s64 + 11292;
	// stw r10,11292(r9)
	PPC_STORE_U32(ctx.r9.u32 + 11292, ctx.r10.u32);
loc_8232A404:
	// stw r3,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r3.u32);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// li r5,64
	ctx.r5.s64 = 64;
	// addi r30,r11,-176
	ctx.r30.s64 = ctx.r11.s64 + -176;
	// addi r4,r30,-64
	ctx.r4.s64 = ctx.r30.s64 + -64;
	// bl 0x8233e4e0
	ctx.lr = 0x8232A41C;
	sub_8233E4E0(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// li r5,64
	ctx.r5.s64 = 64;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// bl 0x8233e4e0
	ctx.lr = 0x8232A42C;
	sub_8233E4E0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232A444"))) PPC_WEAK_FUNC(sub_8232A444);
PPC_FUNC_IMPL(__imp__sub_8232A444) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232A448"))) PPC_WEAK_FUNC(sub_8232A448);
PPC_FUNC_IMPL(__imp__sub_8232A448) {
	PPC_FUNC_PROLOGUE();
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r8,r3,1
	ctx.r8.s64 = ctx.r3.s64 + 1;
	// addi r6,r3,2
	ctx.r6.s64 = ctx.r3.s64 + 2;
	// addi r31,r3,3
	ctx.r31.s64 = ctx.r3.s64 + 3;
	// li r11,0
	ctx.r11.s64 = 0;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// addi r7,r4,1
	ctx.r7.s64 = ctx.r4.s64 + 1;
	// addi r5,r4,2
	ctx.r5.s64 = ctx.r4.s64 + 2;
	// addi r30,r4,3
	ctx.r30.s64 = ctx.r4.s64 + 3;
	// subf r3,r4,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r4.s64;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
loc_8232A480:
	// add r9,r11,r4
	ctx.r9.u64 = ctx.r11.u64 + ctx.r4.u64;
	// lbzx r9,r3,r9
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + ctx.r9.u32);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lbzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// stbx r9,r11,r4
	PPC_STORE_U8(ctx.r11.u32 + ctx.r4.u32, ctx.r9.u8);
	// lbzx r9,r8,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r11.u32);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lbzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// stbx r9,r7,r11
	PPC_STORE_U8(ctx.r7.u32 + ctx.r11.u32, ctx.r9.u8);
	// lbzx r9,r6,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r11.u32);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lbzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// stbx r9,r5,r11
	PPC_STORE_U8(ctx.r5.u32 + ctx.r11.u32, ctx.r9.u8);
	// lbzx r9,r31,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r31.u32 + ctx.r11.u32);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lbzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// stbx r9,r30,r11
	PPC_STORE_U8(ctx.r30.u32 + ctx.r11.u32, ctx.r9.u8);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x8232a480
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8232A480;
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232A4D8"))) PPC_WEAK_FUNC(sub_8232A4D8);
PPC_FUNC_IMPL(__imp__sub_8232A4D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// lis r9,-32199
	ctx.r9.s64 = -2110193664;
	// lis r8,-32199
	ctx.r8.s64 = -2110193664;
	// addi r5,r9,-20864
	ctx.r5.s64 = ctx.r9.s64 + -20864;
	// addi r4,r8,-20712
	ctx.r4.s64 = ctx.r8.s64 + -20712;
	// lfs f0,96(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// stw r4,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r4.u32);
	// stfs f0,32(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 32, temp.u32);
	// lis r7,32767
	ctx.r7.s64 = 2147418112;
	// lis r6,-32171
	ctx.r6.s64 = -2108358656;
	// ori r9,r7,65535
	ctx.r9.u64 = ctx.r7.u64 | 65535;
	// li r11,0
	ctx.r11.s64 = 0;
	// lis r10,-32768
	ctx.r10.s64 = -2147483648;
	// stw r9,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r9.u32);
	// li r8,256
	ctx.r8.s64 = 256;
	// stb r11,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r11.u8);
	// li r7,512
	ctx.r7.s64 = 512;
	// stb r11,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, ctx.r11.u8);
	// li r5,128
	ctx.r5.s64 = 128;
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r11.u8);
	// lis r4,-32171
	ctx.r4.s64 = -2108358656;
	// stb r11,3(r3)
	PPC_STORE_U8(ctx.r3.u32 + 3, ctx.r11.u8);
	// stb r11,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r11.u8);
	// stb r11,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r11.u8);
	// stb r11,6(r3)
	PPC_STORE_U8(ctx.r3.u32 + 6, ctx.r11.u8);
	// stb r11,7(r3)
	PPC_STORE_U8(ctx.r3.u32 + 7, ctx.r11.u8);
	// stw r10,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r10.u32);
	// stw r11,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r11.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// stw r8,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r8.u32);
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, ctx.r11.u32);
	// stw r11,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, ctx.r11.u32);
	// stw r11,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, ctx.r11.u32);
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, ctx.r11.u32);
	// stw r7,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, ctx.r7.u32);
	// stw r5,60(r3)
	PPC_STORE_U32(ctx.r3.u32 + 60, ctx.r5.u32);
	// lwz r10,11296(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 11296);
	// stw r10,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r10.u32);
	// lwz r10,11292(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 11292);
	// stw r11,72(r3)
	PPC_STORE_U32(ctx.r3.u32 + 72, ctx.r11.u32);
	// stw r11,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, ctx.r11.u32);
	// stw r11,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, ctx.r11.u32);
	// stw r10,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232A58C"))) PPC_WEAK_FUNC(sub_8232A58C);
PPC_FUNC_IMPL(__imp__sub_8232A58C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232A590"))) PPC_WEAK_FUNC(sub_8232A590);
PPC_FUNC_IMPL(__imp__sub_8232A590) {
	PPC_FUNC_PROLOGUE();
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x823251a0
	sub_823251A0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232A598"))) PPC_WEAK_FUNC(sub_8232A598);
PPC_FUNC_IMPL(__imp__sub_8232A598) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32205
	ctx.r11.s64 = -2110586880;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r11,-23152
	ctx.r3.s64 = ctx.r11.s64 + -23152;
	// bl 0x8232b8c0
	ctx.lr = 0x8232A5BC;
	sub_8232B8C0(ctx, base);
	// addi r3,r31,4448
	ctx.r3.s64 = ctx.r31.s64 + 4448;
	// bl 0x8232a3a0
	ctx.lr = 0x8232A5C4;
	sub_8232A3A0(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232A5D8"))) PPC_WEAK_FUNC(sub_8232A5D8);
PPC_FUNC_IMPL(__imp__sub_8232A5D8) {
	PPC_FUNC_PROLOGUE();
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r9,r10,20964
	ctx.r9.s64 = ctx.r10.s64 + 20964;
	// addi r11,r11,100
	ctx.r11.s64 = ctx.r11.s64 + 100;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232A5F0"))) PPC_WEAK_FUNC(sub_8232A5F0);
PPC_FUNC_IMPL(__imp__sub_8232A5F0) {
	PPC_FUNC_PROLOGUE();
	// cmpwi cr6,r4,1
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 1, ctx.xer);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// lis r10,-32199
	ctx.r10.s64 = -2110193664;
	// lis r9,-32199
	ctx.r9.s64 = -2110193664;
	// addi r8,r10,-21192
	ctx.r8.s64 = ctx.r10.s64 + -21192;
	// addi r7,r9,-21040
	ctx.r7.s64 = ctx.r9.s64 + -21040;
	// lwz r11,20976(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20976);
	// stw r8,5120(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5120, ctx.r8.u32);
	// addi r6,r11,4448
	ctx.r6.s64 = ctx.r11.s64 + 4448;
	// stw r7,5124(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5124, ctx.r7.u32);
	// stw r6,4404(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4404, ctx.r6.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232A624"))) PPC_WEAK_FUNC(sub_8232A624);
PPC_FUNC_IMPL(__imp__sub_8232A624) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232A628"))) PPC_WEAK_FUNC(sub_8232A628);
PPC_FUNC_IMPL(__imp__sub_8232A628) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,20964
	ctx.r11.s64 = ctx.r11.s64 + 20964;
	// lwzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stw r8,5092(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5092, ctx.r8.u32);
	// lwzx r7,r10,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r6,4(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// stw r6,5096(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5096, ctx.r6.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232A650"))) PPC_WEAK_FUNC(sub_8232A650);
PPC_FUNC_IMPL(__imp__sub_8232A650) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// stw r3,20976(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20976, ctx.r3.u32);
	// bl 0x8232a598
	ctx.lr = 0x8232A668;
	sub_8232A598(ctx, base);
	// bl 0x8232a5d8
	ctx.lr = 0x8232A66C;
	sub_8232A5D8(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232A67C"))) PPC_WEAK_FUNC(sub_8232A67C);
PPC_FUNC_IMPL(__imp__sub_8232A67C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232A680"))) PPC_WEAK_FUNC(sub_8232A680);
PPC_FUNC_IMPL(__imp__sub_8232A680) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// bl 0x8232a5f0
	ctx.lr = 0x8232A698;
	sub_8232A5F0(ctx, base);
	// bl 0x8232a628
	ctx.lr = 0x8232A69C;
	sub_8232A628(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232A6AC"))) PPC_WEAK_FUNC(sub_8232A6AC);
PPC_FUNC_IMPL(__imp__sub_8232A6AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232A6B0"))) PPC_WEAK_FUNC(sub_8232A6B0);
PPC_FUNC_IMPL(__imp__sub_8232A6B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x8232A6B8;
	__restfpr_14(ctx, base);
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// addi r7,r4,4
	ctx.r7.s64 = ctx.r4.s64 + 4;
	// addi r4,r4,5
	ctx.r4.s64 = ctx.r4.s64 + 5;
	// li r8,8
	ctx.r8.s64 = 8;
	// stw r7,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r7.u32);
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// stw r4,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, ctx.r4.u32);
	// addi r7,r11,6
	ctx.r7.s64 = ctx.r11.s64 + 6;
	// addi r4,r11,7
	ctx.r4.s64 = ctx.r11.s64 + 7;
	// addi r9,r10,208
	ctx.r9.s64 = ctx.r10.s64 + 208;
	// stw r7,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r7.u32);
	// stw r4,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r4.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// addi r31,r9,-64
	ctx.r31.s64 = ctx.r9.s64 + -64;
	// addi r6,r9,-64
	ctx.r6.s64 = ctx.r9.s64 + -64;
	// addi r8,r9,-64
	ctx.r8.s64 = ctx.r9.s64 + -64;
	// addi r29,r9,-64
	ctx.r29.s64 = ctx.r9.s64 + -64;
	// addi r30,r9,-64
	ctx.r30.s64 = ctx.r9.s64 + -64;
	// addi r4,r9,-64
	ctx.r4.s64 = ctx.r9.s64 + -64;
	// addi r7,r9,-64
	ctx.r7.s64 = ctx.r9.s64 + -64;
	// addi r28,r9,7
	ctx.r28.s64 = ctx.r9.s64 + 7;
	// addi r27,r9,-64
	ctx.r27.s64 = ctx.r9.s64 + -64;
	// addi r25,r9,1
	ctx.r25.s64 = ctx.r9.s64 + 1;
	// addi r23,r9,2
	ctx.r23.s64 = ctx.r9.s64 + 2;
	// addi r21,r9,3
	ctx.r21.s64 = ctx.r9.s64 + 3;
	// addi r19,r9,4
	ctx.r19.s64 = ctx.r9.s64 + 4;
	// addi r17,r9,5
	ctx.r17.s64 = ctx.r9.s64 + 5;
	// addi r15,r9,6
	ctx.r15.s64 = ctx.r9.s64 + 6;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// addi r10,r8,7
	ctx.r10.s64 = ctx.r8.s64 + 7;
	// stw r9,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r9.u32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// addi r20,r4,4
	ctx.r20.s64 = ctx.r4.s64 + 4;
	// addi r16,r7,6
	ctx.r16.s64 = ctx.r7.s64 + 6;
	// subf r7,r11,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r11.s64;
	// subf r4,r11,r28
	ctx.r4.s64 = ctx.r28.s64 - ctx.r11.s64;
	// addi r22,r31,3
	ctx.r22.s64 = ctx.r31.s64 + 3;
	// stw r7,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r7.u32);
	// addi r18,r6,5
	ctx.r18.s64 = ctx.r6.s64 + 5;
	// stw r4,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, ctx.r4.u32);
	// addi r26,r29,1
	ctx.r26.s64 = ctx.r29.s64 + 1;
	// lfs f0,11124(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 11124);
	ctx.f0.f64 = double(temp.f32);
	// addi r24,r30,2
	ctx.r24.s64 = ctx.r30.s64 + 2;
	// subf r28,r11,r27
	ctx.r28.s64 = ctx.r27.s64 - ctx.r11.s64;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// addi r6,r11,2
	ctx.r6.s64 = ctx.r11.s64 + 2;
	// addi r31,r11,3
	ctx.r31.s64 = ctx.r11.s64 + 3;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r3,1
	ctx.r7.s64 = ctx.r3.s64 + 1;
	// addi r4,r3,2
	ctx.r4.s64 = ctx.r3.s64 + 2;
	// addi r30,r3,3
	ctx.r30.s64 = ctx.r3.s64 + 3;
	// addi r29,r3,7
	ctx.r29.s64 = ctx.r3.s64 + 7;
	// subf r27,r11,r3
	ctx.r27.s64 = ctx.r3.s64 - ctx.r11.s64;
	// subf r26,r11,r26
	ctx.r26.s64 = ctx.r26.s64 - ctx.r11.s64;
	// subf r25,r11,r25
	ctx.r25.s64 = ctx.r25.s64 - ctx.r11.s64;
	// subf r24,r11,r24
	ctx.r24.s64 = ctx.r24.s64 - ctx.r11.s64;
	// subf r23,r11,r23
	ctx.r23.s64 = ctx.r23.s64 - ctx.r11.s64;
	// subf r22,r11,r22
	ctx.r22.s64 = ctx.r22.s64 - ctx.r11.s64;
	// subf r21,r11,r21
	ctx.r21.s64 = ctx.r21.s64 - ctx.r11.s64;
	// subf r20,r11,r20
	ctx.r20.s64 = ctx.r20.s64 - ctx.r11.s64;
	// subf r19,r11,r19
	ctx.r19.s64 = ctx.r19.s64 - ctx.r11.s64;
	// subf r18,r11,r18
	ctx.r18.s64 = ctx.r18.s64 - ctx.r11.s64;
	// subf r17,r11,r17
	ctx.r17.s64 = ctx.r17.s64 - ctx.r11.s64;
	// subf r16,r11,r16
	ctx.r16.s64 = ctx.r16.s64 - ctx.r11.s64;
	// subf r15,r11,r15
	ctx.r15.s64 = ctx.r15.s64 - ctx.r11.s64;
loc_8232A7BC:
	// lbzx r14,r28,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r11.u32);
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
	// std r28,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r28.u64);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// lwz r28,-272(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// std r29,-240(r1)
	PPC_STORE_U64(ctx.r1.u32 + -240, ctx.r29.u64);
	// lwz r29,-280(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// lbzx r14,r14,r3
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r14.u32 + ctx.r3.u32);
	// stb r14,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r14.u8);
	// lbzx r28,r28,r11
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r11.u32);
	// std r28,-232(r1)
	PPC_STORE_U64(ctx.r1.u32 + -232, ctx.r28.u64);
	// lfd f13,-232(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -232);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// lbzx r14,r27,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r27.u32 + ctx.r11.u32);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// rlwinm r14,r14,2,0,29
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfsx f10,r14,r5
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r14.u32 + ctx.r5.u32, temp.u32);
	// lbzx r14,r26,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r26.u32 + ctx.r11.u32);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// lbzx r14,r14,r3
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r14.u32 + ctx.r3.u32);
	// stbx r14,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, ctx.r14.u8);
	// lbzx r28,r25,r11
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r25.u32 + ctx.r11.u32);
	// std r28,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r28.u64);
	// lfd f9,-200(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f8,f9
	ctx.f8.f64 = double(ctx.f9.s64);
	// lbzx r14,r7,r10
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r10.u32);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// frsp f7,f8
	ctx.f7.f64 = double(float(ctx.f8.f64));
	// rlwinm r14,r14,2,0,29
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfsx f6,r14,r5
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r14.u32 + ctx.r5.u32, temp.u32);
	// lbzx r14,r24,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r24.u32 + ctx.r11.u32);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// lbzx r14,r14,r3
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r14.u32 + ctx.r3.u32);
	// stbx r14,r6,r10
	PPC_STORE_U8(ctx.r6.u32 + ctx.r10.u32, ctx.r14.u8);
	// lbzx r28,r23,r11
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r23.u32 + ctx.r11.u32);
	// std r28,-256(r1)
	PPC_STORE_U64(ctx.r1.u32 + -256, ctx.r28.u64);
	// lfd f5,-256(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -256);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// lbzx r14,r4,r10
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r10.u32);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// rlwinm r14,r14,2,0,29
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// stfsx f2,r14,r5
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r14.u32 + ctx.r5.u32, temp.u32);
	// lbzx r14,r22,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r22.u32 + ctx.r11.u32);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// lbzx r14,r14,r3
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r14.u32 + ctx.r3.u32);
	// stbx r14,r31,r10
	PPC_STORE_U8(ctx.r31.u32 + ctx.r10.u32, ctx.r14.u8);
	// lbzx r28,r21,r11
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r21.u32 + ctx.r11.u32);
	// std r28,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r28.u64);
	// lfd f1,-168(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f13,f1
	ctx.f13.f64 = double(ctx.f1.s64);
	// lbzx r14,r30,r10
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r30.u32 + ctx.r10.u32);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// rlwinm r14,r14,2,0,29
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfsx f11,r14,r5
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r14.u32 + ctx.r5.u32, temp.u32);
	// lbzx r14,r20,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r20.u32 + ctx.r11.u32);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// lbzx r14,r14,r3
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r14.u32 + ctx.r3.u32);
	// stbx r14,r29,r10
	PPC_STORE_U8(ctx.r29.u32 + ctx.r10.u32, ctx.r14.u8);
	// lbzx r14,r19,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r19.u32 + ctx.r11.u32);
	// std r14,-216(r1)
	PPC_STORE_U64(ctx.r1.u32 + -216, ctx.r14.u64);
	// lfd f10,-216(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -216);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// lbzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfsx f7,r9,r5
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r5.u32, temp.u32);
	// lbzx r9,r18,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r18.u32 + ctx.r11.u32);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lbzx r14,r9,r3
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// lwz r9,-264(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// stw r9,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, ctx.r9.u32);
	// addi r9,r3,5
	ctx.r9.s64 = ctx.r3.s64 + 5;
	// std r6,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r6.u64);
	// lwz r6,-260(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// lwz r29,-288(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// lwz r28,-284(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// std r8,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r8.u64);
	// lwz r8,-276(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// stbx r14,r6,r10
	PPC_STORE_U8(ctx.r6.u32 + ctx.r10.u32, ctx.r14.u8);
	// lbzx r14,r17,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r17.u32 + ctx.r11.u32);
	// std r14,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r14.u64);
	// lfd f6,-184(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// lbzx r14,r9,r10
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// std r7,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r7.u64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// rlwinm r14,r14,2,0,29
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r3,6
	ctx.r9.s64 = ctx.r3.s64 + 6;
	// lwz r7,-268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// ld r6,-160(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfsx f3,r14,r5
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r14.u32 + ctx.r5.u32, temp.u32);
	// lbzx r14,r16,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r16.u32 + ctx.r11.u32);
	// extsb r14,r14
	ctx.r14.s64 = ctx.r14.s8;
	// lbzx r14,r14,r3
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r14.u32 + ctx.r3.u32);
	// stbx r14,r29,r10
	PPC_STORE_U8(ctx.r29.u32 + ctx.r10.u32, ctx.r14.u8);
	// lbzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lbzx r14,r15,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r15.u32 + ctx.r11.u32);
	// std r14,-248(r1)
	PPC_STORE_U64(ctx.r1.u32 + -248, ctx.r14.u64);
	// lfd f2,-248(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -248);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// ld r29,-240(r1)
	ctx.r29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -240);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// frsp f13,f1
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfsx f12,r9,r5
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r5.u32, temp.u32);
	// lbzx r9,r28,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r11.u32);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// stbx r9,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, ctx.r9.u8);
	// lbzx r9,r29,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r29.u32 + ctx.r10.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// lbzx r14,r7,r11
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r11.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// std r14,-224(r1)
	PPC_STORE_U64(ctx.r1.u32 + -224, ctx.r14.u64);
	// lfd f11,-224(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -224);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// ld r28,-208(r1)
	ctx.r28.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// ld r8,-192(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// ld r7,-176(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfsx f8,r9,r5
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r5.u32, temp.u32);
	// bdnz 0x8232a7bc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8232A7BC;
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232A9E4"))) PPC_WEAK_FUNC(sub_8232A9E4);
PPC_FUNC_IMPL(__imp__sub_8232A9E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232A9E8"))) PPC_WEAK_FUNC(sub_8232A9E8);
PPC_FUNC_IMPL(__imp__sub_8232A9E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r30,r11,112
	ctx.r30.s64 = ctx.r11.s64 + 112;
	// li r5,8
	ctx.r5.s64 = 8;
	// addi r3,r3,4352
	ctx.r3.s64 = ctx.r3.s64 + 4352;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x82329ab8
	ctx.lr = 0x8232AA18;
	sub_82329AB8(ctx, base);
	// li r9,8
	ctx.r9.s64 = 8;
	// addi r11,r30,160
	ctx.r11.s64 = ctx.r30.s64 + 160;
	// addi r10,r31,4576
	ctx.r10.s64 = ctx.r31.s64 + 4576;
	// addi r11,r11,-4
	ctx.r11.s64 = ctx.r11.s64 + -4;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8232AA30:
	// lwzu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// stwu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x8232aa30
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8232AA30;
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// lis r10,-32171
	ctx.r10.s64 = -2108358656;
	// li r9,21
	ctx.r9.s64 = 21;
	// lis r8,-32171
	ctx.r8.s64 = -2108358656;
	// li r7,19
	ctx.r7.s64 = 19;
	// lwz r11,11368(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 11368);
	// lis r6,-32171
	ctx.r6.s64 = -2108358656;
	// stw r9,4612(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4612, ctx.r9.u32);
	// li r5,18
	ctx.r5.s64 = 18;
	// addi r4,r11,-16
	ctx.r4.s64 = ctx.r11.s64 + -16;
	// lis r3,-32171
	ctx.r3.s64 = -2108358656;
	// stw r4,4608(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4608, ctx.r4.u32);
	// li r9,17
	ctx.r9.s64 = 17;
	// lwz r11,11344(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 11344);
	// addi r4,r11,-32
	ctx.r4.s64 = ctx.r11.s64 + -32;
	// stw r4,4616(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4616, ctx.r4.u32);
	// lis r10,-32171
	ctx.r10.s64 = -2108358656;
	// stw r7,4620(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4620, ctx.r7.u32);
	// li r7,16
	ctx.r7.s64 = 16;
	// lwz r11,11380(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 11380);
	// addi r4,r11,-32
	ctx.r4.s64 = ctx.r11.s64 + -32;
	// stw r5,4628(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4628, ctx.r5.u32);
	// li r8,15
	ctx.r8.s64 = 15;
	// stw r4,4624(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4624, ctx.r4.u32);
	// lwz r11,11300(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 11300);
	// addi r6,r11,-32
	ctx.r6.s64 = ctx.r11.s64 + -32;
	// stw r9,4636(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4636, ctx.r9.u32);
	// stw r6,4632(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4632, ctx.r6.u32);
	// lwz r11,11304(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 11304);
	// addi r5,r11,-32
	ctx.r5.s64 = ctx.r11.s64 + -32;
	// stw r7,4644(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4644, ctx.r7.u32);
	// stw r5,4640(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4640, ctx.r5.u32);
	// lwz r11,11376(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 11376);
	// addi r4,r11,-32
	ctx.r4.s64 = ctx.r11.s64 + -32;
	// stw r8,4652(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4652, ctx.r8.u32);
	// stw r4,4648(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4648, ctx.r4.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232AAE4"))) PPC_WEAK_FUNC(sub_8232AAE4);
PPC_FUNC_IMPL(__imp__sub_8232AAE4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232AAE8"))) PPC_WEAK_FUNC(sub_8232AAE8);
PPC_FUNC_IMPL(__imp__sub_8232AAE8) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// lwz r10,5244(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5244);
	// addi r9,r11,10976
	ctx.r9.s64 = ctx.r11.s64 + 10976;
	// cmpwi cr6,r10,8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 8, ctx.xer);
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// beq cr6,0x8232ab2c
	if (ctx.cr6.eq) goto loc_8232AB2C;
	// lwz r10,5300(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5300);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x8232ab14
	if (!ctx.cr6.eq) goto loc_8232AB14;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// b 0x8232ab18
	goto loc_8232AB18;
loc_8232AB14:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
loc_8232AB18:
	// stw r10,5304(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5304, ctx.r10.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r10,5312(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5312, ctx.r10.u32);
	// stw r11,5308(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5308, ctx.r11.u32);
loc_8232AB2C:
	// lwz r11,5300(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 5300);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232ab54
	if (!ctx.cr6.eq) goto loc_8232AB54;
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// lis r10,-32171
	ctx.r10.s64 = -2108358656;
	// lwz r11,11324(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 11324);
	// stw r11,5320(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5320, ctx.r11.u32);
	// lwz r11,11356(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 11356);
	// stw r11,5324(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5324, ctx.r11.u32);
	// blr 
	return;
loc_8232AB54:
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// lis r10,-32171
	ctx.r10.s64 = -2108358656;
	// lwz r11,11372(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 11372);
	// stw r11,5320(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5320, ctx.r11.u32);
	// lwz r11,11348(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 11348);
	// stw r11,5324(r3)
	PPC_STORE_U32(ctx.r3.u32 + 5324, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232AB70"))) PPC_WEAK_FUNC(sub_8232AB70);
PPC_FUNC_IMPL(__imp__sub_8232AB70) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r10,64
	ctx.r10.s64 = 64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r11,0
	ctx.r11.s64 = 0;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_8232AB98:
	// extsb r10,r11
	ctx.r10.s64 = ctx.r11.s8;
	// stbx r10,r11,r9
	PPC_STORE_U8(ctx.r11.u32 + ctx.r9.u32, ctx.r10.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// bdnz 0x8232ab98
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8232AB98;
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8232a448
	ctx.lr = 0x8232ABB4;
	sub_8232A448(ctx, base);
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// addi r31,r31,4384
	ctx.r31.s64 = ctx.r31.s64 + 4384;
	// addi r30,r11,20984
	ctx.r30.s64 = ctx.r11.s64 + 20984;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x8232a6b0
	ctx.lr = 0x8232ABD0;
	sub_8232A6B0(ctx, base);
	// lis r10,-32171
	ctx.r10.s64 = -2108358656;
	// lis r9,-32171
	ctx.r9.s64 = -2108358656;
	// addi r8,r10,11264
	ctx.r8.s64 = ctx.r10.s64 + 11264;
	// addi r7,r9,11280
	ctx.r7.s64 = ctx.r9.s64 + 11280;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// stw r31,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r31.u32);
	// stw r30,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r30.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232AC04"))) PPC_WEAK_FUNC(sub_8232AC04);
PPC_FUNC_IMPL(__imp__sub_8232AC04) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232AC08"))) PPC_WEAK_FUNC(sub_8232AC08);
PPC_FUNC_IMPL(__imp__sub_8232AC08) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x8232ab70
	ctx.lr = 0x8232AC20;
	sub_8232AB70(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8232a9e8
	ctx.lr = 0x8232AC28;
	sub_8232A9E8(ctx, base);
	// bl 0x82336e10
	ctx.lr = 0x8232AC2C;
	sub_82336E10(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232AC40"))) PPC_WEAK_FUNC(sub_8232AC40);
PPC_FUNC_IMPL(__imp__sub_8232AC40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r9,4772(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4772);
	// li r10,8
	ctx.r10.s64 = 8;
	// addi r11,r3,2816
	ctx.r11.s64 = ctx.r3.s64 + 2816;
	// subfic r8,r9,0
	ctx.xer.ca = ctx.r9.u32 <= 0;
	ctx.r8.s64 = 0 - ctx.r9.s64;
	// subfe r7,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r7.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// clrlwi r9,r7,29
	ctx.r9.u64 = ctx.r7.u32 & 0x7;
	// rlwinm r9,r9,0,31,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// addi r6,r9,-1
	ctx.r6.s64 = ctx.r9.s64 + -1;
	// addi r9,r3,4692
	ctx.r9.s64 = ctx.r3.s64 + 4692;
	// stw r6,4692(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4692, ctx.r6.u32);
	// stw r11,4696(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4696, ctx.r11.u32);
	// stw r11,4704(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4704, ctx.r11.u32);
	// stw r11,4712(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4712, ctx.r11.u32);
	// stw r11,4720(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4720, ctx.r11.u32);
	// stw r11,4728(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4728, ctx.r11.u32);
	// stw r11,4736(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4736, ctx.r11.u32);
	// stw r10,4700(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4700, ctx.r10.u32);
	// stw r10,4708(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4708, ctx.r10.u32);
	// stw r10,4716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4716, ctx.r10.u32);
	// stw r10,4724(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4724, ctx.r10.u32);
	// stw r10,4732(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4732, ctx.r10.u32);
	// stw r10,4740(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4740, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232AC9C"))) PPC_WEAK_FUNC(sub_8232AC9C);
PPC_FUNC_IMPL(__imp__sub_8232AC9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232ACA0"))) PPC_WEAK_FUNC(sub_8232ACA0);
PPC_FUNC_IMPL(__imp__sub_8232ACA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r10,4772(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4772);
	// addi r11,r3,4640
	ctx.r11.s64 = ctx.r3.s64 + 4640;
	// subfic r9,r10,0
	ctx.xer.ca = ctx.r10.u32 <= 0;
	ctx.r9.s64 = 0 - ctx.r10.s64;
	// subfe r8,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r8.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// clrlwi r11,r8,29
	ctx.r11.u64 = ctx.r8.u32 & 0x7;
	// rlwinm r11,r11,0,31,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// addi r7,r11,-1
	ctx.r7.s64 = ctx.r11.s64 + -1;
	// stw r7,4640(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4640, ctx.r7.u32);
	// lhz r6,5004(r3)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5004);
	// extsh r5,r6
	ctx.r5.s64 = ctx.r6.s16;
	// stw r5,4648(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4648, ctx.r5.u32);
	// stw r5,4656(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4656, ctx.r5.u32);
	// lhz r4,5006(r3)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r3.u32 + 5006);
	// extsh r11,r4
	ctx.r11.s64 = ctx.r4.s16;
	// stw r11,4664(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4664, ctx.r11.u32);
	// stw r11,4672(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4672, ctx.r11.u32);
	// stw r11,4680(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4680, ctx.r11.u32);
	// stw r11,4688(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4688, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232ACEC"))) PPC_WEAK_FUNC(sub_8232ACEC);
PPC_FUNC_IMPL(__imp__sub_8232ACEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232ACF0"))) PPC_WEAK_FUNC(sub_8232ACF0);
PPC_FUNC_IMPL(__imp__sub_8232ACF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,4772(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4772);
	// subfic r10,r11,0
	ctx.xer.ca = ctx.r11.u32 <= 0;
	ctx.r10.s64 = 0 - ctx.r11.s64;
	// subfe r9,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// clrlwi r11,r9,29
	ctx.r11.u64 = ctx.r9.u32 & 0x7;
	// rlwinm r11,r11,0,31,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,4692(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4692, ctx.r11.u32);
	// stw r11,4640(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4640, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232AD14"))) PPC_WEAK_FUNC(sub_8232AD14);
PPC_FUNC_IMPL(__imp__sub_8232AD14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232AD18"))) PPC_WEAK_FUNC(sub_8232AD18);
PPC_FUNC_IMPL(__imp__sub_8232AD18) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r31,r3,4556
	ctx.r31.s64 = ctx.r3.s64 + 4556;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82336e30
	ctx.lr = 0x8232AD3C;
	sub_82336E30(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82336e18
	ctx.lr = 0x8232AD44;
	sub_82336E18(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8232ac40
	ctx.lr = 0x8232AD4C;
	sub_8232AC40(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232AD64"))) PPC_WEAK_FUNC(sub_8232AD64);
PPC_FUNC_IMPL(__imp__sub_8232AD64) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232AD68"))) PPC_WEAK_FUNC(sub_8232AD68);
PPC_FUNC_IMPL(__imp__sub_8232AD68) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// addi r31,r11,21240
	ctx.r31.s64 = ctx.r11.s64 + 21240;
	// addi r10,r10,376
	ctx.r10.s64 = ctx.r10.s64 + 376;
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// stw r10,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r10.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,40(r31)
	PPC_STORE_U32(ctx.r31.u32 + 40, ctx.r11.u32);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x8232adb0
	if (!ctx.cr6.eq) goto loc_8232ADB0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r4,32
	ctx.r4.s64 = 32;
	// bl 0x8232b4f0
	ctx.lr = 0x8232ADAC;
	sub_8232B4F0(ctx, base);
	// stw r3,36(r31)
	PPC_STORE_U32(ctx.r31.u32 + 36, ctx.r3.u32);
loc_8232ADB0:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232ADC4"))) PPC_WEAK_FUNC(sub_8232ADC4);
PPC_FUNC_IMPL(__imp__sub_8232ADC4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232ADC8"))) PPC_WEAK_FUNC(sub_8232ADC8);
PPC_FUNC_IMPL(__imp__sub_8232ADC8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// addi r31,r11,21276
	ctx.r31.s64 = ctx.r11.s64 + 21276;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addic. r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// bne 0x8232ae08
	if (!ctx.cr0.eq) goto loc_8232AE08;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8232ae08
	if (ctx.cr6.eq) goto loc_8232AE08;
	// bl 0x8232b5d0
	ctx.lr = 0x8232AE00;
	sub_8232B5D0(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
loc_8232AE08:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232AE1C"))) PPC_WEAK_FUNC(sub_8232AE1C);
PPC_FUNC_IMPL(__imp__sub_8232AE1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232AE20"))) PPC_WEAK_FUNC(sub_8232AE20);
PPC_FUNC_IMPL(__imp__sub_8232AE20) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,1
	ctx.r11.s64 = 1;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,-1
	ctx.r30.s64 = -1;
	// stw r11,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r11.u32);
	// b 0x8232aeac
	goto loc_8232AEAC;
loc_8232AE48:
	// lwz r4,80(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// lwz r11,76(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 76);
	// cmplw cr6,r11,r4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r4.u32, ctx.xer);
	// beq cr6,0x8232ae70
	if (ctx.cr6.eq) goto loc_8232AE70;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// bl 0x82247090
	ctx.lr = 0x8232AE60;
	sub_82247090(ctx, base);
	// cmpwi cr6,r3,-1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, -1, ctx.xer);
	// beq cr6,0x8232ae70
	if (ctx.cr6.eq) goto loc_8232AE70;
	// lwz r11,80(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// stw r11,76(r31)
	PPC_STORE_U32(ctx.r31.u32 + 76, ctx.r11.u32);
loc_8232AE70:
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// bl 0x8232b660
	ctx.lr = 0x8232AE78;
	sub_8232B660(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x8232aea4
	if (!ctx.cr6.eq) goto loc_8232AEA4;
	// lwz r11,52(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8232ae9c
	if (ctx.cr6.eq) goto loc_8232AE9C;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x8232AE9C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_8232AE9C:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
loc_8232AEA4:
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// bl 0x8232b6f8
	ctx.lr = 0x8232AEAC;
	sub_8232B6F8(ctx, base);
loc_8232AEAC:
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x82247250
	ctx.lr = 0x8232AEB8;
	sub_82247250(ctx, base);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x82247218
	ctx.lr = 0x8232AEC0;
	sub_82247218(ctx, base);
	// lwz r11,68(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 68);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x8232ae48
	if (ctx.cr6.eq) goto loc_8232AE48;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232AEE8"))) PPC_WEAK_FUNC(sub_8232AEE8);
PPC_FUNC_IMPL(__imp__sub_8232AEE8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x8232af14
	if (!ctx.cr6.eq) goto loc_8232AF14;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,488
	ctx.r3.s64 = ctx.r11.s64 + 488;
loc_8232AF08:
	// bl 0x8232b878
	ctx.lr = 0x8232AF0C;
	sub_8232B878(ctx, base);
loc_8232AF0C:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8232b020
	goto loc_8232B020;
loc_8232AF14:
	// cmplwi cr6,r4,88
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 88, ctx.xer);
	// bge cr6,0x8232af28
	if (!ctx.cr6.lt) goto loc_8232AF28;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,644
	ctx.r3.s64 = ctx.r11.s64 + 644;
	// b 0x8232af08
	goto loc_8232AF08;
loc_8232AF28:
	// addi r11,r3,3
	ctx.r11.s64 = ctx.r3.s64 + 3;
	// li r5,84
	ctx.r5.s64 = 84;
	// rlwinm r31,r11,0,0,29
	ctx.r31.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x8232AF40;
	sub_8233EAF0(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x82246e60
	ctx.lr = 0x8232AF54;
	sub_82246E60(ctx, base);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r3.u32);
	// bne 0x8232af6c
	if (!ctx.cr0.eq) goto loc_8232AF6C;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,600
	ctx.r3.s64 = ctx.r11.s64 + 600;
	// b 0x8232af08
	goto loc_8232AF08;
loc_8232AF6C:
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r31,20
	ctx.r3.s64 = ctx.r31.s64 + 20;
	// bl 0x8232b4f0
	ctx.lr = 0x8232AF78;
	sub_8232B4F0(ctx, base);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r3.u32);
	// bne 0x8232af9c
	if (!ctx.cr0.eq) goto loc_8232AF9C;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,552
	ctx.r3.s64 = ctx.r11.s64 + 552;
	// bl 0x8232b878
	ctx.lr = 0x8232AF90;
	sub_8232B878(ctx, base);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x82246e18
	ctx.lr = 0x8232AF98;
	sub_82246E18(ctx, base);
	// b 0x8232af0c
	goto loc_8232AF0C;
loc_8232AF9C:
	// lis r11,-32205
	ctx.r11.s64 = -2110586880;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// addi r5,r11,-20960
	ctx.r5.s64 = ctx.r11.s64 + -20960;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8237ac40
	ctx.lr = 0x8232AFBC;
	sub_8237AC40(ctx, base);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r3.u32);
	// bne 0x8232aff0
	if (!ctx.cr0.eq) goto loc_8232AFF0;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,512
	ctx.r3.s64 = ctx.r11.s64 + 512;
	// bl 0x8232b878
	ctx.lr = 0x8232AFD4;
	sub_8232B878(ctx, base);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x82246e18
	ctx.lr = 0x8232AFDC;
	sub_82246E18(ctx, base);
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// bl 0x8232b5d0
	ctx.lr = 0x8232AFE4;
	sub_8232B5D0(ctx, base);
	// b 0x8232af0c
	goto loc_8232AF0C;
loc_8232AFE8:
	// li r3,10
	ctx.r3.s64 = 10;
	// bl 0x82247328
	ctx.lr = 0x8232AFF0;
	sub_82247328(ctx, base);
loc_8232AFF0:
	// lwz r11,64(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 64);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x8232afe8
	if (ctx.cr6.eq) goto loc_8232AFE8;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// bl 0x82246f80
	ctx.lr = 0x8232B004;
	sub_82246F80(ctx, base);
	// li r11,-1
	ctx.r11.s64 = -1;
	// stw r3,72(r31)
	PPC_STORE_U32(ctx.r31.u32 + 72, ctx.r3.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,76(r31)
	PPC_STORE_U32(ctx.r31.u32 + 76, ctx.r11.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,80(r31)
	PPC_STORE_U32(ctx.r31.u32 + 80, ctx.r11.u32);
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
loc_8232B020:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B034"))) PPC_WEAK_FUNC(sub_8232B034);
PPC_FUNC_IMPL(__imp__sub_8232B034) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232B038"))) PPC_WEAK_FUNC(sub_8232B038);
PPC_FUNC_IMPL(__imp__sub_8232B038) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x8232B040;
	__restfpr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r28,-32183
	ctx.r28.s64 = -2109145088;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// lwz r3,21276(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 21276);
	// bl 0x8232b660
	ctx.lr = 0x8232B05C;
	sub_8232B660(ctx, base);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x8232b078
	if (!ctx.cr6.eq) goto loc_8232B078;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,488
	ctx.r3.s64 = ctx.r11.s64 + 488;
loc_8232B06C:
	// bl 0x8232b878
	ctx.lr = 0x8232B070;
	sub_8232B878(ctx, base);
loc_8232B070:
	// li r31,-1
	ctx.r31.s64 = -1;
	// b 0x8232b0ec
	goto loc_8232B0EC;
loc_8232B078:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232b090
	if (!ctx.cr6.eq) goto loc_8232B090;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,460
	ctx.r3.s64 = ctx.r11.s64 + 460;
	// b 0x8232b06c
	goto loc_8232B06C;
loc_8232B090:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232b070
	if (!ctx.cr6.eq) goto loc_8232B070;
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// bl 0x8232b660
	ctx.lr = 0x8232B0A4;
	sub_8232B660(ctx, base);
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// lis r9,32767
	ctx.r9.s64 = 2147418112;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r30,52(r31)
	PPC_STORE_U32(ctx.r31.u32 + 52, ctx.r30.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r29,56(r31)
	PPC_STORE_U32(ctx.r31.u32 + 56, ctx.r29.u32);
	// ori r9,r9,65535
	ctx.r9.u64 = ctx.r9.u64 | 65535;
	// stw r10,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r10.u32);
	// stw r11,60(r31)
	PPC_STORE_U32(ctx.r31.u32 + 60, ctx.r11.u32);
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// bne cr6,0x8232b0d4
	if (!ctx.cr6.eq) goto loc_8232B0D4;
	// stw r10,60(r31)
	PPC_STORE_U32(ctx.r31.u32 + 60, ctx.r10.u32);
loc_8232B0D4:
	// lwz r30,60(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// bl 0x8232b6f8
	ctx.lr = 0x8232B0E0;
	sub_8232B6F8(ctx, base);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x822471d8
	ctx.lr = 0x8232B0E8;
	sub_822471D8(ctx, base);
	// mr r31,r30
	ctx.r31.u64 = ctx.r30.u64;
loc_8232B0EC:
	// lwz r3,21276(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 21276);
	// bl 0x8232b6f8
	ctx.lr = 0x8232B0F4;
	sub_8232B6F8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232B100"))) PPC_WEAK_FUNC(sub_8232B100);
PPC_FUNC_IMPL(__imp__sub_8232B100) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x8232b12c
	if (!ctx.cr6.eq) goto loc_8232B12C;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,488
	ctx.r3.s64 = ctx.r11.s64 + 488;
loc_8232B124:
	// bl 0x8232b878
	ctx.lr = 0x8232B128;
	sub_8232B878(ctx, base);
	// b 0x8232b1dc
	goto loc_8232B1DC;
loc_8232B12C:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232b144
	if (!ctx.cr6.eq) goto loc_8232B144;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,460
	ctx.r3.s64 = ctx.r11.s64 + 460;
	// b 0x8232b124
	goto loc_8232B124;
loc_8232B144:
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// bne cr6,0x8232b1dc
	if (!ctx.cr6.eq) goto loc_8232B1DC;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x8232b1dc
	if (ctx.cr6.eq) goto loc_8232B1DC;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x822471d8
	ctx.lr = 0x8232B164;
	sub_822471D8(ctx, base);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232b180
	if (!ctx.cr6.eq) goto loc_8232B180;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,460
	ctx.r3.s64 = ctx.r11.s64 + 460;
	// bl 0x8232b878
	ctx.lr = 0x8232B17C;
	sub_8232B878(ctx, base);
	// b 0x8232b1a0
	goto loc_8232B1A0;
loc_8232B180:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x8232b194
	if (!ctx.cr6.eq) goto loc_8232B194;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x822471d8
	ctx.lr = 0x8232B194;
	sub_822471D8(ctx, base);
loc_8232B194:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x8232b1dc
	if (ctx.cr6.eq) goto loc_8232B1DC;
loc_8232B1A0:
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// bl 0x8232b660
	ctx.lr = 0x8232B1A8;
	sub_8232B660(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x8232b1d4
	if (!ctx.cr6.eq) goto loc_8232B1D4;
	// lwz r11,52(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8232b1cc
	if (ctx.cr6.eq) goto loc_8232B1CC;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x8232B1CC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_8232B1CC:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
loc_8232B1D4:
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// bl 0x8232b6f8
	ctx.lr = 0x8232B1DC;
	sub_8232B6F8(ctx, base);
loc_8232B1DC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B1F0"))) PPC_WEAK_FUNC(sub_8232B1F0);
PPC_FUNC_IMPL(__imp__sub_8232B1F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r30,-32183
	ctx.r30.s64 = -2109145088;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,21276(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21276);
	// bl 0x8232b660
	ctx.lr = 0x8232B214;
	sub_8232B660(ctx, base);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x8232b230
	if (!ctx.cr6.eq) goto loc_8232B230;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,488
	ctx.r3.s64 = ctx.r11.s64 + 488;
loc_8232B224:
	// bl 0x8232b878
	ctx.lr = 0x8232B228;
	sub_8232B878(ctx, base);
	// li r31,0
	ctx.r31.s64 = 0;
	// b 0x8232b24c
	goto loc_8232B24C;
loc_8232B230:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232b248
	if (!ctx.cr6.eq) goto loc_8232B248;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,460
	ctx.r3.s64 = ctx.r11.s64 + 460;
	// b 0x8232b224
	goto loc_8232B224;
loc_8232B248:
	// lwz r31,72(r31)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
loc_8232B24C:
	// lwz r3,21276(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21276);
	// bl 0x8232b6f8
	ctx.lr = 0x8232B254;
	sub_8232B6F8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B270"))) PPC_WEAK_FUNC(sub_8232B270);
PPC_FUNC_IMPL(__imp__sub_8232B270) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x8232B278;
	__restfpr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r29,-32183
	ctx.r29.s64 = -2109145088;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lwz r3,21276(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 21276);
	// bl 0x8232b660
	ctx.lr = 0x8232B290;
	sub_8232B660(ctx, base);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x8232b2a8
	if (!ctx.cr6.eq) goto loc_8232B2A8;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,488
	ctx.r3.s64 = ctx.r11.s64 + 488;
loc_8232B2A0:
	// bl 0x8232b878
	ctx.lr = 0x8232B2A4;
	sub_8232B878(ctx, base);
	// b 0x8232b2e4
	goto loc_8232B2E4;
loc_8232B2A8:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232b2c0
	if (!ctx.cr6.eq) goto loc_8232B2C0;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,460
	ctx.r3.s64 = ctx.r11.s64 + 460;
	// b 0x8232b2a0
	goto loc_8232B2A0;
loc_8232B2C0:
	// lwz r11,72(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// beq cr6,0x8232b2e4
	if (ctx.cr6.eq) goto loc_8232B2E4;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// bl 0x82246f00
	ctx.lr = 0x8232B2D8;
	sub_82246F00(ctx, base);
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// bl 0x82246f80
	ctx.lr = 0x8232B2E0;
	sub_82246F80(ctx, base);
	// stw r3,72(r31)
	PPC_STORE_U32(ctx.r31.u32 + 72, ctx.r3.u32);
loc_8232B2E4:
	// lwz r3,21276(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 21276);
	// bl 0x8232b6f8
	ctx.lr = 0x8232B2EC;
	sub_8232B6F8(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232B2F4"))) PPC_WEAK_FUNC(sub_8232B2F4);
PPC_FUNC_IMPL(__imp__sub_8232B2F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232B2F8"))) PPC_WEAK_FUNC(sub_8232B2F8);
PPC_FUNC_IMPL(__imp__sub_8232B2F8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x8232B300;
	__restfpr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r29,-32183
	ctx.r29.s64 = -2109145088;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lwz r3,21276(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 21276);
	// bl 0x8232b660
	ctx.lr = 0x8232B318;
	sub_8232B660(ctx, base);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x8232b330
	if (!ctx.cr6.eq) goto loc_8232B330;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,488
	ctx.r3.s64 = ctx.r11.s64 + 488;
loc_8232B328:
	// bl 0x8232b878
	ctx.lr = 0x8232B32C;
	sub_8232B878(ctx, base);
	// b 0x8232b368
	goto loc_8232B368;
loc_8232B330:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8232b348
	if (!ctx.cr6.eq) goto loc_8232B348;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,460
	ctx.r3.s64 = ctx.r11.s64 + 460;
	// b 0x8232b328
	goto loc_8232B328;
loc_8232B348:
	// lwz r11,76(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 76);
	// cmplw cr6,r11,r30
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x8232b368
	if (ctx.cr6.eq) goto loc_8232B368;
	// stw r30,80(r31)
	PPC_STORE_U32(ctx.r31.u32 + 80, ctx.r30.u32);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x822471d8
	ctx.lr = 0x8232B360;
	sub_822471D8(ctx, base);
	// li r3,10
	ctx.r3.s64 = 10;
	// bl 0x82247328
	ctx.lr = 0x8232B368;
	sub_82247328(ctx, base);
loc_8232B368:
	// lwz r3,21276(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 21276);
	// bl 0x8232b6f8
	ctx.lr = 0x8232B370;
	sub_8232B6F8(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232B378"))) PPC_WEAK_FUNC(sub_8232B378);
PPC_FUNC_IMPL(__imp__sub_8232B378) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x8232B380;
	__restfpr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r29,-32183
	ctx.r29.s64 = -2109145088;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lwz r3,21276(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 21276);
	// bl 0x8232b660
	ctx.lr = 0x8232B398;
	sub_8232B660(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8232aee8
	ctx.lr = 0x8232B3A4;
	sub_8232AEE8(ctx, base);
	// lwz r11,21276(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 21276);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// bl 0x8232b6f8
	ctx.lr = 0x8232B3B4;
	sub_8232B6F8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232B3C0"))) PPC_WEAK_FUNC(sub_8232B3C0);
PPC_FUNC_IMPL(__imp__sub_8232B3C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x8232B3C8;
	__restfpr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r29,-32183
	ctx.r29.s64 = -2109145088;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lwz r3,21276(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 21276);
	// bl 0x8232b660
	ctx.lr = 0x8232B3E0;
	sub_8232B660(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8232b100
	ctx.lr = 0x8232B3EC;
	sub_8232B100(ctx, base);
	// lwz r3,21276(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 21276);
	// bl 0x8232b6f8
	ctx.lr = 0x8232B3F4;
	sub_8232B6F8(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232B3FC"))) PPC_WEAK_FUNC(sub_8232B3FC);
PPC_FUNC_IMPL(__imp__sub_8232B3FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232B400"))) PPC_WEAK_FUNC(sub_8232B400);
PPC_FUNC_IMPL(__imp__sub_8232B400) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x8232B408;
	__restfpr_29(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r29,-32183
	ctx.r29.s64 = -2109145088;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,21276(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 21276);
	// bl 0x8232b660
	ctx.lr = 0x8232B41C;
	sub_8232B660(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x8232b434
	if (ctx.cr6.eq) goto loc_8232B434;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,60(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// bl 0x8232b100
	ctx.lr = 0x8232B434;
	sub_8232B100(ctx, base);
loc_8232B434:
	// li r30,1
	ctx.r30.s64 = 1;
	// b 0x8232b450
	goto loc_8232B450;
loc_8232B43C:
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,259
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 259, ctx.xer);
	// bne cr6,0x8232b470
	if (!ctx.cr6.eq) goto loc_8232B470;
	// li r3,10
	ctx.r3.s64 = 10;
	// bl 0x82247328
	ctx.lr = 0x8232B450;
	sub_82247328(ctx, base);
loc_8232B450:
	// stw r30,68(r31)
	PPC_STORE_U32(ctx.r31.u32 + 68, ctx.r30.u32);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x822471d8
	ctx.lr = 0x8232B45C;
	sub_822471D8(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// bl 0x82247000
	ctx.lr = 0x8232B468;
	sub_82247000(ctx, base);
	// cmpwi r3,0
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne 0x8232b43c
	if (!ctx.cr0.eq) goto loc_8232B43C;
loc_8232B470:
	// li r4,-1
	ctx.r4.s64 = -1;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// bl 0x82247250
	ctx.lr = 0x8232B47C;
	sub_82247250(ctx, base);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x82247218
	ctx.lr = 0x8232B484;
	sub_82247218(ctx, base);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// li r30,0
	ctx.r30.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8232b49c
	if (ctx.cr6.eq) goto loc_8232B49C;
	// bl 0x82246e18
	ctx.lr = 0x8232B498;
	sub_82246E18(ctx, base);
	// stw r30,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r30.u32);
loc_8232B49C:
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8232b4b0
	if (ctx.cr6.eq) goto loc_8232B4B0;
	// bl 0x8232b5d0
	ctx.lr = 0x8232B4AC;
	sub_8232B5D0(ctx, base);
	// stw r30,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r30.u32);
loc_8232B4B0:
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8232b4c4
	if (ctx.cr6.eq) goto loc_8232B4C4;
	// bl 0x82246e18
	ctx.lr = 0x8232B4C0;
	sub_82246E18(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r30.u32);
loc_8232B4C4:
	// li r5,84
	ctx.r5.s64 = 84;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x8232B4D4;
	sub_8233EAF0(ctx, base);
	// lwz r3,21276(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 21276);
	// bl 0x8232b6f8
	ctx.lr = 0x8232B4DC;
	sub_8232B6F8(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232B4E4"))) PPC_WEAK_FUNC(sub_8232B4E4);
PPC_FUNC_IMPL(__imp__sub_8232B4E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232B4E8"))) PPC_WEAK_FUNC(sub_8232B4E8);
PPC_FUNC_IMPL(__imp__sub_8232B4E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lwz r17,-30396(r24)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r24.u32 + -30396);
	// lwz r16,-13512(r6)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r6.u32 + -13512);
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r31,r1,-112
	ctx.r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// addi r11,r11,752
	ctx.r11.s64 = ctx.r11.s64 + 752;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r11,21284(r10)
	PPC_STORE_U32(ctx.r10.u32 + 21284, ctx.r11.u32);
	// bne cr6,0x8232b534
	if (!ctx.cr6.eq) goto loc_8232B534;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,936
	ctx.r3.s64 = ctx.r11.s64 + 936;
loc_8232B528:
	// bl 0x8232b878
	ctx.lr = 0x8232B52C;
	sub_8232B878(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8232b5a0
	// ERROR 8232B5A0
	return;
loc_8232B534:
	// cmplwi cr6,r4,32
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 32, ctx.xer);
	// bge cr6,0x8232b548
	if (!ctx.cr6.lt) goto loc_8232B548;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,888
	ctx.r3.s64 = ctx.r11.s64 + 888;
	// b 0x8232b528
	goto loc_8232B528;
loc_8232B548:
	// addi r11,r3,3
	ctx.r11.s64 = ctx.r3.s64 + 3;
	// li r5,28
	ctx.r5.s64 = 28;
	// rlwinm r30,r11,0,0,29
	ctx.r30.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r30,80(r31)
	PPC_STORE_U32(ctx.r31.u32 + 80, ctx.r30.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x8232B564;
	sub_8233EAF0(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82388bc4
	ctx.lr = 0x8232B574;
	__imp__RtlInitializeCriticalSection(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x8232b59c
	sub_8232B59C(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232B4F0"))) PPC_WEAK_FUNC(sub_8232B4F0);
PPC_FUNC_IMPL(__imp__sub_8232B4F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r31,r1,-112
	ctx.r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// addi r11,r11,752
	ctx.r11.s64 = ctx.r11.s64 + 752;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r11,21284(r10)
	PPC_STORE_U32(ctx.r10.u32 + 21284, ctx.r11.u32);
	// bne cr6,0x8232b534
	if (!ctx.cr6.eq) goto loc_8232B534;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,936
	ctx.r3.s64 = ctx.r11.s64 + 936;
loc_8232B528:
	// bl 0x8232b878
	ctx.lr = 0x8232B52C;
	sub_8232B878(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8232b5a0
	goto loc_8232B5A0;
loc_8232B534:
	// cmplwi cr6,r4,32
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 32, ctx.xer);
	// bge cr6,0x8232b548
	if (!ctx.cr6.lt) goto loc_8232B548;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,888
	ctx.r3.s64 = ctx.r11.s64 + 888;
	// b 0x8232b528
	goto loc_8232B528;
loc_8232B548:
	// addi r11,r3,3
	ctx.r11.s64 = ctx.r3.s64 + 3;
	// li r5,28
	ctx.r5.s64 = 28;
	// rlwinm r30,r11,0,0,29
	ctx.r30.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r30,80(r31)
	PPC_STORE_U32(ctx.r31.u32 + 80, ctx.r30.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x8232B564;
	sub_8233EAF0(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82388bc4
	ctx.lr = 0x8232B574;
	__imp__RtlInitializeCriticalSection(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x8232b59c
	goto loc_8232B59C;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,836
	ctx.r3.s64 = ctx.r11.s64 + 836;
	// bl 0x8232b878
	ctx.lr = 0x8232B590;
	sub_8232B878(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r30,80(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// b 0x8232b5a0
	goto loc_8232B5A0;
loc_8232B59C:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
loc_8232B5A0:
	// addi r1,r31,112
	ctx.r1.s64 = ctx.r31.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B584"))) PPC_WEAK_FUNC(sub_8232B584);
PPC_FUNC_IMPL(__imp__sub_8232B584) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,836
	ctx.r3.s64 = ctx.r11.s64 + 836;
	// bl 0x8232b878
	ctx.lr = 0x8232B590;
	sub_8232B878(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r30,80(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// b 0x8232b5a0
	// ERROR 8232B5A0
	return;
}

__attribute__((alias("__imp__sub_8232B59C"))) PPC_WEAK_FUNC(sub_8232B59C);
PPC_FUNC_IMPL(__imp__sub_8232B59C) {
	PPC_FUNC_PROLOGUE();
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r31,112
	ctx.r1.s64 = ctx.r31.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B5B8"))) PPC_WEAK_FUNC(sub_8232B5B8);
PPC_FUNC_IMPL(__imp__sub_8232B5B8) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B5C4"))) PPC_WEAK_FUNC(sub_8232B5C4);
PPC_FUNC_IMPL(__imp__sub_8232B5C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232B5C8"))) PPC_WEAK_FUNC(sub_8232B5C8);
PPC_FUNC_IMPL(__imp__sub_8232B5C8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lwz r17,-30396(r24)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r24.u32 + -30396);
	// lwz r16,-13488(r6)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r6.u32 + -13488);
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r31,r1,-96
	ctx.r31.s64 = ctx.r1.s64 + -96;
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r3,116(r31)
	PPC_STORE_U32(ctx.r31.u32 + 116, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x8232b600
	if (!ctx.cr6.eq) goto loc_8232B600;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,1032
	ctx.r3.s64 = ctx.r11.s64 + 1032;
	// bl 0x8232b878
	ctx.lr = 0x8232B5FC;
	sub_8232B878(ctx, base);
	// b 0x8232b638
	// ERROR 8232B638
	return;
loc_8232B600:
	// nop 
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x8232b62c
	// ERROR 8232B62C
	return;
}

__attribute__((alias("__imp__sub_8232B5D0"))) PPC_WEAK_FUNC(sub_8232B5D0);
PPC_FUNC_IMPL(__imp__sub_8232B5D0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r31,r1,-96
	ctx.r31.s64 = ctx.r1.s64 + -96;
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r3,116(r31)
	PPC_STORE_U32(ctx.r31.u32 + 116, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x8232b600
	if (!ctx.cr6.eq) goto loc_8232B600;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,1032
	ctx.r3.s64 = ctx.r11.s64 + 1032;
	// bl 0x8232b878
	ctx.lr = 0x8232B5FC;
	sub_8232B878(ctx, base);
	// b 0x8232b638
	goto loc_8232B638;
loc_8232B600:
	// nop 
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x8232b62c
	goto loc_8232B62C;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,984
	ctx.r3.s64 = ctx.r11.s64 + 984;
	// bl 0x8232b878
	ctx.lr = 0x8232B628;
	sub_8232B878(ctx, base);
	// lwz r3,116(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 116);
loc_8232B62C:
	// li r5,28
	ctx.r5.s64 = 28;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8233eaf0
	ctx.lr = 0x8232B638;
	sub_8233EAF0(ctx, base);
loc_8232B638:
	// addi r1,r31,96
	ctx.r1.s64 = ctx.r31.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B61C"))) PPC_WEAK_FUNC(sub_8232B61C);
PPC_FUNC_IMPL(__imp__sub_8232B61C) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,984
	ctx.r3.s64 = ctx.r11.s64 + 984;
	// bl 0x8232b878
	ctx.lr = 0x8232B628;
	sub_8232B878(ctx, base);
	// lwz r3,116(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	// li r5,28
	ctx.r5.s64 = 28;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8233eaf0
	ctx.lr = 0x8232B638;
	sub_8233EAF0(ctx, base);
	// addi r1,r31,96
	ctx.r1.s64 = ctx.r31.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B64C"))) PPC_WEAK_FUNC(sub_8232B64C);
PPC_FUNC_IMPL(__imp__sub_8232B64C) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B658"))) PPC_WEAK_FUNC(sub_8232B658);
PPC_FUNC_IMPL(__imp__sub_8232B658) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lwz r17,-30396(r24)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r24.u32 + -30396);
	// lwz r16,-13464(r6)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r6.u32 + -13464);
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r31,r1,-112
	ctx.r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r30,0
	ctx.r30.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x8232b698
	if (!ctx.cr6.eq) goto loc_8232B698;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,1120
	ctx.r3.s64 = ctx.r11.s64 + 1120;
	// bl 0x8232b878
	ctx.lr = 0x8232B690;
	sub_8232B878(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x8232b6cc
	// ERROR 8232B6CC
	return;
loc_8232B698:
	// nop 
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// bl 0x82388aa4
	ctx.lr = 0x8232B6A8;
	__imp__RtlEnterCriticalSection(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x8232b6c8
	// ERROR 8232B6C8
	return;
}

__attribute__((alias("__imp__sub_8232B660"))) PPC_WEAK_FUNC(sub_8232B660);
PPC_FUNC_IMPL(__imp__sub_8232B660) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r31,r1,-112
	ctx.r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r30,0
	ctx.r30.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x8232b698
	if (!ctx.cr6.eq) goto loc_8232B698;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,1120
	ctx.r3.s64 = ctx.r11.s64 + 1120;
	// bl 0x8232b878
	ctx.lr = 0x8232B690;
	sub_8232B878(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x8232b6cc
	goto loc_8232B6CC;
loc_8232B698:
	// nop 
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// bl 0x82388aa4
	ctx.lr = 0x8232B6A8;
	__imp__RtlEnterCriticalSection(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x8232b6c8
	goto loc_8232B6C8;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,1076
	ctx.r3.s64 = ctx.r11.s64 + 1076;
	// bl 0x8232b878
	ctx.lr = 0x8232B6C4;
	sub_8232B878(ctx, base);
	// li r30,-1
	ctx.r30.s64 = -1;
loc_8232B6C8:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
loc_8232B6CC:
	// addi r1,r31,112
	ctx.r1.s64 = ctx.r31.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B6B8"))) PPC_WEAK_FUNC(sub_8232B6B8);
PPC_FUNC_IMPL(__imp__sub_8232B6B8) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,1076
	ctx.r3.s64 = ctx.r11.s64 + 1076;
	// bl 0x8232b878
	ctx.lr = 0x8232B6C4;
	sub_8232B878(ctx, base);
	// li r30,-1
	ctx.r30.s64 = -1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r31,112
	ctx.r1.s64 = ctx.r31.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B6E4"))) PPC_WEAK_FUNC(sub_8232B6E4);
PPC_FUNC_IMPL(__imp__sub_8232B6E4) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B6F0"))) PPC_WEAK_FUNC(sub_8232B6F0);
PPC_FUNC_IMPL(__imp__sub_8232B6F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lwz r17,-30396(r24)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r24.u32 + -30396);
	// lwz r16,-13440(r6)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r6.u32 + -13440);
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r31,r1,-112
	ctx.r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r30,0
	ctx.r30.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x8232b730
	if (!ctx.cr6.eq) goto loc_8232B730;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,1204
	ctx.r3.s64 = ctx.r11.s64 + 1204;
	// bl 0x8232b878
	ctx.lr = 0x8232B728;
	sub_8232B878(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x8232b764
	// ERROR 8232B764
	return;
loc_8232B730:
	// nop 
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// bl 0x82388a64
	ctx.lr = 0x8232B740;
	__imp__RtlLeaveCriticalSection(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x8232b760
	// ERROR 8232B760
	return;
}

__attribute__((alias("__imp__sub_8232B6F8"))) PPC_WEAK_FUNC(sub_8232B6F8);
PPC_FUNC_IMPL(__imp__sub_8232B6F8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r31,r1,-112
	ctx.r31.s64 = ctx.r1.s64 + -112;
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r30,0
	ctx.r30.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x8232b730
	if (!ctx.cr6.eq) goto loc_8232B730;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,1204
	ctx.r3.s64 = ctx.r11.s64 + 1204;
	// bl 0x8232b878
	ctx.lr = 0x8232B728;
	sub_8232B878(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x8232b764
	goto loc_8232B764;
loc_8232B730:
	// nop 
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// bl 0x82388a64
	ctx.lr = 0x8232B740;
	__imp__RtlLeaveCriticalSection(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x8232b760
	goto loc_8232B760;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,1160
	ctx.r3.s64 = ctx.r11.s64 + 1160;
	// bl 0x8232b878
	ctx.lr = 0x8232B75C;
	sub_8232B878(ctx, base);
	// li r30,-1
	ctx.r30.s64 = -1;
loc_8232B760:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
loc_8232B764:
	// addi r1,r31,112
	ctx.r1.s64 = ctx.r31.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B750"))) PPC_WEAK_FUNC(sub_8232B750);
PPC_FUNC_IMPL(__imp__sub_8232B750) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,1160
	ctx.r3.s64 = ctx.r11.s64 + 1160;
	// bl 0x8232b878
	ctx.lr = 0x8232B75C;
	sub_8232B878(ctx, base);
	// li r30,-1
	ctx.r30.s64 = -1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r31,112
	ctx.r1.s64 = ctx.r31.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B77C"))) PPC_WEAK_FUNC(sub_8232B77C);
PPC_FUNC_IMPL(__imp__sub_8232B77C) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B788"))) PPC_WEAK_FUNC(sub_8232B788);
PPC_FUNC_IMPL(__imp__sub_8232B788) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// stw r11,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r11.u32);
	// rotlwi r11,r11,0
	ctx.r11.u64 = rotl32(ctx.r11.u32, 0);
	// stw r11,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r11.u32);
	// lwz r11,4(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// ble cr6,0x8232b7b0
	if (!ctx.cr6.gt) goto loc_8232B7B0;
	// stw r4,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r4.u32);
loc_8232B7B0:
	// lwz r11,4(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// lwz r10,4(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// subf. r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r11.u32);
	// bne 0x8232b7cc
	if (!ctx.cr0.eq) goto loc_8232B7CC;
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x8232b7d8
	goto loc_8232B7D8;
loc_8232B7CC:
	// lwz r10,4(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// lwz r11,0(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_8232B7D8:
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B7E0"))) PPC_WEAK_FUNC(sub_8232B7E0);
PPC_FUNC_IMPL(__imp__sub_8232B7E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x8232b81c
	if (!ctx.cr6.eq) goto loc_8232B81C;
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r10,r11,21904
	ctx.r10.s64 = ctx.r11.s64 + 21904;
	// lwz r11,21904(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21904);
	// lwz r3,4(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x8232B818;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// b 0x8232b864
	goto loc_8232B864;
loc_8232B81C:
	// lis r11,-32171
	ctx.r11.s64 = -2108358656;
	// li r6,255
	ctx.r6.s64 = 255;
	// addi r31,r11,11008
	ctx.r31.s64 = ctx.r11.s64 + 11008;
	// li r4,256
	ctx.r4.s64 = 256;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82327d40
	ctx.lr = 0x8232B834;
	sub_82327D40(ctx, base);
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// addi r11,r11,21904
	ctx.r11.s64 = ctx.r11.s64 + 21904;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8232b85c
	if (ctx.cr6.eq) goto loc_8232B85C;
	// rotlwi r10,r10,0
	ctx.r10.u64 = rotl32(ctx.r10.u32, 0);
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8232B85C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_8232B85C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8232b878
	ctx.lr = 0x8232B864;
	sub_8232B878(ctx, base);
loc_8232B864:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B878"))) PPC_WEAK_FUNC(sub_8232B878);
PPC_FUNC_IMPL(__imp__sub_8232B878) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r11,r11,22284
	ctx.r11.s64 = ctx.r11.s64 + 22284;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmpwi cr6,r10,-1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, -1, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// lis r10,-32198
	ctx.r10.s64 = -2110128128;
	// lwz r3,-4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// lwz r11,-21272(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -21272);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__sub_8232B8A4"))) PPC_WEAK_FUNC(sub_8232B8A4);
PPC_FUNC_IMPL(__imp__sub_8232B8A4) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B8A8"))) PPC_WEAK_FUNC(sub_8232B8A8);
PPC_FUNC_IMPL(__imp__sub_8232B8A8) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// addi r9,r11,1816
	ctx.r9.s64 = ctx.r11.s64 + 1816;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// stw r9,22288(r10)
	PPC_STORE_U32(ctx.r10.u32 + 22288, ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B8C0"))) PPC_WEAK_FUNC(sub_8232B8C0);
PPC_FUNC_IMPL(__imp__sub_8232B8C0) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// stw r3,22292(r11)
	PPC_STORE_U32(ctx.r11.u32 + 22292, ctx.r3.u32);
	// stw r4,22296(r10)
	PPC_STORE_U32(ctx.r10.u32 + 22296, ctx.r4.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B8D4"))) PPC_WEAK_FUNC(sub_8232B8D4);
PPC_FUNC_IMPL(__imp__sub_8232B8D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8232B8D8"))) PPC_WEAK_FUNC(sub_8232B8D8);
PPC_FUNC_IMPL(__imp__sub_8232B8D8) {
	PPC_FUNC_PROLOGUE();
	// lis r10,-32171
	ctx.r10.s64 = -2108358656;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r9,r10,10976
	ctx.r9.s64 = ctx.r10.s64 + 10976;
	// addi r11,r11,1972
	ctx.r11.s64 = ctx.r11.s64 + 1972;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8232B8F0"))) PPC_WEAK_FUNC(sub_8232B8F0);
PPC_FUNC_IMPL(__imp__sub_8232B8F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x8232B8F8;
	__restfpr_14(ctx, base);
	// lwz r31,4352(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4352);
	// lwz r6,4360(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4360);
	// lwz r7,4356(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4356);
	// rlwinm r11,r31,16,16,31
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 16) & 0xFFFF;
	// lwz r9,4364(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4364);
	// cmpwi cr6,r6,16
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 16, ctx.xer);
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// ble cr6,0x8232b924
	if (!ctx.cr6.gt) goto loc_8232B924;
	// subfic r10,r6,48
	ctx.xer.ca = ctx.r6.u32 <= 48;
	ctx.r10.s64 = 48 - ctx.r6.s64;
	// srw r8,r7,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r10.u8 & 0x3F));
	// or r11,r8,r11
	ctx.r11.u64 = ctx.r8.u64 | ctx.r11.u64;
loc_8232B924:
	// lwz r10,44(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	// rlwinm r8,r11,23,9,31
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 23) & 0x7FFFFF;
	// lbzx r10,r8,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// rlwinm r5,r10,28,4,31
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r10,r10,28
	ctx.r10.u64 = ctx.r10.u32 & 0xF;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// beq cr6,0x8232b988
	if (ctx.cr6.eq) goto loc_8232B988;
	// lwz r8,4400(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4400);
	// rlwinm r30,r10,1,0,30
	ctx.r30.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// li r28,1
	ctx.r28.s64 = 1;
	// subfic r29,r10,16
	ctx.xer.ca = ctx.r10.u32 <= 16;
	ctx.r29.s64 = 16 - ctx.r10.s64;
	// lhzx r30,r8,r30
	ctx.r30.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r30.u32);
	// slw r8,r28,r5
	ctx.r8.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r28.u32 << (ctx.r5.u8 & 0x3F));
	// extsh r5,r30
	ctx.r5.s64 = ctx.r30.s16;
	// and r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 & ctx.r11.u64;
	// srw r11,r11,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r11.u32 >> (ctx.r29.u8 & 0x3F));
	// and r5,r8,r11
	ctx.r5.u64 = ctx.r8.u64 & ctx.r11.u64;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// bne cr6,0x8232b984
	if (!ctx.cr6.eq) goto loc_8232B984;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r8,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r8.s64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
loc_8232B984:
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
loc_8232B988:
	// add r11,r10,r6
	ctx.r11.u64 = ctx.r10.u64 + ctx.r6.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x8232b9a8
	if (ctx.cr6.lt) goto loc_8232B9A8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232b9ac
	goto loc_8232B9AC;
loc_8232B9A8:
	// slw r6,r31,r10
	ctx.r6.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r10.u8 & 0x3F));
loc_8232B9AC:
	// lwz r30,40(r4)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	// li r31,0
	ctx.r31.s64 = 0;
	// lis r24,-1
	ctx.r24.s64 = -65536;
	// lis r21,-1
	ctx.r21.s64 = -65536;
	// lis r26,-1
	ctx.r26.s64 = -65536;
	// lis r23,-1
	ctx.r23.s64 = -65536;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// ori r22,r24,10
	ctx.r22.u64 = ctx.r24.u64 | 10;
	// ori r24,r21,8
	ctx.r24.u64 = ctx.r21.u64 | 8;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// addi r8,r4,20
	ctx.r8.s64 = ctx.r4.s64 + 20;
	// stw r10,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r10.u32);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// li r29,2
	ctx.r29.s64 = 2;
	// std r10,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r10.u64);
	// lfd f0,-160(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// li r27,4
	ctx.r27.s64 = 4;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// stfs f12,0(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// stw r31,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, ctx.r31.u32);
	// lis r5,-1
	ctx.r5.s64 = -65536;
	// stw r31,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, ctx.r31.u32);
	// lis r31,-1
	ctx.r31.s64 = -65536;
	// lwz r10,4396(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4396);
	// ori r30,r5,2
	ctx.r30.u64 = ctx.r5.u64 | 2;
	// ori r28,r31,4
	ctx.r28.u64 = ctx.r31.u64 | 4;
	// ori r26,r26,6
	ctx.r26.u64 = ctx.r26.u64 | 6;
	// li r25,6
	ctx.r25.s64 = 6;
	// ori r23,r23,12
	ctx.r23.u64 = ctx.r23.u64 | 12;
	// li r16,20
	ctx.r16.s64 = 20;
	// li r17,11
	ctx.r17.s64 = 11;
	// li r18,13
	ctx.r18.s64 = 13;
	// li r19,14
	ctx.r19.s64 = 14;
	// li r20,15
	ctx.r20.s64 = 15;
	// li r21,17
	ctx.r21.s64 = 17;
loc_8232BA40:
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x8232ba58
	if (ctx.cr6.eq) goto loc_8232BA58;
	// subfic r5,r11,32
	ctx.xer.ca = ctx.r11.u32 <= 32;
	ctx.r5.s64 = 32 - ctx.r11.s64;
	// srw r5,r7,r5
	ctx.r5.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r5.u8 & 0x3F));
	// or r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 | ctx.r6.u64;
loc_8232BA58:
	// rlwinm r31,r5,9,23,31
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 9) & 0x1FF;
	// cmplwi cr6,r31,511
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 511, ctx.xer);
	// bgt cr6,0x8232f028
	if (ctx.cr6.gt) goto loc_8232F028;
	// lis r12,-32205
	ctx.r12.s64 = -2110586880;
	// rlwinm r0,r31,2,0,29
	ctx.r0.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r12,r12,-17796
	ctx.r12.s64 = ctx.r12.s64 + -17796;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr 
	switch (ctx.r31.u64) {
	case 0:
		goto loc_8232E894;
	case 1:
		goto loc_8232E840;
	case 2:
		goto loc_8232E7EC;
	case 3:
		goto loc_8232E7EC;
	case 4:
		goto loc_8232E754;
	case 5:
		goto loc_8232E754;
	case 6:
		goto loc_8232E754;
	case 7:
		goto loc_8232E754;
	case 8:
		goto loc_8232E694;
	case 9:
		goto loc_8232E694;
	case 10:
		goto loc_8232E694;
	case 11:
		goto loc_8232E694;
	case 12:
		goto loc_8232E694;
	case 13:
		goto loc_8232E694;
	case 14:
		goto loc_8232E694;
	case 15:
		goto loc_8232E694;
	case 16:
		goto loc_8232E654;
	case 17:
		goto loc_8232E654;
	case 18:
		goto loc_8232E61C;
	case 19:
		goto loc_8232E61C;
	case 20:
		goto loc_8232E5E4;
	case 21:
		goto loc_8232E5E4;
	case 22:
		goto loc_8232E5AC;
	case 23:
		goto loc_8232E5AC;
	case 24:
		goto loc_8232E570;
	case 25:
		goto loc_8232E570;
	case 26:
		goto loc_8232E538;
	case 27:
		goto loc_8232E538;
	case 28:
		goto loc_8232E500;
	case 29:
		goto loc_8232E500;
	case 30:
		goto loc_8232E4C8;
	case 31:
		goto loc_8232E4C8;
	case 32:
		goto loc_8232E488;
	case 33:
		goto loc_8232E488;
	case 34:
		goto loc_8232EFEC;
	case 35:
		goto loc_8232E488;
	case 36:
		goto loc_8232E450;
	case 37:
		goto loc_8232E450;
	case 38:
		goto loc_8232EFD0;
	case 39:
		goto loc_8232E450;
	case 40:
		goto loc_8232E418;
	case 41:
		goto loc_8232E418;
	case 42:
		goto loc_8232EFC8;
	case 43:
		goto loc_8232E418;
	case 44:
		goto loc_8232E3E0;
	case 45:
		goto loc_8232E3E0;
	case 46:
		goto loc_8232EFAC;
	case 47:
		goto loc_8232E3E0;
	case 48:
		goto loc_8232E3A8;
	case 49:
		goto loc_8232E3A8;
	case 50:
		goto loc_8232EF90;
	case 51:
		goto loc_8232E3A8;
	case 52:
		goto loc_8232E370;
	case 53:
		goto loc_8232E370;
	case 54:
		goto loc_8232EF74;
	case 55:
		goto loc_8232E370;
	case 56:
		goto loc_8232E338;
	case 57:
		goto loc_8232E338;
	case 58:
		goto loc_8232EF6C;
	case 59:
		goto loc_8232E338;
	case 60:
		goto loc_8232E300;
	case 61:
		goto loc_8232E300;
	case 62:
		goto loc_8232EF50;
	case 63:
		goto loc_8232E300;
	case 64:
		goto loc_8232E2C8;
	case 65:
		goto loc_8232E290;
	case 66:
		goto loc_8232E254;
	case 67:
		goto loc_8232E21C;
	case 68:
		goto loc_8232E1E4;
	case 69:
		goto loc_8232E1AC;
	case 70:
		goto loc_8232E174;
	case 71:
		goto loc_8232E13C;
	case 72:
		goto loc_8232E104;
	case 73:
		goto loc_8232E0CC;
	case 74:
		goto loc_8232E094;
	case 75:
		goto loc_8232E05C;
	case 76:
		goto loc_8232E020;
	case 77:
		goto loc_8232DFE8;
	case 78:
		goto loc_8232DFB0;
	case 79:
		goto loc_8232DF78;
	case 80:
		goto loc_8232DF38;
	case 81:
		goto loc_8232DF38;
	case 82:
		goto loc_8232DF38;
	case 83:
		goto loc_8232DF38;
	case 84:
		goto loc_8232EF18;
	case 85:
		goto loc_8232EF18;
	case 86:
		goto loc_8232DEE8;
	case 87:
		goto loc_8232DE98;
	case 88:
		goto loc_8232DE60;
	case 89:
		goto loc_8232DE60;
	case 90:
		goto loc_8232DE60;
	case 91:
		goto loc_8232DE60;
	case 92:
		goto loc_8232EEE0;
	case 93:
		goto loc_8232EEE0;
	case 94:
		goto loc_8232DE10;
	case 95:
		goto loc_8232DDC0;
	case 96:
		goto loc_8232DD88;
	case 97:
		goto loc_8232DD88;
	case 98:
		goto loc_8232DD88;
	case 99:
		goto loc_8232DD88;
	case 100:
		goto loc_8232EEA8;
	case 101:
		goto loc_8232EEA8;
	case 102:
		goto loc_8232DD38;
	case 103:
		goto loc_8232DCE8;
	case 104:
		goto loc_8232DCB0;
	case 105:
		goto loc_8232DCB0;
	case 106:
		goto loc_8232DCB0;
	case 107:
		goto loc_8232DCB0;
	case 108:
		goto loc_8232EE70;
	case 109:
		goto loc_8232EE70;
	case 110:
		goto loc_8232DC60;
	case 111:
		goto loc_8232DC10;
	case 112:
		goto loc_8232DBD8;
	case 113:
		goto loc_8232DBD8;
	case 114:
		goto loc_8232DBD8;
	case 115:
		goto loc_8232DBD8;
	case 116:
		goto loc_8232EE38;
	case 117:
		goto loc_8232EE38;
	case 118:
		goto loc_8232DB88;
	case 119:
		goto loc_8232DB38;
	case 120:
		goto loc_8232DB00;
	case 121:
		goto loc_8232DB00;
	case 122:
		goto loc_8232DB00;
	case 123:
		goto loc_8232DB00;
	case 124:
		goto loc_8232EE00;
	case 125:
		goto loc_8232EE00;
	case 126:
		goto loc_8232DAB0;
	case 127:
		goto loc_8232DA60;
	case 128:
		goto loc_8232DA20;
	case 129:
		goto loc_8232DA20;
	case 130:
		goto loc_8232DA20;
	case 131:
		goto loc_8232DA20;
	case 132:
		goto loc_8232DA20;
	case 133:
		goto loc_8232DA20;
	case 134:
		goto loc_8232D9D0;
	case 135:
		goto loc_8232D980;
	case 136:
		goto loc_8232EDC8;
	case 137:
		goto loc_8232EDC8;
	case 138:
		goto loc_8232EDC8;
	case 139:
		goto loc_8232EDC8;
	case 140:
		goto loc_8232D930;
	case 141:
		goto loc_8232D930;
	case 142:
		goto loc_8232D8E0;
	case 143:
		goto loc_8232D8E0;
	case 144:
		goto loc_8232D8A8;
	case 145:
		goto loc_8232D8A8;
	case 146:
		goto loc_8232D8A8;
	case 147:
		goto loc_8232D8A8;
	case 148:
		goto loc_8232D8A8;
	case 149:
		goto loc_8232D8A8;
	case 150:
		goto loc_8232D858;
	case 151:
		goto loc_8232D808;
	case 152:
		goto loc_8232ED90;
	case 153:
		goto loc_8232ED90;
	case 154:
		goto loc_8232ED90;
	case 155:
		goto loc_8232ED90;
	case 156:
		goto loc_8232D7B8;
	case 157:
		goto loc_8232D7B8;
	case 158:
		goto loc_8232D768;
	case 159:
		goto loc_8232D768;
	case 160:
		goto loc_8232D730;
	case 161:
		goto loc_8232D730;
	case 162:
		goto loc_8232D730;
	case 163:
		goto loc_8232D730;
	case 164:
		goto loc_8232D730;
	case 165:
		goto loc_8232D730;
	case 166:
		goto loc_8232D6E0;
	case 167:
		goto loc_8232D690;
	case 168:
		goto loc_8232ED58;
	case 169:
		goto loc_8232ED58;
	case 170:
		goto loc_8232ED58;
	case 171:
		goto loc_8232ED58;
	case 172:
		goto loc_8232D640;
	case 173:
		goto loc_8232D640;
	case 174:
		goto loc_8232D5F0;
	case 175:
		goto loc_8232D5F0;
	case 176:
		goto loc_8232D5B8;
	case 177:
		goto loc_8232D5B8;
	case 178:
		goto loc_8232D5B8;
	case 179:
		goto loc_8232D5B8;
	case 180:
		goto loc_8232D5B8;
	case 181:
		goto loc_8232D5B8;
	case 182:
		goto loc_8232D568;
	case 183:
		goto loc_8232D518;
	case 184:
		goto loc_8232ED18;
	case 185:
		goto loc_8232ED18;
	case 186:
		goto loc_8232ED18;
	case 187:
		goto loc_8232ED18;
	case 188:
		goto loc_8232D4C8;
	case 189:
		goto loc_8232D4C8;
	case 190:
		goto loc_8232D478;
	case 191:
		goto loc_8232D478;
	case 192:
		goto loc_8232D438;
	case 193:
		goto loc_8232D438;
	case 194:
		goto loc_8232D438;
	case 195:
		goto loc_8232D438;
	case 196:
		goto loc_8232D438;
	case 197:
		goto loc_8232D438;
	case 198:
		goto loc_8232D438;
	case 199:
		goto loc_8232D438;
	case 200:
		goto loc_8232D3E8;
	case 201:
		goto loc_8232D398;
	case 202:
		goto loc_8232D348;
	case 203:
		goto loc_8232D2F8;
	case 204:
		goto loc_8232D2A8;
	case 205:
		goto loc_8232D2A8;
	case 206:
		goto loc_8232D258;
	case 207:
		goto loc_8232D258;
	case 208:
		goto loc_8232ECE0;
	case 209:
		goto loc_8232ECE0;
	case 210:
		goto loc_8232ECE0;
	case 211:
		goto loc_8232ECE0;
	case 212:
		goto loc_8232ECE0;
	case 213:
		goto loc_8232ECE0;
	case 214:
		goto loc_8232ECE0;
	case 215:
		goto loc_8232ECE0;
	case 216:
		goto loc_8232D208;
	case 217:
		goto loc_8232D208;
	case 218:
		goto loc_8232ECC0;
	case 219:
		goto loc_8232D208;
	case 220:
		goto loc_8232D1B8;
	case 221:
		goto loc_8232D1B8;
	case 222:
		goto loc_8232EC8C;
	case 223:
		goto loc_8232D1B8;
	case 224:
		goto loc_8232D180;
	case 225:
		goto loc_8232D180;
	case 226:
		goto loc_8232D180;
	case 227:
		goto loc_8232D180;
	case 228:
		goto loc_8232D180;
	case 229:
		goto loc_8232D180;
	case 230:
		goto loc_8232D180;
	case 231:
		goto loc_8232D180;
	case 232:
		goto loc_8232D130;
	case 233:
		goto loc_8232D0E0;
	case 234:
		goto loc_8232D090;
	case 235:
		goto loc_8232D040;
	case 236:
		goto loc_8232CFF0;
	case 237:
		goto loc_8232CFF0;
	case 238:
		goto loc_8232CFA0;
	case 239:
		goto loc_8232CFA0;
	case 240:
		goto loc_8232EC4C;
	case 241:
		goto loc_8232EC4C;
	case 242:
		goto loc_8232EC4C;
	case 243:
		goto loc_8232EC4C;
	case 244:
		goto loc_8232EC4C;
	case 245:
		goto loc_8232EC4C;
	case 246:
		goto loc_8232EC4C;
	case 247:
		goto loc_8232EC4C;
	case 248:
		goto loc_8232CF50;
	case 249:
		goto loc_8232CF50;
	case 250:
		goto loc_8232EC2C;
	case 251:
		goto loc_8232CF50;
	case 252:
		goto loc_8232CF00;
	case 253:
		goto loc_8232CF00;
	case 254:
		goto loc_8232EBF8;
	case 255:
		goto loc_8232CF00;
	case 256:
		goto loc_8232EBD0;
	case 257:
		goto loc_8232EBD0;
	case 258:
		goto loc_8232EBD0;
	case 259:
		goto loc_8232EBD0;
	case 260:
		goto loc_8232EBD0;
	case 261:
		goto loc_8232EBD0;
	case 262:
		goto loc_8232EBD0;
	case 263:
		goto loc_8232EBD0;
	case 264:
		goto loc_8232EBD0;
	case 265:
		goto loc_8232EBD0;
	case 266:
		goto loc_8232EBD0;
	case 267:
		goto loc_8232EBD0;
	case 268:
		goto loc_8232EBD0;
	case 269:
		goto loc_8232EBD0;
	case 270:
		goto loc_8232EBD0;
	case 271:
		goto loc_8232EBD0;
	case 272:
		goto loc_8232EBD0;
	case 273:
		goto loc_8232EBD0;
	case 274:
		goto loc_8232EBD0;
	case 275:
		goto loc_8232EBD0;
	case 276:
		goto loc_8232EBD0;
	case 277:
		goto loc_8232EBD0;
	case 278:
		goto loc_8232EBD0;
	case 279:
		goto loc_8232EBD0;
	case 280:
		goto loc_8232EBD0;
	case 281:
		goto loc_8232EBD0;
	case 282:
		goto loc_8232EBD0;
	case 283:
		goto loc_8232EBD0;
	case 284:
		goto loc_8232EBD0;
	case 285:
		goto loc_8232EBD0;
	case 286:
		goto loc_8232EBD0;
	case 287:
		goto loc_8232EBD0;
	case 288:
		goto loc_8232EBD0;
	case 289:
		goto loc_8232EBD0;
	case 290:
		goto loc_8232EBD0;
	case 291:
		goto loc_8232EBD0;
	case 292:
		goto loc_8232EBD0;
	case 293:
		goto loc_8232EBD0;
	case 294:
		goto loc_8232EBD0;
	case 295:
		goto loc_8232EBD0;
	case 296:
		goto loc_8232EBD0;
	case 297:
		goto loc_8232EBD0;
	case 298:
		goto loc_8232EBD0;
	case 299:
		goto loc_8232EBD0;
	case 300:
		goto loc_8232EBD0;
	case 301:
		goto loc_8232EBD0;
	case 302:
		goto loc_8232EBD0;
	case 303:
		goto loc_8232EBD0;
	case 304:
		goto loc_8232EBD0;
	case 305:
		goto loc_8232EBD0;
	case 306:
		goto loc_8232EBD0;
	case 307:
		goto loc_8232EBD0;
	case 308:
		goto loc_8232EBD0;
	case 309:
		goto loc_8232EBD0;
	case 310:
		goto loc_8232EBD0;
	case 311:
		goto loc_8232EBD0;
	case 312:
		goto loc_8232EBD0;
	case 313:
		goto loc_8232EBD0;
	case 314:
		goto loc_8232EBD0;
	case 315:
		goto loc_8232EBD0;
	case 316:
		goto loc_8232EBD0;
	case 317:
		goto loc_8232EBD0;
	case 318:
		goto loc_8232EBD0;
	case 319:
		goto loc_8232EBD0;
	case 320:
		goto loc_8232EBD0;
	case 321:
		goto loc_8232EBD0;
	case 322:
		goto loc_8232EBD0;
	case 323:
		goto loc_8232EBD0;
	case 324:
		goto loc_8232EBD0;
	case 325:
		goto loc_8232EBD0;
	case 326:
		goto loc_8232EBD0;
	case 327:
		goto loc_8232EBD0;
	case 328:
		goto loc_8232EBD0;
	case 329:
		goto loc_8232EBD0;
	case 330:
		goto loc_8232EBD0;
	case 331:
		goto loc_8232EBD0;
	case 332:
		goto loc_8232EBD0;
	case 333:
		goto loc_8232EBD0;
	case 334:
		goto loc_8232EBD0;
	case 335:
		goto loc_8232EBD0;
	case 336:
		goto loc_8232EBD0;
	case 337:
		goto loc_8232EBD0;
	case 338:
		goto loc_8232EBD0;
	case 339:
		goto loc_8232EBD0;
	case 340:
		goto loc_8232EBD0;
	case 341:
		goto loc_8232EBD0;
	case 342:
		goto loc_8232EBD0;
	case 343:
		goto loc_8232EBD0;
	case 344:
		goto loc_8232EBD0;
	case 345:
		goto loc_8232EBD0;
	case 346:
		goto loc_8232EBD0;
	case 347:
		goto loc_8232EBD0;
	case 348:
		goto loc_8232EBD0;
	case 349:
		goto loc_8232EBD0;
	case 350:
		goto loc_8232EBD0;
	case 351:
		goto loc_8232EBD0;
	case 352:
		goto loc_8232EBD0;
	case 353:
		goto loc_8232EBD0;
	case 354:
		goto loc_8232EBD0;
	case 355:
		goto loc_8232EBD0;
	case 356:
		goto loc_8232EBD0;
	case 357:
		goto loc_8232EBD0;
	case 358:
		goto loc_8232EBD0;
	case 359:
		goto loc_8232EBD0;
	case 360:
		goto loc_8232EBD0;
	case 361:
		goto loc_8232EBD0;
	case 362:
		goto loc_8232EBD0;
	case 363:
		goto loc_8232EBD0;
	case 364:
		goto loc_8232EBD0;
	case 365:
		goto loc_8232EBD0;
	case 366:
		goto loc_8232EBD0;
	case 367:
		goto loc_8232EBD0;
	case 368:
		goto loc_8232EBD0;
	case 369:
		goto loc_8232EBD0;
	case 370:
		goto loc_8232EBD0;
	case 371:
		goto loc_8232EBD0;
	case 372:
		goto loc_8232EBD0;
	case 373:
		goto loc_8232EBD0;
	case 374:
		goto loc_8232EBD0;
	case 375:
		goto loc_8232EBD0;
	case 376:
		goto loc_8232EBD0;
	case 377:
		goto loc_8232EBD0;
	case 378:
		goto loc_8232EBD0;
	case 379:
		goto loc_8232EBD0;
	case 380:
		goto loc_8232EBD0;
	case 381:
		goto loc_8232EBD0;
	case 382:
		goto loc_8232EBD0;
	case 383:
		goto loc_8232EBD0;
	case 384:
		goto loc_8232CEC0;
	case 385:
		goto loc_8232CEC0;
	case 386:
		goto loc_8232CEC0;
	case 387:
		goto loc_8232CEC0;
	case 388:
		goto loc_8232CEC0;
	case 389:
		goto loc_8232CEC0;
	case 390:
		goto loc_8232CEC0;
	case 391:
		goto loc_8232CEC0;
	case 392:
		goto loc_8232CEC0;
	case 393:
		goto loc_8232CEC0;
	case 394:
		goto loc_8232CE70;
	case 395:
		goto loc_8232CE20;
	case 396:
		goto loc_8232CDD0;
	case 397:
		goto loc_8232CD80;
	case 398:
		goto loc_8232CD30;
	case 399:
		goto loc_8232CCE0;
	case 400:
		goto loc_8232CC90;
	case 401:
		goto loc_8232CC90;
	case 402:
		goto loc_8232CC40;
	case 403:
		goto loc_8232CC40;
	case 404:
		goto loc_8232CBF0;
	case 405:
		goto loc_8232CBF0;
	case 406:
		goto loc_8232CBA0;
	case 407:
		goto loc_8232CBA0;
	case 408:
		goto loc_8232CB50;
	case 409:
		goto loc_8232CB50;
	case 410:
		goto loc_8232EBB0;
	case 411:
		goto loc_8232CB50;
	case 412:
		goto loc_8232CB00;
	case 413:
		goto loc_8232CB00;
	case 414:
		goto loc_8232EB7C;
	case 415:
		goto loc_8232CB00;
	case 416:
		goto loc_8232EB44;
	case 417:
		goto loc_8232EB44;
	case 418:
		goto loc_8232EB44;
	case 419:
		goto loc_8232EB44;
	case 420:
		goto loc_8232EB44;
	case 421:
		goto loc_8232EB44;
	case 422:
		goto loc_8232EB44;
	case 423:
		goto loc_8232EB44;
	case 424:
		goto loc_8232EB44;
	case 425:
		goto loc_8232EB44;
	case 426:
		goto loc_8232EB44;
	case 427:
		goto loc_8232EB44;
	case 428:
		goto loc_8232EB44;
	case 429:
		goto loc_8232EB44;
	case 430:
		goto loc_8232EB44;
	case 431:
		goto loc_8232EB44;
	case 432:
		goto loc_8232CAB0;
	case 433:
		goto loc_8232CAB0;
	case 434:
		goto loc_8232CAB0;
	case 435:
		goto loc_8232CAB0;
	case 436:
		goto loc_8232EAF4;
	case 437:
		goto loc_8232EAF4;
	case 438:
		goto loc_8232CA48;
	case 439:
		goto loc_8232C9E0;
	case 440:
		goto loc_8232C990;
	case 441:
		goto loc_8232C990;
	case 442:
		goto loc_8232C990;
	case 443:
		goto loc_8232C990;
	case 444:
		goto loc_8232EAA4;
	case 445:
		goto loc_8232EAA4;
	case 446:
		goto loc_8232C928;
	case 447:
		goto loc_8232C8C0;
	case 448:
		goto loc_8232C888;
	case 449:
		goto loc_8232C888;
	case 450:
		goto loc_8232C888;
	case 451:
		goto loc_8232C888;
	case 452:
		goto loc_8232C888;
	case 453:
		goto loc_8232C888;
	case 454:
		goto loc_8232C888;
	case 455:
		goto loc_8232C888;
	case 456:
		goto loc_8232C888;
	case 457:
		goto loc_8232C888;
	case 458:
		goto loc_8232C838;
	case 459:
		goto loc_8232C7E8;
	case 460:
		goto loc_8232C798;
	case 461:
		goto loc_8232C748;
	case 462:
		goto loc_8232C6F8;
	case 463:
		goto loc_8232C6A8;
	case 464:
		goto loc_8232C658;
	case 465:
		goto loc_8232C658;
	case 466:
		goto loc_8232C608;
	case 467:
		goto loc_8232C608;
	case 468:
		goto loc_8232C5B8;
	case 469:
		goto loc_8232C5B8;
	case 470:
		goto loc_8232C568;
	case 471:
		goto loc_8232C568;
	case 472:
		goto loc_8232C518;
	case 473:
		goto loc_8232C518;
	case 474:
		goto loc_8232EA84;
	case 475:
		goto loc_8232C518;
	case 476:
		goto loc_8232C4C8;
	case 477:
		goto loc_8232C4C8;
	case 478:
		goto loc_8232EA50;
	case 479:
		goto loc_8232C4C8;
	case 480:
		goto loc_8232EA10;
	case 481:
		goto loc_8232EA10;
	case 482:
		goto loc_8232EA10;
	case 483:
		goto loc_8232EA10;
	case 484:
		goto loc_8232EA10;
	case 485:
		goto loc_8232EA10;
	case 486:
		goto loc_8232EA10;
	case 487:
		goto loc_8232EA10;
	case 488:
		goto loc_8232EA10;
	case 489:
		goto loc_8232EA10;
	case 490:
		goto loc_8232EA10;
	case 491:
		goto loc_8232EA10;
	case 492:
		goto loc_8232EA10;
	case 493:
		goto loc_8232EA10;
	case 494:
		goto loc_8232EA10;
	case 495:
		goto loc_8232EA10;
	case 496:
		goto loc_8232C478;
	case 497:
		goto loc_8232C478;
	case 498:
		goto loc_8232C478;
	case 499:
		goto loc_8232C478;
	case 500:
		goto loc_8232E9C0;
	case 501:
		goto loc_8232E9C0;
	case 502:
		goto loc_8232C410;
	case 503:
		goto loc_8232C3A8;
	case 504:
		goto loc_8232C358;
	case 505:
		goto loc_8232C358;
	case 506:
		goto loc_8232C358;
	case 507:
		goto loc_8232C358;
	case 508:
		goto loc_8232E968;
	case 509:
		goto loc_8232E968;
	case 510:
		goto loc_8232C2F0;
	case 511:
		goto loc_8232C27C;
	default:
		__builtin_unreachable();
	}
	// lwz r17,-5996(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5996);
	// lwz r17,-6080(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6080);
	// lwz r17,-6164(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6164);
	// lwz r17,-6164(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6164);
	// lwz r17,-6316(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6316);
	// lwz r17,-6316(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6316);
	// lwz r17,-6316(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6316);
	// lwz r17,-6316(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6316);
	// lwz r17,-6508(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6508);
	// lwz r17,-6508(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6508);
	// lwz r17,-6508(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6508);
	// lwz r17,-6508(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6508);
	// lwz r17,-6508(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6508);
	// lwz r17,-6508(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6508);
	// lwz r17,-6508(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6508);
	// lwz r17,-6508(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6508);
	// lwz r17,-6572(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6572);
	// lwz r17,-6572(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6572);
	// lwz r17,-6628(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6628);
	// lwz r17,-6628(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6628);
	// lwz r17,-6684(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6684);
	// lwz r17,-6684(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6684);
	// lwz r17,-6740(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6740);
	// lwz r17,-6740(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6740);
	// lwz r17,-6800(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6800);
	// lwz r17,-6800(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6800);
	// lwz r17,-6856(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6856);
	// lwz r17,-6856(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6856);
	// lwz r17,-6912(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6912);
	// lwz r17,-6912(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6912);
	// lwz r17,-6968(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6968);
	// lwz r17,-6968(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -6968);
	// lwz r17,-7032(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7032);
	// lwz r17,-7032(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7032);
	// lwz r17,-4116(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4116);
	// lwz r17,-7032(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7032);
	// lwz r17,-7088(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7088);
	// lwz r17,-7088(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7088);
	// lwz r17,-4144(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4144);
	// lwz r17,-7088(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7088);
	// lwz r17,-7144(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7144);
	// lwz r17,-7144(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7144);
	// lwz r17,-4152(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4152);
	// lwz r17,-7144(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7144);
	// lwz r17,-7200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7200);
	// lwz r17,-7200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7200);
	// lwz r17,-4180(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4180);
	// lwz r17,-7200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7200);
	// lwz r17,-7256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7256);
	// lwz r17,-7256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7256);
	// lwz r17,-4208(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4208);
	// lwz r17,-7256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7256);
	// lwz r17,-7312(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7312);
	// lwz r17,-7312(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7312);
	// lwz r17,-4236(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4236);
	// lwz r17,-7312(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7312);
	// lwz r17,-7368(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7368);
	// lwz r17,-7368(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7368);
	// lwz r17,-4244(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4244);
	// lwz r17,-7368(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7368);
	// lwz r17,-7424(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7424);
	// lwz r17,-7424(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7424);
	// lwz r17,-4272(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4272);
	// lwz r17,-7424(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7424);
	// lwz r17,-7480(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7480);
	// lwz r17,-7536(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7536);
	// lwz r17,-7596(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7596);
	// lwz r17,-7652(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7652);
	// lwz r17,-7708(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7708);
	// lwz r17,-7764(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7764);
	// lwz r17,-7820(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7820);
	// lwz r17,-7876(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7876);
	// lwz r17,-7932(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7932);
	// lwz r17,-7988(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7988);
	// lwz r17,-8044(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8044);
	// lwz r17,-8100(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8100);
	// lwz r17,-8160(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8160);
	// lwz r17,-8216(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8216);
	// lwz r17,-8272(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8272);
	// lwz r17,-8328(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8328);
	// lwz r17,-8392(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8392);
	// lwz r17,-8392(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8392);
	// lwz r17,-8392(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8392);
	// lwz r17,-8392(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8392);
	// lwz r17,-4328(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4328);
	// lwz r17,-4328(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4328);
	// lwz r17,-8472(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8472);
	// lwz r17,-8552(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8552);
	// lwz r17,-8608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8608);
	// lwz r17,-8608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8608);
	// lwz r17,-8608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8608);
	// lwz r17,-8608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8608);
	// lwz r17,-4384(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4384);
	// lwz r17,-4384(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4384);
	// lwz r17,-8688(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8688);
	// lwz r17,-8768(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8768);
	// lwz r17,-8824(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8824);
	// lwz r17,-8824(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8824);
	// lwz r17,-8824(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8824);
	// lwz r17,-8824(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8824);
	// lwz r17,-4440(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4440);
	// lwz r17,-4440(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4440);
	// lwz r17,-8904(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8904);
	// lwz r17,-8984(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -8984);
	// lwz r17,-9040(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9040);
	// lwz r17,-9040(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9040);
	// lwz r17,-9040(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9040);
	// lwz r17,-9040(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9040);
	// lwz r17,-4496(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4496);
	// lwz r17,-4496(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4496);
	// lwz r17,-9120(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9120);
	// lwz r17,-9200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9200);
	// lwz r17,-9256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9256);
	// lwz r17,-9256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9256);
	// lwz r17,-9256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9256);
	// lwz r17,-9256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9256);
	// lwz r17,-4552(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4552);
	// lwz r17,-4552(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4552);
	// lwz r17,-9336(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9336);
	// lwz r17,-9416(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9416);
	// lwz r17,-9472(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9472);
	// lwz r17,-9472(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9472);
	// lwz r17,-9472(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9472);
	// lwz r17,-9472(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9472);
	// lwz r17,-4608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4608);
	// lwz r17,-4608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4608);
	// lwz r17,-9552(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9552);
	// lwz r17,-9632(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9632);
	// lwz r17,-9696(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9696);
	// lwz r17,-9696(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9696);
	// lwz r17,-9696(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9696);
	// lwz r17,-9696(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9696);
	// lwz r17,-9696(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9696);
	// lwz r17,-9696(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9696);
	// lwz r17,-9776(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9776);
	// lwz r17,-9856(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9856);
	// lwz r17,-4664(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4664);
	// lwz r17,-4664(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4664);
	// lwz r17,-4664(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4664);
	// lwz r17,-4664(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4664);
	// lwz r17,-9936(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9936);
	// lwz r17,-9936(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -9936);
	// lwz r17,-10016(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10016);
	// lwz r17,-10016(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10016);
	// lwz r17,-10072(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10072);
	// lwz r17,-10072(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10072);
	// lwz r17,-10072(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10072);
	// lwz r17,-10072(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10072);
	// lwz r17,-10072(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10072);
	// lwz r17,-10072(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10072);
	// lwz r17,-10152(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10152);
	// lwz r17,-10232(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10232);
	// lwz r17,-4720(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4720);
	// lwz r17,-4720(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4720);
	// lwz r17,-4720(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4720);
	// lwz r17,-4720(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4720);
	// lwz r17,-10312(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10312);
	// lwz r17,-10312(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10312);
	// lwz r17,-10392(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10392);
	// lwz r17,-10392(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10392);
	// lwz r17,-10448(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10448);
	// lwz r17,-10448(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10448);
	// lwz r17,-10448(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10448);
	// lwz r17,-10448(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10448);
	// lwz r17,-10448(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10448);
	// lwz r17,-10448(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10448);
	// lwz r17,-10528(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10528);
	// lwz r17,-10608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10608);
	// lwz r17,-4776(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4776);
	// lwz r17,-4776(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4776);
	// lwz r17,-4776(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4776);
	// lwz r17,-4776(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4776);
	// lwz r17,-10688(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10688);
	// lwz r17,-10688(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10688);
	// lwz r17,-10768(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10768);
	// lwz r17,-10768(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10768);
	// lwz r17,-10824(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10824);
	// lwz r17,-10824(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10824);
	// lwz r17,-10824(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10824);
	// lwz r17,-10824(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10824);
	// lwz r17,-10824(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10824);
	// lwz r17,-10824(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10824);
	// lwz r17,-10904(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10904);
	// lwz r17,-10984(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -10984);
	// lwz r17,-4840(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4840);
	// lwz r17,-4840(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4840);
	// lwz r17,-4840(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4840);
	// lwz r17,-4840(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4840);
	// lwz r17,-11064(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11064);
	// lwz r17,-11064(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11064);
	// lwz r17,-11144(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11144);
	// lwz r17,-11144(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11144);
	// lwz r17,-11208(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11208);
	// lwz r17,-11208(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11208);
	// lwz r17,-11208(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11208);
	// lwz r17,-11208(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11208);
	// lwz r17,-11208(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11208);
	// lwz r17,-11208(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11208);
	// lwz r17,-11208(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11208);
	// lwz r17,-11208(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11208);
	// lwz r17,-11288(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11288);
	// lwz r17,-11368(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11368);
	// lwz r17,-11448(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11448);
	// lwz r17,-11528(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11528);
	// lwz r17,-11608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11608);
	// lwz r17,-11608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11608);
	// lwz r17,-11688(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11688);
	// lwz r17,-11688(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11688);
	// lwz r17,-4896(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4896);
	// lwz r17,-4896(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4896);
	// lwz r17,-4896(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4896);
	// lwz r17,-4896(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4896);
	// lwz r17,-4896(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4896);
	// lwz r17,-4896(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4896);
	// lwz r17,-4896(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4896);
	// lwz r17,-4896(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4896);
	// lwz r17,-11768(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11768);
	// lwz r17,-11768(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11768);
	// lwz r17,-4928(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4928);
	// lwz r17,-11768(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11768);
	// lwz r17,-11848(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11848);
	// lwz r17,-11848(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11848);
	// lwz r17,-4980(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -4980);
	// lwz r17,-11848(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11848);
	// lwz r17,-11904(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11904);
	// lwz r17,-11904(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11904);
	// lwz r17,-11904(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11904);
	// lwz r17,-11904(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11904);
	// lwz r17,-11904(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11904);
	// lwz r17,-11904(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11904);
	// lwz r17,-11904(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11904);
	// lwz r17,-11904(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11904);
	// lwz r17,-11984(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -11984);
	// lwz r17,-12064(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12064);
	// lwz r17,-12144(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12144);
	// lwz r17,-12224(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12224);
	// lwz r17,-12304(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12304);
	// lwz r17,-12304(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12304);
	// lwz r17,-12384(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12384);
	// lwz r17,-12384(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12384);
	// lwz r17,-5044(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5044);
	// lwz r17,-5044(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5044);
	// lwz r17,-5044(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5044);
	// lwz r17,-5044(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5044);
	// lwz r17,-5044(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5044);
	// lwz r17,-5044(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5044);
	// lwz r17,-5044(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5044);
	// lwz r17,-5044(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5044);
	// lwz r17,-12464(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12464);
	// lwz r17,-12464(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12464);
	// lwz r17,-5076(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5076);
	// lwz r17,-12464(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12464);
	// lwz r17,-12544(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12544);
	// lwz r17,-12544(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12544);
	// lwz r17,-5128(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5128);
	// lwz r17,-12544(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12544);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-5168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5168);
	// lwz r17,-12608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12608);
	// lwz r17,-12608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12608);
	// lwz r17,-12608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12608);
	// lwz r17,-12608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12608);
	// lwz r17,-12608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12608);
	// lwz r17,-12608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12608);
	// lwz r17,-12608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12608);
	// lwz r17,-12608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12608);
	// lwz r17,-12608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12608);
	// lwz r17,-12608(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12608);
	// lwz r17,-12688(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12688);
	// lwz r17,-12768(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12768);
	// lwz r17,-12848(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12848);
	// lwz r17,-12928(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -12928);
	// lwz r17,-13008(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13008);
	// lwz r17,-13088(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13088);
	// lwz r17,-13168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13168);
	// lwz r17,-13168(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13168);
	// lwz r17,-13248(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13248);
	// lwz r17,-13248(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13248);
	// lwz r17,-13328(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13328);
	// lwz r17,-13328(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13328);
	// lwz r17,-13408(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13408);
	// lwz r17,-13408(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13408);
	// lwz r17,-13488(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13488);
	// lwz r17,-13488(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13488);
	// lwz r17,-5200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5200);
	// lwz r17,-13488(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13488);
	// lwz r17,-13568(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13568);
	// lwz r17,-13568(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13568);
	// lwz r17,-5252(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5252);
	// lwz r17,-13568(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13568);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-5308(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5308);
	// lwz r17,-13648(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13648);
	// lwz r17,-13648(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13648);
	// lwz r17,-13648(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13648);
	// lwz r17,-13648(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13648);
	// lwz r17,-5388(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5388);
	// lwz r17,-5388(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5388);
	// lwz r17,-13752(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13752);
	// lwz r17,-13856(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13856);
	// lwz r17,-13936(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13936);
	// lwz r17,-13936(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13936);
	// lwz r17,-13936(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13936);
	// lwz r17,-13936(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -13936);
	// lwz r17,-5468(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5468);
	// lwz r17,-5468(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5468);
	// lwz r17,-14040(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14040);
	// lwz r17,-14144(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14144);
	// lwz r17,-14200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14200);
	// lwz r17,-14200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14200);
	// lwz r17,-14200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14200);
	// lwz r17,-14200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14200);
	// lwz r17,-14200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14200);
	// lwz r17,-14200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14200);
	// lwz r17,-14200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14200);
	// lwz r17,-14200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14200);
	// lwz r17,-14200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14200);
	// lwz r17,-14200(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14200);
	// lwz r17,-14280(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14280);
	// lwz r17,-14360(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14360);
	// lwz r17,-14440(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14440);
	// lwz r17,-14520(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14520);
	// lwz r17,-14600(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14600);
	// lwz r17,-14680(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14680);
	// lwz r17,-14760(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14760);
	// lwz r17,-14760(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14760);
	// lwz r17,-14840(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14840);
	// lwz r17,-14840(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14840);
	// lwz r17,-14920(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14920);
	// lwz r17,-14920(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -14920);
	// lwz r17,-15000(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15000);
	// lwz r17,-15000(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15000);
	// lwz r17,-15080(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15080);
	// lwz r17,-15080(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15080);
	// lwz r17,-5500(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5500);
	// lwz r17,-15080(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15080);
	// lwz r17,-15160(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15160);
	// lwz r17,-15160(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15160);
	// lwz r17,-5552(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5552);
	// lwz r17,-15160(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15160);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-5616(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5616);
	// lwz r17,-15240(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15240);
	// lwz r17,-15240(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15240);
	// lwz r17,-15240(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15240);
	// lwz r17,-15240(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15240);
	// lwz r17,-5696(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5696);
	// lwz r17,-5696(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5696);
	// lwz r17,-15344(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15344);
	// lwz r17,-15448(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15448);
	// lwz r17,-15528(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15528);
	// lwz r17,-15528(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15528);
	// lwz r17,-15528(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15528);
	// lwz r17,-15528(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15528);
	// lwz r17,-5784(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5784);
	// lwz r17,-5784(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -5784);
	// lwz r17,-15632(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15632);
	// lwz r17,-15748(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -15748);
loc_8232C27C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r3,r5
	ctx.r3.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r3,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r3.u32);
	// lwz r3,28(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r3,r5
	ctx.r3.s64 = ctx.r5.s8;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r3,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r3.u32);
	// lwz r3,28(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r3,r5
	ctx.r3.s64 = ctx.r5.s8;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r3,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r3.u32);
	// lwz r3,28(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r30.u32);
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C2E8:
	// rlwinm r6,r6,9,0,22
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 9) & 0xFFFFFE00;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C2F0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C358:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// rlwinm r31,r5,2,0,29
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232df70
	if (ctx.cr6.lt) goto loc_8232DF70;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C3A8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r31,r5,2,0,29
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C410:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C478:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232df70
	if (ctx.cr6.lt) goto loc_8232DF70;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C4C8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C518:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C568:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C5B8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C608:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C658:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C6A8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C6F8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C748:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C798:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C7E8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C838:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C888:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232cef8
	if (ctx.cr6.lt) goto loc_8232CEF8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C8C0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// rlwinm r31,r5,2,0,29
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C928:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C990:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232df70
	if (ctx.cr6.lt) goto loc_8232DF70;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232C9E0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CA48:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CAB0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232df70
	if (ctx.cr6.lt) goto loc_8232DF70;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CB00:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CB50:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CBA0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CBF0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CC40:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CC90:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CCE0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CD30:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CD80:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CDD0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CE20:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CE70:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CEC0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232cef8
	if (ctx.cr6.lt) goto loc_8232CEF8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CEF8:
	// rlwinm r6,r6,3,0,28
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CF00:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CF50:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CFA0:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232CFF0:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D040:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D090:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D0E0:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D130:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D180:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232d470
	if (ctx.cr6.lt) goto loc_8232D470;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D1B8:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D208:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D258:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D2A8:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D2F8:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D348:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D398:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D3E8:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D438:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232d470
	if (ctx.cr6.lt) goto loc_8232D470;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D470:
	// rlwinm r6,r6,4,0,27
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D478:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D4C8:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D518:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D568:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D5B8:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232da58
	if (ctx.cr6.lt) goto loc_8232DA58;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D5F0:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D640:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D690:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D6E0:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D730:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232da58
	if (ctx.cr6.lt) goto loc_8232DA58;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D768:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D7B8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D808:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D858:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D8A8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232da58
	if (ctx.cr6.lt) goto loc_8232DA58;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D8E0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D930:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D980:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232D9D0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DA20:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232da58
	if (ctx.cr6.lt) goto loc_8232DA58;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DA58:
	// rlwinm r6,r6,5,0,26
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DA60:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DAB0:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DB00:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232df70
	if (ctx.cr6.lt) goto loc_8232DF70;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DB38:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DB88:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DBD8:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232df70
	if (ctx.cr6.lt) goto loc_8232DF70;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DC10:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DC60:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DCB0:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232df70
	if (ctx.cr6.lt) goto loc_8232DF70;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DCE8:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DD38:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DD88:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232df70
	if (ctx.cr6.lt) goto loc_8232DF70;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DDC0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DE10:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DE60:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x8232df70
	if (ctx.cr6.lt) goto loc_8232DF70;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DE98:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DEE8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DF38:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// blt cr6,0x8232df70
	if (ctx.cr6.lt) goto loc_8232DF70;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DF70:
	// rlwinm r6,r6,6,0,25
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DF78:
	// lbzu r5,11(r10)
	ea = 11 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DFB0:
	// lbzu r5,11(r10)
	ea = 11 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232DFE8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r22,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r22.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E020:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// li r31,10
	ctx.r31.s64 = 10;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,28(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r31,r5,r15
	PPC_STORE_U32(ctx.r5.u32 + ctx.r15.u32, ctx.r31.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E05C:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E094:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E0CC:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E104:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E13C:
	// lbzu r5,12(r10)
	ea = 12 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E174:
	// lbzu r5,12(r10)
	ea = 12 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E1AC:
	// lbzu r5,13(r10)
	ea = 13 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E1E4:
	// lbzu r5,13(r10)
	ea = 13 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E21C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r23,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r23.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E254:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// li r31,12
	ctx.r31.s64 = 12;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,28(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r31,r5,r15
	PPC_STORE_U32(ctx.r5.u32 + ctx.r15.u32, ctx.r31.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E290:
	// lbzu r5,14(r10)
	ea = 14 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E2C8:
	// lbzu r5,14(r10)
	ea = 14 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232c2e8
	if (ctx.cr6.lt) goto loc_8232C2E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E300:
	// lbzu r5,6(r10)
	ea = 6 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E338:
	// lbzu r5,6(r10)
	ea = 6 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E370:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E3A8:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E3E0:
	// lbzu r5,7(r10)
	ea = 7 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E418:
	// lbzu r5,7(r10)
	ea = 7 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E450:
	// lbzu r5,8(r10)
	ea = 8 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E488:
	// lbzu r5,8(r10)
	ea = 8 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e4c0
	if (ctx.cr6.lt) goto loc_8232E4C0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E4C0:
	// rlwinm r6,r6,7,0,24
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 7) & 0xFFFFFF80;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E4C8:
	// lbzu r5,9(r10)
	ea = 9 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E500:
	// lbzu r5,9(r10)
	ea = 9 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E538:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r24,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r24.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E570:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// li r31,8
	ctx.r31.s64 = 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,28(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r31,r5,r15
	PPC_STORE_U32(ctx.r5.u32 + ctx.r15.u32, ctx.r31.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E5AC:
	// lbzu r5,10(r10)
	ea = 10 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E5E4:
	// lbzu r5,10(r10)
	ea = 10 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E61C:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E654:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232e68c
	if (ctx.cr6.lt) goto loc_8232E68C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E68C:
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E694:
	// rlwinm r31,r5,1,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r31,19,24,31
	ctx.r5.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 19) & 0xFF;
	// rlwinm r15,r31,11,26,31
	ctx.r15.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 11) & 0x3F;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// clrlwi r15,r5,25
	ctx.r15.u64 = ctx.r5.u32 & 0x7F;
	// stw r16,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r16.u32);
	// cmplwi cr6,r15,0
	ctx.cr6.compare<uint32_t>(ctx.r15.u32, 0, ctx.xer);
	// bne cr6,0x8232e6cc
	if (!ctx.cr6.eq) goto loc_8232E6CC;
	// rlwinm r31,r31,27,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 27) & 0xFF;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// li r15,28
	ctx.r15.s64 = 28;
	// or r5,r31,r5
	ctx.r5.u64 = ctx.r31.u64 | ctx.r5.u64;
	// stw r15,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r15.u32);
loc_8232E6CC:
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bge cr6,0x8232e6e0
	if (!ctx.cr6.lt) goto loc_8232E6E0;
	// li r31,1
	ctx.r31.s64 = 1;
	// neg r5,r5
	ctx.r5.s64 = -ctx.r5.s64;
	// b 0x8232e6e4
	goto loc_8232E6E4;
loc_8232E6E0:
	// li r31,0
	ctx.r31.s64 = 0;
loc_8232E6E4:
	// stw r31,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r31.u32);
	// stw r5,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r5.u32);
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,4(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// lwz r14,8(r4)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// neg r14,r14
	ctx.r14.s64 = -ctx.r14.s64;
	// rlwinm r14,r14,15,0,16
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 15) & 0xFFFF8000;
	// or r15,r14,r15
	ctx.r15.u64 = ctx.r14.u64 | ctx.r15.u64;
	// rlwinm r15,r15,1,0,30
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 1) & 0xFFFFFFFE;
	// stwx r15,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r15.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x8232e74c
	if (ctx.cr6.lt) goto loc_8232E74C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E74C:
	// slw r6,r6,r5
	ctx.r6.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E754:
	// stw r17,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r17.u32);
	// rlwinm r5,r5,11,22,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 11) & 0x3FF;
	// rlwinm r15,r5,0,0,30
	ctx.r15.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lwz r31,4372(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4372);
	// lhzx r31,r15,r31
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r15.u32 + ctx.r31.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x8232e7a8
	if (ctx.cr6.lt) goto loc_8232E7A8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232e7ac
	goto loc_8232E7AC;
loc_8232E7A8:
	// slw r6,r6,r5
	ctx.r6.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
loc_8232E7AC:
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,8(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r31,4(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// neg r15,r15
	ctx.r15.s64 = -ctx.r15.s64;
	// lwz r14,28(r4)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// rlwinm r15,r15,15,0,16
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 15) & 0xFFFF8000;
	// or r31,r15,r31
	ctx.r31.u64 = ctx.r15.u64 | ctx.r31.u64;
	// rlwinm r31,r31,1,0,30
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// stwx r31,r5,r14
	PPC_STORE_U32(ctx.r5.u32 + ctx.r14.u32, ctx.r31.u32);
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E7EC:
	// stw r18,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r18.u32);
	// rlwinm r5,r5,13,20,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 13) & 0xFFF;
	// rlwinm r31,r5,0,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lwz r15,4376(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4376);
	// lhzx r31,r31,r15
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r31.u32 + ctx.r15.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x8232e924
	if (ctx.cr6.lt) goto loc_8232E924;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232e928
	goto loc_8232E928;
loc_8232E840:
	// stw r19,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r19.u32);
	// rlwinm r5,r5,14,19,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 14) & 0x1FFF;
	// rlwinm r31,r5,0,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lwz r15,4380(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4380);
	// lhzx r31,r31,r15
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r31.u32 + ctx.r15.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x8232e924
	if (ctx.cr6.lt) goto loc_8232E924;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232e928
	goto loc_8232E928;
loc_8232E894:
	// rlwinm r5,r5,9,0,22
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 9) & 0xFFFFFE00;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bge cr6,0x8232e8b0
	if (!ctx.cr6.lt) goto loc_8232E8B0;
	// stw r20,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r20.u32);
	// rlwinm r5,r5,6,27,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 6) & 0x1F;
	// lwz r15,4384(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4384);
	// b 0x8232e8dc
	goto loc_8232E8DC;
loc_8232E8B0:
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bge cr6,0x8232e8d0
	if (!ctx.cr6.lt) goto loc_8232E8D0;
	// li r31,16
	ctx.r31.s64 = 16;
	// rlwinm r5,r5,6,27,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 6) & 0x1F;
	// stw r31,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r31.u32);
	// lwz r15,4388(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4388);
	// b 0x8232e8dc
	goto loc_8232E8DC;
loc_8232E8D0:
	// stw r21,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r21.u32);
	// rlwinm r5,r5,7,27,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 7) & 0x1F;
	// lwz r15,4392(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4392);
loc_8232E8DC:
	// rlwinm r31,r5,0,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lhzx r31,r31,r15
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r31.u32 + ctx.r15.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x8232e924
	if (ctx.cr6.lt) goto loc_8232E924;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232e928
	goto loc_8232E928;
loc_8232E924:
	// slw r6,r6,r5
	ctx.r6.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
loc_8232E928:
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,8(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r15,4(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// neg r31,r31
	ctx.r31.s64 = -ctx.r31.s64;
	// lwz r14,28(r4)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// rlwinm r31,r31,15,0,16
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 15) & 0xFFFF8000;
	// or r31,r31,r15
	ctx.r31.u64 = ctx.r31.u64 | ctx.r15.u64;
	// rlwinm r31,r31,1,0,30
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// stwx r31,r5,r14
	PPC_STORE_U32(ctx.r5.u32 + ctx.r14.u32, ctx.r31.u32);
	// b 0x8232ba40
	goto loc_8232BA40;
loc_8232E968:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8232e9b8
	if (ctx.cr6.lt) goto loc_8232E9B8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232E9B8:
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232E9C0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8232e9b8
	if (ctx.cr6.lt) goto loc_8232E9B8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EA10:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8232ea48
	if (ctx.cr6.lt) goto loc_8232EA48;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EA48:
	// rlwinm r6,r6,5,0,26
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EA50:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x8232f004
	goto loc_8232F004;
loc_8232EA84:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// b 0x8232eff0
	goto loc_8232EFF0;
loc_8232EAA4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8232e9b8
	if (ctx.cr6.lt) goto loc_8232E9B8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EAF4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8232e9b8
	if (ctx.cr6.lt) goto loc_8232E9B8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EB44:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8232ea48
	if (ctx.cr6.lt) goto loc_8232EA48;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EB7C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x8232f004
	goto loc_8232F004;
loc_8232EBB0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// b 0x8232eff0
	goto loc_8232EFF0;
loc_8232EBD0:
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x8232ebf0
	if (ctx.cr6.lt) goto loc_8232EBF0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EBF0:
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EBF8:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x8232f004
	goto loc_8232F004;
loc_8232EC2C:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// b 0x8232eff0
	goto loc_8232EFF0;
loc_8232EC4C:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8232ec84
	if (ctx.cr6.lt) goto loc_8232EC84;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EC84:
	// rlwinm r6,r6,6,0,25
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EC8C:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x8232f004
	goto loc_8232F004;
loc_8232ECC0:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// b 0x8232eff0
	goto loc_8232EFF0;
loc_8232ECE0:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8232ec84
	if (ctx.cr6.lt) goto loc_8232EC84;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232ED18:
	// lbz r10,3(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8232ed50
	if (ctx.cr6.lt) goto loc_8232ED50;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232ED50:
	// rlwinm r6,r6,7,0,24
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 7) & 0xFFFFFF80;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232ED58:
	// lbz r10,3(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8232ed50
	if (ctx.cr6.lt) goto loc_8232ED50;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232ED90:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r28.u32);
	// blt cr6,0x8232ed50
	if (ctx.cr6.lt) goto loc_8232ED50;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EDC8:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r27.u32);
	// blt cr6,0x8232ed50
	if (ctx.cr6.lt) goto loc_8232ED50;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EE00:
	// lbz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8232e9b8
	if (ctx.cr6.lt) goto loc_8232E9B8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EE38:
	// lbz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8232e9b8
	if (ctx.cr6.lt) goto loc_8232E9B8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EE70:
	// lbz r10,5(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8232e9b8
	if (ctx.cr6.lt) goto loc_8232E9B8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EEA8:
	// lbz r10,5(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8232e9b8
	if (ctx.cr6.lt) goto loc_8232E9B8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EEE0:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r26.u32);
	// blt cr6,0x8232e9b8
	if (ctx.cr6.lt) goto loc_8232E9B8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EF18:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r25.u32);
	// blt cr6,0x8232e9b8
	if (ctx.cr6.lt) goto loc_8232E9B8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232EF50:
	// lbz r10,6(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x8232f004
	goto loc_8232F004;
loc_8232EF6C:
	// lbz r10,6(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// b 0x8232eff0
	goto loc_8232EFF0;
loc_8232EF74:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r28.u32);
	// b 0x8232f004
	goto loc_8232F004;
loc_8232EF90:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r27.u32);
	// b 0x8232f004
	goto loc_8232F004;
loc_8232EFAC:
	// lbz r10,7(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x8232f004
	goto loc_8232F004;
loc_8232EFC8:
	// lbz r10,7(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// b 0x8232eff0
	goto loc_8232EFF0;
loc_8232EFD0:
	// lbz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x8232f004
	goto loc_8232F004;
loc_8232EFEC:
	// lbz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
loc_8232EFF0:
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
loc_8232F004:
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x8232f024
	if (ctx.cr6.lt) goto loc_8232F024;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f028
	goto loc_8232F028;
loc_8232F024:
	// rlwinm r6,r6,9,0,22
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 9) & 0xFFFFFE00;
loc_8232F028:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r5,16(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// cmpw cr6,r10,r5
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, ctx.xer);
	// beq cr6,0x8232f03c
	if (ctx.cr6.eq) goto loc_8232F03C;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
loc_8232F03C:
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// stw r6,4352(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4352, ctx.r6.u32);
	// li r23,36
	ctx.r23.s64 = 36;
	// stw r7,4356(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4356, ctx.r7.u32);
	// addi r21,r10,2016
	ctx.r21.s64 = ctx.r10.s64 + 2016;
	// stw r11,4360(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4360, ctx.r11.u32);
	// li r22,-16
	ctx.r22.s64 = -16;
	// stw r9,4364(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4364, ctx.r9.u32);
	// li r25,48
	ctx.r25.s64 = 48;
	// li r26,64
	ctx.r26.s64 = 64;
	// vspltisw v13,0
	simd::store_i32(ctx.v13.u32, simd::set1_i32(int32_t(0x0)));
	// li r27,80
	ctx.r27.s64 = 80;
	// vspltisw v0,1
	simd::store_i32(ctx.v0.u32, simd::set1_i32(int32_t(0x1)));
	// li r28,96
	ctx.r28.s64 = 96;
	// li r29,112
	ctx.r29.s64 = 112;
	// li r30,128
	ctx.r30.s64 = 128;
	// li r31,144
	ctx.r31.s64 = 144;
	// li r5,160
	ctx.r5.s64 = 160;
	// li r6,176
	ctx.r6.s64 = 176;
	// li r7,192
	ctx.r7.s64 = 192;
	// li r9,208
	ctx.r9.s64 = 208;
	// li r10,224
	ctx.r10.s64 = 224;
	// li r20,-32
	ctx.r20.s64 = -32;
	// li r11,240
	ctx.r11.s64 = 240;
	// li r24,32
	ctx.r24.s64 = 32;
	// lvlx128 v63,r4,r23
	temp.u32 = ctx.r4.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lwz r23,28(r4)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// lvlx v6,r21,r22
	temp.u32 = ctx.r21.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v6,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lwz r3,32(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	// lvlx v7,r21,r20
	temp.u32 = ctx.r21.u32 + ctx.r20.u32;
	simd::store_shuffled(ctx.v7,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v62,v63,0
	simd::store_i32(ctx.v62.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v63.u32), 3));
	// lvlx128 v60,r25,r23
	temp.u32 = ctx.r25.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx128 v59,r26,r23
	temp.u32 = ctx.r26.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v46,v62,0
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v46.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v62.s32)));
	// lvlx128 v58,r27,r23
	temp.u32 = ctx.r27.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v57,r28,r23
	temp.u32 = ctx.r28.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v56,r29,r23
	temp.u32 = ctx.r29.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v55,r30,r23
	temp.u32 = ctx.r30.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v54,r31,r23
	temp.u32 = ctx.r31.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r5,r23
	temp.u32 = ctx.r5.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v52,r6,r23
	temp.u32 = ctx.r6.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v51,r7,r23
	temp.u32 = ctx.r7.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v50,r9,r23
	temp.u32 = ctx.r9.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v49,r10,r23
	temp.u32 = ctx.r10.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v48,r11,r23
	temp.u32 = ctx.r11.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v63,r0,r23
	temp.u32 = ctx.r0.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v12,v63,v63,v7
	simd::store_i8(ctx.v12.u8, simd::permute_bytes(simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v7.u8)));
	// lvlx128 v62,r4,r23
	temp.u32 = ctx.r4.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v44,v63,v62,v6
	simd::store_i8(ctx.v44.u8, simd::permute_bytes(simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v6.u8)));
	// lvlx128 v61,r24,r23
	temp.u32 = ctx.r24.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v41,v44,v96
	simd::store_i32(ctx.v41.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v44.s16)));
	// vupklsb128 v36,v44,v96
	simd::store_i32(ctx.v36.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v44.s16)));
	// lvlx128 v45,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v42,v61,v60,v6
	simd::store_i8(ctx.v42.u8, simd::permute_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v6.u8)));
	// lvlx128 v43,r4,r3
	temp.u32 = ctx.r4.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v11,v62,v62,v7
	simd::store_i8(ctx.v11.u8, simd::permute_bytes(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v7.u8)));
	// lvlx128 v32,r24,r3
	temp.u32 = ctx.r24.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v10,v61,v61,v7
	simd::store_i8(ctx.v10.u8, simd::permute_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v7.u8)));
	// vor128 v47,v63,v63
	simd::store_i8(ctx.v47.u8, simd::load_i8(ctx.v63.u8));
	// vperm128 v9,v60,v60,v7
	simd::store_i8(ctx.v9.u8, simd::permute_bytes(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v40,v41,0
	simd::store_f32_aligned(ctx.v40.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v41.s32)));
	// vperm128 v8,v59,v59,v7
	simd::store_i8(ctx.v8.u8, simd::permute_bytes(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v33,v36,0
	simd::store_f32_aligned(ctx.v33.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v36.s32)));
	// vperm128 v5,v58,v58,v7
	simd::store_i8(ctx.v5.u8, simd::permute_bytes(simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v4,v57,v57,v7
	simd::store_i8(ctx.v4.u8, simd::permute_bytes(simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v3,v56,v56,v7
	simd::store_i8(ctx.v3.u8, simd::permute_bytes(simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v2,v55,v55,v7
	simd::store_i8(ctx.v2.u8, simd::permute_bytes(simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v1,v54,v54,v7
	simd::store_i8(ctx.v1.u8, simd::permute_bytes(simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v31,v53,v53,v7
	simd::store_i8(ctx.v31.u8, simd::permute_bytes(simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v30,v52,v52,v7
	simd::store_i8(ctx.v30.u8, simd::permute_bytes(simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v29,v51,v51,v7
	simd::store_i8(ctx.v29.u8, simd::permute_bytes(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v28,v50,v50,v7
	simd::store_i8(ctx.v28.u8, simd::permute_bytes(simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v27,v49,v49,v7
	simd::store_i8(ctx.v27.u8, simd::permute_bytes(simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v39,v59,v58,v6
	simd::store_i8(ctx.v39.u8, simd::permute_bytes(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v6.u8)));
	// vperm128 v38,v57,v56,v6
	simd::store_i8(ctx.v38.u8, simd::permute_bytes(simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v6.u8)));
	// vperm128 v7,v48,v48,v7
	simd::store_i8(ctx.v7.u8, simd::permute_bytes(simd::load_i8(ctx.v48.u8), simd::load_i8(ctx.v48.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v37,v55,v54,v6
	simd::store_i8(ctx.v37.u8, simd::permute_bytes(simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v6.u8)));
	// vupkhsb128 v35,v42,v96
	simd::store_i32(ctx.v35.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v42.s16)));
	// vperm128 v34,v53,v52,v6
	simd::store_i8(ctx.v34.u8, simd::permute_bytes(simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v6.u8)));
	// vmulfp128 v63,v40,v45
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v40.f32), simd::load_f32_aligned(ctx.v45.f32)));
	// vupklsb128 v61,v42,v96
	simd::store_i32(ctx.v61.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v42.s16)));
	// vmulfp128 v36,v33,v43
	simd::store_f32_aligned(ctx.v36.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v33.f32), simd::load_f32_aligned(ctx.v43.f32)));
	// vupkhsb128 v58,v39,v96
	simd::store_i32(ctx.v58.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v39.s16)));
	// vcsxwfp128 v62,v35,0
	simd::store_f32_aligned(ctx.v62.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v35.s32)));
	// vperm128 v35,v51,v50,v6
	simd::store_i8(ctx.v35.u8, simd::permute_bytes(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v6.u8)));
	// vupklsb128 v56,v39,v96
	simd::store_i32(ctx.v56.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v39.s16)));
	// lvlx128 v60,r25,r3
	temp.u32 = ctx.r25.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v55,v61,0
	simd::store_f32_aligned(ctx.v55.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v61.s32)));
	// vupkhsb128 v54,v38,v96
	simd::store_i32(ctx.v54.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v38.s16)));
	// vupklsb128 v52,v38,v96
	simd::store_i32(ctx.v52.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v38.s16)));
	// vcsxwfp128 v44,v58,0
	simd::store_f32_aligned(ctx.v44.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v58.s32)));
	// vupkhsb128 v42,v37,v96
	simd::store_i32(ctx.v42.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v37.s16)));
	// lvlx128 v59,r26,r3
	temp.u32 = ctx.r26.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v41,v56,0
	simd::store_f32_aligned(ctx.v41.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v56.s32)));
	// lvlx128 v57,r27,r3
	temp.u32 = ctx.r27.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v39,v54,0
	simd::store_f32_aligned(ctx.v39.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v54.s32)));
	// lvlx128 v53,r28,r3
	temp.u32 = ctx.r28.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v58,v52,0
	simd::store_f32_aligned(ctx.v58.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v52.s32)));
	// lvlx128 v45,r29,r3
	temp.u32 = ctx.r29.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v54,v42,0
	simd::store_f32_aligned(ctx.v54.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v42.s32)));
	// lvlx128 v40,r30,r3
	temp.u32 = ctx.r30.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v38,v7,v7
	simd::store_i8(ctx.v38.u8, simd::load_i8(ctx.v7.u8));
	// lvlx v7,0,r21
	temp.u32 = ctx.r0.u32 + ctx.r21.u32;
	simd::store_shuffled(ctx.v7,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v33,v63,v46
	simd::store_f32_aligned(ctx.v33.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vperm128 v63,v49,v48,v6
	simd::store_i8(ctx.v63.u8, simd::permute_bytes(simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v48.u8), simd::load_i8(ctx.v6.u8)));
	// vmulfp128 v36,v36,v46
	simd::store_f32_aligned(ctx.v36.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v36.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// lvlx128 v61,r31,r3
	temp.u32 = ctx.r31.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v62,v62,v32
	simd::store_f32_aligned(ctx.v62.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v32.f32)));
	// lvlx128 v56,r5,r3
	temp.u32 = ctx.r5.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v52,r6,r3
	temp.u32 = ctx.r6.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v50,v55,v60
	simd::store_f32_aligned(ctx.v50.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v55.f32), simd::load_f32_aligned(ctx.v60.f32)));
	// lvlx128 v51,r7,r3
	temp.u32 = ctx.r7.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v42,v44,v59
	simd::store_f32_aligned(ctx.v42.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v44.f32), simd::load_f32_aligned(ctx.v59.f32)));
	// lvlx128 v49,r9,r3
	temp.u32 = ctx.r9.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v48,r10,r3
	temp.u32 = ctx.r10.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v44,v34,v96
	simd::store_i32(ctx.v44.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v34.s16)));
	// vmulfp128 v41,v41,v57
	simd::store_f32_aligned(ctx.v41.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v41.f32), simd::load_f32_aligned(ctx.v57.f32)));
	// lvlx128 v43,r11,r3
	temp.u32 = ctx.r11.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v39,v39,v53
	simd::store_f32_aligned(ctx.v39.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v53.f32)));
	// vmulfp128 v32,v58,v45
	simd::store_f32_aligned(ctx.v32.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v58.f32), simd::load_f32_aligned(ctx.v45.f32)));
	// vupklsb128 v45,v37,v96
	simd::store_i32(ctx.v45.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v37.s16)));
	// vcfpsxws128 v6,v33,0
	simd::store_i32(ctx.v6.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v33.f32)));
	// vcfpsxws128 v26,v36,0
	simd::store_i32(ctx.v26.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v36.f32)));
	// vmulfp128 v33,v62,v46
	simd::store_f32_aligned(ctx.v33.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v62,v54,v40
	simd::store_f32_aligned(ctx.v62.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v54.f32), simd::load_f32_aligned(ctx.v40.f32)));
	// vmulfp128 v60,v50,v46
	simd::store_f32_aligned(ctx.v60.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v50.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v59,v42,v46
	simd::store_f32_aligned(ctx.v59.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v42.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vupklsb128 v42,v34,v96
	simd::store_i32(ctx.v42.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v34.s16)));
	// vmulfp128 v58,v41,v46
	simd::store_f32_aligned(ctx.v58.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v41.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vupkhsb128 v41,v35,v96
	simd::store_i32(ctx.v41.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v35.s16)));
	// vmulfp128 v57,v39,v46
	simd::store_f32_aligned(ctx.v57.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v55,v32,v46
	simd::store_f32_aligned(ctx.v55.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v32.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vsubsws v25,v6,v0
	simd::store_i32(ctx.v25.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v6), simd::to_vec128i(ctx.v0)));
	// vsubsws v18,v26,v0
	simd::store_i32(ctx.v18.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v26), simd::to_vec128i(ctx.v0)));
	// vcfpsxws128 v24,v33,0
	simd::store_i32(ctx.v24.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v33.f32)));
	// vmulfp128 v54,v62,v46
	simd::store_f32_aligned(ctx.v54.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vor v23,v25,v0
	simd::store_i8(ctx.v23.u8, simd::or_i8(simd::load_i8(ctx.v25.u8), simd::load_i8(ctx.v0.u8)));
	// vcfpsxws128 v22,v60,0
	simd::store_i32(ctx.v22.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v60.f32)));
	// vor v15,v18,v0
	simd::store_i8(ctx.v15.u8, simd::or_i8(simd::load_i8(ctx.v18.u8), simd::load_i8(ctx.v0.u8)));
	// vcfpsxws128 v20,v59,0
	simd::store_i32(ctx.v20.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v59.f32)));
	// vcfpsxws128 v19,v58,0
	simd::store_i32(ctx.v19.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v58.f32)));
	// vmaxsw v21,v13,v23
	simd::store_i32(ctx.v21.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v23.u32)));
	// vcfpsxws128 v18,v57,0
	simd::store_i32(ctx.v18.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v57.f32)));
	// vmaxsw v6,v13,v15
	simd::store_i32(ctx.v6.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v15.u32)));
	// vxor v17,v21,v12
	simd::store_u8(ctx.v17.u8, simd::xor_i8(simd::load_u8(ctx.v21.u8), simd::load_u8(ctx.v12.u8)));
	// vxor v25,v6,v11
	simd::store_u8(ctx.v25.u8, simd::xor_i8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v11.u8)));
	// vsubsws v16,v24,v0
	simd::store_i32(ctx.v16.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v24), simd::to_vec128i(ctx.v0)));
	// vsubsws v14,v17,v12
	simd::store_i32(ctx.v14.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v17), simd::to_vec128i(ctx.v12)));
	// vcfpsxws128 v17,v55,0
	simd::store_i32(ctx.v17.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v55.f32)));
	// vsubsws v23,v25,v11
	simd::store_i32(ctx.v23.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v25), simd::to_vec128i(ctx.v11)));
	// vor v12,v16,v0
	simd::store_i8(ctx.v12.u8, simd::or_i8(simd::load_i8(ctx.v16.u8), simd::load_i8(ctx.v0.u8)));
	// vsubsws v16,v22,v0
	simd::store_i32(ctx.v16.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v22), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v53,v14,0
	simd::store_f32_aligned(ctx.v53.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v14.s32)));
	// vcsxwfp128 v50,v23,0
	simd::store_f32_aligned(ctx.v50.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v23.s32)));
	// vmaxsw v26,v13,v12
	simd::store_i32(ctx.v26.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v12.u32)));
	// vxor v24,v26,v10
	simd::store_u8(ctx.v24.u8, simd::xor_i8(simd::load_u8(ctx.v26.u8), simd::load_u8(ctx.v10.u8)));
	// vsubsws v21,v24,v10
	simd::store_i32(ctx.v21.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v24), simd::to_vec128i(ctx.v10)));
	// vperm128 v47,v47,v53,v7
	simd::store_i8(ctx.v47.u8, simd::permute_bytes(simd::load_i8(ctx.v47.u8), simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v7.u8)));
	// stvlx128 v47,r0,r23
{
	uint32_t addr = 
ctx.r23.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v47), 15 - i));
}
	// stvlx128 v50,r23,r4
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r4.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v50), 15 - i));
}
	// vupklsb128 v39,v35,v96
	simd::store_i32(ctx.v39.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v35.s16)));
	// vcsxwfp128 v40,v45,0
	simd::store_f32_aligned(ctx.v40.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v45.s32)));
	// vcsxwfp128 v37,v44,0
	simd::store_f32_aligned(ctx.v37.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v44.s32)));
	// vupkhsb128 v36,v63,v96
	simd::store_i32(ctx.v36.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v63.s16)));
	// vcsxwfp128 v35,v42,0
	simd::store_f32_aligned(ctx.v35.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v42.s32)));
	// vsubsws v15,v20,v0
	simd::store_i32(ctx.v15.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v20), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v34,v41,0
	simd::store_f32_aligned(ctx.v34.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v41.s32)));
	// vsubsws v14,v19,v0
	simd::store_i32(ctx.v14.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v19), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v33,v39,0
	simd::store_f32_aligned(ctx.v33.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v39.s32)));
	// vsubsws v25,v18,v0
	simd::store_i32(ctx.v25.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v18), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v32,v36,0
	simd::store_f32_aligned(ctx.v32.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v36.s32)));
	// vsubsws v24,v17,v0
	simd::store_i32(ctx.v24.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v17), simd::to_vec128i(ctx.v0)));
	// vcfpsxws128 v10,v54,0
	simd::store_i32(ctx.v10.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v54.f32)));
	// vor v12,v16,v0
	simd::store_i8(ctx.v12.u8, simd::or_i8(simd::load_i8(ctx.v16.u8), simd::load_i8(ctx.v0.u8)));
	// vor v11,v15,v0
	simd::store_i8(ctx.v11.u8, simd::or_i8(simd::load_i8(ctx.v15.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v62,v21,0
	simd::store_f32_aligned(ctx.v62.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v21.s32)));
	// vor v6,v14,v0
	simd::store_i8(ctx.v6.u8, simd::or_i8(simd::load_i8(ctx.v14.u8), simd::load_i8(ctx.v0.u8)));
	// vupklsb128 v41,v63,v96
	simd::store_i32(ctx.v41.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v63.s16)));
	// vor v21,v25,v0
	simd::store_i8(ctx.v21.u8, simd::or_i8(simd::load_i8(ctx.v25.u8), simd::load_i8(ctx.v0.u8)));
	// vor v20,v24,v0
	simd::store_i8(ctx.v20.u8, simd::or_i8(simd::load_i8(ctx.v24.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v26,v13,v12
	simd::store_i32(ctx.v26.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v12.u32)));
	// vmulfp128 v61,v40,v61
	simd::store_f32_aligned(ctx.v61.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v40.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vmaxsw v23,v13,v11
	simd::store_i32(ctx.v23.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v11.u32)));
	// vmulfp128 v60,v37,v56
	simd::store_f32_aligned(ctx.v60.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v37.f32), simd::load_f32_aligned(ctx.v56.f32)));
	// vmaxsw v22,v13,v6
	simd::store_i32(ctx.v22.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v6.u32)));
	// vmulfp128 v59,v35,v52
	simd::store_f32_aligned(ctx.v59.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v35.f32), simd::load_f32_aligned(ctx.v52.f32)));
	// vmaxsw v17,v13,v21
	simd::store_i32(ctx.v17.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v21.u32)));
	// vmaxsw v16,v13,v20
	simd::store_i32(ctx.v16.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v20.u32)));
	// vmulfp128 v58,v34,v51
	simd::store_f32_aligned(ctx.v58.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v34.f32), simd::load_f32_aligned(ctx.v51.f32)));
	// vmulfp128 v57,v33,v49
	simd::store_f32_aligned(ctx.v57.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v33.f32), simd::load_f32_aligned(ctx.v49.f32)));
	// vxor v19,v26,v9
	simd::store_u8(ctx.v19.u8, simd::xor_i8(simd::load_u8(ctx.v26.u8), simd::load_u8(ctx.v9.u8)));
	// vmulfp128 v56,v32,v48
	simd::store_f32_aligned(ctx.v56.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v32.f32), simd::load_f32_aligned(ctx.v48.f32)));
	// vxor v18,v23,v8
	simd::store_u8(ctx.v18.u8, simd::xor_i8(simd::load_u8(ctx.v23.u8), simd::load_u8(ctx.v8.u8)));
	// vsubsws v14,v10,v0
	simd::store_i32(ctx.v14.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v10), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v40,v41,0
	simd::store_f32_aligned(ctx.v40.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v41.s32)));
	// vxor v15,v22,v5
	simd::store_u8(ctx.v15.u8, simd::xor_i8(simd::load_u8(ctx.v22.u8), simd::load_u8(ctx.v5.u8)));
	// stvlx128 v62,r23,r24
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r24.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v62), 15 - i));
}
	// vxor v12,v17,v4
	simd::store_u8(ctx.v12.u8, simd::xor_i8(simd::load_u8(ctx.v17.u8), simd::load_u8(ctx.v4.u8)));
	// vxor v11,v16,v3
	simd::store_u8(ctx.v11.u8, simd::xor_i8(simd::load_u8(ctx.v16.u8), simd::load_u8(ctx.v3.u8)));
	// vsubsws v10,v19,v9
	simd::store_i32(ctx.v10.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v19), simd::to_vec128i(ctx.v9)));
	// vor v6,v14,v0
	simd::store_i8(ctx.v6.u8, simd::or_i8(simd::load_i8(ctx.v14.u8), simd::load_i8(ctx.v0.u8)));
	// vsubsws v9,v18,v8
	simd::store_i32(ctx.v9.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v18), simd::to_vec128i(ctx.v8)));
	// vmulfp128 v55,v61,v46
	simd::store_f32_aligned(ctx.v55.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v61.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vsubsws v8,v15,v5
	simd::store_i32(ctx.v8.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v15), simd::to_vec128i(ctx.v5)));
	// vmulfp128 v54,v60,v46
	simd::store_f32_aligned(ctx.v54.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v60.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v53,v59,v46
	simd::store_f32_aligned(ctx.v53.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vsubsws v5,v12,v4
	simd::store_i32(ctx.v5.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v12), simd::to_vec128i(ctx.v4)));
	// vsubsws v4,v11,v3
	simd::store_i32(ctx.v4.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v11), simd::to_vec128i(ctx.v3)));
	// vmulfp128 v52,v58,v46
	simd::store_f32_aligned(ctx.v52.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v58.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmaxsw v3,v13,v6
	simd::store_i32(ctx.v3.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v6.u32)));
	// vmulfp128 v45,v57,v46
	simd::store_f32_aligned(ctx.v45.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v57.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v44,v56,v46
	simd::store_f32_aligned(ctx.v44.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v56.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vcsxwfp128 v51,v10,0
	simd::store_f32_aligned(ctx.v51.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v10.s32)));
	// vcsxwfp128 v50,v9,0
	simd::store_f32_aligned(ctx.v50.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v9.s32)));
	// vxor v26,v3,v2
	simd::store_u8(ctx.v26.u8, simd::xor_i8(simd::load_u8(ctx.v3.u8), simd::load_u8(ctx.v2.u8)));
	// vcsxwfp128 v49,v8,0
	simd::store_f32_aligned(ctx.v49.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v8.s32)));
	// vcsxwfp128 v48,v5,0
	simd::store_f32_aligned(ctx.v48.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v5.s32)));
	// vcsxwfp128 v47,v4,0
	simd::store_f32_aligned(ctx.v47.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v4.s32)));
	// vcfpsxws128 v25,v55,0
	simd::store_i32(ctx.v25.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v55.f32)));
	// vsubsws v23,v26,v2
	simd::store_i32(ctx.v23.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v26), simd::to_vec128i(ctx.v2)));
	// vcfpsxws128 v24,v54,0
	simd::store_i32(ctx.v24.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v54.f32)));
	// vcfpsxws128 v22,v53,0
	simd::store_i32(ctx.v22.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v53.f32)));
	// vcfpsxws128 v21,v52,0
	simd::store_i32(ctx.v21.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v52.f32)));
	// vcsxwfp128 v42,v23,0
	simd::store_f32_aligned(ctx.v42.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v23.s32)));
	// vcfpsxws128 v20,v45,0
	simd::store_i32(ctx.v20.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v45.f32)));
	// vcfpsxws128 v19,v44,0
	simd::store_i32(ctx.v19.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v44.f32)));
	// stvlx128 v51,r23,r25
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r25.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v51), 15 - i));
}
	// stvlx128 v50,r23,r26
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r26.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v50), 15 - i));
}
	// stvlx128 v49,r23,r27
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r27.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v49), 15 - i));
}
	// stvlx128 v48,r23,r28
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r28.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v48), 15 - i));
}
	// stvlx128 v47,r23,r29
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r29.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v47), 15 - i));
}
	// vsubsws v18,v25,v0
	simd::store_i32(ctx.v18.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v25), simd::to_vec128i(ctx.v0)));
	// vsubsws v17,v24,v0
	simd::store_i32(ctx.v17.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v24), simd::to_vec128i(ctx.v0)));
	// vsubsws v16,v22,v0
	simd::store_i32(ctx.v16.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v22), simd::to_vec128i(ctx.v0)));
	// vsubsws v15,v21,v0
	simd::store_i32(ctx.v15.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v21), simd::to_vec128i(ctx.v0)));
	// vor v14,v18,v0
	simd::store_i8(ctx.v14.u8, simd::or_i8(simd::load_i8(ctx.v18.u8), simd::load_i8(ctx.v0.u8)));
	// stvlx128 v42,r23,r30
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r30.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v42), 15 - i));
}
	// vsubsws v12,v20,v0
	simd::store_i32(ctx.v12.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v20), simd::to_vec128i(ctx.v0)));
	// vsubsws v11,v19,v0
	simd::store_i32(ctx.v11.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v19), simd::to_vec128i(ctx.v0)));
	// vor v10,v17,v0
	simd::store_i8(ctx.v10.u8, simd::or_i8(simd::load_i8(ctx.v17.u8), simd::load_i8(ctx.v0.u8)));
	// vor v9,v16,v0
	simd::store_i8(ctx.v9.u8, simd::or_i8(simd::load_i8(ctx.v16.u8), simd::load_i8(ctx.v0.u8)));
	// vmulfp128 v39,v40,v43
	simd::store_f32_aligned(ctx.v39.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v40.f32), simd::load_f32_aligned(ctx.v43.f32)));
	// vmaxsw v3,v13,v10
	simd::store_i32(ctx.v3.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v10.u32)));
	// vor v8,v15,v0
	simd::store_i8(ctx.v8.u8, simd::or_i8(simd::load_i8(ctx.v15.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v2,v13,v9
	simd::store_i32(ctx.v2.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v9.u32)));
	// vor v6,v12,v0
	simd::store_i8(ctx.v6.u8, simd::or_i8(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v5,v13,v14
	simd::store_i32(ctx.v5.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v14.u32)));
	// vor v4,v11,v0
	simd::store_i8(ctx.v4.u8, simd::or_i8(simd::load_i8(ctx.v11.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v26,v13,v8
	simd::store_i32(ctx.v26.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v8.u32)));
	// vmaxsw v25,v13,v6
	simd::store_i32(ctx.v25.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v6.u32)));
	// vxor v24,v5,v1
	simd::store_u8(ctx.v24.u8, simd::xor_i8(simd::load_u8(ctx.v5.u8), simd::load_u8(ctx.v1.u8)));
	// vmaxsw v23,v13,v4
	simd::store_i32(ctx.v23.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v4.u32)));
	// vor128 v7,v38,v38
	simd::store_i8(ctx.v7.u8, simd::load_i8(ctx.v38.u8));
	// vxor v22,v3,v31
	simd::store_u8(ctx.v22.u8, simd::xor_i8(simd::load_u8(ctx.v3.u8), simd::load_u8(ctx.v31.u8)));
	// vmulfp128 v32,v39,v46
	simd::store_f32_aligned(ctx.v32.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vxor v21,v2,v30
	simd::store_u8(ctx.v21.u8, simd::xor_i8(simd::load_u8(ctx.v2.u8), simd::load_u8(ctx.v30.u8)));
	// vxor v20,v26,v29
	simd::store_u8(ctx.v20.u8, simd::xor_i8(simd::load_u8(ctx.v26.u8), simd::load_u8(ctx.v29.u8)));
	// vxor v19,v25,v28
	simd::store_u8(ctx.v19.u8, simd::xor_i8(simd::load_u8(ctx.v25.u8), simd::load_u8(ctx.v28.u8)));
	// vsubsws v18,v24,v1
	simd::store_i32(ctx.v18.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v24), simd::to_vec128i(ctx.v1)));
	// vxor v17,v23,v27
	simd::store_u8(ctx.v17.u8, simd::xor_i8(simd::load_u8(ctx.v23.u8), simd::load_u8(ctx.v27.u8)));
	// vsubsws v16,v22,v31
	simd::store_i32(ctx.v16.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v22), simd::to_vec128i(ctx.v31)));
	// vsubsws v15,v21,v30
	simd::store_i32(ctx.v15.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v21), simd::to_vec128i(ctx.v30)));
	// vsubsws v14,v20,v29
	simd::store_i32(ctx.v14.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v20), simd::to_vec128i(ctx.v29)));
	// vcsxwfp128 v37,v18,0
	simd::store_f32_aligned(ctx.v37.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v18.s32)));
	// vsubsws v12,v19,v28
	simd::store_i32(ctx.v12.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v19), simd::to_vec128i(ctx.v28)));
	// vsubsws v11,v17,v27
	simd::store_i32(ctx.v11.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v17), simd::to_vec128i(ctx.v27)));
	// vcsxwfp128 v36,v16,0
	simd::store_f32_aligned(ctx.v36.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v16.s32)));
	// vcsxwfp128 v35,v15,0
	simd::store_f32_aligned(ctx.v35.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v15.s32)));
	// vcsxwfp128 v34,v14,0
	simd::store_f32_aligned(ctx.v34.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v14.s32)));
	// vcfpsxws128 v10,v32,0
	simd::store_i32(ctx.v10.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v32.f32)));
	// vcsxwfp128 v33,v12,0
	simd::store_f32_aligned(ctx.v33.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v12.s32)));
	// vcsxwfp128 v63,v11,0
	simd::store_f32_aligned(ctx.v63.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v11.s32)));
	// stvlx128 v37,r23,r31
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r31.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v37), 15 - i));
}
	// stvlx128 v36,r23,r5
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r5.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v36), 15 - i));
}
	// stvlx128 v35,r23,r6
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r6.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v35), 15 - i));
}
	// stvlx128 v34,r23,r7
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v34), 15 - i));
}
	// vsubsws v9,v10,v0
	simd::store_i32(ctx.v9.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v10), simd::to_vec128i(ctx.v0)));
	// stvlx128 v33,r23,r9
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v33), 15 - i));
}
	// stvlx128 v63,r23,r10
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v63), 15 - i));
}
	// vor v8,v9,v0
	simd::store_i8(ctx.v8.u8, simd::or_i8(simd::load_i8(ctx.v9.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v6,v13,v8
	simd::store_i32(ctx.v6.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v8.u32)));
	// vxor v5,v6,v7
	simd::store_u8(ctx.v5.u8, simd::xor_i8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v7.u8)));
	// vsubsws v4,v5,v7
	simd::store_i32(ctx.v4.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v5), simd::to_vec128i(ctx.v7)));
	// vcsxwfp128 v62,v4,0
	simd::store_f32_aligned(ctx.v62.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v4.s32)));
	// stvlx128 v62,r23,r11
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v62), 15 - i));
}
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8232F4C0"))) PPC_WEAK_FUNC(sub_8232F4C0);
PPC_FUNC_IMPL(__imp__sub_8232F4C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x8232F4C8;
	__restfpr_14(ctx, base);
	// lwz r31,4352(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4352);
	// lwz r8,4360(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4360);
	// lwz r7,4356(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4356);
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// lwz r9,4364(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4364);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// beq cr6,0x8232f4f4
	if (ctx.cr6.eq) goto loc_8232F4F4;
	// subfic r11,r8,32
	ctx.xer.ca = ctx.r8.u32 <= 32;
	ctx.r11.s64 = 32 - ctx.r8.s64;
	// srw r10,r7,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r11.u8 & 0x3F));
	// or r11,r10,r31
	ctx.r11.u64 = ctx.r10.u64 | ctx.r31.u64;
loc_8232F4F4:
	// lwz r10,44(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	// rlwinm r6,r11,10,22,31
	ctx.r6.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 10) & 0x3FF;
	// lbzx r10,r6,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// rlwinm r5,r10,28,4,31
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r10,r10,28
	ctx.r10.u64 = ctx.r10.u32 & 0xF;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// beq cr6,0x8232f530
	if (ctx.cr6.eq) goto loc_8232F530;
	// slw r11,r11,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r11.u32 << (ctx.r10.u8 & 0x3F));
	// srawi r6,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r11.s32 >> 1;
	// subfic r11,r5,31
	ctx.xer.ca = ctx.r5.u32 <= 31;
	ctx.r11.s64 = 31 - ctx.r5.s64;
	// xoris r30,r6,32768
	ctx.r30.u64 = ctx.r6.u64 ^ 2147483648;
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// sraw r6,r30,r11
	temp.u32 = ctx.r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r30.s32 < 0) & (((ctx.r30.s32 >> temp.u32) << temp.u32) != ctx.r30.s32);
	ctx.r6.s64 = ctx.r30.s32 >> temp.u32;
	// rlwinm r11,r30,1,31,31
	ctx.r11.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0x1;
	// add r5,r6,r11
	ctx.r5.u64 = ctx.r6.u64 + ctx.r11.u64;
loc_8232F530:
	// add r11,r10,r8
	ctx.r11.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x8232f550
	if (ctx.cr6.lt) goto loc_8232F550;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f554
	goto loc_8232F554;
loc_8232F550:
	// slw r6,r31,r10
	ctx.r6.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r31.u32 << (ctx.r10.u8 & 0x3F));
loc_8232F554:
	// lwz r30,40(r4)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	// li r31,0
	ctx.r31.s64 = 0;
	// lis r24,-1
	ctx.r24.s64 = -65536;
	// lis r21,-1
	ctx.r21.s64 = -65536;
	// lis r26,-1
	ctx.r26.s64 = -65536;
	// lis r23,-1
	ctx.r23.s64 = -65536;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// ori r22,r24,10
	ctx.r22.u64 = ctx.r24.u64 | 10;
	// ori r24,r21,8
	ctx.r24.u64 = ctx.r21.u64 | 8;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// addi r8,r4,20
	ctx.r8.s64 = ctx.r4.s64 + 20;
	// stw r10,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r10.u32);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// li r29,2
	ctx.r29.s64 = 2;
	// std r10,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r10.u64);
	// lfd f0,-160(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// li r27,4
	ctx.r27.s64 = 4;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// stfs f12,0(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// stw r31,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, ctx.r31.u32);
	// lis r5,-1
	ctx.r5.s64 = -65536;
	// stw r31,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, ctx.r31.u32);
	// lis r31,-1
	ctx.r31.s64 = -65536;
	// lwz r10,4396(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4396);
	// ori r30,r5,2
	ctx.r30.u64 = ctx.r5.u64 | 2;
	// ori r28,r31,4
	ctx.r28.u64 = ctx.r31.u64 | 4;
	// ori r26,r26,6
	ctx.r26.u64 = ctx.r26.u64 | 6;
	// li r25,6
	ctx.r25.s64 = 6;
	// ori r23,r23,12
	ctx.r23.u64 = ctx.r23.u64 | 12;
	// li r16,20
	ctx.r16.s64 = 20;
	// li r17,11
	ctx.r17.s64 = 11;
	// li r18,13
	ctx.r18.s64 = 13;
	// li r19,14
	ctx.r19.s64 = 14;
	// li r20,15
	ctx.r20.s64 = 15;
	// li r21,17
	ctx.r21.s64 = 17;
loc_8232F5E8:
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x8232f600
	if (ctx.cr6.eq) goto loc_8232F600;
	// subfic r5,r11,32
	ctx.xer.ca = ctx.r11.u32 <= 32;
	ctx.r5.s64 = 32 - ctx.r11.s64;
	// srw r5,r7,r5
	ctx.r5.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r5.u8 & 0x3F));
	// or r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 | ctx.r6.u64;
loc_8232F600:
	// rlwinm r31,r5,9,23,31
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 9) & 0x1FF;
	// cmplwi cr6,r31,511
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 511, ctx.xer);
	// bgt cr6,0x82332bd0
	if (ctx.cr6.gt) goto loc_82332BD0;
	// lis r12,-32205
	ctx.r12.s64 = -2110586880;
	// rlwinm r0,r31,2,0,29
	ctx.r0.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r12,r12,-2524
	ctx.r12.s64 = ctx.r12.s64 + -2524;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr 
	switch (ctx.r31.u64) {
	case 0:
		goto loc_8233243C;
	case 1:
		goto loc_823323E8;
	case 2:
		goto loc_82332394;
	case 3:
		goto loc_82332394;
	case 4:
		goto loc_823322FC;
	case 5:
		goto loc_823322FC;
	case 6:
		goto loc_823322FC;
	case 7:
		goto loc_823322FC;
	case 8:
		goto loc_8233223C;
	case 9:
		goto loc_8233223C;
	case 10:
		goto loc_8233223C;
	case 11:
		goto loc_8233223C;
	case 12:
		goto loc_8233223C;
	case 13:
		goto loc_8233223C;
	case 14:
		goto loc_8233223C;
	case 15:
		goto loc_8233223C;
	case 16:
		goto loc_823321FC;
	case 17:
		goto loc_823321FC;
	case 18:
		goto loc_823321C4;
	case 19:
		goto loc_823321C4;
	case 20:
		goto loc_8233218C;
	case 21:
		goto loc_8233218C;
	case 22:
		goto loc_82332154;
	case 23:
		goto loc_82332154;
	case 24:
		goto loc_82332118;
	case 25:
		goto loc_82332118;
	case 26:
		goto loc_823320E0;
	case 27:
		goto loc_823320E0;
	case 28:
		goto loc_823320A8;
	case 29:
		goto loc_823320A8;
	case 30:
		goto loc_82332070;
	case 31:
		goto loc_82332070;
	case 32:
		goto loc_82332030;
	case 33:
		goto loc_82332030;
	case 34:
		goto loc_82332B94;
	case 35:
		goto loc_82332030;
	case 36:
		goto loc_82331FF8;
	case 37:
		goto loc_82331FF8;
	case 38:
		goto loc_82332B78;
	case 39:
		goto loc_82331FF8;
	case 40:
		goto loc_82331FC0;
	case 41:
		goto loc_82331FC0;
	case 42:
		goto loc_82332B70;
	case 43:
		goto loc_82331FC0;
	case 44:
		goto loc_82331F88;
	case 45:
		goto loc_82331F88;
	case 46:
		goto loc_82332B54;
	case 47:
		goto loc_82331F88;
	case 48:
		goto loc_82331F50;
	case 49:
		goto loc_82331F50;
	case 50:
		goto loc_82332B38;
	case 51:
		goto loc_82331F50;
	case 52:
		goto loc_82331F18;
	case 53:
		goto loc_82331F18;
	case 54:
		goto loc_82332B1C;
	case 55:
		goto loc_82331F18;
	case 56:
		goto loc_82331EE0;
	case 57:
		goto loc_82331EE0;
	case 58:
		goto loc_82332B14;
	case 59:
		goto loc_82331EE0;
	case 60:
		goto loc_82331EA8;
	case 61:
		goto loc_82331EA8;
	case 62:
		goto loc_82332AF8;
	case 63:
		goto loc_82331EA8;
	case 64:
		goto loc_82331E70;
	case 65:
		goto loc_82331E38;
	case 66:
		goto loc_82331DFC;
	case 67:
		goto loc_82331DC4;
	case 68:
		goto loc_82331D8C;
	case 69:
		goto loc_82331D54;
	case 70:
		goto loc_82331D1C;
	case 71:
		goto loc_82331CE4;
	case 72:
		goto loc_82331CAC;
	case 73:
		goto loc_82331C74;
	case 74:
		goto loc_82331C3C;
	case 75:
		goto loc_82331C04;
	case 76:
		goto loc_82331BC8;
	case 77:
		goto loc_82331B90;
	case 78:
		goto loc_82331B58;
	case 79:
		goto loc_82331B20;
	case 80:
		goto loc_82331AE0;
	case 81:
		goto loc_82331AE0;
	case 82:
		goto loc_82331AE0;
	case 83:
		goto loc_82331AE0;
	case 84:
		goto loc_82332AC0;
	case 85:
		goto loc_82332AC0;
	case 86:
		goto loc_82331A90;
	case 87:
		goto loc_82331A40;
	case 88:
		goto loc_82331A08;
	case 89:
		goto loc_82331A08;
	case 90:
		goto loc_82331A08;
	case 91:
		goto loc_82331A08;
	case 92:
		goto loc_82332A88;
	case 93:
		goto loc_82332A88;
	case 94:
		goto loc_823319B8;
	case 95:
		goto loc_82331968;
	case 96:
		goto loc_82331930;
	case 97:
		goto loc_82331930;
	case 98:
		goto loc_82331930;
	case 99:
		goto loc_82331930;
	case 100:
		goto loc_82332A50;
	case 101:
		goto loc_82332A50;
	case 102:
		goto loc_823318E0;
	case 103:
		goto loc_82331890;
	case 104:
		goto loc_82331858;
	case 105:
		goto loc_82331858;
	case 106:
		goto loc_82331858;
	case 107:
		goto loc_82331858;
	case 108:
		goto loc_82332A18;
	case 109:
		goto loc_82332A18;
	case 110:
		goto loc_82331808;
	case 111:
		goto loc_823317B8;
	case 112:
		goto loc_82331780;
	case 113:
		goto loc_82331780;
	case 114:
		goto loc_82331780;
	case 115:
		goto loc_82331780;
	case 116:
		goto loc_823329E0;
	case 117:
		goto loc_823329E0;
	case 118:
		goto loc_82331730;
	case 119:
		goto loc_823316E0;
	case 120:
		goto loc_823316A8;
	case 121:
		goto loc_823316A8;
	case 122:
		goto loc_823316A8;
	case 123:
		goto loc_823316A8;
	case 124:
		goto loc_823329A8;
	case 125:
		goto loc_823329A8;
	case 126:
		goto loc_82331658;
	case 127:
		goto loc_82331608;
	case 128:
		goto loc_823315C8;
	case 129:
		goto loc_823315C8;
	case 130:
		goto loc_823315C8;
	case 131:
		goto loc_823315C8;
	case 132:
		goto loc_823315C8;
	case 133:
		goto loc_823315C8;
	case 134:
		goto loc_82331578;
	case 135:
		goto loc_82331528;
	case 136:
		goto loc_82332970;
	case 137:
		goto loc_82332970;
	case 138:
		goto loc_82332970;
	case 139:
		goto loc_82332970;
	case 140:
		goto loc_823314D8;
	case 141:
		goto loc_823314D8;
	case 142:
		goto loc_82331488;
	case 143:
		goto loc_82331488;
	case 144:
		goto loc_82331450;
	case 145:
		goto loc_82331450;
	case 146:
		goto loc_82331450;
	case 147:
		goto loc_82331450;
	case 148:
		goto loc_82331450;
	case 149:
		goto loc_82331450;
	case 150:
		goto loc_82331400;
	case 151:
		goto loc_823313B0;
	case 152:
		goto loc_82332938;
	case 153:
		goto loc_82332938;
	case 154:
		goto loc_82332938;
	case 155:
		goto loc_82332938;
	case 156:
		goto loc_82331360;
	case 157:
		goto loc_82331360;
	case 158:
		goto loc_82331310;
	case 159:
		goto loc_82331310;
	case 160:
		goto loc_823312D8;
	case 161:
		goto loc_823312D8;
	case 162:
		goto loc_823312D8;
	case 163:
		goto loc_823312D8;
	case 164:
		goto loc_823312D8;
	case 165:
		goto loc_823312D8;
	case 166:
		goto loc_82331288;
	case 167:
		goto loc_82331238;
	case 168:
		goto loc_82332900;
	case 169:
		goto loc_82332900;
	case 170:
		goto loc_82332900;
	case 171:
		goto loc_82332900;
	case 172:
		goto loc_823311E8;
	case 173:
		goto loc_823311E8;
	case 174:
		goto loc_82331198;
	case 175:
		goto loc_82331198;
	case 176:
		goto loc_82331160;
	case 177:
		goto loc_82331160;
	case 178:
		goto loc_82331160;
	case 179:
		goto loc_82331160;
	case 180:
		goto loc_82331160;
	case 181:
		goto loc_82331160;
	case 182:
		goto loc_82331110;
	case 183:
		goto loc_823310C0;
	case 184:
		goto loc_823328C0;
	case 185:
		goto loc_823328C0;
	case 186:
		goto loc_823328C0;
	case 187:
		goto loc_823328C0;
	case 188:
		goto loc_82331070;
	case 189:
		goto loc_82331070;
	case 190:
		goto loc_82331020;
	case 191:
		goto loc_82331020;
	case 192:
		goto loc_82330FE0;
	case 193:
		goto loc_82330FE0;
	case 194:
		goto loc_82330FE0;
	case 195:
		goto loc_82330FE0;
	case 196:
		goto loc_82330FE0;
	case 197:
		goto loc_82330FE0;
	case 198:
		goto loc_82330FE0;
	case 199:
		goto loc_82330FE0;
	case 200:
		goto loc_82330F90;
	case 201:
		goto loc_82330F40;
	case 202:
		goto loc_82330EF0;
	case 203:
		goto loc_82330EA0;
	case 204:
		goto loc_82330E50;
	case 205:
		goto loc_82330E50;
	case 206:
		goto loc_82330E00;
	case 207:
		goto loc_82330E00;
	case 208:
		goto loc_82332888;
	case 209:
		goto loc_82332888;
	case 210:
		goto loc_82332888;
	case 211:
		goto loc_82332888;
	case 212:
		goto loc_82332888;
	case 213:
		goto loc_82332888;
	case 214:
		goto loc_82332888;
	case 215:
		goto loc_82332888;
	case 216:
		goto loc_82330DB0;
	case 217:
		goto loc_82330DB0;
	case 218:
		goto loc_82332868;
	case 219:
		goto loc_82330DB0;
	case 220:
		goto loc_82330D60;
	case 221:
		goto loc_82330D60;
	case 222:
		goto loc_82332834;
	case 223:
		goto loc_82330D60;
	case 224:
		goto loc_82330D28;
	case 225:
		goto loc_82330D28;
	case 226:
		goto loc_82330D28;
	case 227:
		goto loc_82330D28;
	case 228:
		goto loc_82330D28;
	case 229:
		goto loc_82330D28;
	case 230:
		goto loc_82330D28;
	case 231:
		goto loc_82330D28;
	case 232:
		goto loc_82330CD8;
	case 233:
		goto loc_82330C88;
	case 234:
		goto loc_82330C38;
	case 235:
		goto loc_82330BE8;
	case 236:
		goto loc_82330B98;
	case 237:
		goto loc_82330B98;
	case 238:
		goto loc_82330B48;
	case 239:
		goto loc_82330B48;
	case 240:
		goto loc_823327F4;
	case 241:
		goto loc_823327F4;
	case 242:
		goto loc_823327F4;
	case 243:
		goto loc_823327F4;
	case 244:
		goto loc_823327F4;
	case 245:
		goto loc_823327F4;
	case 246:
		goto loc_823327F4;
	case 247:
		goto loc_823327F4;
	case 248:
		goto loc_82330AF8;
	case 249:
		goto loc_82330AF8;
	case 250:
		goto loc_823327D4;
	case 251:
		goto loc_82330AF8;
	case 252:
		goto loc_82330AA8;
	case 253:
		goto loc_82330AA8;
	case 254:
		goto loc_823327A0;
	case 255:
		goto loc_82330AA8;
	case 256:
		goto loc_82332778;
	case 257:
		goto loc_82332778;
	case 258:
		goto loc_82332778;
	case 259:
		goto loc_82332778;
	case 260:
		goto loc_82332778;
	case 261:
		goto loc_82332778;
	case 262:
		goto loc_82332778;
	case 263:
		goto loc_82332778;
	case 264:
		goto loc_82332778;
	case 265:
		goto loc_82332778;
	case 266:
		goto loc_82332778;
	case 267:
		goto loc_82332778;
	case 268:
		goto loc_82332778;
	case 269:
		goto loc_82332778;
	case 270:
		goto loc_82332778;
	case 271:
		goto loc_82332778;
	case 272:
		goto loc_82332778;
	case 273:
		goto loc_82332778;
	case 274:
		goto loc_82332778;
	case 275:
		goto loc_82332778;
	case 276:
		goto loc_82332778;
	case 277:
		goto loc_82332778;
	case 278:
		goto loc_82332778;
	case 279:
		goto loc_82332778;
	case 280:
		goto loc_82332778;
	case 281:
		goto loc_82332778;
	case 282:
		goto loc_82332778;
	case 283:
		goto loc_82332778;
	case 284:
		goto loc_82332778;
	case 285:
		goto loc_82332778;
	case 286:
		goto loc_82332778;
	case 287:
		goto loc_82332778;
	case 288:
		goto loc_82332778;
	case 289:
		goto loc_82332778;
	case 290:
		goto loc_82332778;
	case 291:
		goto loc_82332778;
	case 292:
		goto loc_82332778;
	case 293:
		goto loc_82332778;
	case 294:
		goto loc_82332778;
	case 295:
		goto loc_82332778;
	case 296:
		goto loc_82332778;
	case 297:
		goto loc_82332778;
	case 298:
		goto loc_82332778;
	case 299:
		goto loc_82332778;
	case 300:
		goto loc_82332778;
	case 301:
		goto loc_82332778;
	case 302:
		goto loc_82332778;
	case 303:
		goto loc_82332778;
	case 304:
		goto loc_82332778;
	case 305:
		goto loc_82332778;
	case 306:
		goto loc_82332778;
	case 307:
		goto loc_82332778;
	case 308:
		goto loc_82332778;
	case 309:
		goto loc_82332778;
	case 310:
		goto loc_82332778;
	case 311:
		goto loc_82332778;
	case 312:
		goto loc_82332778;
	case 313:
		goto loc_82332778;
	case 314:
		goto loc_82332778;
	case 315:
		goto loc_82332778;
	case 316:
		goto loc_82332778;
	case 317:
		goto loc_82332778;
	case 318:
		goto loc_82332778;
	case 319:
		goto loc_82332778;
	case 320:
		goto loc_82332778;
	case 321:
		goto loc_82332778;
	case 322:
		goto loc_82332778;
	case 323:
		goto loc_82332778;
	case 324:
		goto loc_82332778;
	case 325:
		goto loc_82332778;
	case 326:
		goto loc_82332778;
	case 327:
		goto loc_82332778;
	case 328:
		goto loc_82332778;
	case 329:
		goto loc_82332778;
	case 330:
		goto loc_82332778;
	case 331:
		goto loc_82332778;
	case 332:
		goto loc_82332778;
	case 333:
		goto loc_82332778;
	case 334:
		goto loc_82332778;
	case 335:
		goto loc_82332778;
	case 336:
		goto loc_82332778;
	case 337:
		goto loc_82332778;
	case 338:
		goto loc_82332778;
	case 339:
		goto loc_82332778;
	case 340:
		goto loc_82332778;
	case 341:
		goto loc_82332778;
	case 342:
		goto loc_82332778;
	case 343:
		goto loc_82332778;
	case 344:
		goto loc_82332778;
	case 345:
		goto loc_82332778;
	case 346:
		goto loc_82332778;
	case 347:
		goto loc_82332778;
	case 348:
		goto loc_82332778;
	case 349:
		goto loc_82332778;
	case 350:
		goto loc_82332778;
	case 351:
		goto loc_82332778;
	case 352:
		goto loc_82332778;
	case 353:
		goto loc_82332778;
	case 354:
		goto loc_82332778;
	case 355:
		goto loc_82332778;
	case 356:
		goto loc_82332778;
	case 357:
		goto loc_82332778;
	case 358:
		goto loc_82332778;
	case 359:
		goto loc_82332778;
	case 360:
		goto loc_82332778;
	case 361:
		goto loc_82332778;
	case 362:
		goto loc_82332778;
	case 363:
		goto loc_82332778;
	case 364:
		goto loc_82332778;
	case 365:
		goto loc_82332778;
	case 366:
		goto loc_82332778;
	case 367:
		goto loc_82332778;
	case 368:
		goto loc_82332778;
	case 369:
		goto loc_82332778;
	case 370:
		goto loc_82332778;
	case 371:
		goto loc_82332778;
	case 372:
		goto loc_82332778;
	case 373:
		goto loc_82332778;
	case 374:
		goto loc_82332778;
	case 375:
		goto loc_82332778;
	case 376:
		goto loc_82332778;
	case 377:
		goto loc_82332778;
	case 378:
		goto loc_82332778;
	case 379:
		goto loc_82332778;
	case 380:
		goto loc_82332778;
	case 381:
		goto loc_82332778;
	case 382:
		goto loc_82332778;
	case 383:
		goto loc_82332778;
	case 384:
		goto loc_82330A68;
	case 385:
		goto loc_82330A68;
	case 386:
		goto loc_82330A68;
	case 387:
		goto loc_82330A68;
	case 388:
		goto loc_82330A68;
	case 389:
		goto loc_82330A68;
	case 390:
		goto loc_82330A68;
	case 391:
		goto loc_82330A68;
	case 392:
		goto loc_82330A68;
	case 393:
		goto loc_82330A68;
	case 394:
		goto loc_82330A18;
	case 395:
		goto loc_823309C8;
	case 396:
		goto loc_82330978;
	case 397:
		goto loc_82330928;
	case 398:
		goto loc_823308D8;
	case 399:
		goto loc_82330888;
	case 400:
		goto loc_82330838;
	case 401:
		goto loc_82330838;
	case 402:
		goto loc_823307E8;
	case 403:
		goto loc_823307E8;
	case 404:
		goto loc_82330798;
	case 405:
		goto loc_82330798;
	case 406:
		goto loc_82330748;
	case 407:
		goto loc_82330748;
	case 408:
		goto loc_823306F8;
	case 409:
		goto loc_823306F8;
	case 410:
		goto loc_82332758;
	case 411:
		goto loc_823306F8;
	case 412:
		goto loc_823306A8;
	case 413:
		goto loc_823306A8;
	case 414:
		goto loc_82332724;
	case 415:
		goto loc_823306A8;
	case 416:
		goto loc_823326EC;
	case 417:
		goto loc_823326EC;
	case 418:
		goto loc_823326EC;
	case 419:
		goto loc_823326EC;
	case 420:
		goto loc_823326EC;
	case 421:
		goto loc_823326EC;
	case 422:
		goto loc_823326EC;
	case 423:
		goto loc_823326EC;
	case 424:
		goto loc_823326EC;
	case 425:
		goto loc_823326EC;
	case 426:
		goto loc_823326EC;
	case 427:
		goto loc_823326EC;
	case 428:
		goto loc_823326EC;
	case 429:
		goto loc_823326EC;
	case 430:
		goto loc_823326EC;
	case 431:
		goto loc_823326EC;
	case 432:
		goto loc_82330658;
	case 433:
		goto loc_82330658;
	case 434:
		goto loc_82330658;
	case 435:
		goto loc_82330658;
	case 436:
		goto loc_8233269C;
	case 437:
		goto loc_8233269C;
	case 438:
		goto loc_823305F0;
	case 439:
		goto loc_82330588;
	case 440:
		goto loc_82330538;
	case 441:
		goto loc_82330538;
	case 442:
		goto loc_82330538;
	case 443:
		goto loc_82330538;
	case 444:
		goto loc_8233264C;
	case 445:
		goto loc_8233264C;
	case 446:
		goto loc_823304D0;
	case 447:
		goto loc_82330468;
	case 448:
		goto loc_82330430;
	case 449:
		goto loc_82330430;
	case 450:
		goto loc_82330430;
	case 451:
		goto loc_82330430;
	case 452:
		goto loc_82330430;
	case 453:
		goto loc_82330430;
	case 454:
		goto loc_82330430;
	case 455:
		goto loc_82330430;
	case 456:
		goto loc_82330430;
	case 457:
		goto loc_82330430;
	case 458:
		goto loc_823303E0;
	case 459:
		goto loc_82330390;
	case 460:
		goto loc_82330340;
	case 461:
		goto loc_823302F0;
	case 462:
		goto loc_823302A0;
	case 463:
		goto loc_82330250;
	case 464:
		goto loc_82330200;
	case 465:
		goto loc_82330200;
	case 466:
		goto loc_823301B0;
	case 467:
		goto loc_823301B0;
	case 468:
		goto loc_82330160;
	case 469:
		goto loc_82330160;
	case 470:
		goto loc_82330110;
	case 471:
		goto loc_82330110;
	case 472:
		goto loc_823300C0;
	case 473:
		goto loc_823300C0;
	case 474:
		goto loc_8233262C;
	case 475:
		goto loc_823300C0;
	case 476:
		goto loc_82330070;
	case 477:
		goto loc_82330070;
	case 478:
		goto loc_823325F8;
	case 479:
		goto loc_82330070;
	case 480:
		goto loc_823325B8;
	case 481:
		goto loc_823325B8;
	case 482:
		goto loc_823325B8;
	case 483:
		goto loc_823325B8;
	case 484:
		goto loc_823325B8;
	case 485:
		goto loc_823325B8;
	case 486:
		goto loc_823325B8;
	case 487:
		goto loc_823325B8;
	case 488:
		goto loc_823325B8;
	case 489:
		goto loc_823325B8;
	case 490:
		goto loc_823325B8;
	case 491:
		goto loc_823325B8;
	case 492:
		goto loc_823325B8;
	case 493:
		goto loc_823325B8;
	case 494:
		goto loc_823325B8;
	case 495:
		goto loc_823325B8;
	case 496:
		goto loc_82330020;
	case 497:
		goto loc_82330020;
	case 498:
		goto loc_82330020;
	case 499:
		goto loc_82330020;
	case 500:
		goto loc_82332568;
	case 501:
		goto loc_82332568;
	case 502:
		goto loc_8232FFB8;
	case 503:
		goto loc_8232FF50;
	case 504:
		goto loc_8232FF00;
	case 505:
		goto loc_8232FF00;
	case 506:
		goto loc_8232FF00;
	case 507:
		goto loc_8232FF00;
	case 508:
		goto loc_82332510;
	case 509:
		goto loc_82332510;
	case 510:
		goto loc_8232FE98;
	case 511:
		goto loc_8232FE24;
	default:
		__builtin_unreachable();
	}
	// lwz r17,9276(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9276);
	// lwz r17,9192(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9192);
	// lwz r17,9108(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9108);
	// lwz r17,9108(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9108);
	// lwz r17,8956(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8956);
	// lwz r17,8956(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8956);
	// lwz r17,8956(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8956);
	// lwz r17,8956(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8956);
	// lwz r17,8764(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8764);
	// lwz r17,8764(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8764);
	// lwz r17,8764(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8764);
	// lwz r17,8764(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8764);
	// lwz r17,8764(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8764);
	// lwz r17,8764(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8764);
	// lwz r17,8764(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8764);
	// lwz r17,8764(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8764);
	// lwz r17,8700(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8700);
	// lwz r17,8700(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8700);
	// lwz r17,8644(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8644);
	// lwz r17,8644(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8644);
	// lwz r17,8588(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8588);
	// lwz r17,8588(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8588);
	// lwz r17,8532(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8532);
	// lwz r17,8532(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8532);
	// lwz r17,8472(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8472);
	// lwz r17,8472(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8472);
	// lwz r17,8416(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8416);
	// lwz r17,8416(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8416);
	// lwz r17,8360(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8360);
	// lwz r17,8360(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8360);
	// lwz r17,8304(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8304);
	// lwz r17,8304(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8304);
	// lwz r17,8240(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8240);
	// lwz r17,8240(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8240);
	// lwz r17,11156(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 11156);
	// lwz r17,8240(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8240);
	// lwz r17,8184(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8184);
	// lwz r17,8184(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8184);
	// lwz r17,11128(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 11128);
	// lwz r17,8184(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8184);
	// lwz r17,8128(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8128);
	// lwz r17,8128(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8128);
	// lwz r17,11120(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 11120);
	// lwz r17,8128(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8128);
	// lwz r17,8072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8072);
	// lwz r17,8072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8072);
	// lwz r17,11092(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 11092);
	// lwz r17,8072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8072);
	// lwz r17,8016(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8016);
	// lwz r17,8016(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8016);
	// lwz r17,11064(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 11064);
	// lwz r17,8016(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8016);
	// lwz r17,7960(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7960);
	// lwz r17,7960(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7960);
	// lwz r17,11036(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 11036);
	// lwz r17,7960(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7960);
	// lwz r17,7904(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7904);
	// lwz r17,7904(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7904);
	// lwz r17,11028(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 11028);
	// lwz r17,7904(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7904);
	// lwz r17,7848(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7848);
	// lwz r17,7848(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7848);
	// lwz r17,11000(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 11000);
	// lwz r17,7848(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7848);
	// lwz r17,7792(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7792);
	// lwz r17,7736(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7736);
	// lwz r17,7676(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7676);
	// lwz r17,7620(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7620);
	// lwz r17,7564(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7564);
	// lwz r17,7508(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7508);
	// lwz r17,7452(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7452);
	// lwz r17,7396(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7396);
	// lwz r17,7340(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7340);
	// lwz r17,7284(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7284);
	// lwz r17,7228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7228);
	// lwz r17,7172(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7172);
	// lwz r17,7112(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7112);
	// lwz r17,7056(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7056);
	// lwz r17,7000(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 7000);
	// lwz r17,6944(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6944);
	// lwz r17,6880(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6880);
	// lwz r17,6880(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6880);
	// lwz r17,6880(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6880);
	// lwz r17,6880(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6880);
	// lwz r17,10944(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10944);
	// lwz r17,10944(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10944);
	// lwz r17,6800(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6800);
	// lwz r17,6720(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6720);
	// lwz r17,6664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6664);
	// lwz r17,6664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6664);
	// lwz r17,6664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6664);
	// lwz r17,6664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6664);
	// lwz r17,10888(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10888);
	// lwz r17,10888(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10888);
	// lwz r17,6584(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6584);
	// lwz r17,6504(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6504);
	// lwz r17,6448(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6448);
	// lwz r17,6448(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6448);
	// lwz r17,6448(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6448);
	// lwz r17,6448(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6448);
	// lwz r17,10832(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10832);
	// lwz r17,10832(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10832);
	// lwz r17,6368(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6368);
	// lwz r17,6288(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6288);
	// lwz r17,6232(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6232);
	// lwz r17,6232(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6232);
	// lwz r17,6232(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6232);
	// lwz r17,6232(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6232);
	// lwz r17,10776(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10776);
	// lwz r17,10776(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10776);
	// lwz r17,6152(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6152);
	// lwz r17,6072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6072);
	// lwz r17,6016(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6016);
	// lwz r17,6016(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6016);
	// lwz r17,6016(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6016);
	// lwz r17,6016(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 6016);
	// lwz r17,10720(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10720);
	// lwz r17,10720(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10720);
	// lwz r17,5936(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5936);
	// lwz r17,5856(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5856);
	// lwz r17,5800(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5800);
	// lwz r17,5800(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5800);
	// lwz r17,5800(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5800);
	// lwz r17,5800(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5800);
	// lwz r17,10664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10664);
	// lwz r17,10664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10664);
	// lwz r17,5720(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5720);
	// lwz r17,5640(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5640);
	// lwz r17,5576(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5576);
	// lwz r17,5576(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5576);
	// lwz r17,5576(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5576);
	// lwz r17,5576(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5576);
	// lwz r17,5576(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5576);
	// lwz r17,5576(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5576);
	// lwz r17,5496(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5496);
	// lwz r17,5416(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5416);
	// lwz r17,10608(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10608);
	// lwz r17,10608(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10608);
	// lwz r17,10608(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10608);
	// lwz r17,10608(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10608);
	// lwz r17,5336(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5336);
	// lwz r17,5336(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5336);
	// lwz r17,5256(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5256);
	// lwz r17,5256(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5256);
	// lwz r17,5200(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5200);
	// lwz r17,5200(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5200);
	// lwz r17,5200(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5200);
	// lwz r17,5200(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5200);
	// lwz r17,5200(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5200);
	// lwz r17,5200(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5200);
	// lwz r17,5120(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5120);
	// lwz r17,5040(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 5040);
	// lwz r17,10552(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10552);
	// lwz r17,10552(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10552);
	// lwz r17,10552(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10552);
	// lwz r17,10552(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10552);
	// lwz r17,4960(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4960);
	// lwz r17,4960(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4960);
	// lwz r17,4880(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4880);
	// lwz r17,4880(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4880);
	// lwz r17,4824(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4824);
	// lwz r17,4824(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4824);
	// lwz r17,4824(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4824);
	// lwz r17,4824(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4824);
	// lwz r17,4824(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4824);
	// lwz r17,4824(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4824);
	// lwz r17,4744(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4744);
	// lwz r17,4664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4664);
	// lwz r17,10496(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10496);
	// lwz r17,10496(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10496);
	// lwz r17,10496(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10496);
	// lwz r17,10496(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10496);
	// lwz r17,4584(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4584);
	// lwz r17,4584(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4584);
	// lwz r17,4504(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4504);
	// lwz r17,4504(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4504);
	// lwz r17,4448(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4448);
	// lwz r17,4448(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4448);
	// lwz r17,4448(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4448);
	// lwz r17,4448(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4448);
	// lwz r17,4448(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4448);
	// lwz r17,4448(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4448);
	// lwz r17,4368(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4368);
	// lwz r17,4288(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4288);
	// lwz r17,10432(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10432);
	// lwz r17,10432(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10432);
	// lwz r17,10432(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10432);
	// lwz r17,10432(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10432);
	// lwz r17,4208(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4208);
	// lwz r17,4208(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4208);
	// lwz r17,4128(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4128);
	// lwz r17,4128(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4128);
	// lwz r17,4064(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4064);
	// lwz r17,4064(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4064);
	// lwz r17,4064(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4064);
	// lwz r17,4064(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4064);
	// lwz r17,4064(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4064);
	// lwz r17,4064(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4064);
	// lwz r17,4064(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4064);
	// lwz r17,4064(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 4064);
	// lwz r17,3984(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3984);
	// lwz r17,3904(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3904);
	// lwz r17,3824(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3824);
	// lwz r17,3744(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3744);
	// lwz r17,3664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3664);
	// lwz r17,3664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3664);
	// lwz r17,3584(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3584);
	// lwz r17,3584(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3584);
	// lwz r17,10376(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10376);
	// lwz r17,10376(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10376);
	// lwz r17,10376(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10376);
	// lwz r17,10376(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10376);
	// lwz r17,10376(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10376);
	// lwz r17,10376(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10376);
	// lwz r17,10376(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10376);
	// lwz r17,10376(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10376);
	// lwz r17,3504(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3504);
	// lwz r17,3504(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3504);
	// lwz r17,10344(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10344);
	// lwz r17,3504(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3504);
	// lwz r17,3424(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3424);
	// lwz r17,3424(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3424);
	// lwz r17,10292(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10292);
	// lwz r17,3424(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3424);
	// lwz r17,3368(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3368);
	// lwz r17,3368(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3368);
	// lwz r17,3368(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3368);
	// lwz r17,3368(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3368);
	// lwz r17,3368(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3368);
	// lwz r17,3368(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3368);
	// lwz r17,3368(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3368);
	// lwz r17,3368(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3368);
	// lwz r17,3288(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3288);
	// lwz r17,3208(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3208);
	// lwz r17,3128(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3128);
	// lwz r17,3048(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 3048);
	// lwz r17,2968(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2968);
	// lwz r17,2968(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2968);
	// lwz r17,2888(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2888);
	// lwz r17,2888(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2888);
	// lwz r17,10228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10228);
	// lwz r17,10228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10228);
	// lwz r17,10228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10228);
	// lwz r17,10228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10228);
	// lwz r17,10228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10228);
	// lwz r17,10228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10228);
	// lwz r17,10228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10228);
	// lwz r17,10228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10228);
	// lwz r17,2808(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2808);
	// lwz r17,2808(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2808);
	// lwz r17,10196(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10196);
	// lwz r17,2808(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2808);
	// lwz r17,2728(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2728);
	// lwz r17,2728(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2728);
	// lwz r17,10144(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10144);
	// lwz r17,2728(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2728);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,10104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10104);
	// lwz r17,2664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2664);
	// lwz r17,2664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2664);
	// lwz r17,2664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2664);
	// lwz r17,2664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2664);
	// lwz r17,2664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2664);
	// lwz r17,2664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2664);
	// lwz r17,2664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2664);
	// lwz r17,2664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2664);
	// lwz r17,2664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2664);
	// lwz r17,2664(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2664);
	// lwz r17,2584(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2584);
	// lwz r17,2504(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2504);
	// lwz r17,2424(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2424);
	// lwz r17,2344(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2344);
	// lwz r17,2264(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2264);
	// lwz r17,2184(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2184);
	// lwz r17,2104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2104);
	// lwz r17,2104(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2104);
	// lwz r17,2024(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2024);
	// lwz r17,2024(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 2024);
	// lwz r17,1944(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1944);
	// lwz r17,1944(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1944);
	// lwz r17,1864(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1864);
	// lwz r17,1864(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1864);
	// lwz r17,1784(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1784);
	// lwz r17,1784(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1784);
	// lwz r17,10072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10072);
	// lwz r17,1784(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1784);
	// lwz r17,1704(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1704);
	// lwz r17,1704(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1704);
	// lwz r17,10020(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 10020);
	// lwz r17,1704(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1704);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,9964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9964);
	// lwz r17,1624(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1624);
	// lwz r17,1624(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1624);
	// lwz r17,1624(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1624);
	// lwz r17,1624(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1624);
	// lwz r17,9884(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9884);
	// lwz r17,9884(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9884);
	// lwz r17,1520(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1520);
	// lwz r17,1416(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1416);
	// lwz r17,1336(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1336);
	// lwz r17,1336(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1336);
	// lwz r17,1336(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1336);
	// lwz r17,1336(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1336);
	// lwz r17,9804(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9804);
	// lwz r17,9804(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9804);
	// lwz r17,1232(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1232);
	// lwz r17,1128(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1128);
	// lwz r17,1072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1072);
	// lwz r17,1072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1072);
	// lwz r17,1072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1072);
	// lwz r17,1072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1072);
	// lwz r17,1072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1072);
	// lwz r17,1072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1072);
	// lwz r17,1072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1072);
	// lwz r17,1072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1072);
	// lwz r17,1072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1072);
	// lwz r17,1072(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 1072);
	// lwz r17,992(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 992);
	// lwz r17,912(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 912);
	// lwz r17,832(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 832);
	// lwz r17,752(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 752);
	// lwz r17,672(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 672);
	// lwz r17,592(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 592);
	// lwz r17,512(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 512);
	// lwz r17,512(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 512);
	// lwz r17,432(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 432);
	// lwz r17,432(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 432);
	// lwz r17,352(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 352);
	// lwz r17,352(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 352);
	// lwz r17,272(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 272);
	// lwz r17,272(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 272);
	// lwz r17,192(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 192);
	// lwz r17,192(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 192);
	// lwz r17,9772(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9772);
	// lwz r17,192(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 192);
	// lwz r17,112(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 112);
	// lwz r17,112(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 112);
	// lwz r17,9720(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9720);
	// lwz r17,112(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 112);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,9656(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9656);
	// lwz r17,32(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 32);
	// lwz r17,32(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 32);
	// lwz r17,32(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 32);
	// lwz r17,32(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 32);
	// lwz r17,9576(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9576);
	// lwz r17,9576(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9576);
	// lwz r17,-72(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -72);
	// lwz r17,-176(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -176);
	// lwz r17,-256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -256);
	// lwz r17,-256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -256);
	// lwz r17,-256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -256);
	// lwz r17,-256(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -256);
	// lwz r17,9488(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9488);
	// lwz r17,9488(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 9488);
	// lwz r17,-360(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -360);
	// lwz r17,-476(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + -476);
loc_8232FE24:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r3,r5
	ctx.r3.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r3,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r3.u32);
	// lwz r3,28(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r3,r5
	ctx.r3.s64 = ctx.r5.s8;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r3,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r3.u32);
	// lwz r3,28(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r3,r5
	ctx.r3.s64 = ctx.r5.s8;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r3,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r3.u32);
	// lwz r3,28(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r30.u32);
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_8232FE90:
	// rlwinm r6,r6,9,0,22
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 9) & 0xFFFFFE00;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_8232FE98:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_8232FF00:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// rlwinm r31,r5,2,0,29
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82331b18
	if (ctx.cr6.lt) goto loc_82331B18;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_8232FF50:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r31,r5,2,0,29
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_8232FFB8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330020:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82331b18
	if (ctx.cr6.lt) goto loc_82331B18;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330070:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823300C0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330110:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330160:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823301B0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330200:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330250:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823302A0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823302F0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330340:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330390:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823303E0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330430:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82330aa0
	if (ctx.cr6.lt) goto loc_82330AA0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330468:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// rlwinm r31,r5,2,0,29
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823304D0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330538:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82331b18
	if (ctx.cr6.lt) goto loc_82331B18;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330588:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823305F0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330658:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82331b18
	if (ctx.cr6.lt) goto loc_82331B18;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823306A8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823306F8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330748:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330798:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823307E8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330838:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330888:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823308D8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330928:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330978:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823309C8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330A18:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330A68:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82330aa0
	if (ctx.cr6.lt) goto loc_82330AA0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330AA0:
	// rlwinm r6,r6,3,0,28
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330AA8:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330AF8:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330B48:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330B98:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330BE8:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330C38:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330C88:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330CD8:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330D28:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82331018
	if (ctx.cr6.lt) goto loc_82331018;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330D60:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330DB0:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330E00:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330E50:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330EA0:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330EF0:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330F40:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330F90:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82330FE0:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82331018
	if (ctx.cr6.lt) goto loc_82331018;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331018:
	// rlwinm r6,r6,4,0,27
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331020:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331070:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823310C0:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331110:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331160:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82331600
	if (ctx.cr6.lt) goto loc_82331600;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331198:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823311E8:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331238:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331288:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823312D8:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82331600
	if (ctx.cr6.lt) goto loc_82331600;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331310:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331360:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823313B0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331400:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331450:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82331600
	if (ctx.cr6.lt) goto loc_82331600;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331488:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823314D8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331528:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331578:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823315C8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82331600
	if (ctx.cr6.lt) goto loc_82331600;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331600:
	// rlwinm r6,r6,5,0,26
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331608:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331658:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823316A8:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82331b18
	if (ctx.cr6.lt) goto loc_82331B18;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823316E0:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331730:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331780:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82331b18
	if (ctx.cr6.lt) goto loc_82331B18;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823317B8:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331808:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331858:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82331b18
	if (ctx.cr6.lt) goto loc_82331B18;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331890:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823318E0:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331930:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82331b18
	if (ctx.cr6.lt) goto loc_82331B18;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331968:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823319B8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331A08:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x82331b18
	if (ctx.cr6.lt) goto loc_82331B18;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331A40:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331A90:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331AE0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// blt cr6,0x82331b18
	if (ctx.cr6.lt) goto loc_82331B18;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331B18:
	// rlwinm r6,r6,6,0,25
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331B20:
	// lbzu r5,11(r10)
	ea = 11 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331B58:
	// lbzu r5,11(r10)
	ea = 11 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331B90:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r22,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r22.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331BC8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// li r31,10
	ctx.r31.s64 = 10;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,28(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r31,r5,r15
	PPC_STORE_U32(ctx.r5.u32 + ctx.r15.u32, ctx.r31.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331C04:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331C3C:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r25.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331C74:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331CAC:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331CE4:
	// lbzu r5,12(r10)
	ea = 12 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331D1C:
	// lbzu r5,12(r10)
	ea = 12 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331D54:
	// lbzu r5,13(r10)
	ea = 13 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331D8C:
	// lbzu r5,13(r10)
	ea = 13 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331DC4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r23,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r23.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331DFC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// li r31,12
	ctx.r31.s64 = 12;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,28(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r31,r5,r15
	PPC_STORE_U32(ctx.r5.u32 + ctx.r15.u32, ctx.r31.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331E38:
	// lbzu r5,14(r10)
	ea = 14 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331E70:
	// lbzu r5,14(r10)
	ea = 14 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x8232fe90
	if (ctx.cr6.lt) goto loc_8232FE90;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331EA8:
	// lbzu r5,6(r10)
	ea = 6 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331EE0:
	// lbzu r5,6(r10)
	ea = 6 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331F18:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331F50:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331F88:
	// lbzu r5,7(r10)
	ea = 7 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331FC0:
	// lbzu r5,7(r10)
	ea = 7 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82331FF8:
	// lbzu r5,8(r10)
	ea = 8 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82332030:
	// lbzu r5,8(r10)
	ea = 8 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332068
	if (ctx.cr6.lt) goto loc_82332068;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82332068:
	// rlwinm r6,r6,7,0,24
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 7) & 0xFFFFFF80;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82332070:
	// lbzu r5,9(r10)
	ea = 9 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823320A8:
	// lbzu r5,9(r10)
	ea = 9 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823320E0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r24,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r24.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82332118:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// li r31,8
	ctx.r31.s64 = 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,28(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r31,r5,r15
	PPC_STORE_U32(ctx.r5.u32 + ctx.r15.u32, ctx.r31.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82332154:
	// lbzu r5,10(r10)
	ea = 10 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_8233218C:
	// lbzu r5,10(r10)
	ea = 10 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823321C4:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823321FC:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82332234
	if (ctx.cr6.lt) goto loc_82332234;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82332234:
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_8233223C:
	// rlwinm r31,r5,1,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r31,19,24,31
	ctx.r5.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 19) & 0xFF;
	// rlwinm r15,r31,11,26,31
	ctx.r15.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 11) & 0x3F;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// clrlwi r15,r5,25
	ctx.r15.u64 = ctx.r5.u32 & 0x7F;
	// stw r16,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r16.u32);
	// cmplwi cr6,r15,0
	ctx.cr6.compare<uint32_t>(ctx.r15.u32, 0, ctx.xer);
	// bne cr6,0x82332274
	if (!ctx.cr6.eq) goto loc_82332274;
	// rlwinm r31,r31,27,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 27) & 0xFF;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// li r15,28
	ctx.r15.s64 = 28;
	// or r5,r31,r5
	ctx.r5.u64 = ctx.r31.u64 | ctx.r5.u64;
	// stw r15,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r15.u32);
loc_82332274:
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bge cr6,0x82332288
	if (!ctx.cr6.lt) goto loc_82332288;
	// li r31,1
	ctx.r31.s64 = 1;
	// neg r5,r5
	ctx.r5.s64 = -ctx.r5.s64;
	// b 0x8233228c
	goto loc_8233228C;
loc_82332288:
	// li r31,0
	ctx.r31.s64 = 0;
loc_8233228C:
	// stw r31,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r31.u32);
	// stw r5,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r5.u32);
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,4(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// lwz r14,8(r4)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// neg r14,r14
	ctx.r14.s64 = -ctx.r14.s64;
	// rlwinm r14,r14,15,0,16
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 15) & 0xFFFF8000;
	// or r15,r14,r15
	ctx.r15.u64 = ctx.r14.u64 | ctx.r15.u64;
	// rlwinm r15,r15,1,0,30
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 1) & 0xFFFFFFFE;
	// stwx r15,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r15.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x823322f4
	if (ctx.cr6.lt) goto loc_823322F4;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823322F4:
	// slw r6,r6,r5
	ctx.r6.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_823322FC:
	// stw r17,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r17.u32);
	// rlwinm r5,r5,11,22,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 11) & 0x3FF;
	// rlwinm r15,r5,0,0,30
	ctx.r15.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lwz r31,4372(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4372);
	// lhzx r31,r15,r31
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r15.u32 + ctx.r31.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x82332350
	if (ctx.cr6.lt) goto loc_82332350;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332354
	goto loc_82332354;
loc_82332350:
	// slw r6,r6,r5
	ctx.r6.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
loc_82332354:
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,8(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r31,4(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// neg r15,r15
	ctx.r15.s64 = -ctx.r15.s64;
	// lwz r14,28(r4)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// rlwinm r15,r15,15,0,16
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 15) & 0xFFFF8000;
	// or r31,r15,r31
	ctx.r31.u64 = ctx.r15.u64 | ctx.r31.u64;
	// rlwinm r31,r31,1,0,30
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// stwx r31,r5,r14
	PPC_STORE_U32(ctx.r5.u32 + ctx.r14.u32, ctx.r31.u32);
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82332394:
	// stw r18,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r18.u32);
	// rlwinm r5,r5,13,20,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 13) & 0xFFF;
	// rlwinm r31,r5,0,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lwz r15,4376(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4376);
	// lhzx r31,r31,r15
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r31.u32 + ctx.r15.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x823324cc
	if (ctx.cr6.lt) goto loc_823324CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x823324d0
	goto loc_823324D0;
loc_823323E8:
	// stw r19,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r19.u32);
	// rlwinm r5,r5,14,19,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 14) & 0x1FFF;
	// rlwinm r31,r5,0,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lwz r15,4380(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4380);
	// lhzx r31,r31,r15
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r31.u32 + ctx.r15.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x823324cc
	if (ctx.cr6.lt) goto loc_823324CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x823324d0
	goto loc_823324D0;
loc_8233243C:
	// rlwinm r5,r5,9,0,22
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 9) & 0xFFFFFE00;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bge cr6,0x82332458
	if (!ctx.cr6.lt) goto loc_82332458;
	// stw r20,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r20.u32);
	// rlwinm r5,r5,6,27,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 6) & 0x1F;
	// lwz r15,4384(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4384);
	// b 0x82332484
	goto loc_82332484;
loc_82332458:
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bge cr6,0x82332478
	if (!ctx.cr6.lt) goto loc_82332478;
	// li r31,16
	ctx.r31.s64 = 16;
	// rlwinm r5,r5,6,27,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 6) & 0x1F;
	// stw r31,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r31.u32);
	// lwz r15,4388(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4388);
	// b 0x82332484
	goto loc_82332484;
loc_82332478:
	// stw r21,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r21.u32);
	// rlwinm r5,r5,7,27,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 7) & 0x1F;
	// lwz r15,4392(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4392);
loc_82332484:
	// rlwinm r31,r5,0,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lhzx r31,r31,r15
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r31.u32 + ctx.r15.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x823324cc
	if (ctx.cr6.lt) goto loc_823324CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x823324d0
	goto loc_823324D0;
loc_823324CC:
	// slw r6,r6,r5
	ctx.r6.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
loc_823324D0:
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,8(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r15,4(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// neg r31,r31
	ctx.r31.s64 = -ctx.r31.s64;
	// lwz r14,28(r4)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// rlwinm r31,r31,15,0,16
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 15) & 0xFFFF8000;
	// or r31,r31,r15
	ctx.r31.u64 = ctx.r31.u64 | ctx.r15.u64;
	// rlwinm r31,r31,1,0,30
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// stwx r31,r5,r14
	PPC_STORE_U32(ctx.r5.u32 + ctx.r14.u32, ctx.r31.u32);
	// b 0x8232f5e8
	goto loc_8232F5E8;
loc_82332510:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x82332560
	if (ctx.cr6.lt) goto loc_82332560;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332560:
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332568:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x82332560
	if (ctx.cr6.lt) goto loc_82332560;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_823325B8:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x823325f0
	if (ctx.cr6.lt) goto loc_823325F0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_823325F0:
	// rlwinm r6,r6,5,0,26
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_823325F8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82332bac
	goto loc_82332BAC;
loc_8233262C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// b 0x82332b98
	goto loc_82332B98;
loc_8233264C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x82332560
	if (ctx.cr6.lt) goto loc_82332560;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_8233269C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x82332560
	if (ctx.cr6.lt) goto loc_82332560;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_823326EC:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x823325f0
	if (ctx.cr6.lt) goto loc_823325F0;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332724:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82332bac
	goto loc_82332BAC;
loc_82332758:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// b 0x82332b98
	goto loc_82332B98;
loc_82332778:
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x82332798
	if (ctx.cr6.lt) goto loc_82332798;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332798:
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_823327A0:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82332bac
	goto loc_82332BAC;
loc_823327D4:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// b 0x82332b98
	goto loc_82332B98;
loc_823327F4:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8233282c
	if (ctx.cr6.lt) goto loc_8233282C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_8233282C:
	// rlwinm r6,r6,6,0,25
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332834:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82332bac
	goto loc_82332BAC;
loc_82332868:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// b 0x82332b98
	goto loc_82332B98;
loc_82332888:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8233282c
	if (ctx.cr6.lt) goto loc_8233282C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_823328C0:
	// lbz r10,3(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x823328f8
	if (ctx.cr6.lt) goto loc_823328F8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_823328F8:
	// rlwinm r6,r6,7,0,24
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 7) & 0xFFFFFF80;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332900:
	// lbz r10,3(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x823328f8
	if (ctx.cr6.lt) goto loc_823328F8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332938:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r28.u32);
	// blt cr6,0x823328f8
	if (ctx.cr6.lt) goto loc_823328F8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332970:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r27.u32);
	// blt cr6,0x823328f8
	if (ctx.cr6.lt) goto loc_823328F8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_823329A8:
	// lbz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x82332560
	if (ctx.cr6.lt) goto loc_82332560;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_823329E0:
	// lbz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x82332560
	if (ctx.cr6.lt) goto loc_82332560;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332A18:
	// lbz r10,5(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x82332560
	if (ctx.cr6.lt) goto loc_82332560;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332A50:
	// lbz r10,5(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x82332560
	if (ctx.cr6.lt) goto loc_82332560;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332A88:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r26.u32);
	// blt cr6,0x82332560
	if (ctx.cr6.lt) goto loc_82332560;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332AC0:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r25,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r25.u32);
	// blt cr6,0x82332560
	if (ctx.cr6.lt) goto loc_82332560;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332AF8:
	// lbz r10,6(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82332bac
	goto loc_82332BAC;
loc_82332B14:
	// lbz r10,6(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// b 0x82332b98
	goto loc_82332B98;
loc_82332B1C:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r28.u32);
	// b 0x82332bac
	goto loc_82332BAC;
loc_82332B38:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r27.u32);
	// b 0x82332bac
	goto loc_82332BAC;
loc_82332B54:
	// lbz r10,7(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82332bac
	goto loc_82332BAC;
loc_82332B70:
	// lbz r10,7(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// b 0x82332b98
	goto loc_82332B98;
loc_82332B78:
	// lbz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82332bac
	goto loc_82332BAC;
loc_82332B94:
	// lbz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
loc_82332B98:
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
loc_82332BAC:
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x82332bcc
	if (ctx.cr6.lt) goto loc_82332BCC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82332bd0
	goto loc_82332BD0;
loc_82332BCC:
	// rlwinm r6,r6,9,0,22
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 9) & 0xFFFFFE00;
loc_82332BD0:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r5,16(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// cmpw cr6,r10,r5
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, ctx.xer);
	// beq cr6,0x82332be4
	if (ctx.cr6.eq) goto loc_82332BE4;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
loc_82332BE4:
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// stw r6,4352(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4352, ctx.r6.u32);
	// li r23,36
	ctx.r23.s64 = 36;
	// stw r7,4356(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4356, ctx.r7.u32);
	// addi r21,r10,2016
	ctx.r21.s64 = ctx.r10.s64 + 2016;
	// stw r11,4360(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4360, ctx.r11.u32);
	// li r22,-16
	ctx.r22.s64 = -16;
	// stw r9,4364(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4364, ctx.r9.u32);
	// li r25,48
	ctx.r25.s64 = 48;
	// li r26,64
	ctx.r26.s64 = 64;
	// vspltisw v13,0
	simd::store_i32(ctx.v13.u32, simd::set1_i32(int32_t(0x0)));
	// li r27,80
	ctx.r27.s64 = 80;
	// vspltisw v0,1
	simd::store_i32(ctx.v0.u32, simd::set1_i32(int32_t(0x1)));
	// li r28,96
	ctx.r28.s64 = 96;
	// li r29,112
	ctx.r29.s64 = 112;
	// li r30,128
	ctx.r30.s64 = 128;
	// li r31,144
	ctx.r31.s64 = 144;
	// li r5,160
	ctx.r5.s64 = 160;
	// li r6,176
	ctx.r6.s64 = 176;
	// li r7,192
	ctx.r7.s64 = 192;
	// li r9,208
	ctx.r9.s64 = 208;
	// li r10,224
	ctx.r10.s64 = 224;
	// li r20,-32
	ctx.r20.s64 = -32;
	// li r11,240
	ctx.r11.s64 = 240;
	// li r24,32
	ctx.r24.s64 = 32;
	// lvlx128 v63,r4,r23
	temp.u32 = ctx.r4.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lwz r23,28(r4)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// lvlx v6,r21,r22
	temp.u32 = ctx.r21.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v6,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lwz r3,32(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	// lvlx v7,r21,r20
	temp.u32 = ctx.r21.u32 + ctx.r20.u32;
	simd::store_shuffled(ctx.v7,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v62,v63,0
	simd::store_i32(ctx.v62.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v63.u32), 3));
	// lvlx128 v60,r25,r23
	temp.u32 = ctx.r25.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r4,16
	ctx.r4.s64 = 16;
	// lvlx128 v59,r26,r23
	temp.u32 = ctx.r26.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v46,v62,0
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v46.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v62.s32)));
	// lvlx128 v58,r27,r23
	temp.u32 = ctx.r27.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v57,r28,r23
	temp.u32 = ctx.r28.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v56,r29,r23
	temp.u32 = ctx.r29.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v55,r30,r23
	temp.u32 = ctx.r30.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v54,r31,r23
	temp.u32 = ctx.r31.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r5,r23
	temp.u32 = ctx.r5.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v52,r6,r23
	temp.u32 = ctx.r6.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v51,r7,r23
	temp.u32 = ctx.r7.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v50,r9,r23
	temp.u32 = ctx.r9.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v49,r10,r23
	temp.u32 = ctx.r10.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v48,r11,r23
	temp.u32 = ctx.r11.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v63,r0,r23
	temp.u32 = ctx.r0.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v12,v63,v63,v7
	simd::store_i8(ctx.v12.u8, simd::permute_bytes(simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v7.u8)));
	// lvlx128 v62,r4,r23
	temp.u32 = ctx.r4.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v44,v63,v62,v6
	simd::store_i8(ctx.v44.u8, simd::permute_bytes(simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v6.u8)));
	// lvlx128 v61,r24,r23
	temp.u32 = ctx.r24.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v41,v44,v96
	simd::store_i32(ctx.v41.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v44.s16)));
	// vupklsb128 v36,v44,v96
	simd::store_i32(ctx.v36.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v44.s16)));
	// lvlx128 v45,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v42,v61,v60,v6
	simd::store_i8(ctx.v42.u8, simd::permute_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v6.u8)));
	// lvlx128 v43,r4,r3
	temp.u32 = ctx.r4.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v11,v62,v62,v7
	simd::store_i8(ctx.v11.u8, simd::permute_bytes(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v7.u8)));
	// lvlx128 v32,r24,r3
	temp.u32 = ctx.r24.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v10,v61,v61,v7
	simd::store_i8(ctx.v10.u8, simd::permute_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v7.u8)));
	// vor128 v47,v63,v63
	simd::store_i8(ctx.v47.u8, simd::load_i8(ctx.v63.u8));
	// vperm128 v9,v60,v60,v7
	simd::store_i8(ctx.v9.u8, simd::permute_bytes(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v40,v41,0
	simd::store_f32_aligned(ctx.v40.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v41.s32)));
	// vperm128 v8,v59,v59,v7
	simd::store_i8(ctx.v8.u8, simd::permute_bytes(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v33,v36,0
	simd::store_f32_aligned(ctx.v33.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v36.s32)));
	// vperm128 v5,v58,v58,v7
	simd::store_i8(ctx.v5.u8, simd::permute_bytes(simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v4,v57,v57,v7
	simd::store_i8(ctx.v4.u8, simd::permute_bytes(simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v3,v56,v56,v7
	simd::store_i8(ctx.v3.u8, simd::permute_bytes(simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v2,v55,v55,v7
	simd::store_i8(ctx.v2.u8, simd::permute_bytes(simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v1,v54,v54,v7
	simd::store_i8(ctx.v1.u8, simd::permute_bytes(simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v31,v53,v53,v7
	simd::store_i8(ctx.v31.u8, simd::permute_bytes(simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v30,v52,v52,v7
	simd::store_i8(ctx.v30.u8, simd::permute_bytes(simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v29,v51,v51,v7
	simd::store_i8(ctx.v29.u8, simd::permute_bytes(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v28,v50,v50,v7
	simd::store_i8(ctx.v28.u8, simd::permute_bytes(simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v27,v49,v49,v7
	simd::store_i8(ctx.v27.u8, simd::permute_bytes(simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v39,v59,v58,v6
	simd::store_i8(ctx.v39.u8, simd::permute_bytes(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v6.u8)));
	// vperm128 v38,v57,v56,v6
	simd::store_i8(ctx.v38.u8, simd::permute_bytes(simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v6.u8)));
	// vperm128 v7,v48,v48,v7
	simd::store_i8(ctx.v7.u8, simd::permute_bytes(simd::load_i8(ctx.v48.u8), simd::load_i8(ctx.v48.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v37,v55,v54,v6
	simd::store_i8(ctx.v37.u8, simd::permute_bytes(simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v6.u8)));
	// vupkhsb128 v35,v42,v96
	simd::store_i32(ctx.v35.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v42.s16)));
	// vperm128 v34,v53,v52,v6
	simd::store_i8(ctx.v34.u8, simd::permute_bytes(simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v6.u8)));
	// vmulfp128 v63,v40,v45
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v40.f32), simd::load_f32_aligned(ctx.v45.f32)));
	// vupklsb128 v61,v42,v96
	simd::store_i32(ctx.v61.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v42.s16)));
	// vmulfp128 v36,v33,v43
	simd::store_f32_aligned(ctx.v36.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v33.f32), simd::load_f32_aligned(ctx.v43.f32)));
	// vupkhsb128 v58,v39,v96
	simd::store_i32(ctx.v58.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v39.s16)));
	// vcsxwfp128 v62,v35,0
	simd::store_f32_aligned(ctx.v62.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v35.s32)));
	// vperm128 v35,v51,v50,v6
	simd::store_i8(ctx.v35.u8, simd::permute_bytes(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v6.u8)));
	// vupklsb128 v56,v39,v96
	simd::store_i32(ctx.v56.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v39.s16)));
	// lvlx128 v60,r25,r3
	temp.u32 = ctx.r25.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v55,v61,0
	simd::store_f32_aligned(ctx.v55.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v61.s32)));
	// vupkhsb128 v54,v38,v96
	simd::store_i32(ctx.v54.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v38.s16)));
	// vupklsb128 v52,v38,v96
	simd::store_i32(ctx.v52.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v38.s16)));
	// vcsxwfp128 v44,v58,0
	simd::store_f32_aligned(ctx.v44.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v58.s32)));
	// vupkhsb128 v42,v37,v96
	simd::store_i32(ctx.v42.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v37.s16)));
	// lvlx128 v59,r26,r3
	temp.u32 = ctx.r26.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v41,v56,0
	simd::store_f32_aligned(ctx.v41.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v56.s32)));
	// lvlx128 v57,r27,r3
	temp.u32 = ctx.r27.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v39,v54,0
	simd::store_f32_aligned(ctx.v39.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v54.s32)));
	// lvlx128 v53,r28,r3
	temp.u32 = ctx.r28.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v58,v52,0
	simd::store_f32_aligned(ctx.v58.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v52.s32)));
	// lvlx128 v45,r29,r3
	temp.u32 = ctx.r29.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v54,v42,0
	simd::store_f32_aligned(ctx.v54.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v42.s32)));
	// lvlx128 v40,r30,r3
	temp.u32 = ctx.r30.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v38,v7,v7
	simd::store_i8(ctx.v38.u8, simd::load_i8(ctx.v7.u8));
	// lvlx v7,0,r21
	temp.u32 = ctx.r0.u32 + ctx.r21.u32;
	simd::store_shuffled(ctx.v7,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v33,v63,v46
	simd::store_f32_aligned(ctx.v33.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vperm128 v63,v49,v48,v6
	simd::store_i8(ctx.v63.u8, simd::permute_bytes(simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v48.u8), simd::load_i8(ctx.v6.u8)));
	// vmulfp128 v36,v36,v46
	simd::store_f32_aligned(ctx.v36.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v36.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// lvlx128 v61,r31,r3
	temp.u32 = ctx.r31.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v62,v62,v32
	simd::store_f32_aligned(ctx.v62.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v32.f32)));
	// lvlx128 v56,r5,r3
	temp.u32 = ctx.r5.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v52,r6,r3
	temp.u32 = ctx.r6.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v50,v55,v60
	simd::store_f32_aligned(ctx.v50.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v55.f32), simd::load_f32_aligned(ctx.v60.f32)));
	// lvlx128 v51,r7,r3
	temp.u32 = ctx.r7.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v42,v44,v59
	simd::store_f32_aligned(ctx.v42.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v44.f32), simd::load_f32_aligned(ctx.v59.f32)));
	// lvlx128 v49,r9,r3
	temp.u32 = ctx.r9.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v48,r10,r3
	temp.u32 = ctx.r10.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v44,v34,v96
	simd::store_i32(ctx.v44.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v34.s16)));
	// vmulfp128 v41,v41,v57
	simd::store_f32_aligned(ctx.v41.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v41.f32), simd::load_f32_aligned(ctx.v57.f32)));
	// lvlx128 v43,r11,r3
	temp.u32 = ctx.r11.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v39,v39,v53
	simd::store_f32_aligned(ctx.v39.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v53.f32)));
	// vmulfp128 v32,v58,v45
	simd::store_f32_aligned(ctx.v32.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v58.f32), simd::load_f32_aligned(ctx.v45.f32)));
	// vupklsb128 v45,v37,v96
	simd::store_i32(ctx.v45.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v37.s16)));
	// vcfpsxws128 v6,v33,0
	simd::store_i32(ctx.v6.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v33.f32)));
	// vcfpsxws128 v26,v36,0
	simd::store_i32(ctx.v26.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v36.f32)));
	// vmulfp128 v33,v62,v46
	simd::store_f32_aligned(ctx.v33.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v62,v54,v40
	simd::store_f32_aligned(ctx.v62.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v54.f32), simd::load_f32_aligned(ctx.v40.f32)));
	// vmulfp128 v60,v50,v46
	simd::store_f32_aligned(ctx.v60.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v50.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v59,v42,v46
	simd::store_f32_aligned(ctx.v59.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v42.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vupklsb128 v42,v34,v96
	simd::store_i32(ctx.v42.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v34.s16)));
	// vmulfp128 v58,v41,v46
	simd::store_f32_aligned(ctx.v58.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v41.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vupkhsb128 v41,v35,v96
	simd::store_i32(ctx.v41.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v35.s16)));
	// vmulfp128 v57,v39,v46
	simd::store_f32_aligned(ctx.v57.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v55,v32,v46
	simd::store_f32_aligned(ctx.v55.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v32.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vsubsws v25,v6,v0
	simd::store_i32(ctx.v25.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v6), simd::to_vec128i(ctx.v0)));
	// vsubsws v18,v26,v0
	simd::store_i32(ctx.v18.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v26), simd::to_vec128i(ctx.v0)));
	// vcfpsxws128 v24,v33,0
	simd::store_i32(ctx.v24.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v33.f32)));
	// vmulfp128 v54,v62,v46
	simd::store_f32_aligned(ctx.v54.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vor v23,v25,v0
	simd::store_i8(ctx.v23.u8, simd::or_i8(simd::load_i8(ctx.v25.u8), simd::load_i8(ctx.v0.u8)));
	// vcfpsxws128 v22,v60,0
	simd::store_i32(ctx.v22.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v60.f32)));
	// vor v15,v18,v0
	simd::store_i8(ctx.v15.u8, simd::or_i8(simd::load_i8(ctx.v18.u8), simd::load_i8(ctx.v0.u8)));
	// vcfpsxws128 v20,v59,0
	simd::store_i32(ctx.v20.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v59.f32)));
	// vcfpsxws128 v19,v58,0
	simd::store_i32(ctx.v19.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v58.f32)));
	// vmaxsw v21,v13,v23
	simd::store_i32(ctx.v21.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v23.u32)));
	// vcfpsxws128 v18,v57,0
	simd::store_i32(ctx.v18.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v57.f32)));
	// vmaxsw v6,v13,v15
	simd::store_i32(ctx.v6.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v15.u32)));
	// vxor v17,v21,v12
	simd::store_u8(ctx.v17.u8, simd::xor_i8(simd::load_u8(ctx.v21.u8), simd::load_u8(ctx.v12.u8)));
	// vxor v25,v6,v11
	simd::store_u8(ctx.v25.u8, simd::xor_i8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v11.u8)));
	// vsubsws v16,v24,v0
	simd::store_i32(ctx.v16.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v24), simd::to_vec128i(ctx.v0)));
	// vsubsws v14,v17,v12
	simd::store_i32(ctx.v14.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v17), simd::to_vec128i(ctx.v12)));
	// vcfpsxws128 v17,v55,0
	simd::store_i32(ctx.v17.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v55.f32)));
	// vsubsws v23,v25,v11
	simd::store_i32(ctx.v23.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v25), simd::to_vec128i(ctx.v11)));
	// vor v12,v16,v0
	simd::store_i8(ctx.v12.u8, simd::or_i8(simd::load_i8(ctx.v16.u8), simd::load_i8(ctx.v0.u8)));
	// vsubsws v16,v22,v0
	simd::store_i32(ctx.v16.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v22), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v53,v14,0
	simd::store_f32_aligned(ctx.v53.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v14.s32)));
	// vcsxwfp128 v50,v23,0
	simd::store_f32_aligned(ctx.v50.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v23.s32)));
	// vmaxsw v26,v13,v12
	simd::store_i32(ctx.v26.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v12.u32)));
	// vxor v24,v26,v10
	simd::store_u8(ctx.v24.u8, simd::xor_i8(simd::load_u8(ctx.v26.u8), simd::load_u8(ctx.v10.u8)));
	// vsubsws v21,v24,v10
	simd::store_i32(ctx.v21.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v24), simd::to_vec128i(ctx.v10)));
	// vperm128 v47,v47,v53,v7
	simd::store_i8(ctx.v47.u8, simd::permute_bytes(simd::load_i8(ctx.v47.u8), simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v7.u8)));
	// stvlx128 v47,r0,r23
{
	uint32_t addr = 
ctx.r23.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v47), 15 - i));
}
	// stvlx128 v50,r23,r4
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r4.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v50), 15 - i));
}
	// vupklsb128 v39,v35,v96
	simd::store_i32(ctx.v39.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v35.s16)));
	// vcsxwfp128 v40,v45,0
	simd::store_f32_aligned(ctx.v40.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v45.s32)));
	// vcsxwfp128 v37,v44,0
	simd::store_f32_aligned(ctx.v37.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v44.s32)));
	// vupkhsb128 v36,v63,v96
	simd::store_i32(ctx.v36.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v63.s16)));
	// vcsxwfp128 v35,v42,0
	simd::store_f32_aligned(ctx.v35.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v42.s32)));
	// vsubsws v15,v20,v0
	simd::store_i32(ctx.v15.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v20), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v34,v41,0
	simd::store_f32_aligned(ctx.v34.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v41.s32)));
	// vsubsws v14,v19,v0
	simd::store_i32(ctx.v14.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v19), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v33,v39,0
	simd::store_f32_aligned(ctx.v33.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v39.s32)));
	// vsubsws v25,v18,v0
	simd::store_i32(ctx.v25.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v18), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v32,v36,0
	simd::store_f32_aligned(ctx.v32.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v36.s32)));
	// vsubsws v24,v17,v0
	simd::store_i32(ctx.v24.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v17), simd::to_vec128i(ctx.v0)));
	// vcfpsxws128 v10,v54,0
	simd::store_i32(ctx.v10.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v54.f32)));
	// vor v12,v16,v0
	simd::store_i8(ctx.v12.u8, simd::or_i8(simd::load_i8(ctx.v16.u8), simd::load_i8(ctx.v0.u8)));
	// vor v11,v15,v0
	simd::store_i8(ctx.v11.u8, simd::or_i8(simd::load_i8(ctx.v15.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v62,v21,0
	simd::store_f32_aligned(ctx.v62.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v21.s32)));
	// vor v6,v14,v0
	simd::store_i8(ctx.v6.u8, simd::or_i8(simd::load_i8(ctx.v14.u8), simd::load_i8(ctx.v0.u8)));
	// vupklsb128 v41,v63,v96
	simd::store_i32(ctx.v41.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v63.s16)));
	// vor v21,v25,v0
	simd::store_i8(ctx.v21.u8, simd::or_i8(simd::load_i8(ctx.v25.u8), simd::load_i8(ctx.v0.u8)));
	// vor v20,v24,v0
	simd::store_i8(ctx.v20.u8, simd::or_i8(simd::load_i8(ctx.v24.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v26,v13,v12
	simd::store_i32(ctx.v26.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v12.u32)));
	// vmulfp128 v61,v40,v61
	simd::store_f32_aligned(ctx.v61.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v40.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vmaxsw v23,v13,v11
	simd::store_i32(ctx.v23.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v11.u32)));
	// vmulfp128 v60,v37,v56
	simd::store_f32_aligned(ctx.v60.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v37.f32), simd::load_f32_aligned(ctx.v56.f32)));
	// vmaxsw v22,v13,v6
	simd::store_i32(ctx.v22.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v6.u32)));
	// vmulfp128 v59,v35,v52
	simd::store_f32_aligned(ctx.v59.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v35.f32), simd::load_f32_aligned(ctx.v52.f32)));
	// vmaxsw v17,v13,v21
	simd::store_i32(ctx.v17.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v21.u32)));
	// vmaxsw v16,v13,v20
	simd::store_i32(ctx.v16.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v20.u32)));
	// vmulfp128 v58,v34,v51
	simd::store_f32_aligned(ctx.v58.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v34.f32), simd::load_f32_aligned(ctx.v51.f32)));
	// vmulfp128 v57,v33,v49
	simd::store_f32_aligned(ctx.v57.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v33.f32), simd::load_f32_aligned(ctx.v49.f32)));
	// vxor v19,v26,v9
	simd::store_u8(ctx.v19.u8, simd::xor_i8(simd::load_u8(ctx.v26.u8), simd::load_u8(ctx.v9.u8)));
	// vmulfp128 v56,v32,v48
	simd::store_f32_aligned(ctx.v56.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v32.f32), simd::load_f32_aligned(ctx.v48.f32)));
	// vxor v18,v23,v8
	simd::store_u8(ctx.v18.u8, simd::xor_i8(simd::load_u8(ctx.v23.u8), simd::load_u8(ctx.v8.u8)));
	// vsubsws v14,v10,v0
	simd::store_i32(ctx.v14.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v10), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v40,v41,0
	simd::store_f32_aligned(ctx.v40.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v41.s32)));
	// vxor v15,v22,v5
	simd::store_u8(ctx.v15.u8, simd::xor_i8(simd::load_u8(ctx.v22.u8), simd::load_u8(ctx.v5.u8)));
	// stvlx128 v62,r23,r24
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r24.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v62), 15 - i));
}
	// vxor v12,v17,v4
	simd::store_u8(ctx.v12.u8, simd::xor_i8(simd::load_u8(ctx.v17.u8), simd::load_u8(ctx.v4.u8)));
	// vxor v11,v16,v3
	simd::store_u8(ctx.v11.u8, simd::xor_i8(simd::load_u8(ctx.v16.u8), simd::load_u8(ctx.v3.u8)));
	// vsubsws v10,v19,v9
	simd::store_i32(ctx.v10.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v19), simd::to_vec128i(ctx.v9)));
	// vor v6,v14,v0
	simd::store_i8(ctx.v6.u8, simd::or_i8(simd::load_i8(ctx.v14.u8), simd::load_i8(ctx.v0.u8)));
	// vsubsws v9,v18,v8
	simd::store_i32(ctx.v9.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v18), simd::to_vec128i(ctx.v8)));
	// vmulfp128 v55,v61,v46
	simd::store_f32_aligned(ctx.v55.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v61.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vsubsws v8,v15,v5
	simd::store_i32(ctx.v8.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v15), simd::to_vec128i(ctx.v5)));
	// vmulfp128 v54,v60,v46
	simd::store_f32_aligned(ctx.v54.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v60.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v53,v59,v46
	simd::store_f32_aligned(ctx.v53.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vsubsws v5,v12,v4
	simd::store_i32(ctx.v5.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v12), simd::to_vec128i(ctx.v4)));
	// vsubsws v4,v11,v3
	simd::store_i32(ctx.v4.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v11), simd::to_vec128i(ctx.v3)));
	// vmulfp128 v52,v58,v46
	simd::store_f32_aligned(ctx.v52.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v58.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmaxsw v3,v13,v6
	simd::store_i32(ctx.v3.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v6.u32)));
	// vmulfp128 v45,v57,v46
	simd::store_f32_aligned(ctx.v45.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v57.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v44,v56,v46
	simd::store_f32_aligned(ctx.v44.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v56.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vcsxwfp128 v51,v10,0
	simd::store_f32_aligned(ctx.v51.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v10.s32)));
	// vcsxwfp128 v50,v9,0
	simd::store_f32_aligned(ctx.v50.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v9.s32)));
	// vxor v26,v3,v2
	simd::store_u8(ctx.v26.u8, simd::xor_i8(simd::load_u8(ctx.v3.u8), simd::load_u8(ctx.v2.u8)));
	// vcsxwfp128 v49,v8,0
	simd::store_f32_aligned(ctx.v49.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v8.s32)));
	// vcsxwfp128 v48,v5,0
	simd::store_f32_aligned(ctx.v48.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v5.s32)));
	// vcsxwfp128 v47,v4,0
	simd::store_f32_aligned(ctx.v47.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v4.s32)));
	// vcfpsxws128 v25,v55,0
	simd::store_i32(ctx.v25.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v55.f32)));
	// vsubsws v23,v26,v2
	simd::store_i32(ctx.v23.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v26), simd::to_vec128i(ctx.v2)));
	// vcfpsxws128 v24,v54,0
	simd::store_i32(ctx.v24.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v54.f32)));
	// vcfpsxws128 v22,v53,0
	simd::store_i32(ctx.v22.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v53.f32)));
	// vcfpsxws128 v21,v52,0
	simd::store_i32(ctx.v21.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v52.f32)));
	// vcsxwfp128 v42,v23,0
	simd::store_f32_aligned(ctx.v42.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v23.s32)));
	// vcfpsxws128 v20,v45,0
	simd::store_i32(ctx.v20.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v45.f32)));
	// vcfpsxws128 v19,v44,0
	simd::store_i32(ctx.v19.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v44.f32)));
	// stvlx128 v51,r23,r25
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r25.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v51), 15 - i));
}
	// stvlx128 v50,r23,r26
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r26.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v50), 15 - i));
}
	// stvlx128 v49,r23,r27
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r27.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v49), 15 - i));
}
	// stvlx128 v48,r23,r28
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r28.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v48), 15 - i));
}
	// stvlx128 v47,r23,r29
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r29.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v47), 15 - i));
}
	// vsubsws v18,v25,v0
	simd::store_i32(ctx.v18.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v25), simd::to_vec128i(ctx.v0)));
	// vsubsws v17,v24,v0
	simd::store_i32(ctx.v17.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v24), simd::to_vec128i(ctx.v0)));
	// vsubsws v16,v22,v0
	simd::store_i32(ctx.v16.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v22), simd::to_vec128i(ctx.v0)));
	// vsubsws v15,v21,v0
	simd::store_i32(ctx.v15.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v21), simd::to_vec128i(ctx.v0)));
	// vor v14,v18,v0
	simd::store_i8(ctx.v14.u8, simd::or_i8(simd::load_i8(ctx.v18.u8), simd::load_i8(ctx.v0.u8)));
	// stvlx128 v42,r23,r30
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r30.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v42), 15 - i));
}
	// vsubsws v12,v20,v0
	simd::store_i32(ctx.v12.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v20), simd::to_vec128i(ctx.v0)));
	// vsubsws v11,v19,v0
	simd::store_i32(ctx.v11.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v19), simd::to_vec128i(ctx.v0)));
	// vor v10,v17,v0
	simd::store_i8(ctx.v10.u8, simd::or_i8(simd::load_i8(ctx.v17.u8), simd::load_i8(ctx.v0.u8)));
	// vor v9,v16,v0
	simd::store_i8(ctx.v9.u8, simd::or_i8(simd::load_i8(ctx.v16.u8), simd::load_i8(ctx.v0.u8)));
	// vmulfp128 v39,v40,v43
	simd::store_f32_aligned(ctx.v39.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v40.f32), simd::load_f32_aligned(ctx.v43.f32)));
	// vmaxsw v3,v13,v10
	simd::store_i32(ctx.v3.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v10.u32)));
	// vor v8,v15,v0
	simd::store_i8(ctx.v8.u8, simd::or_i8(simd::load_i8(ctx.v15.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v2,v13,v9
	simd::store_i32(ctx.v2.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v9.u32)));
	// vor v6,v12,v0
	simd::store_i8(ctx.v6.u8, simd::or_i8(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v5,v13,v14
	simd::store_i32(ctx.v5.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v14.u32)));
	// vor v4,v11,v0
	simd::store_i8(ctx.v4.u8, simd::or_i8(simd::load_i8(ctx.v11.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v26,v13,v8
	simd::store_i32(ctx.v26.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v8.u32)));
	// vmaxsw v25,v13,v6
	simd::store_i32(ctx.v25.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v6.u32)));
	// vxor v24,v5,v1
	simd::store_u8(ctx.v24.u8, simd::xor_i8(simd::load_u8(ctx.v5.u8), simd::load_u8(ctx.v1.u8)));
	// vmaxsw v23,v13,v4
	simd::store_i32(ctx.v23.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v4.u32)));
	// vor128 v7,v38,v38
	simd::store_i8(ctx.v7.u8, simd::load_i8(ctx.v38.u8));
	// vxor v22,v3,v31
	simd::store_u8(ctx.v22.u8, simd::xor_i8(simd::load_u8(ctx.v3.u8), simd::load_u8(ctx.v31.u8)));
	// vmulfp128 v32,v39,v46
	simd::store_f32_aligned(ctx.v32.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vxor v21,v2,v30
	simd::store_u8(ctx.v21.u8, simd::xor_i8(simd::load_u8(ctx.v2.u8), simd::load_u8(ctx.v30.u8)));
	// vxor v20,v26,v29
	simd::store_u8(ctx.v20.u8, simd::xor_i8(simd::load_u8(ctx.v26.u8), simd::load_u8(ctx.v29.u8)));
	// vxor v19,v25,v28
	simd::store_u8(ctx.v19.u8, simd::xor_i8(simd::load_u8(ctx.v25.u8), simd::load_u8(ctx.v28.u8)));
	// vsubsws v18,v24,v1
	simd::store_i32(ctx.v18.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v24), simd::to_vec128i(ctx.v1)));
	// vxor v17,v23,v27
	simd::store_u8(ctx.v17.u8, simd::xor_i8(simd::load_u8(ctx.v23.u8), simd::load_u8(ctx.v27.u8)));
	// vsubsws v16,v22,v31
	simd::store_i32(ctx.v16.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v22), simd::to_vec128i(ctx.v31)));
	// vsubsws v15,v21,v30
	simd::store_i32(ctx.v15.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v21), simd::to_vec128i(ctx.v30)));
	// vsubsws v14,v20,v29
	simd::store_i32(ctx.v14.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v20), simd::to_vec128i(ctx.v29)));
	// vcsxwfp128 v37,v18,0
	simd::store_f32_aligned(ctx.v37.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v18.s32)));
	// vsubsws v12,v19,v28
	simd::store_i32(ctx.v12.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v19), simd::to_vec128i(ctx.v28)));
	// vsubsws v11,v17,v27
	simd::store_i32(ctx.v11.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v17), simd::to_vec128i(ctx.v27)));
	// vcsxwfp128 v36,v16,0
	simd::store_f32_aligned(ctx.v36.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v16.s32)));
	// vcsxwfp128 v35,v15,0
	simd::store_f32_aligned(ctx.v35.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v15.s32)));
	// vcsxwfp128 v34,v14,0
	simd::store_f32_aligned(ctx.v34.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v14.s32)));
	// vcfpsxws128 v10,v32,0
	simd::store_i32(ctx.v10.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v32.f32)));
	// vcsxwfp128 v33,v12,0
	simd::store_f32_aligned(ctx.v33.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v12.s32)));
	// vcsxwfp128 v63,v11,0
	simd::store_f32_aligned(ctx.v63.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v11.s32)));
	// stvlx128 v37,r23,r31
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r31.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v37), 15 - i));
}
	// stvlx128 v36,r23,r5
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r5.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v36), 15 - i));
}
	// stvlx128 v35,r23,r6
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r6.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v35), 15 - i));
}
	// stvlx128 v34,r23,r7
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v34), 15 - i));
}
	// vsubsws v9,v10,v0
	simd::store_i32(ctx.v9.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v10), simd::to_vec128i(ctx.v0)));
	// stvlx128 v33,r23,r9
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v33), 15 - i));
}
	// stvlx128 v63,r23,r10
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v63), 15 - i));
}
	// vor v8,v9,v0
	simd::store_i8(ctx.v8.u8, simd::or_i8(simd::load_i8(ctx.v9.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v6,v13,v8
	simd::store_i32(ctx.v6.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v8.u32)));
	// vxor v5,v6,v7
	simd::store_u8(ctx.v5.u8, simd::xor_i8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v7.u8)));
	// vsubsws v4,v5,v7
	simd::store_i32(ctx.v4.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v5), simd::to_vec128i(ctx.v7)));
	// vcsxwfp128 v62,v4,0
	simd::store_f32_aligned(ctx.v62.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v4.s32)));
	// stvlx128 v62,r23,r11
{
	uint32_t addr = 
ctx.r23.u32 + ctx.r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v62), 15 - i));
}
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82333068"))) PPC_WEAK_FUNC(sub_82333068);
PPC_FUNC_IMPL(__imp__sub_82333068) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x82333070;
	__restfpr_14(ctx, base);
	// lwz r11,28(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// li r25,0
	ctx.r25.s64 = 0;
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// std r25,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r25.u64);
	// std r25,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r25.u64);
	// std r25,16(r11)
	PPC_STORE_U64(ctx.r11.u32 + 16, ctx.r25.u64);
	// std r25,24(r11)
	PPC_STORE_U64(ctx.r11.u32 + 24, ctx.r25.u64);
	// std r25,32(r11)
	PPC_STORE_U64(ctx.r11.u32 + 32, ctx.r25.u64);
	// std r25,40(r11)
	PPC_STORE_U64(ctx.r11.u32 + 40, ctx.r25.u64);
	// std r25,48(r11)
	PPC_STORE_U64(ctx.r11.u32 + 48, ctx.r25.u64);
	// std r25,56(r11)
	PPC_STORE_U64(ctx.r11.u32 + 56, ctx.r25.u64);
	// std r25,64(r11)
	PPC_STORE_U64(ctx.r11.u32 + 64, ctx.r25.u64);
	// std r25,72(r11)
	PPC_STORE_U64(ctx.r11.u32 + 72, ctx.r25.u64);
	// std r25,80(r11)
	PPC_STORE_U64(ctx.r11.u32 + 80, ctx.r25.u64);
	// std r25,88(r11)
	PPC_STORE_U64(ctx.r11.u32 + 88, ctx.r25.u64);
	// std r25,96(r11)
	PPC_STORE_U64(ctx.r11.u32 + 96, ctx.r25.u64);
	// std r25,104(r11)
	PPC_STORE_U64(ctx.r11.u32 + 104, ctx.r25.u64);
	// std r25,112(r11)
	PPC_STORE_U64(ctx.r11.u32 + 112, ctx.r25.u64);
	// std r25,120(r11)
	PPC_STORE_U64(ctx.r11.u32 + 120, ctx.r25.u64);
	// std r25,128(r11)
	PPC_STORE_U64(ctx.r11.u32 + 128, ctx.r25.u64);
	// std r25,136(r11)
	PPC_STORE_U64(ctx.r11.u32 + 136, ctx.r25.u64);
	// std r25,144(r11)
	PPC_STORE_U64(ctx.r11.u32 + 144, ctx.r25.u64);
	// std r25,152(r11)
	PPC_STORE_U64(ctx.r11.u32 + 152, ctx.r25.u64);
	// std r25,160(r11)
	PPC_STORE_U64(ctx.r11.u32 + 160, ctx.r25.u64);
	// std r25,168(r11)
	PPC_STORE_U64(ctx.r11.u32 + 168, ctx.r25.u64);
	// std r25,176(r11)
	PPC_STORE_U64(ctx.r11.u32 + 176, ctx.r25.u64);
	// std r25,184(r11)
	PPC_STORE_U64(ctx.r11.u32 + 184, ctx.r25.u64);
	// std r25,192(r11)
	PPC_STORE_U64(ctx.r11.u32 + 192, ctx.r25.u64);
	// std r25,200(r11)
	PPC_STORE_U64(ctx.r11.u32 + 200, ctx.r25.u64);
	// std r25,208(r11)
	PPC_STORE_U64(ctx.r11.u32 + 208, ctx.r25.u64);
	// std r25,216(r11)
	PPC_STORE_U64(ctx.r11.u32 + 216, ctx.r25.u64);
	// std r25,224(r11)
	PPC_STORE_U64(ctx.r11.u32 + 224, ctx.r25.u64);
	// std r25,232(r11)
	PPC_STORE_U64(ctx.r11.u32 + 232, ctx.r25.u64);
	// std r25,240(r11)
	PPC_STORE_U64(ctx.r11.u32 + 240, ctx.r25.u64);
	// std r25,248(r11)
	PPC_STORE_U64(ctx.r11.u32 + 248, ctx.r25.u64);
	// lwz r6,4352(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4352);
	// lwz r8,4360(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4360);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// lwz r7,4356(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4356);
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r9,4364(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4364);
	// beq cr6,0x82333124
	if (ctx.cr6.eq) goto loc_82333124;
	// subfic r11,r8,32
	ctx.xer.ca = ctx.r8.u32 <= 32;
	ctx.r11.s64 = 32 - ctx.r8.s64;
	// srw r10,r7,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r11.u8 & 0x3F));
	// or r11,r10,r6
	ctx.r11.u64 = ctx.r10.u64 | ctx.r6.u64;
loc_82333124:
	// li r18,11
	ctx.r18.s64 = 11;
	// li r19,13
	ctx.r19.s64 = 13;
	// li r16,15
	ctx.r16.s64 = 15;
	// li r17,17
	ctx.r17.s64 = 17;
	// li r5,1
	ctx.r5.s64 = 1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x8233315c
	if (!ctx.cr6.lt) goto loc_8233315C;
	// rlwinm r11,r11,2,31,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0x1;
	// li r10,2
	ctx.r10.s64 = 2;
	// stw r11,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r11.u32);
	// stw r5,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r5.u32);
	// stw r25,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r25.u32);
	// stw r10,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r10.u32);
	// b 0x823332ec
	goto loc_823332EC;
loc_8233315C:
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r11,8,24,31
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFF;
	// cmplwi cr6,r10,7
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 7, ctx.xer);
	// bgt cr6,0x82333254
	if (ctx.cr6.gt) goto loc_82333254;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x823331d8
	if (ctx.cr6.eq) goto loc_823331D8;
	// bdz 0x823331b8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_823331B8;
	// bdz 0x823331a8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_823331A8;
	// bdz 0x823331a8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_823331A8;
	// bdz 0x82333190
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_82333190;
	// bdz 0x82333190
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_82333190;
	// bdz 0x82333190
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_82333190;
loc_82333190:
	// stw r18,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r18.u32);
	// rlwinm r11,r11,10,22,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 10) & 0x3FF;
	// lwz r10,4372(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4372);
	// rlwinm r5,r11,0,0,30
	ctx.r5.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lhzx r10,r5,r10
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r10.u32);
	// b 0x82333234
	goto loc_82333234;
loc_823331A8:
	// stw r19,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r19.u32);
	// rlwinm r11,r11,12,20,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 12) & 0xFFF;
	// lwz r5,4376(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4376);
	// b 0x8233322c
	goto loc_8233322C;
loc_823331B8:
	// li r10,14
	ctx.r10.s64 = 14;
	// rlwinm r11,r11,13,19,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 13) & 0x1FFF;
	// stw r10,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r10.u32);
	// rlwinm r5,r11,0,0,30
	ctx.r5.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,4380(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4380);
	// lhzx r5,r5,r10
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r10.u32);
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// b 0x82333238
	goto loc_82333238;
loc_823331D8:
	// rlwinm r11,r11,8,0,23
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFFFFFF00;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x823331f4
	if (!ctx.cr6.lt) goto loc_823331F4;
	// stw r16,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r16.u32);
	// rlwinm r11,r11,6,27,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 6) & 0x1F;
	// lwz r5,4384(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4384);
	// b 0x8233322c
	goto loc_8233322C;
loc_823331F4:
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x82333220
	if (!ctx.cr6.lt) goto loc_82333220;
	// rlwinm r11,r11,6,27,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 6) & 0x1F;
	// li r10,16
	ctx.r10.s64 = 16;
	// rlwinm r5,r11,0,0,30
	ctx.r5.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r10.u32);
	// lwz r10,4388(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4388);
	// lhzx r5,r5,r10
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r10.u32);
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// b 0x82333238
	goto loc_82333238;
loc_82333220:
	// stw r17,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r17.u32);
	// rlwinm r11,r11,7,27,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 7) & 0x1F;
	// lwz r5,4392(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4392);
loc_8233322C:
	// rlwinm r10,r11,0,0,30
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lhzx r10,r10,r5
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r5.u32);
loc_82333234:
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
loc_82333238:
	// clrlwi r5,r10,24
	ctx.r5.u64 = ctx.r10.u32 & 0xFF;
	// rlwinm r10,r10,24,24,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFF;
	// clrlwi r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	// stw r5,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r5.u32);
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// stw r11,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r11.u32);
	// b 0x823332ec
	goto loc_823332EC;
loc_82333254:
	// lwz r31,4368(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4368);
	// rlwinm r10,r10,1,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFC;
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r31.u32);
	// clrlwi r31,r10,24
	ctx.r31.u64 = ctx.r10.u32 & 0xFF;
	// stw r31,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r31.u32);
	// cmpwi cr6,r31,64
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 64, ctx.xer);
	// beq cr6,0x82333298
	if (ctx.cr6.eq) goto loc_82333298;
	// rlwinm r5,r10,16,16,31
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// rlwinm r10,r10,24,24,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFF;
	// stw r5,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r5.u32);
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// subfic r10,r5,33
	ctx.xer.ca = ctx.r5.u32 <= 33;
	ctx.r10.s64 = 33 - ctx.r5.s64;
	// srw r5,r11,r10
	ctx.r5.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r11.u32 >> (ctx.r10.u8 & 0x3F));
	// clrlwi r11,r5,31
	ctx.r11.u64 = ctx.r5.u32 & 0x1;
	// stw r11,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r11.u32);
	// b 0x823332ec
	goto loc_823332EC;
loc_82333298:
	// rlwinm r10,r11,19,24,31
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 19) & 0xFF;
	// rlwinm r31,r11,11,26,31
	ctx.r31.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 11) & 0x3F;
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// stw r31,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r31.u32);
	// li r30,20
	ctx.r30.s64 = 20;
	// clrlwi r31,r10,25
	ctx.r31.u64 = ctx.r10.u32 & 0x7F;
	// stw r30,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r30.u32);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x823332d0
	if (!ctx.cr6.eq) goto loc_823332D0;
	// rlwinm r11,r11,27,24,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0xFF;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// li r31,28
	ctx.r31.s64 = 28;
	// or r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 | ctx.r10.u64;
	// stw r31,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r31.u32);
loc_823332D0:
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bge cr6,0x823332e4
	if (!ctx.cr6.lt) goto loc_823332E4;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// b 0x823332e8
	goto loc_823332E8;
loc_823332E4:
	// stw r25,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r25.u32);
loc_823332E8:
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
loc_823332EC:
	// lwz r10,12(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r10,r8
	ctx.r11.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x82333310
	if (ctx.cr6.lt) goto loc_82333310;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82333314
	goto loc_82333314;
loc_82333310:
	// slw r6,r6,r10
	ctx.r6.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r10.u8 & 0x3F));
loc_82333314:
	// lwz r5,4396(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4396);
	// lis r23,-1
	ctx.r23.s64 = -65536;
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lis r20,-1
	ctx.r20.s64 = -65536;
	// lis r26,-1
	ctx.r26.s64 = -65536;
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lis r22,-1
	ctx.r22.s64 = -65536;
	// ori r21,r23,11
	ctx.r21.u64 = ctx.r23.u64 | 11;
	// ori r23,r20,9
	ctx.r23.u64 = ctx.r20.u64 | 9;
	// addi r8,r4,20
	ctx.r8.s64 = ctx.r4.s64 + 20;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// li r27,5
	ctx.r27.s64 = 5;
	// ori r26,r26,7
	ctx.r26.u64 = ctx.r26.u64 | 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// li r24,7
	ctx.r24.s64 = 7;
	// stw r5,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, ctx.r5.u32);
	// rlwinm r31,r5,2,0,29
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r5,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, ctx.r5.u32);
	// ori r22,r22,13
	ctx.r22.u64 = ctx.r22.u64 | 13;
	// lwz r5,4(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// li r20,9
	ctx.r20.s64 = 9;
	// lwz r29,28(r4)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// lwz r30,8(r4)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// neg r30,r30
	ctx.r30.s64 = -ctx.r30.s64;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r30,r30,16,0,15
	ctx.r30.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 16) & 0xFFFF0000;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// or r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 | ctx.r30.u64;
	// stwx r5,r31,r29
	PPC_STORE_U32(ctx.r31.u32 + ctx.r29.u32, ctx.r5.u32);
	// lis r5,-1
	ctx.r5.s64 = -65536;
	// lis r31,-1
	ctx.r31.s64 = -65536;
	// ori r30,r5,3
	ctx.r30.u64 = ctx.r5.u64 | 3;
	// li r29,3
	ctx.r29.s64 = 3;
	// ori r28,r31,5
	ctx.r28.u64 = ctx.r31.u64 | 5;
loc_8233339C:
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x823333b4
	if (ctx.cr6.eq) goto loc_823333B4;
	// subfic r5,r11,32
	ctx.xer.ca = ctx.r11.u32 <= 32;
	ctx.r5.s64 = 32 - ctx.r11.s64;
	// srw r5,r7,r5
	ctx.r5.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r5.u8 & 0x3F));
	// or r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 | ctx.r6.u64;
loc_823333B4:
	// rlwinm r31,r5,9,23,31
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 9) & 0x1FF;
	// cmplwi cr6,r31,511
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 511, ctx.xer);
	// bgt cr6,0x8233698c
	if (ctx.cr6.gt) goto loc_8233698C;
	// lis r12,-32205
	ctx.r12.s64 = -2110586880;
	// rlwinm r0,r31,2,0,29
	ctx.r0.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r12,r12,13272
	ctx.r12.s64 = ctx.r12.s64 + 13272;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr 
	switch (ctx.r31.u64) {
	case 0:
		goto loc_823361F4;
	case 1:
		goto loc_82336154;
	case 2:
		goto loc_82336100;
	case 3:
		goto loc_82336100;
	case 4:
		goto loc_823360AC;
	case 5:
		goto loc_823360AC;
	case 6:
		goto loc_823360AC;
	case 7:
		goto loc_823360AC;
	case 8:
		goto loc_82335FE4;
	case 9:
		goto loc_82335FE4;
	case 10:
		goto loc_82335FE4;
	case 11:
		goto loc_82335FE4;
	case 12:
		goto loc_82335FE4;
	case 13:
		goto loc_82335FE4;
	case 14:
		goto loc_82335FE4;
	case 15:
		goto loc_82335FE4;
	case 16:
		goto loc_82335FA4;
	case 17:
		goto loc_82335FA4;
	case 18:
		goto loc_82335F6C;
	case 19:
		goto loc_82335F6C;
	case 20:
		goto loc_82335F34;
	case 21:
		goto loc_82335F34;
	case 22:
		goto loc_82335EFC;
	case 23:
		goto loc_82335EFC;
	case 24:
		goto loc_82335EC4;
	case 25:
		goto loc_82335EC4;
	case 26:
		goto loc_82335E8C;
	case 27:
		goto loc_82335E8C;
	case 28:
		goto loc_82335E54;
	case 29:
		goto loc_82335E54;
	case 30:
		goto loc_82335E1C;
	case 31:
		goto loc_82335E1C;
	case 32:
		goto loc_82335DDC;
	case 33:
		goto loc_82335DDC;
	case 34:
		goto loc_82336950;
	case 35:
		goto loc_82335DDC;
	case 36:
		goto loc_82335DA4;
	case 37:
		goto loc_82335DA4;
	case 38:
		goto loc_82336934;
	case 39:
		goto loc_82335DA4;
	case 40:
		goto loc_82335D6C;
	case 41:
		goto loc_82335D6C;
	case 42:
		goto loc_8233692C;
	case 43:
		goto loc_82335D6C;
	case 44:
		goto loc_82335D34;
	case 45:
		goto loc_82335D34;
	case 46:
		goto loc_82336910;
	case 47:
		goto loc_82335D34;
	case 48:
		goto loc_82335CFC;
	case 49:
		goto loc_82335CFC;
	case 50:
		goto loc_823368F4;
	case 51:
		goto loc_82335CFC;
	case 52:
		goto loc_82335CC4;
	case 53:
		goto loc_82335CC4;
	case 54:
		goto loc_823368D8;
	case 55:
		goto loc_82335CC4;
	case 56:
		goto loc_82335C8C;
	case 57:
		goto loc_82335C8C;
	case 58:
		goto loc_823368D0;
	case 59:
		goto loc_82335C8C;
	case 60:
		goto loc_82335C54;
	case 61:
		goto loc_82335C54;
	case 62:
		goto loc_823368B4;
	case 63:
		goto loc_82335C54;
	case 64:
		goto loc_82335C1C;
	case 65:
		goto loc_82335BE4;
	case 66:
		goto loc_82335BAC;
	case 67:
		goto loc_82335B74;
	case 68:
		goto loc_82335B3C;
	case 69:
		goto loc_82335B04;
	case 70:
		goto loc_82335ACC;
	case 71:
		goto loc_82335A94;
	case 72:
		goto loc_82335A5C;
	case 73:
		goto loc_82335A24;
	case 74:
		goto loc_823359EC;
	case 75:
		goto loc_823359B4;
	case 76:
		goto loc_8233597C;
	case 77:
		goto loc_82335944;
	case 78:
		goto loc_8233590C;
	case 79:
		goto loc_823358D4;
	case 80:
		goto loc_82335894;
	case 81:
		goto loc_82335894;
	case 82:
		goto loc_82335894;
	case 83:
		goto loc_82335894;
	case 84:
		goto loc_8233687C;
	case 85:
		goto loc_8233687C;
	case 86:
		goto loc_82335844;
	case 87:
		goto loc_823357F4;
	case 88:
		goto loc_823357BC;
	case 89:
		goto loc_823357BC;
	case 90:
		goto loc_823357BC;
	case 91:
		goto loc_823357BC;
	case 92:
		goto loc_82336844;
	case 93:
		goto loc_82336844;
	case 94:
		goto loc_8233576C;
	case 95:
		goto loc_8233571C;
	case 96:
		goto loc_823356E4;
	case 97:
		goto loc_823356E4;
	case 98:
		goto loc_823356E4;
	case 99:
		goto loc_823356E4;
	case 100:
		goto loc_8233680C;
	case 101:
		goto loc_8233680C;
	case 102:
		goto loc_82335694;
	case 103:
		goto loc_82335644;
	case 104:
		goto loc_8233560C;
	case 105:
		goto loc_8233560C;
	case 106:
		goto loc_8233560C;
	case 107:
		goto loc_8233560C;
	case 108:
		goto loc_823367D4;
	case 109:
		goto loc_823367D4;
	case 110:
		goto loc_823355BC;
	case 111:
		goto loc_8233556C;
	case 112:
		goto loc_82335534;
	case 113:
		goto loc_82335534;
	case 114:
		goto loc_82335534;
	case 115:
		goto loc_82335534;
	case 116:
		goto loc_8233679C;
	case 117:
		goto loc_8233679C;
	case 118:
		goto loc_823354E4;
	case 119:
		goto loc_82335494;
	case 120:
		goto loc_8233545C;
	case 121:
		goto loc_8233545C;
	case 122:
		goto loc_8233545C;
	case 123:
		goto loc_8233545C;
	case 124:
		goto loc_82336764;
	case 125:
		goto loc_82336764;
	case 126:
		goto loc_8233540C;
	case 127:
		goto loc_823353BC;
	case 128:
		goto loc_8233537C;
	case 129:
		goto loc_8233537C;
	case 130:
		goto loc_8233537C;
	case 131:
		goto loc_8233537C;
	case 132:
		goto loc_8233537C;
	case 133:
		goto loc_8233537C;
	case 134:
		goto loc_8233532C;
	case 135:
		goto loc_823352DC;
	case 136:
		goto loc_8233672C;
	case 137:
		goto loc_8233672C;
	case 138:
		goto loc_8233672C;
	case 139:
		goto loc_8233672C;
	case 140:
		goto loc_8233528C;
	case 141:
		goto loc_8233528C;
	case 142:
		goto loc_8233523C;
	case 143:
		goto loc_8233523C;
	case 144:
		goto loc_82335204;
	case 145:
		goto loc_82335204;
	case 146:
		goto loc_82335204;
	case 147:
		goto loc_82335204;
	case 148:
		goto loc_82335204;
	case 149:
		goto loc_82335204;
	case 150:
		goto loc_823351B4;
	case 151:
		goto loc_82335164;
	case 152:
		goto loc_823366F4;
	case 153:
		goto loc_823366F4;
	case 154:
		goto loc_823366F4;
	case 155:
		goto loc_823366F4;
	case 156:
		goto loc_82335114;
	case 157:
		goto loc_82335114;
	case 158:
		goto loc_823350C4;
	case 159:
		goto loc_823350C4;
	case 160:
		goto loc_8233508C;
	case 161:
		goto loc_8233508C;
	case 162:
		goto loc_8233508C;
	case 163:
		goto loc_8233508C;
	case 164:
		goto loc_8233508C;
	case 165:
		goto loc_8233508C;
	case 166:
		goto loc_8233503C;
	case 167:
		goto loc_82334FEC;
	case 168:
		goto loc_823366BC;
	case 169:
		goto loc_823366BC;
	case 170:
		goto loc_823366BC;
	case 171:
		goto loc_823366BC;
	case 172:
		goto loc_82334F9C;
	case 173:
		goto loc_82334F9C;
	case 174:
		goto loc_82334F4C;
	case 175:
		goto loc_82334F4C;
	case 176:
		goto loc_82334F14;
	case 177:
		goto loc_82334F14;
	case 178:
		goto loc_82334F14;
	case 179:
		goto loc_82334F14;
	case 180:
		goto loc_82334F14;
	case 181:
		goto loc_82334F14;
	case 182:
		goto loc_82334EC4;
	case 183:
		goto loc_82334E74;
	case 184:
		goto loc_8233667C;
	case 185:
		goto loc_8233667C;
	case 186:
		goto loc_8233667C;
	case 187:
		goto loc_8233667C;
	case 188:
		goto loc_82334E24;
	case 189:
		goto loc_82334E24;
	case 190:
		goto loc_82334DD4;
	case 191:
		goto loc_82334DD4;
	case 192:
		goto loc_82334D94;
	case 193:
		goto loc_82334D94;
	case 194:
		goto loc_82334D94;
	case 195:
		goto loc_82334D94;
	case 196:
		goto loc_82334D94;
	case 197:
		goto loc_82334D94;
	case 198:
		goto loc_82334D94;
	case 199:
		goto loc_82334D94;
	case 200:
		goto loc_82334D44;
	case 201:
		goto loc_82334CF4;
	case 202:
		goto loc_82334CA4;
	case 203:
		goto loc_82334C54;
	case 204:
		goto loc_82334C04;
	case 205:
		goto loc_82334C04;
	case 206:
		goto loc_82334BB4;
	case 207:
		goto loc_82334BB4;
	case 208:
		goto loc_82336644;
	case 209:
		goto loc_82336644;
	case 210:
		goto loc_82336644;
	case 211:
		goto loc_82336644;
	case 212:
		goto loc_82336644;
	case 213:
		goto loc_82336644;
	case 214:
		goto loc_82336644;
	case 215:
		goto loc_82336644;
	case 216:
		goto loc_82334B64;
	case 217:
		goto loc_82334B64;
	case 218:
		goto loc_82336624;
	case 219:
		goto loc_82334B64;
	case 220:
		goto loc_82334B14;
	case 221:
		goto loc_82334B14;
	case 222:
		goto loc_823365F0;
	case 223:
		goto loc_82334B14;
	case 224:
		goto loc_82334ADC;
	case 225:
		goto loc_82334ADC;
	case 226:
		goto loc_82334ADC;
	case 227:
		goto loc_82334ADC;
	case 228:
		goto loc_82334ADC;
	case 229:
		goto loc_82334ADC;
	case 230:
		goto loc_82334ADC;
	case 231:
		goto loc_82334ADC;
	case 232:
		goto loc_82334A8C;
	case 233:
		goto loc_82334A3C;
	case 234:
		goto loc_823349EC;
	case 235:
		goto loc_8233499C;
	case 236:
		goto loc_8233494C;
	case 237:
		goto loc_8233494C;
	case 238:
		goto loc_823348FC;
	case 239:
		goto loc_823348FC;
	case 240:
		goto loc_823365B0;
	case 241:
		goto loc_823365B0;
	case 242:
		goto loc_823365B0;
	case 243:
		goto loc_823365B0;
	case 244:
		goto loc_823365B0;
	case 245:
		goto loc_823365B0;
	case 246:
		goto loc_823365B0;
	case 247:
		goto loc_823365B0;
	case 248:
		goto loc_823348AC;
	case 249:
		goto loc_823348AC;
	case 250:
		goto loc_82336590;
	case 251:
		goto loc_823348AC;
	case 252:
		goto loc_8233485C;
	case 253:
		goto loc_8233485C;
	case 254:
		goto loc_8233655C;
	case 255:
		goto loc_8233485C;
	case 256:
		goto loc_82336534;
	case 257:
		goto loc_82336534;
	case 258:
		goto loc_82336534;
	case 259:
		goto loc_82336534;
	case 260:
		goto loc_82336534;
	case 261:
		goto loc_82336534;
	case 262:
		goto loc_82336534;
	case 263:
		goto loc_82336534;
	case 264:
		goto loc_82336534;
	case 265:
		goto loc_82336534;
	case 266:
		goto loc_82336534;
	case 267:
		goto loc_82336534;
	case 268:
		goto loc_82336534;
	case 269:
		goto loc_82336534;
	case 270:
		goto loc_82336534;
	case 271:
		goto loc_82336534;
	case 272:
		goto loc_82336534;
	case 273:
		goto loc_82336534;
	case 274:
		goto loc_82336534;
	case 275:
		goto loc_82336534;
	case 276:
		goto loc_82336534;
	case 277:
		goto loc_82336534;
	case 278:
		goto loc_82336534;
	case 279:
		goto loc_82336534;
	case 280:
		goto loc_82336534;
	case 281:
		goto loc_82336534;
	case 282:
		goto loc_82336534;
	case 283:
		goto loc_82336534;
	case 284:
		goto loc_82336534;
	case 285:
		goto loc_82336534;
	case 286:
		goto loc_82336534;
	case 287:
		goto loc_82336534;
	case 288:
		goto loc_82336534;
	case 289:
		goto loc_82336534;
	case 290:
		goto loc_82336534;
	case 291:
		goto loc_82336534;
	case 292:
		goto loc_82336534;
	case 293:
		goto loc_82336534;
	case 294:
		goto loc_82336534;
	case 295:
		goto loc_82336534;
	case 296:
		goto loc_82336534;
	case 297:
		goto loc_82336534;
	case 298:
		goto loc_82336534;
	case 299:
		goto loc_82336534;
	case 300:
		goto loc_82336534;
	case 301:
		goto loc_82336534;
	case 302:
		goto loc_82336534;
	case 303:
		goto loc_82336534;
	case 304:
		goto loc_82336534;
	case 305:
		goto loc_82336534;
	case 306:
		goto loc_82336534;
	case 307:
		goto loc_82336534;
	case 308:
		goto loc_82336534;
	case 309:
		goto loc_82336534;
	case 310:
		goto loc_82336534;
	case 311:
		goto loc_82336534;
	case 312:
		goto loc_82336534;
	case 313:
		goto loc_82336534;
	case 314:
		goto loc_82336534;
	case 315:
		goto loc_82336534;
	case 316:
		goto loc_82336534;
	case 317:
		goto loc_82336534;
	case 318:
		goto loc_82336534;
	case 319:
		goto loc_82336534;
	case 320:
		goto loc_82336534;
	case 321:
		goto loc_82336534;
	case 322:
		goto loc_82336534;
	case 323:
		goto loc_82336534;
	case 324:
		goto loc_82336534;
	case 325:
		goto loc_82336534;
	case 326:
		goto loc_82336534;
	case 327:
		goto loc_82336534;
	case 328:
		goto loc_82336534;
	case 329:
		goto loc_82336534;
	case 330:
		goto loc_82336534;
	case 331:
		goto loc_82336534;
	case 332:
		goto loc_82336534;
	case 333:
		goto loc_82336534;
	case 334:
		goto loc_82336534;
	case 335:
		goto loc_82336534;
	case 336:
		goto loc_82336534;
	case 337:
		goto loc_82336534;
	case 338:
		goto loc_82336534;
	case 339:
		goto loc_82336534;
	case 340:
		goto loc_82336534;
	case 341:
		goto loc_82336534;
	case 342:
		goto loc_82336534;
	case 343:
		goto loc_82336534;
	case 344:
		goto loc_82336534;
	case 345:
		goto loc_82336534;
	case 346:
		goto loc_82336534;
	case 347:
		goto loc_82336534;
	case 348:
		goto loc_82336534;
	case 349:
		goto loc_82336534;
	case 350:
		goto loc_82336534;
	case 351:
		goto loc_82336534;
	case 352:
		goto loc_82336534;
	case 353:
		goto loc_82336534;
	case 354:
		goto loc_82336534;
	case 355:
		goto loc_82336534;
	case 356:
		goto loc_82336534;
	case 357:
		goto loc_82336534;
	case 358:
		goto loc_82336534;
	case 359:
		goto loc_82336534;
	case 360:
		goto loc_82336534;
	case 361:
		goto loc_82336534;
	case 362:
		goto loc_82336534;
	case 363:
		goto loc_82336534;
	case 364:
		goto loc_82336534;
	case 365:
		goto loc_82336534;
	case 366:
		goto loc_82336534;
	case 367:
		goto loc_82336534;
	case 368:
		goto loc_82336534;
	case 369:
		goto loc_82336534;
	case 370:
		goto loc_82336534;
	case 371:
		goto loc_82336534;
	case 372:
		goto loc_82336534;
	case 373:
		goto loc_82336534;
	case 374:
		goto loc_82336534;
	case 375:
		goto loc_82336534;
	case 376:
		goto loc_82336534;
	case 377:
		goto loc_82336534;
	case 378:
		goto loc_82336534;
	case 379:
		goto loc_82336534;
	case 380:
		goto loc_82336534;
	case 381:
		goto loc_82336534;
	case 382:
		goto loc_82336534;
	case 383:
		goto loc_82336534;
	case 384:
		goto loc_8233481C;
	case 385:
		goto loc_8233481C;
	case 386:
		goto loc_8233481C;
	case 387:
		goto loc_8233481C;
	case 388:
		goto loc_8233481C;
	case 389:
		goto loc_8233481C;
	case 390:
		goto loc_8233481C;
	case 391:
		goto loc_8233481C;
	case 392:
		goto loc_8233481C;
	case 393:
		goto loc_8233481C;
	case 394:
		goto loc_823347CC;
	case 395:
		goto loc_8233477C;
	case 396:
		goto loc_8233472C;
	case 397:
		goto loc_823346DC;
	case 398:
		goto loc_8233468C;
	case 399:
		goto loc_8233463C;
	case 400:
		goto loc_823345EC;
	case 401:
		goto loc_823345EC;
	case 402:
		goto loc_8233459C;
	case 403:
		goto loc_8233459C;
	case 404:
		goto loc_8233454C;
	case 405:
		goto loc_8233454C;
	case 406:
		goto loc_823344FC;
	case 407:
		goto loc_823344FC;
	case 408:
		goto loc_823344AC;
	case 409:
		goto loc_823344AC;
	case 410:
		goto loc_82336514;
	case 411:
		goto loc_823344AC;
	case 412:
		goto loc_8233445C;
	case 413:
		goto loc_8233445C;
	case 414:
		goto loc_823364E0;
	case 415:
		goto loc_8233445C;
	case 416:
		goto loc_823364A8;
	case 417:
		goto loc_823364A8;
	case 418:
		goto loc_823364A8;
	case 419:
		goto loc_823364A8;
	case 420:
		goto loc_823364A8;
	case 421:
		goto loc_823364A8;
	case 422:
		goto loc_823364A8;
	case 423:
		goto loc_823364A8;
	case 424:
		goto loc_823364A8;
	case 425:
		goto loc_823364A8;
	case 426:
		goto loc_823364A8;
	case 427:
		goto loc_823364A8;
	case 428:
		goto loc_823364A8;
	case 429:
		goto loc_823364A8;
	case 430:
		goto loc_823364A8;
	case 431:
		goto loc_823364A8;
	case 432:
		goto loc_8233440C;
	case 433:
		goto loc_8233440C;
	case 434:
		goto loc_8233440C;
	case 435:
		goto loc_8233440C;
	case 436:
		goto loc_82336458;
	case 437:
		goto loc_82336458;
	case 438:
		goto loc_823343A4;
	case 439:
		goto loc_8233433C;
	case 440:
		goto loc_823342EC;
	case 441:
		goto loc_823342EC;
	case 442:
		goto loc_823342EC;
	case 443:
		goto loc_823342EC;
	case 444:
		goto loc_82336408;
	case 445:
		goto loc_82336408;
	case 446:
		goto loc_82334284;
	case 447:
		goto loc_8233421C;
	case 448:
		goto loc_823341E4;
	case 449:
		goto loc_823341E4;
	case 450:
		goto loc_823341E4;
	case 451:
		goto loc_823341E4;
	case 452:
		goto loc_823341E4;
	case 453:
		goto loc_823341E4;
	case 454:
		goto loc_823341E4;
	case 455:
		goto loc_823341E4;
	case 456:
		goto loc_823341E4;
	case 457:
		goto loc_823341E4;
	case 458:
		goto loc_82334194;
	case 459:
		goto loc_82334144;
	case 460:
		goto loc_823340F4;
	case 461:
		goto loc_823340A4;
	case 462:
		goto loc_82334054;
	case 463:
		goto loc_82334004;
	case 464:
		goto loc_82333FB4;
	case 465:
		goto loc_82333FB4;
	case 466:
		goto loc_82333F64;
	case 467:
		goto loc_82333F64;
	case 468:
		goto loc_82333F14;
	case 469:
		goto loc_82333F14;
	case 470:
		goto loc_82333EC4;
	case 471:
		goto loc_82333EC4;
	case 472:
		goto loc_82333E74;
	case 473:
		goto loc_82333E74;
	case 474:
		goto loc_823363E8;
	case 475:
		goto loc_82333E74;
	case 476:
		goto loc_82333E24;
	case 477:
		goto loc_82333E24;
	case 478:
		goto loc_823363B4;
	case 479:
		goto loc_82333E24;
	case 480:
		goto loc_82336374;
	case 481:
		goto loc_82336374;
	case 482:
		goto loc_82336374;
	case 483:
		goto loc_82336374;
	case 484:
		goto loc_82336374;
	case 485:
		goto loc_82336374;
	case 486:
		goto loc_82336374;
	case 487:
		goto loc_82336374;
	case 488:
		goto loc_82336374;
	case 489:
		goto loc_82336374;
	case 490:
		goto loc_82336374;
	case 491:
		goto loc_82336374;
	case 492:
		goto loc_82336374;
	case 493:
		goto loc_82336374;
	case 494:
		goto loc_82336374;
	case 495:
		goto loc_82336374;
	case 496:
		goto loc_82333DD4;
	case 497:
		goto loc_82333DD4;
	case 498:
		goto loc_82333DD4;
	case 499:
		goto loc_82333DD4;
	case 500:
		goto loc_82336324;
	case 501:
		goto loc_82336324;
	case 502:
		goto loc_82333D6C;
	case 503:
		goto loc_82333D04;
	case 504:
		goto loc_82333CB4;
	case 505:
		goto loc_82333CB4;
	case 506:
		goto loc_82333CB4;
	case 507:
		goto loc_82333CB4;
	case 508:
		goto loc_823362CC;
	case 509:
		goto loc_823362CC;
	case 510:
		goto loc_82333C4C;
	case 511:
		goto loc_82333BD8;
	default:
		__builtin_unreachable();
	}
	// lwz r17,25076(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25076);
	// lwz r17,24916(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24916);
	// lwz r17,24832(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24832);
	// lwz r17,24832(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24832);
	// lwz r17,24748(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24748);
	// lwz r17,24748(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24748);
	// lwz r17,24748(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24748);
	// lwz r17,24748(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24748);
	// lwz r17,24548(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24548);
	// lwz r17,24548(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24548);
	// lwz r17,24548(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24548);
	// lwz r17,24548(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24548);
	// lwz r17,24548(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24548);
	// lwz r17,24548(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24548);
	// lwz r17,24548(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24548);
	// lwz r17,24548(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24548);
	// lwz r17,24484(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24484);
	// lwz r17,24484(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24484);
	// lwz r17,24428(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24428);
	// lwz r17,24428(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24428);
	// lwz r17,24372(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24372);
	// lwz r17,24372(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24372);
	// lwz r17,24316(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24316);
	// lwz r17,24316(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24316);
	// lwz r17,24260(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24260);
	// lwz r17,24260(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24260);
	// lwz r17,24204(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24204);
	// lwz r17,24204(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24204);
	// lwz r17,24148(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24148);
	// lwz r17,24148(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24148);
	// lwz r17,24092(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24092);
	// lwz r17,24092(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24092);
	// lwz r17,24028(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24028);
	// lwz r17,24028(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24028);
	// lwz r17,26960(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26960);
	// lwz r17,24028(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 24028);
	// lwz r17,23972(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23972);
	// lwz r17,23972(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23972);
	// lwz r17,26932(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26932);
	// lwz r17,23972(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23972);
	// lwz r17,23916(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23916);
	// lwz r17,23916(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23916);
	// lwz r17,26924(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26924);
	// lwz r17,23916(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23916);
	// lwz r17,23860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23860);
	// lwz r17,23860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23860);
	// lwz r17,26896(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26896);
	// lwz r17,23860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23860);
	// lwz r17,23804(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23804);
	// lwz r17,23804(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23804);
	// lwz r17,26868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26868);
	// lwz r17,23804(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23804);
	// lwz r17,23748(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23748);
	// lwz r17,23748(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23748);
	// lwz r17,26840(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26840);
	// lwz r17,23748(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23748);
	// lwz r17,23692(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23692);
	// lwz r17,23692(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23692);
	// lwz r17,26832(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26832);
	// lwz r17,23692(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23692);
	// lwz r17,23636(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23636);
	// lwz r17,23636(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23636);
	// lwz r17,26804(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26804);
	// lwz r17,23636(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23636);
	// lwz r17,23580(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23580);
	// lwz r17,23524(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23524);
	// lwz r17,23468(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23468);
	// lwz r17,23412(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23412);
	// lwz r17,23356(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23356);
	// lwz r17,23300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23300);
	// lwz r17,23244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23244);
	// lwz r17,23188(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23188);
	// lwz r17,23132(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23132);
	// lwz r17,23076(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23076);
	// lwz r17,23020(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 23020);
	// lwz r17,22964(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22964);
	// lwz r17,22908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22908);
	// lwz r17,22852(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22852);
	// lwz r17,22796(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22796);
	// lwz r17,22740(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22740);
	// lwz r17,22676(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22676);
	// lwz r17,22676(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22676);
	// lwz r17,22676(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22676);
	// lwz r17,22676(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22676);
	// lwz r17,26748(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26748);
	// lwz r17,26748(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26748);
	// lwz r17,22596(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22596);
	// lwz r17,22516(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22516);
	// lwz r17,22460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22460);
	// lwz r17,22460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22460);
	// lwz r17,22460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22460);
	// lwz r17,22460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22460);
	// lwz r17,26692(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26692);
	// lwz r17,26692(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26692);
	// lwz r17,22380(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22380);
	// lwz r17,22300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22300);
	// lwz r17,22244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22244);
	// lwz r17,22244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22244);
	// lwz r17,22244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22244);
	// lwz r17,22244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22244);
	// lwz r17,26636(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26636);
	// lwz r17,26636(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26636);
	// lwz r17,22164(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22164);
	// lwz r17,22084(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22084);
	// lwz r17,22028(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22028);
	// lwz r17,22028(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22028);
	// lwz r17,22028(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22028);
	// lwz r17,22028(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 22028);
	// lwz r17,26580(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26580);
	// lwz r17,26580(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26580);
	// lwz r17,21948(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21948);
	// lwz r17,21868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21868);
	// lwz r17,21812(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21812);
	// lwz r17,21812(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21812);
	// lwz r17,21812(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21812);
	// lwz r17,21812(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21812);
	// lwz r17,26524(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26524);
	// lwz r17,26524(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26524);
	// lwz r17,21732(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21732);
	// lwz r17,21652(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21652);
	// lwz r17,21596(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21596);
	// lwz r17,21596(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21596);
	// lwz r17,21596(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21596);
	// lwz r17,21596(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21596);
	// lwz r17,26468(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26468);
	// lwz r17,26468(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26468);
	// lwz r17,21516(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21516);
	// lwz r17,21436(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21436);
	// lwz r17,21372(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21372);
	// lwz r17,21372(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21372);
	// lwz r17,21372(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21372);
	// lwz r17,21372(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21372);
	// lwz r17,21372(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21372);
	// lwz r17,21372(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21372);
	// lwz r17,21292(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21292);
	// lwz r17,21212(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21212);
	// lwz r17,26412(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26412);
	// lwz r17,26412(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26412);
	// lwz r17,26412(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26412);
	// lwz r17,26412(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26412);
	// lwz r17,21132(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21132);
	// lwz r17,21132(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21132);
	// lwz r17,21052(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21052);
	// lwz r17,21052(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 21052);
	// lwz r17,20996(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20996);
	// lwz r17,20996(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20996);
	// lwz r17,20996(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20996);
	// lwz r17,20996(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20996);
	// lwz r17,20996(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20996);
	// lwz r17,20996(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20996);
	// lwz r17,20916(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20916);
	// lwz r17,20836(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20836);
	// lwz r17,26356(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26356);
	// lwz r17,26356(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26356);
	// lwz r17,26356(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26356);
	// lwz r17,26356(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26356);
	// lwz r17,20756(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20756);
	// lwz r17,20756(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20756);
	// lwz r17,20676(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20676);
	// lwz r17,20676(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20676);
	// lwz r17,20620(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20620);
	// lwz r17,20620(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20620);
	// lwz r17,20620(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20620);
	// lwz r17,20620(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20620);
	// lwz r17,20620(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20620);
	// lwz r17,20620(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20620);
	// lwz r17,20540(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20540);
	// lwz r17,20460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20460);
	// lwz r17,26300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26300);
	// lwz r17,26300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26300);
	// lwz r17,26300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26300);
	// lwz r17,26300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26300);
	// lwz r17,20380(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20380);
	// lwz r17,20380(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20380);
	// lwz r17,20300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20300);
	// lwz r17,20300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20300);
	// lwz r17,20244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20244);
	// lwz r17,20244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20244);
	// lwz r17,20244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20244);
	// lwz r17,20244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20244);
	// lwz r17,20244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20244);
	// lwz r17,20244(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20244);
	// lwz r17,20164(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20164);
	// lwz r17,20084(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20084);
	// lwz r17,26236(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26236);
	// lwz r17,26236(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26236);
	// lwz r17,26236(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26236);
	// lwz r17,26236(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26236);
	// lwz r17,20004(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20004);
	// lwz r17,20004(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 20004);
	// lwz r17,19924(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19924);
	// lwz r17,19924(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19924);
	// lwz r17,19860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19860);
	// lwz r17,19860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19860);
	// lwz r17,19860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19860);
	// lwz r17,19860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19860);
	// lwz r17,19860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19860);
	// lwz r17,19860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19860);
	// lwz r17,19860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19860);
	// lwz r17,19860(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19860);
	// lwz r17,19780(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19780);
	// lwz r17,19700(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19700);
	// lwz r17,19620(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19620);
	// lwz r17,19540(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19540);
	// lwz r17,19460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19460);
	// lwz r17,19460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19460);
	// lwz r17,19380(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19380);
	// lwz r17,19380(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19380);
	// lwz r17,26180(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26180);
	// lwz r17,26180(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26180);
	// lwz r17,26180(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26180);
	// lwz r17,26180(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26180);
	// lwz r17,26180(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26180);
	// lwz r17,26180(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26180);
	// lwz r17,26180(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26180);
	// lwz r17,26180(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26180);
	// lwz r17,19300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19300);
	// lwz r17,19300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19300);
	// lwz r17,26148(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26148);
	// lwz r17,19300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19300);
	// lwz r17,19220(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19220);
	// lwz r17,19220(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19220);
	// lwz r17,26096(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26096);
	// lwz r17,19220(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19220);
	// lwz r17,19164(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19164);
	// lwz r17,19164(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19164);
	// lwz r17,19164(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19164);
	// lwz r17,19164(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19164);
	// lwz r17,19164(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19164);
	// lwz r17,19164(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19164);
	// lwz r17,19164(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19164);
	// lwz r17,19164(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19164);
	// lwz r17,19084(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19084);
	// lwz r17,19004(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 19004);
	// lwz r17,18924(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18924);
	// lwz r17,18844(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18844);
	// lwz r17,18764(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18764);
	// lwz r17,18764(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18764);
	// lwz r17,18684(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18684);
	// lwz r17,18684(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18684);
	// lwz r17,26032(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26032);
	// lwz r17,26032(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26032);
	// lwz r17,26032(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26032);
	// lwz r17,26032(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26032);
	// lwz r17,26032(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26032);
	// lwz r17,26032(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26032);
	// lwz r17,26032(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26032);
	// lwz r17,26032(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26032);
	// lwz r17,18604(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18604);
	// lwz r17,18604(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18604);
	// lwz r17,26000(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 26000);
	// lwz r17,18604(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18604);
	// lwz r17,18524(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18524);
	// lwz r17,18524(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18524);
	// lwz r17,25948(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25948);
	// lwz r17,18524(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18524);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,25908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25908);
	// lwz r17,18460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18460);
	// lwz r17,18460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18460);
	// lwz r17,18460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18460);
	// lwz r17,18460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18460);
	// lwz r17,18460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18460);
	// lwz r17,18460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18460);
	// lwz r17,18460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18460);
	// lwz r17,18460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18460);
	// lwz r17,18460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18460);
	// lwz r17,18460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18460);
	// lwz r17,18380(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18380);
	// lwz r17,18300(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18300);
	// lwz r17,18220(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18220);
	// lwz r17,18140(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18140);
	// lwz r17,18060(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 18060);
	// lwz r17,17980(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17980);
	// lwz r17,17900(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17900);
	// lwz r17,17900(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17900);
	// lwz r17,17820(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17820);
	// lwz r17,17820(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17820);
	// lwz r17,17740(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17740);
	// lwz r17,17740(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17740);
	// lwz r17,17660(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17660);
	// lwz r17,17660(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17660);
	// lwz r17,17580(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17580);
	// lwz r17,17580(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17580);
	// lwz r17,25876(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25876);
	// lwz r17,17580(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17580);
	// lwz r17,17500(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17500);
	// lwz r17,17500(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17500);
	// lwz r17,25824(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25824);
	// lwz r17,17500(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17500);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,25768(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25768);
	// lwz r17,17420(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17420);
	// lwz r17,17420(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17420);
	// lwz r17,17420(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17420);
	// lwz r17,17420(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17420);
	// lwz r17,25688(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25688);
	// lwz r17,25688(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25688);
	// lwz r17,17316(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17316);
	// lwz r17,17212(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17212);
	// lwz r17,17132(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17132);
	// lwz r17,17132(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17132);
	// lwz r17,17132(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17132);
	// lwz r17,17132(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17132);
	// lwz r17,25608(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25608);
	// lwz r17,25608(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25608);
	// lwz r17,17028(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 17028);
	// lwz r17,16924(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16924);
	// lwz r17,16868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16868);
	// lwz r17,16868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16868);
	// lwz r17,16868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16868);
	// lwz r17,16868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16868);
	// lwz r17,16868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16868);
	// lwz r17,16868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16868);
	// lwz r17,16868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16868);
	// lwz r17,16868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16868);
	// lwz r17,16868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16868);
	// lwz r17,16868(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16868);
	// lwz r17,16788(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16788);
	// lwz r17,16708(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16708);
	// lwz r17,16628(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16628);
	// lwz r17,16548(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16548);
	// lwz r17,16468(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16468);
	// lwz r17,16388(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16388);
	// lwz r17,16308(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16308);
	// lwz r17,16308(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16308);
	// lwz r17,16228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16228);
	// lwz r17,16228(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16228);
	// lwz r17,16148(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16148);
	// lwz r17,16148(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16148);
	// lwz r17,16068(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16068);
	// lwz r17,16068(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 16068);
	// lwz r17,15988(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15988);
	// lwz r17,15988(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15988);
	// lwz r17,25576(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25576);
	// lwz r17,15988(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15988);
	// lwz r17,15908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15908);
	// lwz r17,15908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15908);
	// lwz r17,25524(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25524);
	// lwz r17,15908(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15908);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,25460(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25460);
	// lwz r17,15828(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15828);
	// lwz r17,15828(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15828);
	// lwz r17,15828(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15828);
	// lwz r17,15828(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15828);
	// lwz r17,25380(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25380);
	// lwz r17,25380(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25380);
	// lwz r17,15724(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15724);
	// lwz r17,15620(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15620);
	// lwz r17,15540(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15540);
	// lwz r17,15540(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15540);
	// lwz r17,15540(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15540);
	// lwz r17,15540(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15540);
	// lwz r17,25292(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25292);
	// lwz r17,25292(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 25292);
	// lwz r17,15436(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15436);
	// lwz r17,15320(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 15320);
loc_82333BD8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r3,r5
	ctx.r3.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r3,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r3.u32);
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,28(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r3,r5
	ctx.r3.s64 = ctx.r5.s8;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r3,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r3.u32);
	// lwz r3,28(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r3,r5
	ctx.r3.s64 = ctx.r5.s8;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r3,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r3.u32);
	// lwz r3,28(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r30.u32);
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333C44:
	// rlwinm r6,r6,9,0,22
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 9) & 0xFFFFFE00;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333C4C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333CB4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x823358cc
	if (ctx.cr6.lt) goto loc_823358CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333D04:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333D6C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333DD4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x823358cc
	if (ctx.cr6.lt) goto loc_823358CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333E24:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333E74:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333EC4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333F14:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333F64:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82333FB4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334004:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334054:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823340A4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823340F4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334144:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334194:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r24,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r24.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823341E4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82334854
	if (ctx.cr6.lt) goto loc_82334854;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233421C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334284:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823342EC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x823358cc
	if (ctx.cr6.lt) goto loc_823358CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233433C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823343A4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233440C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x823358cc
	if (ctx.cr6.lt) goto loc_823358CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233445C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823344AC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823344FC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233454C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233459C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823345EC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233463C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233468C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823346DC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233472C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233477C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823347CC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r24,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r24.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233481C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82334854
	if (ctx.cr6.lt) goto loc_82334854;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334854:
	// rlwinm r6,r6,3,0,28
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233485C:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823348AC:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823348FC:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233494C:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233499C:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823349EC:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334A3C:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334A8C:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334ADC:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82334dcc
	if (ctx.cr6.lt) goto loc_82334DCC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334B14:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334B64:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334BB4:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334C04:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334C54:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334CA4:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334CF4:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334D44:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334D94:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82334dcc
	if (ctx.cr6.lt) goto loc_82334DCC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334DCC:
	// rlwinm r6,r6,4,0,27
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334DD4:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334E24:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334E74:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334EC4:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334F14:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x823353b4
	if (ctx.cr6.lt) goto loc_823353B4;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334F4C:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334F9C:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82334FEC:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233503C:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233508C:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x823353b4
	if (ctx.cr6.lt) goto loc_823353B4;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823350C4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335114:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335164:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823351B4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335204:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x823353b4
	if (ctx.cr6.lt) goto loc_823353B4;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233523C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233528C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823352DC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233532C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233537C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x823353b4
	if (ctx.cr6.lt) goto loc_823353B4;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823353B4:
	// rlwinm r6,r6,5,0,26
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// b 0x8233339c
	goto loc_8233339C;
loc_823353BC:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233540C:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233545C:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x823358cc
	if (ctx.cr6.lt) goto loc_823358CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335494:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823354E4:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335534:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x823358cc
	if (ctx.cr6.lt) goto loc_823358CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233556C:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823355BC:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233560C:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x823358cc
	if (ctx.cr6.lt) goto loc_823358CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335644:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335694:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823356E4:
	// lbzu r5,5(r10)
	ea = 5 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x823358cc
	if (ctx.cr6.lt) goto loc_823358CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233571C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233576C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823357BC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x823358cc
	if (ctx.cr6.lt) goto loc_823358CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823357F4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r24,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r24.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335844:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r24,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r24.u32);
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335894:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r24,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r24.u32);
	// blt cr6,0x823358cc
	if (ctx.cr6.lt) goto loc_823358CC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823358CC:
	// rlwinm r6,r6,6,0,25
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
	// b 0x8233339c
	goto loc_8233339C;
loc_823358D4:
	// lbzu r5,11(r10)
	ea = 11 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233590C:
	// lbzu r5,11(r10)
	ea = 11 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335944:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r21,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r21.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_8233597C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r18,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r18.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823359B4:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r26.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823359EC:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r24,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r24.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335A24:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335A5C:
	// lbzu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335A94:
	// lbzu r5,12(r10)
	ea = 12 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335ACC:
	// lbzu r5,12(r10)
	ea = 12 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335B04:
	// lbzu r5,13(r10)
	ea = 13 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335B3C:
	// lbzu r5,13(r10)
	ea = 13 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335B74:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r22,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r22.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335BAC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r19,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r19.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335BE4:
	// lbzu r5,14(r10)
	ea = 14 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335C1C:
	// lbzu r5,14(r10)
	ea = 14 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82333c44
	if (ctx.cr6.lt) goto loc_82333C44;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335C54:
	// lbzu r5,6(r10)
	ea = 6 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335C8C:
	// lbzu r5,6(r10)
	ea = 6 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335CC4:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335CFC:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335D34:
	// lbzu r5,7(r10)
	ea = 7 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335D6C:
	// lbzu r5,7(r10)
	ea = 7 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335DA4:
	// lbzu r5,8(r10)
	ea = 8 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335DDC:
	// lbzu r5,8(r10)
	ea = 8 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335e14
	if (ctx.cr6.lt) goto loc_82335E14;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335E14:
	// rlwinm r6,r6,7,0,24
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 7) & 0xFFFFFF80;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335E1C:
	// lbzu r5,9(r10)
	ea = 9 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335E54:
	// lbzu r5,9(r10)
	ea = 9 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335E8C:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r23,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r23.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335EC4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r20,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r20.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335EFC:
	// lbzu r5,10(r10)
	ea = 10 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335F34:
	// lbzu r5,10(r10)
	ea = 10 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335F6C:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r28.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335FA4:
	// lbzu r5,3(r10)
	ea = 3 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r27.u32);
	// blt cr6,0x82335fdc
	if (ctx.cr6.lt) goto loc_82335FDC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335FDC:
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x8233339c
	goto loc_8233339C;
loc_82335FE4:
	// rlwinm r31,r5,1,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// li r15,20
	ctx.r15.s64 = 20;
	// rlwinm r5,r31,19,24,31
	ctx.r5.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 19) & 0xFF;
	// rlwinm r14,r31,11,26,31
	ctx.r14.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 11) & 0x3F;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r14,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r14.u32);
	// clrlwi r14,r5,25
	ctx.r14.u64 = ctx.r5.u32 & 0x7F;
	// stw r15,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r15.u32);
	// cmplwi cr6,r14,0
	ctx.cr6.compare<uint32_t>(ctx.r14.u32, 0, ctx.xer);
	// bne cr6,0x82336020
	if (!ctx.cr6.eq) goto loc_82336020;
	// rlwinm r31,r31,27,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 27) & 0xFF;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// li r15,28
	ctx.r15.s64 = 28;
	// or r5,r31,r5
	ctx.r5.u64 = ctx.r31.u64 | ctx.r5.u64;
	// stw r15,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r15.u32);
loc_82336020:
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bge cr6,0x82336038
	if (!ctx.cr6.lt) goto loc_82336038;
	// li r31,1
	ctx.r31.s64 = 1;
	// neg r5,r5
	ctx.r5.s64 = -ctx.r5.s64;
	// stw r31,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r31.u32);
	// b 0x8233603c
	goto loc_8233603C;
loc_82336038:
	// stw r25,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r25.u32);
loc_8233603C:
	// stw r5,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r5.u32);
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r31,r5,2,0,29
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r14,28(r4)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// lwz r5,4(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r15,8(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// neg r15,r15
	ctx.r15.s64 = -ctx.r15.s64;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// rlwinm r15,r15,16,0,15
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 16) & 0xFFFF0000;
	// or r5,r5,r15
	ctx.r5.u64 = ctx.r5.u64 | ctx.r15.u64;
	// stwx r5,r31,r14
	PPC_STORE_U32(ctx.r31.u32 + ctx.r14.u32, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x823360a4
	if (ctx.cr6.lt) goto loc_823360A4;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233339c
	goto loc_8233339C;
loc_823360A4:
	// slw r6,r6,r5
	ctx.r6.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
	// b 0x8233339c
	goto loc_8233339C;
loc_823360AC:
	// stw r18,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r18.u32);
	// rlwinm r5,r5,11,22,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 11) & 0x3FF;
	// lwz r15,4372(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4372);
	// rlwinm r31,r5,0,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lhzx r31,r31,r15
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r31.u32 + ctx.r15.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x823361ac
	if (ctx.cr6.lt) goto loc_823361AC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x823361b0
	goto loc_823361B0;
loc_82336100:
	// stw r19,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r19.u32);
	// rlwinm r5,r5,13,20,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 13) & 0xFFF;
	// rlwinm r31,r5,0,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lwz r15,4376(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4376);
	// lhzx r31,r31,r15
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r31.u32 + ctx.r15.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x823361ac
	if (ctx.cr6.lt) goto loc_823361AC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x823361b0
	goto loc_823361B0;
loc_82336154:
	// li r31,14
	ctx.r31.s64 = 14;
	// rlwinm r5,r5,14,19,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 14) & 0x1FFF;
	// stw r31,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r31.u32);
	// rlwinm r31,r5,0,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lwz r15,4380(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4380);
	// lhzx r31,r31,r15
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r31.u32 + ctx.r15.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x823361ac
	if (ctx.cr6.lt) goto loc_823361AC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x823361b0
	goto loc_823361B0;
loc_823361AC:
	// slw r6,r6,r5
	ctx.r6.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
loc_823361B0:
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r31,r5,2,0,29
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r14,28(r4)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// lwz r15,8(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// neg r15,r15
	ctx.r15.s64 = -ctx.r15.s64;
	// lwz r5,4(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// rlwinm r15,r15,16,0,15
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// or r5,r5,r15
	ctx.r5.u64 = ctx.r5.u64 | ctx.r15.u64;
	// stwx r5,r31,r14
	PPC_STORE_U32(ctx.r31.u32 + ctx.r14.u32, ctx.r5.u32);
	// b 0x8233339c
	goto loc_8233339C;
loc_823361F4:
	// rlwinm r5,r5,9,0,22
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 9) & 0xFFFFFE00;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bge cr6,0x82336210
	if (!ctx.cr6.lt) goto loc_82336210;
	// stw r16,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r16.u32);
	// rlwinm r5,r5,6,27,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 6) & 0x1F;
	// lwz r15,4384(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4384);
	// b 0x8233623c
	goto loc_8233623C;
loc_82336210:
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bge cr6,0x82336230
	if (!ctx.cr6.lt) goto loc_82336230;
	// li r31,16
	ctx.r31.s64 = 16;
	// rlwinm r5,r5,6,27,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 6) & 0x1F;
	// stw r31,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r31.u32);
	// lwz r15,4388(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4388);
	// b 0x8233623c
	goto loc_8233623C;
loc_82336230:
	// stw r17,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r17.u32);
	// rlwinm r5,r5,7,27,31
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 7) & 0x1F;
	// lwz r15,4392(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4392);
loc_8233623C:
	// rlwinm r31,r5,0,0,30
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// lhzx r31,r31,r15
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r31.u32 + ctx.r15.u32);
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// clrlwi r15,r31,24
	ctx.r15.u64 = ctx.r31.u32 & 0xFF;
	// rlwinm r31,r31,24,24,31
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 24) & 0xFF;
	// stw r15,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r15.u32);
	// stw r31,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r31.u32);
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// add r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 + ctx.r11.u64;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x82336284
	if (ctx.cr6.lt) goto loc_82336284;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82336288
	goto loc_82336288;
loc_82336284:
	// slw r6,r6,r5
	ctx.r6.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r5.u8 & 0x3F));
loc_82336288:
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r31,r5,2,0,29
	ctx.r31.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,4(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r15,8(r4)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r14,28(r4)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// neg r15,r15
	ctx.r15.s64 = -ctx.r15.s64;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// rlwinm r15,r15,16,0,15
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 16) & 0xFFFF0000;
	// or r5,r5,r15
	ctx.r5.u64 = ctx.r5.u64 | ctx.r15.u64;
	// stwx r5,r31,r14
	PPC_STORE_U32(ctx.r31.u32 + ctx.r14.u32, ctx.r5.u32);
	// b 0x8233339c
	goto loc_8233339C;
loc_823362CC:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8233631c
	if (ctx.cr6.lt) goto loc_8233631C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_8233631C:
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x8233698c
	goto loc_8233698C;
loc_82336324:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8233631c
	if (ctx.cr6.lt) goto loc_8233631C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_82336374:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x823363ac
	if (ctx.cr6.lt) goto loc_823363AC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_823363AC:
	// rlwinm r6,r6,5,0,26
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// b 0x8233698c
	goto loc_8233698C;
loc_823363B4:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82336968
	goto loc_82336968;
loc_823363E8:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// b 0x82336954
	goto loc_82336954;
loc_82336408:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8233631c
	if (ctx.cr6.lt) goto loc_8233631C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_82336458:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8233631c
	if (ctx.cr6.lt) goto loc_8233631C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_823364A8:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,5
	ctx.r11.s64 = ctx.r11.s64 + 5;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x823363ac
	if (ctx.cr6.lt) goto loc_823363AC;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_823364E0:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82336968
	goto loc_82336968;
loc_82336514:
	// lbzu r5,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// b 0x82336954
	goto loc_82336954;
loc_82336534:
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x82336554
	if (ctx.cr6.lt) goto loc_82336554;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_82336554:
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x8233698c
	goto loc_8233698C;
loc_8233655C:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82336968
	goto loc_82336968;
loc_82336590:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r30.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// b 0x82336954
	goto loc_82336954;
loc_823365B0:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x823365e8
	if (ctx.cr6.lt) goto loc_823365E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_823365E8:
	// rlwinm r6,r6,6,0,25
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
	// b 0x8233698c
	goto loc_8233698C;
loc_823365F0:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82336968
	goto loc_82336968;
loc_82336624:
	// lbzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,28(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + ctx.r31.u32, ctx.r29.u32);
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// b 0x82336954
	goto loc_82336954;
loc_82336644:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x823365e8
	if (ctx.cr6.lt) goto loc_823365E8;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_8233667C:
	// lbz r10,3(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x823366b4
	if (ctx.cr6.lt) goto loc_823366B4;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_823366B4:
	// rlwinm r6,r6,7,0,24
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 7) & 0xFFFFFF80;
	// b 0x8233698c
	goto loc_8233698C;
loc_823366BC:
	// lbz r10,3(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x823366b4
	if (ctx.cr6.lt) goto loc_823366B4;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_823366F4:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r28.u32);
	// blt cr6,0x823366b4
	if (ctx.cr6.lt) goto loc_823366B4;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_8233672C:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r27.u32);
	// blt cr6,0x823366b4
	if (ctx.cr6.lt) goto loc_823366B4;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_82336764:
	// lbz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8233631c
	if (ctx.cr6.lt) goto loc_8233631C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_8233679C:
	// lbz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8233631c
	if (ctx.cr6.lt) goto loc_8233631C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_823367D4:
	// lbz r10,5(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// blt cr6,0x8233631c
	if (ctx.cr6.lt) goto loc_8233631C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_8233680C:
	// lbz r10,5(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
	// blt cr6,0x8233631c
	if (ctx.cr6.lt) goto loc_8233631C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_82336844:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r26,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r26.u32);
	// blt cr6,0x8233631c
	if (ctx.cr6.lt) goto loc_8233631C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_8233687C:
	// lbz r10,1(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r24,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r24.u32);
	// blt cr6,0x8233631c
	if (ctx.cr6.lt) goto loc_8233631C;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_823368B4:
	// lbz r10,6(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82336968
	goto loc_82336968;
loc_823368D0:
	// lbz r10,6(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// b 0x82336954
	goto loc_82336954;
loc_823368D8:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r28,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r28.u32);
	// b 0x82336968
	goto loc_82336968;
loc_823368F4:
	// lbz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r27,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r27.u32);
	// b 0x82336968
	goto loc_82336968;
loc_82336910:
	// lbz r10,7(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82336968
	goto loc_82336968;
loc_8233692C:
	// lbz r10,7(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// b 0x82336954
	goto loc_82336954;
loc_82336934:
	// lbz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r30,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r30.u32);
	// b 0x82336968
	goto loc_82336968;
loc_82336950:
	// lbz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
loc_82336954:
	// extsb r5,r10
	ctx.r5.s64 = ctx.r10.s8;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,28(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// stwx r29,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, ctx.r29.u32);
loc_82336968:
	// addi r11,r11,9
	ctx.r11.s64 = ctx.r11.s64 + 9;
	// cmpwi cr6,r11,32
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 32, ctx.xer);
	// blt cr6,0x82336988
	if (ctx.cr6.lt) goto loc_82336988;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// slw r6,r7,r11
	ctx.r6.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r11.u8 & 0x3F));
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x8233698c
	goto loc_8233698C;
loc_82336988:
	// rlwinm r6,r6,9,0,22
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 9) & 0xFFFFFE00;
loc_8233698C:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r5,16(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// cmpw cr6,r10,r5
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, ctx.xer);
	// beq cr6,0x823369a0
	if (ctx.cr6.eq) goto loc_823369A0;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
loc_823369A0:
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// li r21,36
	ctx.r21.s64 = 36;
	// stw r6,4352(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4352, ctx.r6.u32);
	// li r24,32
	ctx.r24.s64 = 32;
	// stw r7,4356(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4356, ctx.r7.u32);
	// li r25,48
	ctx.r25.s64 = 48;
	// stw r11,4360(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4360, ctx.r11.u32);
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// stw r9,4364(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4364, ctx.r9.u32);
	// li r26,64
	ctx.r26.s64 = 64;
	// addi r23,r10,2000
	ctx.r23.s64 = ctx.r10.s64 + 2000;
	// vspltisw v13,0
	simd::store_i32(ctx.v13.u32, simd::set1_i32(int32_t(0x0)));
	// li r27,80
	ctx.r27.s64 = 80;
	// vspltisw v0,1
	simd::store_i32(ctx.v0.u32, simd::set1_i32(int32_t(0x1)));
	// li r28,96
	ctx.r28.s64 = 96;
	// li r29,112
	ctx.r29.s64 = 112;
	// li r30,128
	ctx.r30.s64 = 128;
	// li r31,144
	ctx.r31.s64 = 144;
	// li r5,160
	ctx.r5.s64 = 160;
	// li r6,176
	ctx.r6.s64 = 176;
	// li r7,192
	ctx.r7.s64 = 192;
	// li r9,208
	ctx.r9.s64 = 208;
	// li r10,224
	ctx.r10.s64 = 224;
	// li r20,-16
	ctx.r20.s64 = -16;
	// li r3,16
	ctx.r3.s64 = 16;
	// li r11,240
	ctx.r11.s64 = 240;
	// lwz r22,32(r4)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	// lvlx128 v63,r4,r21
	temp.u32 = ctx.r4.u32 + ctx.r21.u32;
	simd::store_shuffled(ctx.v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lwz r4,28(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// lvlx v7,r23,r20
	temp.u32 = ctx.r23.u32 + ctx.r20.u32;
	simd::store_shuffled(ctx.v7,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v62,v63,0
	simd::store_i32(ctx.v62.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v63.u32), 3));
	// lvlx v6,0,r23
	temp.u32 = ctx.r0.u32 + ctx.r23.u32;
	simd::store_shuffled(ctx.v6,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v61,r24,r4
	temp.u32 = ctx.r24.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v46,v62,0
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v46.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v62.s32)));
	// lvlx128 v60,r25,r4
	temp.u32 = ctx.r25.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v59,r26,r4
	temp.u32 = ctx.r26.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v58,r27,r4
	temp.u32 = ctx.r27.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v57,r28,r4
	temp.u32 = ctx.r28.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v56,r29,r4
	temp.u32 = ctx.r29.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v55,r30,r4
	temp.u32 = ctx.r30.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v54,r31,r4
	temp.u32 = ctx.r31.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r5,r4
	temp.u32 = ctx.r5.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v52,r6,r4
	temp.u32 = ctx.r6.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v51,r7,r4
	temp.u32 = ctx.r7.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v50,r9,r4
	temp.u32 = ctx.r9.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v49,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v47,r0,r22
	temp.u32 = ctx.r0.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v45,r3,r22
	temp.u32 = ctx.r3.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v48,r11,r4
	temp.u32 = ctx.r11.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v63,r0,r4
	temp.u32 = ctx.r0.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v12,v63,v63,v7
	simd::store_i8(ctx.v12.u8, simd::permute_bytes(simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v7.u8)));
	// lvlx128 v62,r3,r4
	temp.u32 = ctx.r3.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v43,v63,v62,v6
	simd::store_i8(ctx.v43.u8, simd::permute_bytes(simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v6.u8)));
	// vupkhsb128 v41,v43,v96
	simd::store_i32(ctx.v41.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v43.s16)));
	// lvlx128 v44,r24,r22
	temp.u32 = ctx.r24.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupklsb128 v40,v43,v96
	simd::store_i32(ctx.v40.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v43.s16)));
	// vperm128 v42,v61,v60,v6
	simd::store_i8(ctx.v42.u8, simd::permute_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v6.u8)));
	// vperm128 v11,v62,v62,v7
	simd::store_i8(ctx.v11.u8, simd::permute_bytes(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v38,v41,0
	simd::store_f32_aligned(ctx.v38.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v41.s32)));
	// vperm128 v10,v61,v61,v7
	simd::store_i8(ctx.v10.u8, simd::permute_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v37,v40,0
	simd::store_f32_aligned(ctx.v37.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v40.s32)));
	// vperm128 v9,v60,v60,v7
	simd::store_i8(ctx.v9.u8, simd::permute_bytes(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v7.u8)));
	// vupkhsb128 v39,v42,v96
	simd::store_i32(ctx.v39.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v42.s16)));
	// vperm128 v8,v59,v59,v7
	simd::store_i8(ctx.v8.u8, simd::permute_bytes(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v5,v58,v58,v7
	simd::store_i8(ctx.v5.u8, simd::permute_bytes(simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v4,v57,v57,v7
	simd::store_i8(ctx.v4.u8, simd::permute_bytes(simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v3,v56,v56,v7
	simd::store_i8(ctx.v3.u8, simd::permute_bytes(simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v36,v39,0
	simd::store_f32_aligned(ctx.v36.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v39.s32)));
	// vperm128 v2,v55,v55,v7
	simd::store_i8(ctx.v2.u8, simd::permute_bytes(simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v1,v54,v54,v7
	simd::store_i8(ctx.v1.u8, simd::permute_bytes(simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v31,v53,v53,v7
	simd::store_i8(ctx.v31.u8, simd::permute_bytes(simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v30,v52,v52,v7
	simd::store_i8(ctx.v30.u8, simd::permute_bytes(simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v29,v51,v51,v7
	simd::store_i8(ctx.v29.u8, simd::permute_bytes(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v28,v50,v50,v7
	simd::store_i8(ctx.v28.u8, simd::permute_bytes(simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v7.u8)));
	// vmulfp128 v32,v38,v47
	simd::store_f32_aligned(ctx.v32.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v38.f32), simd::load_f32_aligned(ctx.v47.f32)));
	// vperm128 v27,v49,v49,v7
	simd::store_i8(ctx.v27.u8, simd::permute_bytes(simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v7.u8)));
	// vmulfp128 v63,v37,v45
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v37.f32), simd::load_f32_aligned(ctx.v45.f32)));
	// vperm128 v7,v48,v48,v7
	simd::store_i8(ctx.v7.u8, simd::permute_bytes(simd::load_i8(ctx.v48.u8), simd::load_i8(ctx.v48.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v35,v59,v58,v6
	simd::store_i8(ctx.v35.u8, simd::permute_bytes(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v6.u8)));
	// vperm128 v34,v57,v56,v6
	simd::store_i8(ctx.v34.u8, simd::permute_bytes(simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v6.u8)));
	// vperm128 v33,v55,v54,v6
	simd::store_i8(ctx.v33.u8, simd::permute_bytes(simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v6.u8)));
	// vmulfp128 v62,v36,v44
	simd::store_f32_aligned(ctx.v62.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v36.f32), simd::load_f32_aligned(ctx.v44.f32)));
	// vupkhsb128 v55,v34,v96
	simd::store_i32(ctx.v55.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v34.s16)));
	// vupklsb128 v47,v34,v96
	simd::store_i32(ctx.v47.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v34.s16)));
	// vmulfp128 v36,v32,v46
	simd::store_f32_aligned(ctx.v36.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v32.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v34,v63,v46
	simd::store_f32_aligned(ctx.v34.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vupklsb128 v61,v42,v96
	simd::store_i32(ctx.v61.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v42.s16)));
	// vupkhsb128 v59,v35,v96
	simd::store_i32(ctx.v59.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v35.s16)));
	// lvlx128 v60,r25,r22
	temp.u32 = ctx.r25.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupklsb128 v57,v35,v96
	simd::store_i32(ctx.v57.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v35.s16)));
	// vcsxwfp128 v40,v55,0
	simd::store_f32_aligned(ctx.v40.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v55.s32)));
	// vperm128 v37,v53,v52,v6
	simd::store_i8(ctx.v37.u8, simd::permute_bytes(simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v6.u8)));
	// lvlx128 v58,r26,r22
	temp.u32 = ctx.r26.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v35,v51,v50,v6
	simd::store_i8(ctx.v35.u8, simd::permute_bytes(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v6.u8)));
	// vcsxwfp128 v45,v61,0
	simd::store_f32_aligned(ctx.v45.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v61.s32)));
	// vperm128 v32,v49,v48,v6
	simd::store_i8(ctx.v32.u8, simd::permute_bytes(simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v48.u8), simd::load_i8(ctx.v6.u8)));
	// vcsxwfp128 v43,v59,0
	simd::store_f32_aligned(ctx.v43.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v59.s32)));
	// vcsxwfp128 v42,v57,0
	simd::store_f32_aligned(ctx.v42.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v57.s32)));
	// vupkhsb128 v44,v33,v96
	simd::store_i32(ctx.v44.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v33.s16)));
	// lvlx128 v56,r27,r22
	temp.u32 = ctx.r27.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v38,v47,0
	simd::store_f32_aligned(ctx.v38.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v47.s32)));
	// lvlx128 v54,r28,r22
	temp.u32 = ctx.r28.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v41,r29,r22
	temp.u32 = ctx.r29.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v63,v62,v46
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// lvlx128 v39,r30,r22
	temp.u32 = ctx.r30.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcfpsxws128 v6,v36,0
	simd::store_i32(ctx.v6.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v36.f32)));
	// lvlx128 v62,r31,r22
	temp.u32 = ctx.r31.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcfpsxws128 v26,v34,0
	simd::store_i32(ctx.v26.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v34.f32)));
	// lvlx128 v59,r5,r22
	temp.u32 = ctx.r5.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v61,v44,0
	simd::store_f32_aligned(ctx.v61.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v44.s32)));
	// lvlx128 v57,r6,r22
	temp.u32 = ctx.r6.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v47,v40,v54
	simd::store_f32_aligned(ctx.v47.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v40.f32), simd::load_f32_aligned(ctx.v54.f32)));
	// lvlx128 v55,r7,r22
	temp.u32 = ctx.r7.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r9,r22
	temp.u32 = ctx.r9.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v52,v45,v60
	simd::store_f32_aligned(ctx.v52.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v45.f32), simd::load_f32_aligned(ctx.v60.f32)));
	// lvlx128 v51,r10,r22
	temp.u32 = ctx.r10.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v50,v43,v58
	simd::store_f32_aligned(ctx.v50.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v43.f32), simd::load_f32_aligned(ctx.v58.f32)));
	// lvlx128 v49,r11,r22
	temp.u32 = ctx.r11.u32 + ctx.r22.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmulfp128 v48,v42,v56
	simd::store_f32_aligned(ctx.v48.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v42.f32), simd::load_f32_aligned(ctx.v56.f32)));
	// vupkhsb128 v60,v37,v96
	simd::store_i32(ctx.v60.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v37.s16)));
	// vmulfp128 v45,v38,v41
	simd::store_f32_aligned(ctx.v45.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v38.f32), simd::load_f32_aligned(ctx.v41.f32)));
	// vupklsb128 v58,v37,v96
	simd::store_i32(ctx.v58.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v37.s16)));
	// vupkhsb128 v56,v35,v96
	simd::store_i32(ctx.v56.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v35.s16)));
	// vcfpsxws128 v25,v63,0
	simd::store_i32(ctx.v25.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v63.f32)));
	// vsubsws v24,v6,v0
	simd::store_i32(ctx.v24.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v6), simd::to_vec128i(ctx.v0)));
	// vsubsws v23,v26,v0
	simd::store_i32(ctx.v23.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v26), simd::to_vec128i(ctx.v0)));
	// vmulfp128 v44,v61,v39
	simd::store_f32_aligned(ctx.v44.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v61.f32), simd::load_f32_aligned(ctx.v39.f32)));
	// vupklsb128 v61,v33,v96
	simd::store_i32(ctx.v61.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v33.s16)));
	// vmulfp128 v40,v47,v46
	simd::store_f32_aligned(ctx.v40.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v47.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vor v21,v24,v0
	simd::store_i8(ctx.v21.u8, simd::or_i8(simd::load_i8(ctx.v24.u8), simd::load_i8(ctx.v0.u8)));
	// vor v20,v23,v0
	simd::store_i8(ctx.v20.u8, simd::or_i8(simd::load_i8(ctx.v23.u8), simd::load_i8(ctx.v0.u8)));
	// vmulfp128 v43,v52,v46
	simd::store_f32_aligned(ctx.v43.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v52.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v42,v50,v46
	simd::store_f32_aligned(ctx.v42.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v50.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v41,v48,v46
	simd::store_f32_aligned(ctx.v41.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v48.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmaxsw v17,v13,v21
	simd::store_i32(ctx.v17.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v21.u32)));
	// vmulfp128 v39,v45,v46
	simd::store_f32_aligned(ctx.v39.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v45.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmaxsw v15,v13,v20
	simd::store_i32(ctx.v15.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v20.u32)));
	// vsubsws v22,v25,v0
	simd::store_i32(ctx.v22.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v25), simd::to_vec128i(ctx.v0)));
	// vxor v24,v17,v12
	simd::store_u8(ctx.v24.u8, simd::xor_i8(simd::load_u8(ctx.v17.u8), simd::load_u8(ctx.v12.u8)));
	// vxor v23,v15,v11
	simd::store_u8(ctx.v23.u8, simd::xor_i8(simd::load_u8(ctx.v15.u8), simd::load_u8(ctx.v11.u8)));
	// vmulfp128 v38,v44,v46
	simd::store_f32_aligned(ctx.v38.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v44.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vor v19,v22,v0
	simd::store_i8(ctx.v19.u8, simd::or_i8(simd::load_i8(ctx.v22.u8), simd::load_i8(ctx.v0.u8)));
	// vcfpsxws128 v26,v40,0
	simd::store_i32(ctx.v26.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v40.f32)));
	// vsubsws v20,v24,v12
	simd::store_i32(ctx.v20.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v24), simd::to_vec128i(ctx.v12)));
	// vcfpsxws128 v18,v43,0
	simd::store_i32(ctx.v18.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v43.f32)));
	// vcfpsxws128 v16,v42,0
	simd::store_i32(ctx.v16.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v42.f32)));
	// vmaxsw v6,v13,v19
	simd::store_i32(ctx.v6.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v19.u32)));
	// vcfpsxws128 v14,v41,0
	simd::store_i32(ctx.v14.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v41.f32)));
	// vsubsws v19,v23,v11
	simd::store_i32(ctx.v19.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v23), simd::to_vec128i(ctx.v11)));
	// vcsxwfp128 v36,v20,0
	simd::store_f32_aligned(ctx.v36.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v20.s32)));
	// vcfpsxws128 v25,v39,0
	simd::store_i32(ctx.v25.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v39.f32)));
	// vxor v22,v6,v10
	simd::store_u8(ctx.v22.u8, simd::xor_i8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v10.u8)));
	// vcsxwfp128 v34,v19,0
	simd::store_f32_aligned(ctx.v34.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v19.s32)));
	// vcfpsxws128 v21,v38,0
	simd::store_i32(ctx.v21.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v38.f32)));
	// vsubsws v17,v22,v10
	simd::store_i32(ctx.v17.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v22), simd::to_vec128i(ctx.v10)));
	// vsubsws v10,v26,v0
	simd::store_i32(ctx.v10.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v26), simd::to_vec128i(ctx.v0)));
	// vsubsws v15,v18,v0
	simd::store_i32(ctx.v15.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v18), simd::to_vec128i(ctx.v0)));
	// vsubsws v12,v16,v0
	simd::store_i32(ctx.v12.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v16), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v63,v17,0
	simd::store_f32_aligned(ctx.v63.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v17.s32)));
	// vsubsws v11,v14,v0
	simd::store_i32(ctx.v11.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v14), simd::to_vec128i(ctx.v0)));
	// stvlx128 v36,r0,r4
{
	uint32_t addr = 
ctx.r4.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v36), 15 - i));
}
	// stvlx128 v34,r4,r3
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r3.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v34), 15 - i));
}
	// vcsxwfp128 v54,v61,0
	simd::store_f32_aligned(ctx.v54.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v61.s32)));
	// vupklsb128 v52,v35,v96
	simd::store_i32(ctx.v52.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v35.s16)));
	// vcsxwfp128 v50,v60,0
	simd::store_f32_aligned(ctx.v50.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v60.s32)));
	// vupkhsb128 v48,v32,v96
	simd::store_i32(ctx.v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v32.s16)));
	// vcsxwfp128 v47,v58,0
	simd::store_f32_aligned(ctx.v47.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v58.s32)));
	// vsubsws v6,v25,v0
	simd::store_i32(ctx.v6.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v25), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v45,v56,0
	simd::store_f32_aligned(ctx.v45.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v56.s32)));
	// vor v26,v15,v0
	simd::store_i8(ctx.v26.u8, simd::or_i8(simd::load_i8(ctx.v15.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v44,v52,0
	simd::store_f32_aligned(ctx.v44.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v52.s32)));
	// vsubsws v25,v21,v0
	simd::store_i32(ctx.v25.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v21), simd::to_vec128i(ctx.v0)));
	// vcsxwfp128 v43,v48,0
	simd::store_f32_aligned(ctx.v43.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v48.s32)));
	// vor v24,v12,v0
	simd::store_i8(ctx.v24.u8, simd::or_i8(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v0.u8)));
	// vor v23,v11,v0
	simd::store_i8(ctx.v23.u8, simd::or_i8(simd::load_i8(ctx.v11.u8), simd::load_i8(ctx.v0.u8)));
	// vupklsb128 v42,v32,v96
	simd::store_i32(ctx.v42.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v32.s16)));
	// vor v22,v10,v0
	simd::store_i8(ctx.v22.u8, simd::or_i8(simd::load_i8(ctx.v10.u8), simd::load_i8(ctx.v0.u8)));
	// stvlx128 v63,r4,r24
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r24.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v63), 15 - i));
}
	// vor v21,v6,v0
	simd::store_i8(ctx.v21.u8, simd::or_i8(simd::load_i8(ctx.v6.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v20,v13,v26
	simd::store_i32(ctx.v20.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v26.u32)));
	// vor v19,v25,v0
	simd::store_i8(ctx.v19.u8, simd::or_i8(simd::load_i8(ctx.v25.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v18,v13,v24
	simd::store_i32(ctx.v18.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v24.u32)));
	// vmulfp128 v41,v54,v62
	simd::store_f32_aligned(ctx.v41.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v54.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// vmaxsw v17,v13,v23
	simd::store_i32(ctx.v17.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v23.u32)));
	// vmulfp128 v40,v50,v59
	simd::store_f32_aligned(ctx.v40.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v50.f32), simd::load_f32_aligned(ctx.v59.f32)));
	// vmaxsw v16,v13,v22
	simd::store_i32(ctx.v16.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v22.u32)));
	// vmulfp128 v39,v47,v57
	simd::store_f32_aligned(ctx.v39.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v47.f32), simd::load_f32_aligned(ctx.v57.f32)));
	// vmaxsw v15,v13,v21
	simd::store_i32(ctx.v15.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v21.u32)));
	// vmulfp128 v38,v45,v55
	simd::store_f32_aligned(ctx.v38.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v45.f32), simd::load_f32_aligned(ctx.v55.f32)));
	// vxor v14,v20,v9
	simd::store_u8(ctx.v14.u8, simd::xor_i8(simd::load_u8(ctx.v20.u8), simd::load_u8(ctx.v9.u8)));
	// vmulfp128 v37,v44,v53
	simd::store_f32_aligned(ctx.v37.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v44.f32), simd::load_f32_aligned(ctx.v53.f32)));
	// vmaxsw v12,v13,v19
	simd::store_i32(ctx.v12.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v19.u32)));
	// vmulfp128 v36,v43,v51
	simd::store_f32_aligned(ctx.v36.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v43.f32), simd::load_f32_aligned(ctx.v51.f32)));
	// vxor v11,v18,v8
	simd::store_u8(ctx.v11.u8, simd::xor_i8(simd::load_u8(ctx.v18.u8), simd::load_u8(ctx.v8.u8)));
	// vxor v10,v17,v5
	simd::store_u8(ctx.v10.u8, simd::xor_i8(simd::load_u8(ctx.v17.u8), simd::load_u8(ctx.v5.u8)));
	// vcsxwfp128 v55,v42,0
	simd::store_f32_aligned(ctx.v55.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v42.s32)));
	// vxor v6,v16,v4
	simd::store_u8(ctx.v6.u8, simd::xor_i8(simd::load_u8(ctx.v16.u8), simd::load_u8(ctx.v4.u8)));
	// vxor v26,v15,v3
	simd::store_u8(ctx.v26.u8, simd::xor_i8(simd::load_u8(ctx.v15.u8), simd::load_u8(ctx.v3.u8)));
	// vsubsws v25,v14,v9
	simd::store_i32(ctx.v25.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v14), simd::to_vec128i(ctx.v9)));
	// vxor v24,v12,v2
	simd::store_u8(ctx.v24.u8, simd::xor_i8(simd::load_u8(ctx.v12.u8), simd::load_u8(ctx.v2.u8)));
	// vsubsws v23,v11,v8
	simd::store_i32(ctx.v23.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v11), simd::to_vec128i(ctx.v8)));
	// vmulfp128 v35,v41,v46
	simd::store_f32_aligned(ctx.v35.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v41.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vsubsws v22,v10,v5
	simd::store_i32(ctx.v22.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v10), simd::to_vec128i(ctx.v5)));
	// vmulfp128 v34,v40,v46
	simd::store_f32_aligned(ctx.v34.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v40.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vsubsws v21,v6,v4
	simd::store_i32(ctx.v21.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v6), simd::to_vec128i(ctx.v4)));
	// vmulfp128 v33,v39,v46
	simd::store_f32_aligned(ctx.v33.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vsubsws v20,v26,v3
	simd::store_i32(ctx.v20.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v26), simd::to_vec128i(ctx.v3)));
	// vmulfp128 v32,v38,v46
	simd::store_f32_aligned(ctx.v32.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v38.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vsubsws v19,v24,v2
	simd::store_i32(ctx.v19.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v24), simd::to_vec128i(ctx.v2)));
	// vmulfp128 v57,v37,v46
	simd::store_f32_aligned(ctx.v57.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v37.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vmulfp128 v56,v36,v46
	simd::store_f32_aligned(ctx.v56.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v36.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vcsxwfp128 v63,v25,0
	simd::store_f32_aligned(ctx.v63.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v25.s32)));
	// vcsxwfp128 v62,v23,0
	simd::store_f32_aligned(ctx.v62.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v23.s32)));
	// vcsxwfp128 v61,v22,0
	simd::store_f32_aligned(ctx.v61.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v22.s32)));
	// vcsxwfp128 v60,v21,0
	simd::store_f32_aligned(ctx.v60.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v21.s32)));
	// vcsxwfp128 v59,v20,0
	simd::store_f32_aligned(ctx.v59.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v20.s32)));
	// vcsxwfp128 v58,v19,0
	simd::store_f32_aligned(ctx.v58.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v19.s32)));
	// vcfpsxws128 v18,v35,0
	simd::store_i32(ctx.v18.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v35.f32)));
	// vcfpsxws128 v17,v34,0
	simd::store_i32(ctx.v17.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v34.f32)));
	// vcfpsxws128 v16,v33,0
	simd::store_i32(ctx.v16.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v33.f32)));
	// vcfpsxws128 v15,v32,0
	simd::store_i32(ctx.v15.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v32.f32)));
	// vcfpsxws128 v14,v57,0
	simd::store_i32(ctx.v14.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v57.f32)));
	// vcfpsxws128 v12,v56,0
	simd::store_i32(ctx.v12.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v56.f32)));
	// stvlx128 v63,r4,r25
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r25.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v63), 15 - i));
}
	// stvlx128 v62,r4,r26
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r26.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v62), 15 - i));
}
	// stvlx128 v61,r4,r27
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r27.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v61), 15 - i));
}
	// stvlx128 v60,r4,r28
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r28.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v60), 15 - i));
}
	// stvlx128 v59,r4,r29
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r29.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v59), 15 - i));
}
	// stvlx128 v58,r4,r30
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r30.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v58), 15 - i));
}
	// vsubsws v11,v18,v0
	simd::store_i32(ctx.v11.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v18), simd::to_vec128i(ctx.v0)));
	// vsubsws v10,v17,v0
	simd::store_i32(ctx.v10.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v17), simd::to_vec128i(ctx.v0)));
	// vsubsws v9,v16,v0
	simd::store_i32(ctx.v9.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v16), simd::to_vec128i(ctx.v0)));
	// vsubsws v8,v15,v0
	simd::store_i32(ctx.v8.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v15), simd::to_vec128i(ctx.v0)));
	// vor v6,v11,v0
	simd::store_i8(ctx.v6.u8, simd::or_i8(simd::load_i8(ctx.v11.u8), simd::load_i8(ctx.v0.u8)));
	// vor v5,v10,v0
	simd::store_i8(ctx.v5.u8, simd::or_i8(simd::load_i8(ctx.v10.u8), simd::load_i8(ctx.v0.u8)));
	// vsubsws v3,v14,v0
	simd::store_i32(ctx.v3.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v14), simd::to_vec128i(ctx.v0)));
	// vsubsws v2,v12,v0
	simd::store_i32(ctx.v2.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v12), simd::to_vec128i(ctx.v0)));
	// vor v4,v9,v0
	simd::store_i8(ctx.v4.u8, simd::or_i8(simd::load_i8(ctx.v9.u8), simd::load_i8(ctx.v0.u8)));
	// vor v26,v8,v0
	simd::store_i8(ctx.v26.u8, simd::or_i8(simd::load_i8(ctx.v8.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v25,v13,v6
	simd::store_i32(ctx.v25.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v6.u32)));
	// vor v24,v3,v0
	simd::store_i8(ctx.v24.u8, simd::or_i8(simd::load_i8(ctx.v3.u8), simd::load_i8(ctx.v0.u8)));
	// vor v23,v2,v0
	simd::store_i8(ctx.v23.u8, simd::or_i8(simd::load_i8(ctx.v2.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v22,v13,v5
	simd::store_i32(ctx.v22.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v5.u32)));
	// vmulfp128 v54,v55,v49
	simd::store_f32_aligned(ctx.v54.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v55.f32), simd::load_f32_aligned(ctx.v49.f32)));
	// vxor v18,v25,v1
	simd::store_u8(ctx.v18.u8, simd::xor_i8(simd::load_u8(ctx.v25.u8), simd::load_u8(ctx.v1.u8)));
	// vmaxsw v21,v13,v4
	simd::store_i32(ctx.v21.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v4.u32)));
	// vxor v16,v22,v31
	simd::store_u8(ctx.v16.u8, simd::xor_i8(simd::load_u8(ctx.v22.u8), simd::load_u8(ctx.v31.u8)));
	// vmaxsw v20,v13,v26
	simd::store_i32(ctx.v20.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v26.u32)));
	// vsubsws v11,v18,v1
	simd::store_i32(ctx.v11.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v18), simd::to_vec128i(ctx.v1)));
	// vmaxsw v19,v13,v24
	simd::store_i32(ctx.v19.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v24.u32)));
	// vxor v15,v21,v30
	simd::store_u8(ctx.v15.u8, simd::xor_i8(simd::load_u8(ctx.v21.u8), simd::load_u8(ctx.v30.u8)));
	// vsubsws v9,v16,v31
	simd::store_i32(ctx.v9.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v16), simd::to_vec128i(ctx.v31)));
	// vmaxsw v17,v13,v23
	simd::store_i32(ctx.v17.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v23.u32)));
	// vcsxwfp128 v53,v11,0
	simd::store_f32_aligned(ctx.v53.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v11.s32)));
	// vxor v14,v20,v29
	simd::store_u8(ctx.v14.u8, simd::xor_i8(simd::load_u8(ctx.v20.u8), simd::load_u8(ctx.v29.u8)));
	// stvlx128 v53,r4,r31
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r31.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v53), 15 - i));
}
	// vxor v12,v19,v28
	simd::store_u8(ctx.v12.u8, simd::xor_i8(simd::load_u8(ctx.v19.u8), simd::load_u8(ctx.v28.u8)));
	// vsubsws v8,v15,v30
	simd::store_i32(ctx.v8.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v15), simd::to_vec128i(ctx.v30)));
	// vcsxwfp128 v52,v9,0
	simd::store_f32_aligned(ctx.v52.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v9.s32)));
	// vmulfp128 v50,v54,v46
	simd::store_f32_aligned(ctx.v50.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v54.f32), simd::load_f32_aligned(ctx.v46.f32)));
	// vxor v10,v17,v27
	simd::store_u8(ctx.v10.u8, simd::xor_i8(simd::load_u8(ctx.v17.u8), simd::load_u8(ctx.v27.u8)));
	// vsubsws v6,v14,v29
	simd::store_i32(ctx.v6.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v14), simd::to_vec128i(ctx.v29)));
	// vsubsws v5,v12,v28
	simd::store_i32(ctx.v5.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v12), simd::to_vec128i(ctx.v28)));
	// vcsxwfp128 v51,v8,0
	simd::store_f32_aligned(ctx.v51.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v8.s32)));
	// vsubsws v4,v10,v27
	simd::store_i32(ctx.v4.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v10), simd::to_vec128i(ctx.v27)));
	// vcsxwfp128 v49,v6,0
	simd::store_f32_aligned(ctx.v49.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v6.s32)));
	// vcsxwfp128 v48,v5,0
	simd::store_f32_aligned(ctx.v48.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v5.s32)));
	// vcsxwfp128 v47,v4,0
	simd::store_f32_aligned(ctx.v47.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v4.s32)));
	// stvlx128 v52,r4,r5
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r5.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v52), 15 - i));
}
	// vcfpsxws128 v3,v50,0
	simd::store_i32(ctx.v3.s32, simd::vctsxs(simd::load_f32_aligned(ctx.v50.f32)));
	// stvlx128 v51,r4,r6
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r6.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v51), 15 - i));
}
	// stvlx128 v49,r4,r7
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v49), 15 - i));
}
	// stvlx128 v48,r4,r9
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v48), 15 - i));
}
	// stvlx128 v47,r4,r10
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v47), 15 - i));
}
	// vsubsws v2,v3,v0
	simd::store_i32(ctx.v2.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v3), simd::to_vec128i(ctx.v0)));
	// vor v1,v2,v0
	simd::store_i8(ctx.v1.u8, simd::or_i8(simd::load_i8(ctx.v2.u8), simd::load_i8(ctx.v0.u8)));
	// vmaxsw v31,v13,v1
	simd::store_i32(ctx.v31.u32, simd::max_i32(simd::load_i32(ctx.v13.u32), simd::load_i32(ctx.v1.u32)));
	// vxor v30,v31,v7
	simd::store_u8(ctx.v30.u8, simd::xor_i8(simd::load_u8(ctx.v31.u8), simd::load_u8(ctx.v7.u8)));
	// vsubsws v29,v30,v7
	simd::store_i32(ctx.v29.u32, simd::sub_saturate_i32(simd::to_vec128i(ctx.v30), simd::to_vec128i(ctx.v7)));
	// vcsxwfp128 v46,v29,0
	simd::store_f32_aligned(ctx.v46.f32, simd::cvtepi32_f32(simd::load_i32(ctx.v29.s32)));
	// stvlx128 v46,r4,r11
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v46), 15 - i));
}
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82336E0C"))) PPC_WEAK_FUNC(sub_82336E0C);
PPC_FUNC_IMPL(__imp__sub_82336E0C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82336E10"))) PPC_WEAK_FUNC(sub_82336E10);
PPC_FUNC_IMPL(__imp__sub_82336E10) {
	PPC_FUNC_PROLOGUE();
	// b 0x8232b8d8
	sub_8232B8D8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82336E14"))) PPC_WEAK_FUNC(sub_82336E14);
PPC_FUNC_IMPL(__imp__sub_82336E14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82336E18"))) PPC_WEAK_FUNC(sub_82336E18);
PPC_FUNC_IMPL(__imp__sub_82336E18) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, ctx.r11.u32);
	// stw r11,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, ctx.r11.u32);
	// stw r11,60(r3)
	PPC_STORE_U32(ctx.r3.u32 + 60, ctx.r11.u32);
	// stw r11,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82336E30"))) PPC_WEAK_FUNC(sub_82336E30);
PPC_FUNC_IMPL(__imp__sub_82336E30) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r11.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82336E48"))) PPC_WEAK_FUNC(sub_82336E48);
PPC_FUNC_IMPL(__imp__sub_82336E48) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x82336E50;
	__restfpr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// li r31,1
	ctx.r31.s64 = 1;
	// addi r29,r11,2848
	ctx.r29.s64 = ctx.r11.s64 + 2848;
loc_82336E64:
	// rlwinm r11,r31,2,0,29
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r29.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82336ea4
	if (!ctx.cr6.eq) goto loc_82336EA4;
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// cmplwi cr6,r31,46
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 46, ctx.xer);
	// bge cr6,0x82336ea4
	if (!ctx.cr6.lt) goto loc_82336EA4;
	// rlwinm r11,r31,2,0,29
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r29
	ctx.r11.u64 = ctx.r11.u64 + ctx.r29.u64;
loc_82336E88:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82336ea4
	if (ctx.cr6.eq) goto loc_82336EA4;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplwi cr6,r31,46
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 46, ctx.xer);
	// blt cr6,0x82336e88
	if (ctx.cr6.lt) goto loc_82336E88;
loc_82336EA4:
	// rlwinm r11,r31,2,0,29
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// lwzx r4,r11,r29
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r29.u32);
	// bl 0x82342440
	ctx.lr = 0x82336EB8;
	sub_82342440(ctx, base);
	// cmpwi r3,0
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq 0x82336ee4
	if (ctx.cr0.eq) goto loc_82336EE4;
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// cmplwi cr6,r31,46
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 46, ctx.xer);
	// blt cr6,0x82336e64
	if (ctx.cr6.lt) goto loc_82336E64;
	// b 0x82336efc
	goto loc_82336EFC;
loc_82336ED0:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82336eec
	if (ctx.cr6.eq) goto loc_82336EEC;
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
loc_82336EE4:
	// cmplwi cr6,r31,46
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 46, ctx.xer);
	// blt cr6,0x82336ed0
	if (ctx.cr6.lt) goto loc_82336ED0;
loc_82336EEC:
	// rlwinm r11,r31,2,0,29
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r11,r29
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r29.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x82336f04
	if (!ctx.cr6.eq) goto loc_82336F04;
loc_82336EFC:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r3,r11,2064
	ctx.r3.s64 = ctx.r11.s64 + 2064;
loc_82336F04:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82336F0C"))) PPC_WEAK_FUNC(sub_82336F0C);
PPC_FUNC_IMPL(__imp__sub_82336F0C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82336F10"))) PPC_WEAK_FUNC(sub_82336F10);
PPC_FUNC_IMPL(__imp__sub_82336F10) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32183
	ctx.r10.s64 = -2109145088;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// addi r31,r10,22304
	ctx.r31.s64 = ctx.r10.s64 + 22304;
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r5,r11
	ctx.r5.u64 = ctx.r11.u64;
	// li r4,512
	ctx.r4.s64 = 512;
	// bl 0x8233b7a8
	ctx.lr = 0x82336F44;
	sub_8233B7A8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82336F5C"))) PPC_WEAK_FUNC(sub_82336F5C);
PPC_FUNC_IMPL(__imp__sub_82336F5C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82336F60"))) PPC_WEAK_FUNC(sub_82336F60);
PPC_FUNC_IMPL(__imp__sub_82336F60) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x8233b7f0
	ctx.lr = 0x82336F78;
	sub_8233B7F0(ctx, base);
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// addi r9,r11,23076
	ctx.r9.s64 = ctx.r11.s64 + 23076;
	// addi r10,r10,3344
	ctx.r10.s64 = ctx.r10.s64 + 3344;
	// stw r31,23076(r11)
	PPC_STORE_U32(ctx.r11.u32 + 23076, ctx.r31.u32);
	// stw r10,-4(r9)
	PPC_STORE_U32(ctx.r9.u32 + -4, ctx.r10.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82336FA4"))) PPC_WEAK_FUNC(sub_82336FA4);
PPC_FUNC_IMPL(__imp__sub_82336FA4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82336FA8"))) PPC_WEAK_FUNC(sub_82336FA8);
PPC_FUNC_IMPL(__imp__sub_82336FA8) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// stw r3,23080(r11)
	PPC_STORE_U32(ctx.r11.u32 + 23080, ctx.r3.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82336FB4"))) PPC_WEAK_FUNC(sub_82336FB4);
PPC_FUNC_IMPL(__imp__sub_82336FB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82336FB8"))) PPC_WEAK_FUNC(sub_82336FB8);
PPC_FUNC_IMPL(__imp__sub_82336FB8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x82336FC0;
	__restfpr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// bl 0x8233b7f0
	ctx.lr = 0x82336FDC;
	sub_8233B7F0(ctx, base);
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// addi r11,r11,23076
	ctx.r11.s64 = ctx.r11.s64 + 23076;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x82337008
	if (!ctx.cr6.eq) goto loc_82337008;
	// cmpwi cr6,r31,1
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 1, ctx.xer);
	// bne cr6,0x82337008
	if (!ctx.cr6.eq) goto loc_82337008;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r10.u32);
	// b 0x8233703c
	goto loc_8233703C;
loc_82337008:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// stw r10,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
	// beq cr6,0x8233703c
	if (ctx.cr6.eq) goto loc_8233703C;
	// rotlwi r11,r9,0
	ctx.r11.u64 = rotl32(ctx.r9.u32, 0);
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x8233703C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_8233703C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82337044"))) PPC_WEAK_FUNC(sub_82337044);
PPC_FUNC_IMPL(__imp__sub_82337044) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82337048"))) PPC_WEAK_FUNC(sub_82337048);
PPC_FUNC_IMPL(__imp__sub_82337048) {
	PPC_FUNC_PROLOGUE();
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// b 0x82336fb8
	sub_82336FB8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82337058"))) PPC_WEAK_FUNC(sub_82337058);
PPC_FUNC_IMPL(__imp__sub_82337058) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x82337060;
	__restfpr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// addi r11,r11,3432
	ctx.r11.s64 = ctx.r11.s64 + 3432;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x823370a8
	if (ctx.cr6.eq) goto loc_823370A8;
	// li r10,0
	ctx.r10.s64 = 0;
loc_82337084:
	// lwzx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// cmpw cr6,r8,r5
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, ctx.xer);
	// beq cr6,0x8233710c
	if (ctx.cr6.eq) goto loc_8233710C;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r8,r11,4
	ctx.r8.s64 = ctx.r11.s64 + 4;
	// rlwinm r10,r9,3,0,28
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r8,r10,r8
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x82337084
	if (!ctx.cr6.eq) goto loc_82337084;
loc_823370A8:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r31,r11,3256
	ctx.r31.s64 = ctx.r11.s64 + 3256;
loc_823370B0:
	// lis r11,-32183
	ctx.r11.s64 = -2109145088;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// addi r29,r11,22816
	ctx.r29.s64 = ctx.r11.s64 + 22816;
	// li r4,256
	ctx.r4.s64 = 256;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233b708
	ctx.lr = 0x823370C8;
	sub_8233B708(ctx, base);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r5,r11,3496
	ctx.r5.s64 = ctx.r11.s64 + 3496;
	// li r4,256
	ctx.r4.s64 = 256;
	// bl 0x8233b750
	ctx.lr = 0x823370DC;
	sub_8233B750(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// li r4,256
	ctx.r4.s64 = 256;
	// bl 0x8233b750
	ctx.lr = 0x823370EC;
	sub_8233B750(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82336fb8
	ctx.lr = 0x82337104;
	sub_82336FB8(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_8233710C:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lwzx r31,r10,r11
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// b 0x823370b0
	goto loc_823370B0;
}

__attribute__((alias("__imp__sub_82337118"))) PPC_WEAK_FUNC(sub_82337118);
PPC_FUNC_IMPL(__imp__sub_82337118) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x82337120;
	__restfpr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r4,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r4.u32);
	// addi r11,r3,24
	ctx.r11.s64 = ctx.r3.s64 + 24;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// addi r10,r11,7
	ctx.r10.s64 = ctx.r11.s64 + 7;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// rlwinm r31,r10,0,0,28
	ctx.r31.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF8;
	// li r5,24
	ctx.r5.s64 = 24;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// subf r29,r11,r31
	ctx.r29.s64 = ctx.r31.s64 - ctx.r11.s64;
	// bl 0x8233eaf0
	ctx.lr = 0x82337150;
	sub_8233EAF0(ctx, base);
	// addi r11,r31,-4
	ctx.r11.s64 = ctx.r31.s64 + -4;
	// clrlwi. r11,r11,29
	ctx.r11.u64 = ctx.r11.u32 & 0x7;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82337160
	if (ctx.cr0.eq) goto loc_82337160;
	// subfic r11,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r11.s64 = 8 - ctx.r11.s64;
loc_82337160:
	// clrlwi r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r8,r11,28
	ctx.r8.s64 = ctx.r11.s64 + 28;
	// sth r11,14(r31)
	PPC_STORE_U16(ctx.r31.u32 + 14, ctx.r11.u16);
	// add r9,r11,r31
	ctx.r9.u64 = ctx.r11.u64 + ctx.r31.u64;
	// stw r10,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r10.u32);
	// clrlwi r8,r8,16
	ctx.r8.u64 = ctx.r8.u32 & 0xFFFF;
	// stb r10,13(r31)
	PPC_STORE_U8(ctx.r31.u32 + 13, ctx.r10.u8);
	// sth r10,16(r31)
	PPC_STORE_U16(ctx.r31.u32 + 16, ctx.r10.u16);
	// addi r9,r9,31
	ctx.r9.s64 = ctx.r9.s64 + 31;
	// subf r10,r8,r28
	ctx.r10.s64 = ctx.r28.s64 - ctx.r8.s64;
	// rlwinm r8,r9,0,0,28
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF8;
	// subf r9,r29,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r29.s64;
	// addi r10,r11,28
	ctx.r10.s64 = ctx.r11.s64 + 28;
	// addi r9,r9,-24
	ctx.r9.s64 = ctx.r9.s64 + -24;
	// addi r11,r29,24
	ctx.r11.s64 = ctx.r29.s64 + 24;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stw r10,-4(r8)
	PPC_STORE_U32(ctx.r8.u32 + -4, ctx.r10.u32);
	// stw r31,16(r30)
	PPC_STORE_U32(ctx.r30.u32 + 16, ctx.r31.u32);
	// stw r31,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r31.u32);
	// stw r11,12(r30)
	PPC_STORE_U32(ctx.r30.u32 + 12, ctx.r11.u32);
	// stw r11,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823371C4"))) PPC_WEAK_FUNC(sub_823371C4);
PPC_FUNC_IMPL(__imp__sub_823371C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823371C8"))) PPC_WEAK_FUNC(sub_823371C8);
PPC_FUNC_IMPL(__imp__sub_823371C8) {
	PPC_FUNC_PROLOGUE();
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lwz r3,20(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
loc_823371D0:
	// lbz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 12);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne 0x82337288
	if (!ctx.cr0.eq) goto loc_82337288;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r10,r5,16
	ctx.r10.u64 = ctx.r5.u32 & 0xFFFF;
	// lhz r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 16);
	// addi r6,r3,28
	ctx.r6.s64 = ctx.r3.s64 + 28;
	// lhz r31,14(r3)
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// cmpwi cr6,r10,8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 8, ctx.xer);
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// add r7,r11,r31
	ctx.r7.u64 = ctx.r11.u64 + ctx.r31.u64;
	// blt cr6,0x82337208
	if (ctx.cr6.lt) goto loc_82337208;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
loc_82337208:
	// divwu r11,r6,r9
	ctx.r11.u32 = ctx.r6.u32 / ctx.r9.u32;
	// twllei r9,0
	if (ctx.r9.u32 <= 0) __builtin_debugtrap();
	// mullw r11,r11,r9
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r9.s32);
	// subf. r11,r11,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82337220
	if (ctx.cr0.eq) goto loc_82337220;
	// subf r11,r11,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r11.s64;
loc_82337220:
	// clrlwi r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// cmpw cr6,r11,r7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r7.s32, ctx.xer);
	// bgt cr6,0x82337244
	if (ctx.cr6.gt) goto loc_82337244;
	// lbz r11,13(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq 0x82337294
	if (ctx.cr0.eq) goto loc_82337294;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// beq cr6,0x82337294
	if (ctx.cr6.eq) goto loc_82337294;
loc_82337244:
	// cmpw cr6,r8,r4
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r4.s32, ctx.xer);
	// bne cr6,0x82337288
	if (!ctx.cr6.eq) goto loc_82337288;
	// cmpwi cr6,r10,8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 8, ctx.xer);
	// bge cr6,0x82337258
	if (!ctx.cr6.lt) goto loc_82337258;
	// li r10,8
	ctx.r10.s64 = 8;
loc_82337258:
	// divwu r11,r6,r10
	ctx.r11.u32 = ctx.r6.u32 / ctx.r10.u32;
	// twllei r10,0
	if (ctx.r10.u32 <= 0) __builtin_debugtrap();
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// subf. r11,r11,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82337270
	if (ctx.cr0.eq) goto loc_82337270;
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
loc_82337270:
	// clrlwi r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	// cmplw cr6,r31,r11
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x82337288
	if (!ctx.cr6.eq) goto loc_82337288;
	// lbz r11,13(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// beq cr6,0x82337294
	if (ctx.cr6.eq) goto loc_82337294;
loc_82337288:
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x823371d0
	if (!ctx.cr6.eq) goto loc_823371D0;
loc_82337294:
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8233729C"))) PPC_WEAK_FUNC(sub_8233729C);
PPC_FUNC_IMPL(__imp__sub_8233729C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823372A0"))) PPC_WEAK_FUNC(sub_823372A0);
PPC_FUNC_IMPL(__imp__sub_823372A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x82337304
	if (ctx.cr6.eq) goto loc_82337304;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x823372f4
	if (ctx.cr6.eq) goto loc_823372F4;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// beq cr6,0x823372e4
	if (ctx.cr6.eq) goto loc_823372E4;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r4,r11,3772
	ctx.r4.s64 = ctx.r11.s64 + 3772;
	// b 0x823372ec
	goto loc_823372EC;
loc_823372E4:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r4,r11,3712
	ctx.r4.s64 = ctx.r11.s64 + 3712;
loc_823372EC:
	// bl 0x82337048
	ctx.lr = 0x823372F0;
	sub_82337048(ctx, base);
	// b 0x82337374
	goto loc_82337374;
loc_823372F4:
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// bl 0x823371c8
	ctx.lr = 0x823372FC;
	sub_823371C8(ctx, base);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// b 0x82337374
	goto loc_82337374;
loc_82337304:
	// lwz r31,16(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
loc_82337308:
	// lbz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 12);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne 0x82337368
	if (!ctx.cr0.eq) goto loc_82337368;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// bne cr6,0x8233735c
	if (!ctx.cr6.eq) goto loc_8233735C;
	// clrlwi r10,r6,16
	ctx.r10.u64 = ctx.r6.u32 & 0xFFFF;
	// addi r11,r31,28
	ctx.r11.s64 = ctx.r31.s64 + 28;
	// cmpwi cr6,r10,8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 8, ctx.xer);
	// bge cr6,0x82337334
	if (!ctx.cr6.lt) goto loc_82337334;
	// li r10,8
	ctx.r10.s64 = 8;
loc_82337334:
	// divwu r9,r11,r10
	ctx.r9.u32 = ctx.r11.u32 / ctx.r10.u32;
	// twllei r10,0
	if (ctx.r10.u32 <= 0) __builtin_debugtrap();
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// subf. r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x8233734c
	if (ctx.cr0.eq) goto loc_8233734C;
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
loc_8233734C:
	// lhz r10,14(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 14);
	// clrlwi r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82337374
	if (ctx.cr6.eq) goto loc_82337374;
loc_8233735C:
	// lbz r11,13(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 13);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq 0x82337374
	if (ctx.cr0.eq) goto loc_82337374;
loc_82337368:
	// lwz r31,4(r31)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x82337308
	if (!ctx.cr6.eq) goto loc_82337308;
loc_82337374:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8233738C"))) PPC_WEAK_FUNC(sub_8233738C);
PPC_FUNC_IMPL(__imp__sub_8233738C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82337390"))) PPC_WEAK_FUNC(sub_82337390);
PPC_FUNC_IMPL(__imp__sub_82337390) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e454
	ctx.lr = 0x82337398;
	__restfpr_23(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lhz r11,16(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 16);
	// mr r23,r3
	ctx.r23.u64 = ctx.r3.u64;
	// lhz r9,14(r4)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r4.u32 + 14);
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// mr r24,r6
	ctx.r24.u64 = ctx.r6.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r8,r4,28
	ctx.r8.s64 = ctx.r4.s64 + 28;
	// cmpwi cr6,r7,8
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 8, ctx.xer);
	// addi r25,r11,28
	ctx.r25.s64 = ctx.r11.s64 + 28;
	// bge cr6,0x823373d4
	if (!ctx.cr6.lt) goto loc_823373D4;
	// li r7,8
	ctx.r7.s64 = 8;
loc_823373D4:
	// divwu r11,r8,r7
	ctx.r11.u32 = ctx.r8.u32 / ctx.r7.u32;
	// twllei r7,0
	if (ctx.r7.u32 <= 0) __builtin_debugtrap();
	// mullw r11,r11,r7
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r7.s32);
	// subf. r11,r11,r8
	ctx.r11.s64 = ctx.r8.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x823373ec
	if (ctx.cr0.eq) goto loc_823373EC;
	// subf r11,r11,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r11.s64;
loc_823373EC:
	// clrlwi r28,r11,16
	ctx.r28.u64 = ctx.r11.u32 & 0xFFFF;
	// li r10,4
	ctx.r10.s64 = 4;
	// add r11,r28,r26
	ctx.r11.u64 = ctx.r28.u64 + ctx.r26.u64;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// addi r11,r11,28
	ctx.r11.s64 = ctx.r11.s64 + 28;
	// beq cr6,0x82337408
	if (ctx.cr6.eq) goto loc_82337408;
	// subfic r10,r10,8
	ctx.xer.ca = ctx.r10.u32 <= 8;
	ctx.r10.s64 = 8 - ctx.r10.s64;
loc_82337408:
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r10,r10,28
	ctx.r10.s64 = ctx.r10.s64 + 28;
	// cmpw cr6,r25,r10
	ctx.cr6.compare<int32_t>(ctx.r25.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x82337420
	if (!ctx.cr6.lt) goto loc_82337420;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x823374cc
	goto loc_823374CC;
loc_82337420:
	// add r10,r11,r31
	ctx.r10.u64 = ctx.r11.u64 + ctx.r31.u64;
	// li r5,24
	ctx.r5.s64 = 24;
	// addi r9,r10,7
	ctx.r9.s64 = ctx.r10.s64 + 7;
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r30,r9,0,0,28
	ctx.r30.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF8;
	// subf r10,r10,r30
	ctx.r10.s64 = ctx.r30.s64 - ctx.r10.s64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// clrlwi r29,r10,16
	ctx.r29.u64 = ctx.r10.u32 & 0xFFFF;
	// add r27,r29,r11
	ctx.r27.u64 = ctx.r29.u64 + ctx.r11.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x82337448;
	sub_8233EAF0(ctx, base);
	// addi r11,r30,-4
	ctx.r11.s64 = ctx.r30.s64 + -4;
	// clrlwi. r11,r11,29
	ctx.r11.u64 = ctx.r11.u32 & 0x7;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82337458
	if (ctx.cr0.eq) goto loc_82337458;
	// subfic r11,r11,8
	ctx.xer.ca = ctx.r11.u32 <= 8;
	ctx.r11.s64 = 8 - ctx.r11.s64;
loc_82337458:
	// clrlwi r10,r11,16
	ctx.r10.u64 = ctx.r11.u32 & 0xFFFF;
	// stw r31,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r31.u32);
	// add r11,r28,r31
	ctx.r11.u64 = ctx.r28.u64 + ctx.r31.u64;
	// subf r9,r10,r25
	ctx.r9.s64 = ctx.r25.s64 - ctx.r10.s64;
	// sth r10,14(r30)
	PPC_STORE_U16(ctx.r30.u32 + 14, ctx.r10.u16);
	// addi r10,r11,31
	ctx.r10.s64 = ctx.r11.s64 + 31;
	// subf r11,r27,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r27.s64;
	// rlwinm r9,r10,0,0,28
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF8;
	// addi r11,r11,-28
	ctx.r11.s64 = ctx.r11.s64 + -28;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r11.u32);
	// addi r11,r28,28
	ctx.r11.s64 = ctx.r28.s64 + 28;
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stw r8,4(r30)
	PPC_STORE_U32(ctx.r30.u32 + 4, ctx.r8.u32);
	// sth r28,14(r31)
	PPC_STORE_U16(ctx.r31.u32 + 14, ctx.r28.u16);
	// stw r30,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r30.u32);
	// stw r26,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r26.u32);
	// stb r10,13(r31)
	PPC_STORE_U8(ctx.r31.u32 + 13, ctx.r10.u8);
	// stw r24,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r24.u32);
	// sth r29,16(r31)
	PPC_STORE_U16(ctx.r31.u32 + 16, ctx.r29.u16);
	// stb r10,12(r31)
	PPC_STORE_U8(ctx.r31.u32 + 12, ctx.r10.u8);
	// stw r11,-4(r9)
	PPC_STORE_U32(ctx.r9.u32 + -4, ctx.r11.u32);
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x823374c4
	if (!ctx.cr6.eq) goto loc_823374C4;
	// stw r30,20(r23)
	PPC_STORE_U32(ctx.r23.u32 + 20, ctx.r30.u32);
	// b 0x823374c8
	goto loc_823374C8;
loc_823374C4:
	// stw r30,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r30.u32);
loc_823374C8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
loc_823374CC:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8233e4a4
	__restgprlr_23(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823374D4"))) PPC_WEAK_FUNC(sub_823374D4);
PPC_FUNC_IMPL(__imp__sub_823374D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823374D8"))) PPC_WEAK_FUNC(sub_823374D8);
PPC_FUNC_IMPL(__imp__sub_823374D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// mr r11,r7
	ctx.r11.u64 = ctx.r7.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x823374f4
	if (!ctx.cr6.eq) goto loc_823374F4;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
loc_823374F4:
	// subf r8,r5,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r5.s64;
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// bgt cr6,0x82337504
	if (ctx.cr6.gt) goto loc_82337504;
	// li r11,8
	ctx.r11.s64 = 8;
loc_82337504:
	// srawi r10,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// subf. r10,r10,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r10.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x82337528
	if (ctx.cr0.eq) goto loc_82337528;
	// addi r11,r11,-8
	ctx.r11.s64 = ctx.r11.s64 + -8;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
loc_82337528:
	// divwu r10,r8,r11
	ctx.r10.u32 = ctx.r8.u32 / ctx.r11.u32;
	// twllei r11,0
	if (ctx.r11.u32 <= 0) __builtin_debugtrap();
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// lhz r11,14(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 14);
	// add r9,r11,r4
	ctx.r9.u64 = ctx.r11.u64 + ctx.r4.u64;
	// addi r11,r10,-28
	ctx.r11.s64 = ctx.r10.s64 + -28;
	// addi r7,r9,28
	ctx.r7.s64 = ctx.r9.s64 + 28;
	// subf r9,r10,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r10.s64;
	// rlwinm r11,r11,0,0,28
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF8;
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// cmplw cr6,r11,r7
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r7.u32, ctx.xer);
	// bge cr6,0x823375b8
	if (!ctx.cr6.lt) goto loc_823375B8;
	// lbz r11,13(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 13);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne 0x8233756c
	if (!ctx.cr0.eq) goto loc_8233756C;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82337664
	goto loc_82337664;
loc_8233756C:
	// subf r11,r4,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r4.s64;
	// sth r9,16(r4)
	PPC_STORE_U16(ctx.r4.u32 + 16, ctx.r9.u16);
	// li r10,2
	ctx.r10.s64 = 2;
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// addis r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 65536;
	// stw r6,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, ctx.r6.u32);
	// stb r10,13(r4)
	PPC_STORE_U8(ctx.r4.u32 + 13, ctx.r10.u8);
	// li r9,1
	ctx.r9.s64 = 1;
	// addi r11,r11,-28
	ctx.r11.s64 = ctx.r11.s64 + -28;
	// stb r9,12(r4)
	PPC_STORE_U8(ctx.r4.u32 + 12, ctx.r9.u8);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// clrlwi r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	// add r10,r11,r4
	ctx.r10.u64 = ctx.r11.u64 + ctx.r4.u64;
	// sth r11,14(r4)
	PPC_STORE_U16(ctx.r4.u32 + 14, ctx.r11.u16);
	// addi r11,r11,28
	ctx.r11.s64 = ctx.r11.s64 + 28;
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// rlwinm r10,r10,0,0,28
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF8;
	// stw r11,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r11.u32);
	// b 0x82337664
	goto loc_82337664;
loc_823375B8:
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	// stw r5,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r5.u32);
	// li r8,2
	ctx.r8.s64 = 2;
	// addis r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 65536;
	// stb r8,13(r11)
	PPC_STORE_U8(ctx.r11.u32 + 13, ctx.r8.u8);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r10,r10,-28
	ctx.r10.s64 = ctx.r10.s64 + -28;
	// addi r7,r4,-4
	ctx.r7.s64 = ctx.r4.s64 + -4;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
	// sth r10,14(r11)
	PPC_STORE_U16(ctx.r11.u32 + 14, ctx.r10.u16);
	// addi r31,r10,28
	ctx.r31.s64 = ctx.r10.s64 + 28;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// addi r8,r8,31
	ctx.r8.s64 = ctx.r8.s64 + 31;
	// stw r10,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
	// rlwinm r8,r8,0,0,28
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFF8;
	// stw r4,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r4.u32);
	// sth r9,16(r11)
	PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r9.u16);
	// clrlwi. r10,r7,29
	ctx.r10.u64 = ctx.r7.u32 & 0x7;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r6,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r6.u32);
	// stb r5,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r5.u8);
	// stw r31,-4(r8)
	PPC_STORE_U32(ctx.r8.u32 + -4, ctx.r31.u32);
	// beq 0x82337618
	if (ctx.cr0.eq) goto loc_82337618;
	// subfic r10,r10,8
	ctx.xer.ca = ctx.r10.u32 <= 8;
	ctx.r10.s64 = 8 - ctx.r10.s64;
loc_82337618:
	// lwz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// sth r10,14(r4)
	PPC_STORE_U16(ctx.r4.u32 + 14, ctx.r10.u16);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8233762c
	if (ctx.cr6.eq) goto loc_8233762C;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r11.u32);
loc_8233762C:
	// lhz r10,14(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 14);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r11.u32);
	// addi r10,r10,28
	ctx.r10.s64 = ctx.r10.s64 + 28;
	// sth r9,16(r4)
	PPC_STORE_U16(ctx.r4.u32 + 16, ctx.r9.u16);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// subf r10,r10,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r10.s64;
	// subf r10,r4,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r4.s64;
	// stw r10,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82337660
	if (!ctx.cr6.eq) goto loc_82337660;
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r11.u32);
loc_82337660:
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
loc_82337664:
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8233766C"))) PPC_WEAK_FUNC(sub_8233766C);
PPC_FUNC_IMPL(__imp__sub_8233766C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82337670"))) PPC_WEAK_FUNC(sub_82337670);
PPC_FUNC_IMPL(__imp__sub_82337670) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x82337678;
	__restfpr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// mr r27,r6
	ctx.r27.u64 = ctx.r6.u64;
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bge cr6,0x823376a0
	if (!ctx.cr6.lt) goto loc_823376A0;
loc_82337698:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x823377c4
	goto loc_823377C4;
loc_823376A0:
	// clrlwi r6,r30,16
	ctx.r6.u64 = ctx.r30.u32 & 0xFFFF;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x823372a0
	ctx.lr = 0x823376B4;
	sub_823372A0(ctx, base);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq 0x82337698
	if (ctx.cr0.eq) goto loc_82337698;
	// addi r10,r3,28
	ctx.r10.s64 = ctx.r3.s64 + 28;
	// cmpwi cr6,r30,8
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 8, ctx.xer);
	// li r11,8
	ctx.r11.s64 = 8;
	// blt cr6,0x823376d0
	if (ctx.cr6.lt) goto loc_823376D0;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_823376D0:
	// divwu r9,r10,r11
	ctx.r9.u32 = ctx.r10.u32 / ctx.r11.u32;
	// twllei r11,0
	if (ctx.r11.u32 <= 0) __builtin_debugtrap();
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// subf. r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x823376e8
	if (ctx.cr0.eq) goto loc_823376E8;
	// subf r10,r10,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r10.s64;
loc_823376E8:
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x82337738
	if (!ctx.cr6.eq) goto loc_82337738;
	// lhz r11,14(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x82337738
	if (!ctx.cr6.eq) goto loc_82337738;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// cmplwi r10,0
	ctx.cr0.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq 0x82337698
	if (ctx.cr0.eq) goto loc_82337698;
	// add r10,r11,r3
	ctx.r10.u64 = ctx.r11.u64 + ctx.r3.u64;
	// stw r27,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r27.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// stb r28,13(r3)
	PPC_STORE_U8(ctx.r3.u32 + 13, ctx.r28.u8);
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// addi r11,r11,28
	ctx.r11.s64 = ctx.r11.s64 + 28;
	// stb r8,12(r3)
	PPC_STORE_U8(ctx.r3.u32 + 12, ctx.r8.u8);
	// rlwinm r10,r10,0,0,28
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF8;
	// stw r11,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r11.u32);
	// b 0x8233777c
	goto loc_8233777C;
loc_82337738:
	// cmpwi cr6,r28,1
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 1, ctx.xer);
	// beq cr6,0x82337764
	if (ctx.cr6.eq) goto loc_82337764;
	// cmpwi cr6,r28,2
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 2, ctx.xer);
	// bne cr6,0x82337698
	if (!ctx.cr6.eq) goto loc_82337698;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r7,r30
	ctx.r7.u64 = ctx.r30.u64;
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x823374d8
	ctx.lr = 0x82337760;
	sub_823374D8(ctx, base);
	// b 0x8233777c
	goto loc_8233777C;
loc_82337764:
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r7,r30
	ctx.r7.u64 = ctx.r30.u64;
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82337390
	ctx.lr = 0x8233777C;
	sub_82337390(ctx, base);
loc_8233777C:
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82337698
	if (ctx.cr6.eq) goto loc_82337698;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lhz r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 16);
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lhz r9,14(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r11,r11,28
	ctx.r11.s64 = ctx.r11.s64 + 28;
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
	// cmpw cr6,r8,r11
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x823377b8
	if (!ctx.cr6.lt) goto loc_823377B8;
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
loc_823377B8:
	// lhz r11,14(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// addi r3,r11,28
	ctx.r3.s64 = ctx.r11.s64 + 28;
loc_823377C4:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823377CC"))) PPC_WEAK_FUNC(sub_823377CC);
PPC_FUNC_IMPL(__imp__sub_823377CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823377D0"))) PPC_WEAK_FUNC(sub_823377D0);
PPC_FUNC_IMPL(__imp__sub_823377D0) {
	PPC_FUNC_PROLOGUE();
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// b 0x82337670
	sub_82337670(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823377E4"))) PPC_WEAK_FUNC(sub_823377E4);
PPC_FUNC_IMPL(__imp__sub_823377E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_823377E8"))) PPC_WEAK_FUNC(sub_823377E8);
PPC_FUNC_IMPL(__imp__sub_823377E8) {
	PPC_FUNC_PROLOGUE();
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// li r4,2
	ctx.r4.s64 = 2;
	// b 0x82337670
	sub_82337670(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823377FC"))) PPC_WEAK_FUNC(sub_823377FC);
PPC_FUNC_IMPL(__imp__sub_823377FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

