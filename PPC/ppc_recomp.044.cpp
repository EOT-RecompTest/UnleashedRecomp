#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_822C6748"))) PPC_WEAK_FUNC(sub_822C6748);
PPC_FUNC_IMPL(__imp__sub_822C6748) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822c6784
	if (ctx.cr6.eq) goto loc_822C6784;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x8234f138
	ctx.lr = 0x822C6774;
	sub_8234F138(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c6550
	ctx.lr = 0x822C677C;
	sub_822C6550(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822e8ab0
	ctx.lr = 0x822C6784;
	sub_822E8AB0(ctx, base);
loc_822C6784:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822C6798"))) PPC_WEAK_FUNC(sub_822C6798);
PPC_FUNC_IMPL(__imp__sub_822C6798) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x822c6550
	ctx.lr = 0x822C67B4;
	sub_822C6550(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c6808
	if (ctx.cr6.lt) goto loc_822C6808;
	// li r5,720
	ctx.r5.s64 = 720;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822C67D0;
	sub_8233EAF0(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,2
	ctx.r9.s64 = 2;
	// stw r11,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r11.u32);
	// stw r10,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r10.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r11.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(ctx.r31.u32 + 36, ctx.r11.u32);
	// stw r11,40(r31)
	PPC_STORE_U32(ctx.r31.u32 + 40, ctx.r11.u32);
	// stw r9,52(r31)
	PPC_STORE_U32(ctx.r31.u32 + 52, ctx.r9.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(ctx.r31.u32 + 56, ctx.r11.u32);
	// stw r10,216(r31)
	PPC_STORE_U32(ctx.r31.u32 + 216, ctx.r10.u32);
	// stw r9,164(r31)
	PPC_STORE_U32(ctx.r31.u32 + 164, ctx.r9.u32);
	// stw r11,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r11.u32);
	// stw r10,696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 696, ctx.r10.u32);
loc_822C6808:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822C6824"))) PPC_WEAK_FUNC(sub_822C6824);
PPC_FUNC_IMPL(__imp__sub_822C6824) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C6828"))) PPC_WEAK_FUNC(sub_822C6828);
PPC_FUNC_IMPL(__imp__sub_822C6828) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822c684c
	if (ctx.cr6.eq) goto loc_822C684C;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// blt cr6,0x822c6890
	if (ctx.cr6.lt) goto loc_822C6890;
loc_822C684C:
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8234f138
	ctx.lr = 0x822C6858;
	sub_8234F138(ctx, base);
	// li r3,720
	ctx.r3.s64 = 720;
	// bl 0x822e8aa0
	ctx.lr = 0x822C6860;
	sub_822E8AA0(ctx, base);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822c6890
	if (ctx.cr6.eq) goto loc_822C6890;
	// li r5,720
	ctx.r5.s64 = 720;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8233eaf0
	ctx.lr = 0x822C6878;
	sub_8233EAF0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c6798
	ctx.lr = 0x822C6880;
	sub_822C6798(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bge cr6,0x822c6894
	if (!ctx.cr6.lt) goto loc_822C6894;
	// bl 0x822e8ab0
	ctx.lr = 0x822C6890;
	sub_822E8AB0(ctx, base);
loc_822C6890:
	// li r3,0
	ctx.r3.s64 = 0;
loc_822C6894:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822C68A8"))) PPC_WEAK_FUNC(sub_822C68A8);
PPC_FUNC_IMPL(__imp__sub_822C68A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822C68B0;
	__restfpr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r30,0(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// li r27,0
	ctx.r27.s64 = 0;
	// lhz r11,34(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 34);
	// rotlwi r3,r11,3
	ctx.r3.u64 = rotl32(ctx.r11.u32, 3);
	// bl 0x822e8aa0
	ctx.lr = 0x822C68D0;
	sub_822E8AA0(ctx, base);
	// stw r3,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822c69bc
	if (ctx.cr6.eq) goto loc_822C69BC;
	// lhz r11,34(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 34);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c69b0
	if (ctx.cr6.eq) goto loc_822C69B0;
	// li r29,0
	ctx.r29.s64 = 0;
loc_822C68EC:
	// mulli r11,r29,1776
	ctx.r11.s64 = ctx.r29.s64 * 1776;
	// li r3,28
	ctx.r3.s64 = 28;
	// add r31,r11,r28
	ctx.r31.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bl 0x822e8aa0
	ctx.lr = 0x822C68FC;
	sub_822E8AA0(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 424, ctx.r3.u32);
	// beq cr6,0x822c69bc
	if (ctx.cr6.eq) goto loc_822C69BC;
	// li r10,7
	ctx.r10.s64 = 7;
	// addi r11,r3,-4
	ctx.r11.s64 = ctx.r3.s64 + -4;
	// li r9,0
	ctx.r9.s64 = 0;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822C6918:
	// stwu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r11.u32 = ea;
	// bdnz 0x822c6918
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822C6918;
	// lwz r11,228(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 228);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r3,r11,7
	ctx.r3.s64 = ctx.r11.s64 + 7;
	// bl 0x822e8aa0
	ctx.lr = 0x822C6930;
	sub_822E8AA0(ctx, base);
	// lwz r10,424(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 424);
	// stw r3,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r3.u32);
	// lwz r9,424(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 424);
	// lwz r3,4(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822c69bc
	if (ctx.cr6.eq) goto loc_822C69BC;
	// lwz r11,228(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 228);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r11,7
	ctx.r5.s64 = ctx.r11.s64 + 7;
	// bl 0x8233eaf0
	ctx.lr = 0x822C695C;
	sub_8233EAF0(ctx, base);
	// lwz r8,424(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 424);
	// addi r10,r29,1
	ctx.r10.s64 = ctx.r29.s64 + 1;
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r29,r9
	ctx.r29.u64 = ctx.r9.u64;
	// addi r7,r11,2
	ctx.r7.s64 = ctx.r11.s64 + 2;
	// stw r7,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r7.u32);
	// lwz r6,424(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 424);
	// lwz r11,228(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 228);
	// addi r5,r11,1
	ctx.r5.s64 = ctx.r11.s64 + 1;
	// rlwinm r11,r5,1,0,30
	ctx.r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,8(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// add r4,r11,r10
	ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r4,12(r6)
	PPC_STORE_U32(ctx.r6.u32 + 12, ctx.r4.u32);
	// lwz r3,424(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 424);
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// stw r11,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r11.u32);
	// lhz r10,34(r30)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r30.u32 + 34);
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x822c68ec
	if (ctx.cr6.lt) goto loc_822C68EC;
loc_822C69B0:
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822C69BC:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C69CC"))) PPC_WEAK_FUNC(sub_822C69CC);
PPC_FUNC_IMPL(__imp__sub_822C69CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C69D0"))) PPC_WEAK_FUNC(sub_822C69D0);
PPC_FUNC_IMPL(__imp__sub_822C69D0) {
	PPC_FUNC_PROLOGUE();
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x822c6a2c
	if (!ctx.cr6.eq) goto loc_822C6A2C;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// cmplwi cr6,r10,16
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 16, ctx.xer);
	// beq cr6,0x822c6a08
	if (ctx.cr6.eq) goto loc_822C6A08;
	// cmplwi cr6,r10,20
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 20, ctx.xer);
	// beq cr6,0x822c6a08
	if (ctx.cr6.eq) goto loc_822C6A08;
	// cmplwi cr6,r10,24
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 24, ctx.xer);
	// beq cr6,0x822c6a08
	if (ctx.cr6.eq) goto loc_822C6A08;
	// cmplwi cr6,r10,32
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 32, ctx.xer);
	// bne cr6,0x822c6a5c
	if (!ctx.cr6.eq) goto loc_822C6A5C;
loc_822C6A08:
	// lwz r11,16(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// beq cr6,0x822c6a4c
	if (ctx.cr6.eq) goto loc_822C6A4C;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// beq cr6,0x822c6a4c
	if (ctx.cr6.eq) goto loc_822C6A4C;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// beq cr6,0x822c6a4c
	if (ctx.cr6.eq) goto loc_822C6A4C;
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// blr 
	return;
loc_822C6A2C:
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x822c6a5c
	if (!ctx.cr6.eq) goto loc_822C6A5C;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// cmplwi cr6,r10,32
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 32, ctx.xer);
	// bne cr6,0x822c6a5c
	if (!ctx.cr6.eq) goto loc_822C6A5C;
	// lwz r11,16(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bne cr6,0x822c6a5c
	if (!ctx.cr6.eq) goto loc_822C6A5C;
loc_822C6A4C:
	// addi r10,r10,7
	ctx.r10.s64 = ctx.r10.s64 + 7;
	// rlwinm r9,r10,29,3,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// bgelr cr6
	if (!ctx.cr6.lt) return;
loc_822C6A5C:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822C6A64"))) PPC_WEAK_FUNC(sub_822C6A64);
PPC_FUNC_IMPL(__imp__sub_822C6A64) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C6A68"))) PPC_WEAK_FUNC(sub_822C6A68);
PPC_FUNC_IMPL(__imp__sub_822C6A68) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x822C6A70;
	__restfpr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822c6c7c
	if (ctx.cr6.eq) goto loc_822C6C7C;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x822c6c7c
	if (ctx.cr6.eq) goto loc_822C6C7C;
	// lhz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,352
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 352, ctx.xer);
	// beq cr6,0x822c6aac
	if (ctx.cr6.eq) goto loc_822C6AAC;
	// cmplwi cr6,r11,353
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 353, ctx.xer);
	// beq cr6,0x822c6aac
	if (ctx.cr6.eq) goto loc_822C6AAC;
	// cmplwi cr6,r11,357
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 357, ctx.xer);
	// li r30,0
	ctx.r30.s64 = 0;
	// bne cr6,0x822c6ab0
	if (!ctx.cr6.eq) goto loc_822C6AB0;
loc_822C6AAC:
	// li r30,1
	ctx.r30.s64 = 1;
loc_822C6AB0:
	// cmplwi cr6,r11,354
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 354, ctx.xer);
	// beq cr6,0x822c6ac4
	if (ctx.cr6.eq) goto loc_822C6AC4;
	// cmplwi cr6,r11,358
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 358, ctx.xer);
	// li r31,0
	ctx.r31.s64 = 0;
	// bne cr6,0x822c6ac8
	if (!ctx.cr6.eq) goto loc_822C6AC8;
loc_822C6AC4:
	// li r31,1
	ctx.r31.s64 = 1;
loc_822C6AC8:
	// cmplwi cr6,r11,355
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 355, ctx.xer);
	// beq cr6,0x822c6adc
	if (ctx.cr6.eq) goto loc_822C6ADC;
	// cmplwi cr6,r11,359
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 359, ctx.xer);
	// li r11,0
	ctx.r11.s64 = 0;
	// bne cr6,0x822c6ae0
	if (!ctx.cr6.eq) goto loc_822C6AE0;
loc_822C6ADC:
	// li r11,1
	ctx.r11.s64 = 1;
loc_822C6AE0:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x822c6b04
	if (!ctx.cr6.eq) goto loc_822C6B04;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// bne cr6,0x822c6b04
	if (!ctx.cr6.eq) goto loc_822C6B04;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c6b0c
	if (!ctx.cr6.eq) goto loc_822C6B0C;
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_822C6B04:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c6b1c
	if (ctx.cr6.eq) goto loc_822C6B1C;
loc_822C6B0C:
	// lwz r10,4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x822c6c70
	if (!ctx.cr6.eq) goto loc_822C6C70;
loc_822C6B1C:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x822c6b54
	if (ctx.cr6.eq) goto loc_822C6B54;
	// lwz r10,4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// cmplwi cr6,r10,48000
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 48000, ctx.xer);
	// bgt cr6,0x822c6c70
	if (ctx.cr6.gt) goto loc_822C6C70;
	// lhz r7,2(r8)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r8.u32 + 2);
	// cmplwi cr6,r7,2
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 2, ctx.xer);
	// bgt cr6,0x822c6c70
	if (ctx.cr6.gt) goto loc_822C6C70;
	// lhz r10,14(r8)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r8.u32 + 14);
	// cmplwi cr6,r10,16
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 16, ctx.xer);
	// beq cr6,0x822c6b60
	if (ctx.cr6.eq) goto loc_822C6B60;
loc_822C6B48:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_822C6B54:
	// lhz r7,2(r8)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r8.u32 + 2);
	// cmplwi cr6,r7,32
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 32, ctx.xer);
	// bgt cr6,0x822c6c70
	if (ctx.cr6.gt) goto loc_822C6C70;
loc_822C6B60:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c6c70
	if (!ctx.cr6.eq) goto loc_822C6C70;
	// lwz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x822c6c70
	if (ctx.cr6.eq) goto loc_822C6C70;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822c6c70
	if (ctx.cr6.eq) goto loc_822C6C70;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x822c69d0
	ctx.lr = 0x822C6B84;
	sub_822C69D0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c6c84
	if (ctx.cr6.lt) goto loc_822C6C84;
	// lhz r11,14(r8)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + 14);
	// cmplwi cr6,r11,16
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 16, ctx.xer);
	// beq cr6,0x822c6bb0
	if (ctx.cr6.eq) goto loc_822C6BB0;
	// cmplwi cr6,r11,20
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 20, ctx.xer);
	// beq cr6,0x822c6bb0
	if (ctx.cr6.eq) goto loc_822C6BB0;
	// cmplwi cr6,r11,24
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 24, ctx.xer);
	// beq cr6,0x822c6bb0
	if (ctx.cr6.eq) goto loc_822C6BB0;
	// cmplwi cr6,r11,32
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 32, ctx.xer);
	// bne cr6,0x822c6c70
	if (!ctx.cr6.eq) goto loc_822C6C70;
loc_822C6BB0:
	// lwz r11,8(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// rlwinm r10,r11,0,0,0
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822c6c7c
	if (!ctx.cr6.eq) goto loc_822C6C7C;
	// lhz r11,12(r8)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c6c7c
	if (ctx.cr6.eq) goto loc_822C6C7C;
	// lwz r11,8(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r10,16(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x822c6be4
	if (ctx.cr6.eq) goto loc_822C6BE4;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x822c6c70
	if (!ctx.cr6.eq) goto loc_822C6C70;
loc_822C6BE4:
	// lwz r11,4(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// cmplw cr6,r11,r7
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r7.u32, ctx.xer);
	// beq cr6,0x822c6bf8
	if (ctx.cr6.eq) goto loc_822C6BF8;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x822c6c70
	if (!ctx.cr6.eq) goto loc_822C6C70;
loc_822C6BF8:
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x822c6c1c
	if (ctx.cr6.eq) goto loc_822C6C1C;
	// lhz r11,24(r5)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 24);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// blt cr6,0x822c6c70
	if (ctx.cr6.lt) goto loc_822C6C70;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bgt cr6,0x822c6c70
	if (ctx.cr6.gt) goto loc_822C6C70;
	// lhz r29,0(r5)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
loc_822C6C1C:
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// beq cr6,0x822c6c48
	if (ctx.cr6.eq) goto loc_822C6C48;
	// rlwinm r10,r29,0,28,28
	ctx.r10.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822c6c70
	if (!ctx.cr6.eq) goto loc_822C6C70;
	// cmplw cr6,r6,r11
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822c6c70
	if (ctx.cr6.lt) goto loc_822C6C70;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r6,r10
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r10.u32, ctx.xer);
	// bgt cr6,0x822c6b48
	if (ctx.cr6.gt) goto loc_822C6B48;
loc_822C6C48:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x822c6c84
	if (ctx.cr6.eq) goto loc_822C6C84;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplw cr6,r6,r10
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r10.u32, ctx.xer);
	// bgt cr6,0x822c6c70
	if (ctx.cr6.gt) goto loc_822C6C70;
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x822c6c84
	if (!ctx.cr6.lt) goto loc_822C6C84;
loc_822C6C70:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_822C6C7C:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
loc_822C6C84:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C6C8C"))) PPC_WEAK_FUNC(sub_822C6C8C);
PPC_FUNC_IMPL(__imp__sub_822C6C8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C6C90"))) PPC_WEAK_FUNC(sub_822C6C90);
PPC_FUNC_IMPL(__imp__sub_822C6C90) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r9,356(r3)
	PPC_STORE_U32(ctx.r3.u32 + 356, ctx.r9.u32);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// stw r10,368(r3)
	PPC_STORE_U32(ctx.r3.u32 + 368, ctx.r10.u32);
	// lhz r8,110(r11)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r11.u32 + 110);
	// sth r8,396(r3)
	PPC_STORE_U16(ctx.r3.u32 + 396, ctx.r8.u16);
	// lhz r7,110(r11)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + 110);
	// cmplwi cr6,r7,16
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 16, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// lwz r10,12(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// cmplwi cr6,r10,16
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 16, ctx.xer);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
	// li r8,16
	ctx.r8.s64 = 16;
	// li r10,1
	ctx.r10.s64 = 1;
	// sth r8,396(r3)
	PPC_STORE_U16(ctx.r3.u32 + 396, ctx.r8.u16);
	// stw r10,356(r3)
	PPC_STORE_U32(ctx.r3.u32 + 356, ctx.r10.u32);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// cmplwi cr6,r10,2
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 2, ctx.xer);
	// ble cr6,0x822c6ce4
	if (!ctx.cr6.gt) goto loc_822C6CE4;
	// li r10,2
	ctx.r10.s64 = 2;
loc_822C6CE4:
	// stw r10,368(r3)
	PPC_STORE_U32(ctx.r3.u32 + 368, ctx.r10.u32);
	// lwz r8,88(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
	// stw r9,356(r3)
	PPC_STORE_U32(ctx.r3.u32 + 356, ctx.r9.u32);
	// lhz r11,110(r11)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + 110);
	// sth r11,396(r3)
	PPC_STORE_U16(ctx.r3.u32 + 396, ctx.r11.u16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822C6D04"))) PPC_WEAK_FUNC(sub_822C6D04);
PPC_FUNC_IMPL(__imp__sub_822C6D04) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C6D08"))) PPC_WEAK_FUNC(sub_822C6D08);
PPC_FUNC_IMPL(__imp__sub_822C6D08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822C6D10;
	__restfpr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,8(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// lwz r10,468(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 468);
	// rotlwi r5,r11,3
	ctx.r5.u64 = rotl32(ctx.r11.u32, 3);
	// extsh r30,r10
	ctx.r30.s64 = ctx.r10.s16;
	// bl 0x8233eaf0
	ctx.lr = 0x822C6D34;
	sub_8233EAF0(ctx, base);
	// lhz r9,34(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x822c6f3c
	if (ctx.cr6.eq) goto loc_822C6F3C;
	// extsh r4,r30
	ctx.r4.s64 = ctx.r30.s16;
	// li r10,0
	ctx.r10.s64 = 0;
	// srawi r11,r4,1
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r4.s32 >> 1;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// addze r3,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r3.s64 = temp.s64;
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// li r30,32767
	ctx.r30.s64 = 32767;
	// li r29,1
	ctx.r29.s64 = 1;
	// lfs f0,-28948(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
loc_822C6D64:
	// lwz r11,320(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// mulli r7,r9,1776
	ctx.r7.s64 = ctx.r9.s64 * 1776;
	// add r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 + ctx.r11.u64;
	// mulli r8,r9,112
	ctx.r8.s64 = ctx.r9.s64 * 112;
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// lwz r5,332(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 332);
	// rlwinm r6,r4,1,0,30
	ctx.r6.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r6,r4,r6
	ctx.r6.u64 = ctx.r4.u64 + ctx.r6.u64;
	// stw r5,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r5.u32);
	// lwz r5,336(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// add r5,r8,r5
	ctx.r5.u64 = ctx.r8.u64 + ctx.r5.u64;
	// srawi r6,r6,1
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 1;
	// stw r5,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r5.u32);
	// stw r10,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r10.u32);
	// addze r8,r6
	temp.s64 = ctx.r6.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r8.s64 = temp.s64;
	// stw r10,28(r11)
	PPC_STORE_U32(ctx.r11.u32 + 28, ctx.r10.u32);
	// stw r10,32(r11)
	PPC_STORE_U32(ctx.r11.u32 + 32, ctx.r10.u32);
	// mullw r6,r8,r9
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// lwz r8,264(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r5,268(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// stw r10,40(r11)
	PPC_STORE_U32(ctx.r11.u32 + 40, ctx.r10.u32);
	// stw r10,48(r11)
	PPC_STORE_U32(ctx.r11.u32 + 48, ctx.r10.u32);
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// subf r5,r8,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r8.s64;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r5,36(r11)
	PPC_STORE_U32(ctx.r11.u32 + 36, ctx.r5.u32);
	// lwz r8,324(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 324);
	// stfs f0,72(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 72, temp.u32);
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// stfs f0,76(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 76, temp.u32);
	// stfs f0,80(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 80, temp.u32);
	// stw r8,56(r11)
	PPC_STORE_U32(ctx.r11.u32 + 56, ctx.r8.u32);
	// stfs f0,84(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 84, temp.u32);
	// stw r8,144(r11)
	PPC_STORE_U32(ctx.r11.u32 + 144, ctx.r8.u32);
	// stfs f0,88(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 88, temp.u32);
	// stw r10,64(r11)
	PPC_STORE_U32(ctx.r11.u32 + 64, ctx.r10.u32);
	// stfs f0,92(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 92, temp.u32);
	// sth r30,112(r11)
	PPC_STORE_U16(ctx.r11.u32 + 112, ctx.r30.u16);
	// stfs f0,96(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 96, temp.u32);
	// sth r10,122(r11)
	PPC_STORE_U16(ctx.r11.u32 + 122, ctx.r10.u16);
	// stfs f0,100(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 100, temp.u32);
	// sth r10,124(r11)
	PPC_STORE_U16(ctx.r11.u32 + 124, ctx.r10.u16);
	// stfs f0,104(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 104, temp.u32);
	// sth r10,126(r11)
	PPC_STORE_U16(ctx.r11.u32 + 126, ctx.r10.u16);
	// stfs f0,108(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 108, temp.u32);
	// sth r10,128(r11)
	PPC_STORE_U16(ctx.r11.u32 + 128, ctx.r10.u16);
	// stfs f0,156(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 156, temp.u32);
	// sth r10,130(r11)
	PPC_STORE_U16(ctx.r11.u32 + 130, ctx.r10.u16);
	// sth r10,132(r11)
	PPC_STORE_U16(ctx.r11.u32 + 132, ctx.r10.u16);
	// sth r10,134(r11)
	PPC_STORE_U16(ctx.r11.u32 + 134, ctx.r10.u16);
	// sth r10,114(r11)
	PPC_STORE_U16(ctx.r11.u32 + 114, ctx.r10.u16);
	// sth r10,118(r11)
	PPC_STORE_U16(ctx.r11.u32 + 118, ctx.r10.u16);
	// lwz r8,280(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 280);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// bne cr6,0x822c6e78
	if (!ctx.cr6.eq) goto loc_822C6E78;
	// lwz r8,256(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 256);
	// lwz r6,436(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 436);
	// mullw r5,r8,r9
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stw r8,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r8.u32);
	// lwz r6,436(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 436);
	// lwz r5,256(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 256);
	// mullw r8,r5,r9
	ctx.r8.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r9.s32);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stw r6,148(r11)
	PPC_STORE_U32(ctx.r11.u32 + 148, ctx.r6.u32);
	// b 0x822c6e98
	goto loc_822C6E98;
loc_822C6E78:
	// lwz r8,320(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lwz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r6,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r6.u32);
	// lwz r8,320(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// add r5,r7,r8
	ctx.r5.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// stw r8,148(r11)
	PPC_STORE_U32(ctx.r11.u32 + 148, ctx.r8.u32);
loc_822C6E98:
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// addi r5,r9,1
	ctx.r5.s64 = ctx.r9.s64 + 1;
	// stw r10,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r10.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r10.u32);
	// lwz r8,304(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 304);
	// mullw r8,r8,r9
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// lwz r6,416(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 416);
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stw r6,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r6.u32);
	// lwz r6,420(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 420);
	// lwz r8,304(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 304);
	// mullw r8,r8,r9
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r28,424(r11)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stw r6,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r6.u32);
	// lwz r6,424(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 424);
	// lwz r8,304(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 304);
	// mullw r9,r8,r9
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// extsh r9,r5
	ctx.r9.s64 = ctx.r5.s16;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stw r8,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r8.u32);
	// sth r29,0(r28)
	PPC_STORE_U16(ctx.r28.u32 + 0, ctx.r29.u16);
	// lwz r5,256(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 256);
	// lwz r6,424(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// lwz r6,8(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// sth r5,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, ctx.r5.u16);
	// lwz r5,424(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// lwz r8,256(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 256);
	// lwz r5,8(r5)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	// sth r8,-2(r5)
	PPC_STORE_U16(ctx.r5.u32 + -2, ctx.r8.u16);
	// lwz r11,424(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// lwz r8,12(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// sth r10,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r10.u16);
	// lwz r11,320(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stw r10,172(r7)
	PPC_STORE_U32(ctx.r7.u32 + 172, ctx.r10.u32);
	// lhz r6,34(r31)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmpw cr6,r9,r6
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, ctx.xer);
	// blt cr6,0x822c6d64
	if (ctx.cr6.lt) goto loc_822C6D64;
loc_822C6F3C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C6F44"))) PPC_WEAK_FUNC(sub_822C6F44);
PPC_FUNC_IMPL(__imp__sub_822C6F44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C6F48"))) PPC_WEAK_FUNC(sub_822C6F48);
PPC_FUNC_IMPL(__imp__sub_822C6F48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822C6F50;
	__restfpr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822c6f6c
	if (!ctx.cr6.eq) goto loc_822C6F6C;
loc_822C6F60:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822C6F6C:
	// lwz r31,0(r30)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x822c6f60
	if (ctx.cr6.eq) goto loc_822C6F60;
	// addi r27,r30,224
	ctx.r27.s64 = ctx.r30.s64 + 224;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x822ed5e8
	ctx.lr = 0x822C6F84;
	sub_822ED5E8(ctx, base);
	// li r29,0
	ctx.r29.s64 = 0;
	// li r28,1
	ctx.r28.s64 = 1;
	// stw r29,236(r30)
	PPC_STORE_U32(ctx.r30.u32 + 236, ctx.r29.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r28,284(r30)
	PPC_STORE_U32(ctx.r30.u32 + 284, ctx.r28.u32);
	// stw r29,240(r30)
	PPC_STORE_U32(ctx.r30.u32 + 240, ctx.r29.u32);
	// lwz r3,356(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 356);
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// rotlwi r5,r11,2
	ctx.r5.u64 = rotl32(ctx.r11.u32, 2);
	// bl 0x8233eaf0
	ctx.lr = 0x822C6FAC;
	sub_8233EAF0(ctx, base);
	// lhz r7,34(r31)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// li r8,-2
	ctx.r8.s64 = -2;
	// stw r29,388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 388, ctx.r29.u32);
	// li r9,3
	ctx.r9.s64 = 3;
	// stw r29,392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 392, ctx.r29.u32);
	// stw r8,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r8.u32);
	// mr r10,r29
	ctx.r10.u64 = ctx.r29.u64;
	// stw r9,72(r31)
	PPC_STORE_U32(ctx.r31.u32 + 72, ctx.r9.u32);
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822c703c
	if (ctx.cr6.eq) goto loc_822C703C;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_822C6FD8:
	// lwz r7,256(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 256);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// srawi r6,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 1;
	// add r5,r9,r11
	ctx.r5.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addze r4,r6
	temp.s64 = ctx.r6.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r4.s64 = temp.s64;
	// sth r4,122(r5)
	PPC_STORE_U16(ctx.r5.u32 + 122, ctx.r4.u16);
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// lhz r7,122(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 122);
	// sth r7,124(r9)
	PPC_STORE_U16(ctx.r9.u32 + 124, ctx.r7.u16);
	// lwz r6,256(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 256);
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// add r5,r9,r11
	ctx.r5.u64 = ctx.r9.u64 + ctx.r11.u64;
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// lwz r3,424(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 424);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// sth r4,-2(r9)
	PPC_STORE_U16(ctx.r9.u32 + -2, ctx.r4.u16);
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + ctx.r11.u64;
	// sth r29,116(r7)
	PPC_STORE_U16(ctx.r7.u32 + 116, ctx.r29.u16);
	// addi r11,r11,1776
	ctx.r11.s64 = ctx.r11.s64 + 1776;
	// lhz r6,34(r31)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmpw cr6,r10,r6
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, ctx.xer);
	// blt cr6,0x822c6fd8
	if (ctx.cr6.lt) goto loc_822C6FD8;
loc_822C703C:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,2
	ctx.r9.s64 = 2;
	// stw r28,12(r30)
	PPC_STORE_U32(ctx.r30.u32 + 12, ctx.r28.u32);
	// cntlzw r7,r11
	ctx.r7.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// stw r29,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r29.u32);
	// rldicr r11,r10,63,63
	ctx.r11.u64 = rotl64(ctx.r10.u64, 63) & 0xFFFFFFFFFFFFFFFF;
	// stw r8,276(r30)
	PPC_STORE_U32(ctx.r30.u32 + 276, ctx.r8.u32);
	// rlwinm r6,r7,27,31,31
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// std r29,184(r30)
	PPC_STORE_U64(ctx.r30.u32 + 184, ctx.r29.u64);
	// stw r29,160(r30)
	PPC_STORE_U32(ctx.r30.u32 + 160, ctx.r29.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// xori r10,r6,1
	ctx.r10.u64 = ctx.r6.u64 ^ 1;
	// stw r9,164(r30)
	PPC_STORE_U32(ctx.r30.u32 + 164, ctx.r9.u32);
	// stw r29,60(r30)
	PPC_STORE_U32(ctx.r30.u32 + 60, ctx.r29.u32);
	// addi r5,r10,1
	ctx.r5.s64 = ctx.r10.s64 + 1;
	// stw r28,696(r30)
	PPC_STORE_U32(ctx.r30.u32 + 696, ctx.r28.u32);
	// stw r29,300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 300, ctx.r29.u32);
	// stw r5,692(r30)
	PPC_STORE_U32(ctx.r30.u32 + 692, ctx.r5.u32);
	// stw r29,156(r30)
	PPC_STORE_U32(ctx.r30.u32 + 156, ctx.r29.u32);
	// std r11,168(r30)
	PPC_STORE_U64(ctx.r30.u32 + 168, ctx.r11.u64);
	// std r11,176(r30)
	PPC_STORE_U64(ctx.r30.u32 + 176, ctx.r11.u64);
	// sth r29,154(r30)
	PPC_STORE_U16(ctx.r30.u32 + 154, ctx.r29.u16);
	// stw r29,32(r30)
	PPC_STORE_U32(ctx.r30.u32 + 32, ctx.r29.u32);
	// bl 0x822e9828
	ctx.lr = 0x822C70A0;
	sub_822E9828(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C70A8"))) PPC_WEAK_FUNC(sub_822C70A8);
PPC_FUNC_IMPL(__imp__sub_822C70A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e45c
	ctx.lr = 0x822C70B0;
	__restfpr_25(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,704(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 704);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r28,16(r4)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// ld r29,24(r4)
	ctx.r29.u64 = PPC_LOAD_U64(ctx.r4.u32 + 24);
	// li r27,1
	ctx.r27.s64 = 1;
	// lwz r25,32(r4)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r26,8(r4)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// beq cr6,0x822c7104
	if (ctx.cr6.eq) goto loc_822C7104;
	// lwz r11,224(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822c7104
	if (!ctx.cr6.eq) goto loc_822C7104;
	// lwz r11,696(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 696);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c7104
	if (!ctx.cr6.eq) goto loc_822C7104;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// beq cr6,0x822c717c
	if (ctx.cr6.eq) goto loc_822C717C;
	// bl 0x822c6f48
	ctx.lr = 0x822C7100;
	sub_822C6F48(ctx, base);
	// stw r27,696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 696, ctx.r27.u32);
loc_822C7104:
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// beq cr6,0x822c717c
	if (ctx.cr6.eq) goto loc_822C717C;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x822c717c
	if (ctx.cr6.eq) goto loc_822C717C;
	// lhz r11,154(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 154);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// ble cr6,0x822c712c
	if (!ctx.cr6.gt) goto loc_822C712C;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// sth r11,154(r31)
	PPC_STORE_U16(ctx.r31.u32 + 154, ctx.r11.u16);
loc_822C712C:
	// ld r11,168(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 168);
	// cmpd cr6,r11,r29
	ctx.cr6.compare<int64_t>(ctx.r11.s64, ctx.r29.s64, ctx.xer);
	// beq cr6,0x822c717c
	if (ctx.cr6.eq) goto loc_822C717C;
	// lwz r11,156(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c7154
	if (!ctx.cr6.eq) goto loc_822C7154;
	// std r29,168(r31)
	PPC_STORE_U64(ctx.r31.u32 + 168, ctx.r29.u64);
	// stw r27,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r27.u32);
	// sth r27,154(r31)
	PPC_STORE_U16(ctx.r31.u32 + 154, ctx.r27.u16);
	// b 0x822c717c
	goto loc_822C717C;
loc_822C7154:
	// lhz r11,154(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 154);
	// std r29,176(r31)
	PPC_STORE_U64(ctx.r31.u32 + 176, ctx.r29.u64);
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// sth r9,154(r31)
	PPC_STORE_U16(ctx.r31.u32 + 154, ctx.r9.u16);
	// cmpwi cr6,r9,2
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 2, ctx.xer);
	// beq cr6,0x822c717c
	if (ctx.cr6.eq) goto loc_822C717C;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r11.u32);
	// sth r11,154(r31)
	PPC_STORE_U16(ctx.r31.u32 + 154, ctx.r11.u16);
loc_822C717C:
	// lwz r11,704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c7198
	if (ctx.cr6.eq) goto loc_822C7198;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// beq cr6,0x822c7198
	if (ctx.cr6.eq) goto loc_822C7198;
	// addi r3,r31,224
	ctx.r3.s64 = ctx.r31.s64 + 224;
	// bl 0x822ed5e8
	ctx.lr = 0x822C7198;
	sub_822ED5E8(ctx, base);
loc_822C7198:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r29,r31,224
	ctx.r29.s64 = ctx.r31.s64 + 224;
	// lwz r10,36(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 36);
	// lwz r8,704(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 704);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r7,12(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// lwz r6,8(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r9,60(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 60);
	// beq cr6,0x822c71d0
	if (ctx.cr6.eq) goto loc_822C71D0;
	// bl 0x822edec8
	ctx.lr = 0x822C71CC;
	sub_822EDEC8(ctx, base);
	// b 0x822c71d4
	goto loc_822C71D4;
loc_822C71D0:
	// bl 0x822edec0
	ctx.lr = 0x822C71D4;
	sub_822EDEC0(ctx, base);
loc_822C71D4:
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c723c
	if (ctx.cr6.lt) goto loc_822C723C;
	// cmpwi cr6,r3,6
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 6, ctx.xer);
	// bne cr6,0x822c71f4
	if (!ctx.cr6.eq) goto loc_822C71F4;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// li r10,6
	ctx.r10.s64 = 6;
	// stw r10,72(r11)
	PPC_STORE_U32(ctx.r11.u32 + 72, ctx.r10.u32);
	// stw r27,696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 696, ctx.r27.u32);
loc_822C71F4:
	// lwz r11,704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c723c
	if (ctx.cr6.eq) goto loc_822C723C;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// bne cr6,0x822c7220
	if (!ctx.cr6.eq) goto loc_822C7220;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// beq cr6,0x822c723c
	if (ctx.cr6.eq) goto loc_822C723C;
loc_822C7210:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
loc_822C7220:
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// beq cr6,0x822c723c
	if (ctx.cr6.eq) goto loc_822C723C;
	// cmpwi cr6,r25,8
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 8, ctx.xer);
	// bge cr6,0x822c7210
	if (!ctx.cr6.lt) goto loc_822C7210;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822ee1d8
	ctx.lr = 0x822C723C;
	sub_822EE1D8(ctx, base);
loc_822C723C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C7244"))) PPC_WEAK_FUNC(sub_822C7244);
PPC_FUNC_IMPL(__imp__sub_822C7244) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C7248"))) PPC_WEAK_FUNC(sub_822C7248);
PPC_FUNC_IMPL(__imp__sub_822C7248) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e45c
	ctx.lr = 0x822C7250;
	__restfpr_25(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r26,0
	ctx.r26.s64 = 0;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// mr r25,r26
	ctx.r25.u64 = ctx.r26.u64;
	// bne cr6,0x822c7278
	if (!ctx.cr6.eq) goto loc_822C7278;
loc_822C7268:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
loc_822C7278:
	// lwz r29,0(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x822c7268
	if (ctx.cr6.eq) goto loc_822C7268;
	// lwz r11,60(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bgt cr6,0x822c7294
	if (ctx.cr6.gt) goto loc_822C7294;
	// li r25,4
	ctx.r25.s64 = 4;
loc_822C7294:
	// lwz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// li r27,1
	ctx.r27.s64 = 1;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x822c7514
	if (ctx.cr6.eq) goto loc_822C7514;
	// lwz r11,704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 704);
	// stw r26,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r26.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// sth r27,16(r31)
	PPC_STORE_U16(ctx.r31.u32 + 16, ctx.r27.u16);
	// bne cr6,0x822c75b0
	if (!ctx.cr6.eq) goto loc_822C75B0;
	// lwz r11,212(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 212);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c7330
	if (!ctx.cr6.eq) goto loc_822C7330;
	// lwz r11,60(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bgt cr6,0x822c7330
	if (ctx.cr6.gt) goto loc_822C7330;
	// stw r26,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r26.u32);
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822c72e8
	if (!ctx.cr6.lt) goto loc_822C72E8;
	// stw r26,4(r29)
	PPC_STORE_U32(ctx.r29.u32 + 4, ctx.r26.u32);
	// stw r27,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r27.u32);
loc_822C72E8:
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r10,236(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 236);
	// subf. r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq 0x822c7300
	if (ctx.cr0.eq) goto loc_822C7300;
	// addi r3,r31,224
	ctx.r3.s64 = ctx.r31.s64 + 224;
	// bl 0x822ed888
	ctx.lr = 0x822C7300;
	sub_822ED888(ctx, base);
loc_822C7300:
	// lwz r11,236(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 236);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x822c7328
	if (!ctx.cr6.eq) goto loc_822C7328;
	// addi r3,r31,224
	ctx.r3.s64 = ctx.r31.s64 + 224;
	// bl 0x822ed5e8
	ctx.lr = 0x822C7318;
	sub_822ED5E8(ctx, base);
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,4
	ctx.r3.u64 = ctx.r3.u64 | 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
loc_822C7328:
	// stw r11,4(r29)
	PPC_STORE_U32(ctx.r29.u32 + 4, ctx.r11.u32);
	// b 0x822c74c0
	goto loc_822C74C0;
loc_822C7330:
	// li r28,-2
	ctx.r28.s64 = -2;
loc_822C7334:
	// lwz r11,236(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 236);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822c7394
	if (!ctx.cr6.eq) goto loc_822C7394;
	// addi r30,r31,224
	ctx.r30.s64 = ctx.r31.s64 + 224;
loc_822C7344:
	// lwz r11,240(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 240);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822c737c
	if (!ctx.cr6.eq) goto loc_822C737C;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x822ed5e8
	ctx.lr = 0x822C7358;
	sub_822ED5E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c6438
	ctx.lr = 0x822C7360;
	sub_822C6438(ctx, base);
	// clrlwi r5,r3,24
	ctx.r5.u64 = ctx.r3.u32 & 0xFF;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x822eded8
	ctx.lr = 0x822C7370;
	sub_822EDED8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c75b4
	if (ctx.cr6.lt) goto loc_822C75B4;
	// b 0x822c7388
	goto loc_822C7388;
loc_822C737C:
	// lwz r11,16(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	// stw r26,16(r30)
	PPC_STORE_U32(ctx.r30.u32 + 16, ctx.r26.u32);
	// stw r11,12(r30)
	PPC_STORE_U32(ctx.r30.u32 + 12, ctx.r11.u32);
loc_822C7388:
	// lwz r11,236(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 236);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c7344
	if (ctx.cr6.eq) goto loc_822C7344;
loc_822C7394:
	// lwz r11,60(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// lwz r11,236(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 236);
	// ble cr6,0x822c7480
	if (!ctx.cr6.gt) goto loc_822C7480;
	// rlwinm r10,r11,5,31,31
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0x1;
	// rlwinm r9,r11,6,0,25
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 6) & 0xFFFFFFC0;
	// sth r10,18(r31)
	PPC_STORE_U16(ctx.r31.u32 + 18, ctx.r10.u16);
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// lwz r7,8(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	// subfic r6,r7,32
	ctx.xer.ca = ctx.r7.u32 <= 32;
	ctx.r6.s64 = 32 - ctx.r7.s64;
	// srw r11,r9,r6
	ctx.r11.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r6.u8 & 0x3F));
	// stw r11,64(r31)
	PPC_STORE_U32(ctx.r31.u32 + 64, ctx.r11.u32);
	// cmpwi cr6,r8,1
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 1, ctx.xer);
	// stw r11,68(r31)
	PPC_STORE_U32(ctx.r31.u32 + 68, ctx.r11.u32);
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
	// bne cr6,0x822c73fc
	if (!ctx.cr6.eq) goto loc_822C73FC;
	// sth r27,16(r31)
	PPC_STORE_U16(ctx.r31.u32 + 16, ctx.r27.u16);
	// lwz r9,12(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 12);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x822c7418
	if (ctx.cr6.lt) goto loc_822C7418;
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
loc_822C73FC:
	// lwz r9,12(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 12);
	// subfc r7,r9,r10
	ctx.xer.ca = ctx.r10.u32 >= ctx.r9.u32;
	ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
	// eqv r6,r9,r10
	ctx.r6.u64 = ~(ctx.r9.u64 ^ ctx.r10.u64);
	// rlwinm r5,r6,1,31,31
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
	// addze r4,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r4.s64 = temp.s64;
	// clrlwi r3,r4,31
	ctx.r3.u64 = ctx.r4.u32 & 0x1;
	// sth r3,16(r31)
	PPC_STORE_U16(ctx.r31.u32 + 16, ctx.r3.u16);
loc_822C7418:
	// lwz r9,12(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x822c7438
	if (!ctx.cr6.eq) goto loc_822C7438;
	// lhz r8,16(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 16);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x822c7460
	if (!ctx.cr6.eq) goto loc_822C7460;
	// stw r26,236(r31)
	PPC_STORE_U32(ctx.r31.u32 + 236, ctx.r26.u32);
	// b 0x822c7334
	goto loc_822C7334;
loc_822C7438:
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x822c7460
	if (!ctx.cr6.eq) goto loc_822C7460;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// bne cr6,0x822c7460
	if (!ctx.cr6.eq) goto loc_822C7460;
	// stw r28,276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 276, ctx.r28.u32);
	// addi r3,r31,224
	ctx.r3.s64 = ctx.r31.s64 + 224;
	// stw r26,236(r31)
	PPC_STORE_U32(ctx.r31.u32 + 236, ctx.r26.u32);
	// bl 0x822ed5e8
	ctx.lr = 0x822C7458;
	sub_822ED5E8(ctx, base);
	// stw r27,284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 284, ctx.r27.u32);
	// b 0x822c7334
	goto loc_822C7334;
loc_822C7460:
	// lwz r8,12(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 12);
	// cmpw cr6,r10,r8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822c74ac
	if (ctx.cr6.lt) goto loc_822C74AC;
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x822c7478
	if (!ctx.cr6.eq) goto loc_822C7478;
	// stw r28,276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 276, ctx.r28.u32);
loc_822C7478:
	// stw r26,236(r31)
	PPC_STORE_U32(ctx.r31.u32 + 236, ctx.r26.u32);
	// b 0x822c7334
	goto loc_822C7334;
loc_822C7480:
	// subfic r10,r25,32
	ctx.xer.ca = ctx.r25.u32 <= 32;
	ctx.r10.s64 = 32 - ctx.r25.s64;
	// rlwinm r8,r11,4,0,27
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r25,4
	ctx.r9.s64 = ctx.r25.s64 + 4;
	// srw r6,r8,r10
	ctx.r6.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
	// sth r6,16(r31)
	PPC_STORE_U16(ctx.r31.u32 + 16, ctx.r6.u16);
	// slw r7,r11,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r11.u32 << (ctx.r9.u8 & 0x3F));
	// lwz r4,8(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	// subfic r3,r4,29
	ctx.xer.ca = ctx.r4.u32 <= 29;
	ctx.r3.s64 = 29 - ctx.r4.s64;
	// srw r11,r7,r3
	ctx.r11.u64 = ctx.r3.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r3.u8 & 0x3F));
	// stw r11,64(r31)
	PPC_STORE_U32(ctx.r31.u32 + 64, ctx.r11.u32);
	// stw r11,68(r31)
	PPC_STORE_U32(ctx.r31.u32 + 68, ctx.r11.u32);
loc_822C74AC:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c74bc
	if (!ctx.cr6.eq) goto loc_822C74BC;
	// addi r3,r31,224
	ctx.r3.s64 = ctx.r31.s64 + 224;
	// bl 0x822ed888
	ctx.lr = 0x822C74BC;
	sub_822ED888(ctx, base);
loc_822C74BC:
	// stw r26,236(r31)
	PPC_STORE_U32(ctx.r31.u32 + 236, ctx.r26.u32);
loc_822C74C0:
	// lwz r11,156(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822c7508
	if (!ctx.cr6.eq) goto loc_822C7508;
	// lhz r11,154(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 154);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822c75bc
	if (!ctx.cr6.eq) goto loc_822C75BC;
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// lis r10,152
	ctx.r10.s64 = 9961472;
	// ld r9,168(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 168);
	// extsw r8,r11
	ctx.r8.s64 = ctx.r11.s32;
	// ori r7,r10,38528
	ctx.r7.u64 = ctx.r10.u64 | 38528;
	// mulld r6,r8,r9
	ctx.r6.s64 = ctx.r8.s64 * ctx.r9.s64;
	// divd r5,r6,r7
	ctx.r5.s64 = ctx.r6.s64 / ctx.r7.s64;
	// std r5,184(r31)
	PPC_STORE_U64(ctx.r31.u32 + 184, ctx.r5.u64);
loc_822C74FC:
	// sth r26,154(r31)
	PPC_STORE_U16(ctx.r31.u32 + 154, ctx.r26.u16);
	// stw r26,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r26.u32);
loc_822C7504:
	// stw r27,160(r31)
	PPC_STORE_U32(ctx.r31.u32 + 160, ctx.r27.u32);
loc_822C7508:
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c75b0
	if (ctx.cr6.eq) goto loc_822C75B0;
loc_822C7514:
	// lwz r11,64(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 64);
	// stw r27,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r27.u32);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// ble cr6,0x822c7554
	if (!ctx.cr6.gt) goto loc_822C7554;
	// addi r30,r31,224
	ctx.r30.s64 = ctx.r31.s64 + 224;
loc_822C7528:
	// li r4,24
	ctx.r4.s64 = 24;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x822ee1d8
	ctx.lr = 0x822C7534;
	sub_822EE1D8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c75b4
	if (ctx.cr6.lt) goto loc_822C75B4;
	// lwz r11,64(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 64);
	// addi r11,r11,-24
	ctx.r11.s64 = ctx.r11.s64 + -24;
	// rotlwi r10,r11,0
	ctx.r10.u64 = rotl32(ctx.r11.u32, 0);
	// stw r11,64(r31)
	PPC_STORE_U32(ctx.r31.u32 + 64, ctx.r11.u32);
	// cmpwi cr6,r10,24
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 24, ctx.xer);
	// bgt cr6,0x822c7528
	if (ctx.cr6.gt) goto loc_822C7528;
loc_822C7554:
	// addi r3,r31,224
	ctx.r3.s64 = ctx.r31.s64 + 224;
	// lwz r4,64(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 64);
	// bl 0x822ee1d8
	ctx.lr = 0x822C7560;
	sub_822EE1D8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c75b4
	if (ctx.cr6.lt) goto loc_822C75B4;
	// lhz r11,34(r29)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r29.u32 + 34);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c75a4
	if (ctx.cr6.eq) goto loc_822C75A4;
	// mr r11,r26
	ctx.r11.u64 = ctx.r26.u64;
	// li r8,32767
	ctx.r8.s64 = 32767;
loc_822C757C:
	// lwz r10,320(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 320);
	// mulli r9,r11,1776
	ctx.r9.s64 = ctx.r11.s64 * 1776;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// sth r8,112(r10)
	PPC_STORE_U16(ctx.r10.u32 + 112, ctx.r8.u16);
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
	// lhz r7,34(r29)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r29.u32 + 34);
	// cmpw cr6,r9,r7
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, ctx.xer);
	// blt cr6,0x822c757c
	if (ctx.cr6.lt) goto loc_822C757C;
loc_822C75A4:
	// li r11,3
	ctx.r11.s64 = 3;
	// stw r11,72(r29)
	PPC_STORE_U32(ctx.r29.u32 + 72, ctx.r11.u32);
	// stw r26,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r26.u32);
loc_822C75B0:
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
loc_822C75B4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
loc_822C75BC:
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x822c74fc
	if (!ctx.cr6.eq) goto loc_822C74FC;
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// lis r10,152
	ctx.r10.s64 = 9961472;
	// ld r9,168(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 168);
	// extsw r8,r11
	ctx.r8.s64 = ctx.r11.s32;
	// ld r7,176(r31)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r31.u32 + 176);
	// ori r6,r10,38528
	ctx.r6.u64 = ctx.r10.u64 | 38528;
	// sth r27,154(r31)
	PPC_STORE_U16(ctx.r31.u32 + 154, ctx.r27.u16);
	// mulld r5,r8,r9
	ctx.r5.s64 = ctx.r8.s64 * ctx.r9.s64;
	// std r7,168(r31)
	PPC_STORE_U64(ctx.r31.u32 + 168, ctx.r7.u64);
	// divd r4,r5,r6
	ctx.r4.s64 = ctx.r5.s64 / ctx.r6.s64;
	// std r4,184(r31)
	PPC_STORE_U64(ctx.r31.u32 + 184, ctx.r4.u64);
	// b 0x822c7504
	goto loc_822C7504;
}

__attribute__((alias("__imp__sub_822C75F4"))) PPC_WEAK_FUNC(sub_822C75F4);
PPC_FUNC_IMPL(__imp__sub_822C75F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C75F8"))) PPC_WEAK_FUNC(sub_822C75F8);
PPC_FUNC_IMPL(__imp__sub_822C75F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e434
	ctx.lr = 0x822C7600;
	__restfpr_15(ctx, base);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r28,0
	ctx.r28.s64 = 0;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r18,r4
	ctx.r18.u64 = ctx.r4.u64;
	// mr r20,r5
	ctx.r20.u64 = ctx.r5.u64;
	// li r16,2
	ctx.r16.s64 = 2;
	// mr r19,r28
	ctx.r19.u64 = ctx.r28.u64;
	// li r15,3
	ctx.r15.s64 = 3;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822c7e8c
	if (ctx.cr6.eq) goto loc_822C7E8C;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x822c7e8c
	if (ctx.cr6.eq) goto loc_822C7E8C;
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x822c7e8c
	if (ctx.cr6.eq) goto loc_822C7E8C;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// sth r28,0(r4)
	PPC_STORE_U16(ctx.r4.u32 + 0, ctx.r28.u16);
	// beq cr6,0x822c764c
	if (ctx.cr6.eq) goto loc_822C764C;
	// sth r28,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, ctx.r28.u16);
loc_822C764C:
	// lis r11,-32764
	ctx.r11.s64 = -2147221504;
	// li r26,32767
	ctx.r26.s64 = 32767;
	// li r23,5
	ctx.r23.s64 = 5;
	// li r21,1
	ctx.r21.s64 = 1;
	// ori r25,r11,4
	ctx.r25.u64 = ctx.r11.u64 | 4;
	// li r27,8
	ctx.r27.s64 = 8;
	// li r17,4
	ctx.r17.s64 = 4;
	// li r24,7
	ctx.r24.s64 = 7;
	// li r22,6
	ctx.r22.s64 = 6;
loc_822C7670:
	// lwz r11,32(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 32);
	// cmplwi cr6,r11,9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 9, ctx.xer);
	// bgt cr6,0x822c7670
	if (ctx.cr6.gt) goto loc_822C7670;
	// lis r12,-32212
	ctx.r12.s64 = -2111045632;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r12,r12,30356
	ctx.r12.s64 = ctx.r12.s64 + 30356;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr 
	switch (ctx.r11.u64) {
	case 0:
		goto loc_822C79F8;
	case 1:
		goto loc_822C7670;
	case 2:
		goto loc_822C7A18;
	case 3:
		goto loc_822C7AE4;
	case 4:
		goto loc_822C76BC;
	case 5:
		goto loc_822C7788;
	case 6:
		goto loc_822C7834;
	case 7:
		goto loc_822C7874;
	case 8:
		goto loc_822C78B8;
	case 9:
		goto loc_822C7984;
	default:
		__builtin_unreachable();
	}
	// lwz r17,31224(r12)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r12.u32 + 31224);
	// lwz r17,30320(r12)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r12.u32 + 30320);
	// lwz r17,31256(r12)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r12.u32 + 31256);
	// lwz r17,31460(r12)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r12.u32 + 31460);
	// lwz r17,30396(r12)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r12.u32 + 30396);
	// lwz r17,30600(r12)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r12.u32 + 30600);
	// lwz r17,30772(r12)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r12.u32 + 30772);
	// lwz r17,30836(r12)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r12.u32 + 30836);
	// lwz r17,30904(r12)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r12.u32 + 30904);
	// lwz r17,31108(r12)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r12.u32 + 31108);
loc_822C76BC:
	// lwz r11,216(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 216);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c7730
	if (!ctx.cr6.eq) goto loc_822C7730;
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822c771c
	if (!ctx.cr6.gt) goto loc_822C771C;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C76E0:
	// lwz r8,584(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// addi r7,r11,1
	ctx.r7.s64 = ctx.r11.s64 + 1;
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lhzx r5,r10,r8
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r4,r5
	ctx.r4.s64 = ctx.r5.s16;
	// mulli r8,r4,1776
	ctx.r8.s64 = ctx.r4.s64 * 1776;
	// add r3,r8,r9
	ctx.r3.u64 = ctx.r8.u64 + ctx.r9.u64;
	// sth r26,112(r3)
	PPC_STORE_U16(ctx.r3.u32 + 112, ctx.r26.u16);
	// lhz r9,580(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// cmpw cr6,r6,r8
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822c76e0
	if (ctx.cr6.lt) goto loc_822C76E0;
loc_822C771C:
	// stw r23,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r23.u32);
	// stw r28,36(r29)
	PPC_STORE_U32(ctx.r29.u32 + 36, ctx.r28.u32);
	// stw r28,40(r29)
	PPC_STORE_U32(ctx.r29.u32 + 40, ctx.r28.u32);
	// sth r28,150(r29)
	PPC_STORE_U16(ctx.r29.u32 + 150, ctx.r28.u16);
	// sth r28,152(r29)
	PPC_STORE_U16(ctx.r29.u32 + 152, ctx.r28.u16);
loc_822C7730:
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822c7784
	if (!ctx.cr6.gt) goto loc_822C7784;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C7748:
	// lwz r8,584(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// addi r7,r11,1
	ctx.r7.s64 = ctx.r11.s64 + 1;
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lhzx r5,r10,r8
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r4,r5
	ctx.r4.s64 = ctx.r5.s16;
	// mulli r8,r4,1776
	ctx.r8.s64 = ctx.r4.s64 * 1776;
	// add r3,r8,r9
	ctx.r3.u64 = ctx.r8.u64 + ctx.r9.u64;
	// sth r21,490(r3)
	PPC_STORE_U16(ctx.r3.u32 + 490, ctx.r21.u16);
	// lhz r9,580(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// cmpw cr6,r6,r8
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822c7748
	if (ctx.cr6.lt) goto loc_822C7748;
loc_822C7784:
	// sth r21,730(r31)
	PPC_STORE_U16(ctx.r31.u32 + 730, ctx.r21.u16);
loc_822C7788:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r10,176(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 176);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// beq cr6,0x822c77ec
	if (ctx.cr6.eq) goto loc_822C77EC;
	// lwz r11,504(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 504);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C77A8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r19,r3
	ctx.r19.u64 = ctx.r3.u64;
	// cmplw cr6,r3,r25
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r25.u32, ctx.xer);
	// bne cr6,0x822c77e4
	if (!ctx.cr6.eq) goto loc_822C77E4;
	// addi r3,r29,224
	ctx.r3.s64 = ctx.r29.s64 + 224;
	// bl 0x822ed990
	ctx.lr = 0x822C77BC;
	sub_822ED990(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x822c7e94
	if (ctx.cr6.eq) goto loc_822C7E94;
	// lwz r11,704(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c7e94
	if (ctx.cr6.eq) goto loc_822C7E94;
	// stw r27,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r27.u32);
	// mr r19,r17
	ctx.r19.u64 = ctx.r17.u64;
	// stw r21,216(r29)
	PPC_STORE_U32(ctx.r29.u32 + 216, ctx.r21.u32);
	// sth r28,16(r29)
	PPC_STORE_U16(ctx.r29.u32 + 16, ctx.r28.u16);
	// b 0x822c7670
	goto loc_822C7670;
loc_822C77E4:
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// blt cr6,0x822c7e94
	if (ctx.cr6.lt) goto loc_822C7E94;
loc_822C77EC:
	// lwz r11,36(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 36);
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// bne cr6,0x822c7834
	if (!ctx.cr6.eq) goto loc_822C7834;
	// lwz r11,216(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 216);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c7828
	if (ctx.cr6.eq) goto loc_822C7828;
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r10,176(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 176);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x822c781c
	if (!ctx.cr6.eq) goto loc_822C781C;
	// stw r24,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r24.u32);
	// b 0x822c7670
	goto loc_822C7670;
loc_822C781C:
	// lbz r11,144(r29)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + 144);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c7830
	if (ctx.cr6.eq) goto loc_822C7830;
loc_822C7828:
	// stw r27,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r27.u32);
	// b 0x822c7670
	goto loc_822C7670;
loc_822C7830:
	// stw r22,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r22.u32);
loc_822C7834:
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// blt cr6,0x822c7870
	if (ctx.cr6.lt) goto loc_822C7870;
	// addi r30,r29,224
	ctx.r30.s64 = ctx.r29.s64 + 224;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r28.u32);
loc_822C7848:
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x822ee068
	ctx.lr = 0x822C7858;
	sub_822EE068(ctx, base);
	// mr r19,r3
	ctx.r19.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c7e94
	if (ctx.cr6.lt) goto loc_822C7E94;
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x822c7848
	if (!ctx.cr6.eq) goto loc_822C7848;
loc_822C7870:
	// stw r24,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r24.u32);
loc_822C7874:
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bgt cr6,0x822c7890
	if (ctx.cr6.gt) goto loc_822C7890;
	// lhz r11,16(r29)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r29.u32 + 16);
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// sth r10,16(r29)
	PPC_STORE_U16(ctx.r29.u32 + 16, ctx.r10.u16);
	// b 0x822c789c
	goto loc_822C789C;
loc_822C7890:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822ee308
	ctx.lr = 0x822C7898;
	sub_822EE308(ctx, base);
	// mr r19,r3
	ctx.r19.u64 = ctx.r3.u64;
loc_822C789C:
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// blt cr6,0x822c7e94
	if (ctx.cr6.lt) goto loc_822C7E94;
	// lwz r11,704(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c78b4
	if (ctx.cr6.eq) goto loc_822C78B4;
	// sth r28,16(r29)
	PPC_STORE_U16(ctx.r29.u32 + 16, ctx.r28.u16);
loc_822C78B4:
	// stw r27,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r27.u32);
loc_822C78B8:
	// cmplwi cr6,r20,0
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, 0, ctx.xer);
	// bne cr6,0x822c78c4
	if (!ctx.cr6.eq) goto loc_822C78C4;
	// addi r20,r1,80
	ctx.r20.s64 = ctx.r1.s64 + 80;
loc_822C78C4:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,176(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 176);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x822c78fc
	if (!ctx.cr6.eq) goto loc_822C78FC;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r7,708(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 708);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r18
	ctx.r4.u64 = ctx.r18.u64;
	// bl 0x822e9ba8
	ctx.lr = 0x822C78EC;
	sub_822E9BA8(ctx, base);
	// cmplwi cr6,r20,0
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, 0, ctx.xer);
	// beq cr6,0x822c7900
	if (ctx.cr6.eq) goto loc_822C7900;
	// sth r28,0(r20)
	PPC_STORE_U16(ctx.r20.u32 + 0, ctx.r28.u16);
	// b 0x822c7900
	goto loc_822C7900;
loc_822C78FC:
	// bl 0x822f2030
	ctx.lr = 0x822C7900;
	sub_822F2030(ctx, base);
loc_822C7900:
	// lhz r11,220(r29)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r29.u32 + 220);
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// sth r10,220(r29)
	PPC_STORE_U16(ctx.r29.u32 + 220, ctx.r10.u16);
	// lhz r8,580(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// ble cr6,0x822c7984
	if (!ctx.cr6.gt) goto loc_822C7984;
	// mr r10,r28
	ctx.r10.u64 = ctx.r28.u64;
	// rlwinm r11,r28,1,0,30
	ctx.r11.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C7924:
	// lwz r8,584(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lhzx r7,r11,r8
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r8.u32);
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// mulli r11,r6,1776
	ctx.r11.s64 = ctx.r6.s64 * 1776;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwz r5,424(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// lhz r4,114(r11)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r11.u32 + 114);
	// extsh r9,r4
	ctx.r9.s64 = ctx.r4.s16;
	// lhz r3,0(r5)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// extsh r8,r3
	ctx.r8.s64 = ctx.r3.s16;
	// cmpw cr6,r9,r8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822c7cb4
	if (!ctx.cr6.lt) goto loc_822C7CB4;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// extsh r6,r8
	ctx.r6.s64 = ctx.r8.s16;
	// sth r7,114(r11)
	PPC_STORE_U16(ctx.r11.u32 + 114, ctx.r7.u16);
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// rlwinm r11,r6,1,0,30
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r5,580(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r4,r5
	ctx.r4.s64 = ctx.r5.s16;
	// cmpw cr6,r6,r4
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r4.s32, ctx.xer);
	// blt cr6,0x822c7924
	if (ctx.cr6.lt) goto loc_822C7924;
loc_822C7984:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r10,176(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 176);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x822c79bc
	if (!ctx.cr6.eq) goto loc_822C79BC;
	// lwz r10,72(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// cmpwi cr6,r10,3
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 3, ctx.xer);
	// bne cr6,0x822c79bc
	if (!ctx.cr6.eq) goto loc_822C79BC;
	// lbz r10,144(r29)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r29.u32 + 144);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822c79b0
	if (ctx.cr6.eq) goto loc_822C79B0;
	// stw r21,372(r11)
	PPC_STORE_U32(ctx.r11.u32 + 372, ctx.r21.u32);
loc_822C79B0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,708(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 708);
	// bl 0x822ef728
	ctx.lr = 0x822C79BC;
	sub_822EF728(ctx, base);
loc_822C79BC:
	// lwz r11,216(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 216);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c7e84
	if (ctx.cr6.eq) goto loc_822C7E84;
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r10,176(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 176);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x822c7da4
	if (!ctx.cr6.eq) goto loc_822C7DA4;
	// lwz r10,392(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 392);
	// lwz r11,388(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x822c7b44
	if (!ctx.cr6.gt) goto loc_822C7B44;
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
	// clrlwi r10,r11,16
	ctx.r10.u64 = ctx.r11.u32 & 0xFFFF;
	// sth r10,0(r18)
	PPC_STORE_U16(ctx.r18.u32 + 0, ctx.r10.u16);
	// b 0x822c7670
	goto loc_822C7670;
loc_822C79F8:
	// lwz r11,164(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 164);
	// stw r16,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r16.u32);
	// stw r16,52(r29)
	PPC_STORE_U32(ctx.r29.u32 + 52, ctx.r16.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x822c7670
	if (!ctx.cr6.gt) goto loc_822C7670;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,164(r29)
	PPC_STORE_U32(ctx.r29.u32 + 164, ctx.r11.u32);
	// b 0x822c7670
	goto loc_822C7670;
loc_822C7A18:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r10,440(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 440);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,440(r11)
	PPC_STORE_U32(ctx.r11.u32 + 440, ctx.r10.u32);
	// stw r28,216(r29)
	PPC_STORE_U32(ctx.r29.u32 + 216, ctx.r28.u32);
	// sth r28,220(r29)
	PPC_STORE_U16(ctx.r29.u32 + 220, ctx.r28.u16);
	// stw r28,380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 380, ctx.r28.u32);
	// stw r28,288(r29)
	PPC_STORE_U32(ctx.r29.u32 + 288, ctx.r28.u32);
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r8,176(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 176);
	// cmpwi cr6,r8,1
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 1, ctx.xer);
	// bne cr6,0x822c7a50
	if (!ctx.cr6.eq) goto loc_822C7A50;
	// sth r28,210(r31)
	PPC_STORE_U16(ctx.r31.u32 + 210, ctx.r28.u16);
	// b 0x822c7a8c
	goto loc_822C7A8C;
loc_822C7A50:
	// lwz r10,392(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 392);
	// lwz r11,388(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x822c7a70
	if (!ctx.cr6.gt) goto loc_822C7A70;
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
	// clrlwi r10,r11,16
	ctx.r10.u64 = ctx.r11.u32 & 0xFFFF;
	// sth r10,0(r18)
	PPC_STORE_U16(ctx.r18.u32 + 0, ctx.r10.u16);
	// b 0x822c7670
	goto loc_822C7670;
loc_822C7A70:
	// lwz r10,468(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 468);
	// srawi r9,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// addze r8,r9
	temp.s64 = ctx.r9.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r8.s64 = temp.s64;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822c7a8c
	if (ctx.cr6.lt) goto loc_822C7A8C;
	// stw r28,392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 392, ctx.r28.u32);
	// stw r28,388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 388, ctx.r28.u32);
loc_822C7A8C:
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c7adc
	if (ctx.cr6.eq) goto loc_822C7ADC;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_822C7A9C:
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// mulli r9,r11,1776
	ctx.r9.s64 = ctx.r11.s64 * 1776;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// sth r28,114(r10)
	PPC_STORE_U16(ctx.r10.u32 + 114, ctx.r28.u16);
	// lwz r9,176(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x822c7ac4
	if (!ctx.cr6.eq) goto loc_822C7AC4;
	// lwz r10,356(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 356);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r28,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, ctx.r28.u32);
loc_822C7AC4:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x822c7a9c
	if (ctx.cr6.lt) goto loc_822C7A9C;
loc_822C7ADC:
	// stw r15,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r15.u32);
	// b 0x822c7670
	goto loc_822C7670;
loc_822C7AE4:
	// lhz r11,16(r29)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r29.u32 + 16);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c7e7c
	if (ctx.cr6.eq) goto loc_822C7E7C;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822f0660
	ctx.lr = 0x822C7AF8;
	sub_822F0660(ctx, base);
	// mr r19,r3
	ctx.r19.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c7e94
	if (ctx.cr6.lt) goto loc_822C7E94;
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c7b3c
	if (ctx.cr6.eq) goto loc_822C7B3C;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_822C7B14:
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// mulli r9,r11,1776
	ctx.r9.s64 = ctx.r11.s64 * 1776;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// sth r28,116(r10)
	PPC_STORE_U16(ctx.r10.u32 + 116, ctx.r28.u16);
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
	// lhz r8,34(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmpw cr6,r9,r8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822c7b14
	if (ctx.cr6.lt) goto loc_822C7B14;
loc_822C7B3C:
	// stw r17,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r17.u32);
	// b 0x822c7670
	goto loc_822C7670;
loc_822C7B44:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// blt cr6,0x822c7e14
	if (ctx.cr6.lt) goto loc_822C7E14;
	// lwz r10,468(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 468);
	// srawi r9,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// addze r8,r9
	temp.s64 = ctx.r9.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r8.s64 = temp.s64;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822c7e14
	if (!ctx.cr6.lt) goto loc_822C7E14;
	// lwz r9,708(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 708);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x822c7b78
	if (ctx.cr6.eq) goto loc_822C7B78;
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// b 0x822c7b88
	goto loc_822C7B88;
loc_822C7B78:
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822e9968
	ctx.lr = 0x822C7B84;
	sub_822E9968(ctx, base);
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_822C7B88:
	// lwz r10,388(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// lwz r11,360(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r8,392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 392, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// ble cr6,0x822c7c4c
	if (!ctx.cr6.gt) goto loc_822C7C4C;
	// lwz r7,708(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 708);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// bne cr6,0x822c7c4c
	if (!ctx.cr6.eq) goto loc_822C7C4C;
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x822c7bd0
	if (ctx.cr6.lt) goto loc_822C7BD0;
	// rotlwi r10,r8,0
	ctx.r10.u64 = rotl32(ctx.r8.u32, 0);
	// subf r9,r9,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r9.s64;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// lwz r8,392(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 392);
	// stw r8,388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 388, ctx.r8.u32);
	// b 0x822c7be0
	goto loc_822C7BE0;
loc_822C7BD0:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 388, ctx.r10.u32);
	// stw r28,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r28.u32);
loc_822C7BE0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822e9ac8
	ctx.lr = 0x822C7BE8;
	sub_822E9AC8(ctx, base);
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// blt cr6,0x822c7c4c
	if (ctx.cr6.lt) goto loc_822C7C4C;
	// lwz r11,380(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 380);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c7c4c
	if (ctx.cr6.eq) goto loc_822C7C4C;
	// lwz r10,444(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 444);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x822c7c18
	if (ctx.cr6.eq) goto loc_822C7C18;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 456);
	// srw r11,r11,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r11.u32 >> (ctx.r10.u8 & 0x3F));
	// b 0x822c7c2c
	goto loc_822C7C2C;
loc_822C7C18:
	// lwz r10,448(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x822c7c2c
	if (ctx.cr6.eq) goto loc_822C7C2C;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 456);
	// slw r11,r11,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r11.u32 << (ctx.r10.u8 & 0x3F));
loc_822C7C2C:
	// lwz r10,392(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 392);
	// lwz r9,388(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r11.s64;
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// bge cr6,0x822c7c4c
	if (!ctx.cr6.lt) goto loc_822C7C4C;
	// stw r28,388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 388, ctx.r28.u32);
	// stw r28,392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 392, ctx.r28.u32);
	// stw r28,380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 380, ctx.r28.u32);
loc_822C7C4C:
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// blt cr6,0x822c7ca4
	if (ctx.cr6.lt) goto loc_822C7CA4;
	// lwz r11,708(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 708);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c7ca4
	if (!ctx.cr6.eq) goto loc_822C7CA4;
	// lwz r11,444(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 444);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c7c80
	if (ctx.cr6.eq) goto loc_822C7C80;
	// lwz r11,380(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 380);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 456);
	// srw r11,r11,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r11.u32 >> (ctx.r10.u8 & 0x3F));
	// b 0x822c7c98
	goto loc_822C7C98;
loc_822C7C80:
	// lwz r11,448(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r11,380(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 380);
	// beq cr6,0x822c7c98
	if (ctx.cr6.eq) goto loc_822C7C98;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 456);
	// slw r11,r11,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r11.u32 << (ctx.r10.u8 & 0x3F));
loc_822C7C98:
	// lwz r10,392(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 392);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	// stw r9,392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 392, ctx.r9.u32);
loc_822C7CA4:
	// lwz r11,392(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 392);
	// lwz r10,388(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x822c7cc0
	if (!ctx.cr6.lt) goto loc_822C7CC0;
loc_822C7CB4:
	// lis r19,-32764
	ctx.r19.s64 = -2147221504;
	// ori r19,r19,2
	ctx.r19.u64 = ctx.r19.u64 | 2;
	// b 0x822c7e94
	goto loc_822C7E94;
loc_822C7CC0:
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// sth r11,0(r18)
	PPC_STORE_U16(ctx.r18.u32 + 0, ctx.r11.u16);
	// lwz r9,76(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 76);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x822c7da4
	if (ctx.cr6.eq) goto loc_822C7DA4;
	// cmplwi cr6,r20,0
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, 0, ctx.xer);
	// bne cr6,0x822c7ce0
	if (!ctx.cr6.eq) goto loc_822C7CE0;
	// addi r20,r1,80
	ctx.r20.s64 = ctx.r1.s64 + 80;
loc_822C7CE0:
	// sth r28,0(r20)
	PPC_STORE_U16(ctx.r20.u32 + 0, ctx.r28.u16);
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bgt cr6,0x822c7d8c
	if (ctx.cr6.gt) goto loc_822C7D8C;
	// lwz r11,320(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lwz r10,444(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 444);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lwz r9,424(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// lhz r5,-2(r8)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r8.u32 + -2);
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// beq cr6,0x822c7d2c
	if (ctx.cr6.eq) goto loc_822C7D2C;
	// lwz r11,456(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 456);
	// extsh r10,r6
	ctx.r10.s64 = ctx.r6.s16;
	// extsh r9,r5
	ctx.r9.s64 = ctx.r5.s16;
	// extsh r8,r11
	ctx.r8.s64 = ctx.r11.s16;
	// sraw r6,r10,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r6.s64 = ctx.r10.s32 >> temp.u32;
	// sraw r5,r9,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r5.s64 = ctx.r9.s32 >> temp.u32;
	// b 0x822c7d58
	goto loc_822C7D58;
loc_822C7D2C:
	// lwz r11,448(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c7d58
	if (ctx.cr6.eq) goto loc_822C7D58;
	// lwz r11,456(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 456);
	// extsh r10,r6
	ctx.r10.s64 = ctx.r6.s16;
	// extsh r9,r5
	ctx.r9.s64 = ctx.r5.s16;
	// extsh r8,r11
	ctx.r8.s64 = ctx.r11.s16;
	// slw r7,r10,r8
	ctx.r7.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r8.u8 & 0x3F));
	// slw r5,r9,r8
	ctx.r5.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
loc_822C7D58:
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// addi r7,r1,82
	ctx.r7.s64 = ctx.r1.s64 + 82;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822ef1a8
	ctx.lr = 0x822C7D6C;
	sub_822EF1A8(ctx, base);
	// lhz r11,0(r20)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r20.u32 + 0);
	// lhz r10,82(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// extsh r8,r11
	ctx.r8.s64 = ctx.r11.s16;
	// lhz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// subf r11,r7,r8
	ctx.r11.s64 = ctx.r8.s64 - ctx.r7.s64;
	// add r6,r11,r9
	ctx.r6.u64 = ctx.r11.u64 + ctx.r9.u64;
	// sth r6,0(r20)
	PPC_STORE_U16(ctx.r20.u32 + 0, ctx.r6.u16);
loc_822C7D8C:
	// stw r28,76(r31)
	PPC_STORE_U32(ctx.r31.u32 + 76, ctx.r28.u32);
	// lhz r11,0(r20)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r20.u32 + 0);
	// ld r10,184(r29)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r29.u32 + 184);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// std r10,184(r29)
	PPC_STORE_U64(ctx.r29.u32 + 184, ctx.r10.u64);
loc_822C7DA4:
	// lwz r11,212(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c7e4c
	if (ctx.cr6.eq) goto loc_822C7E4C;
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c7e4c
	if (ctx.cr6.eq) goto loc_822C7E4C;
	// mr r8,r28
	ctx.r8.u64 = ctx.r28.u64;
loc_822C7DC0:
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// mulli r11,r8,1776
	ctx.r11.s64 = ctx.r8.s64 * 1776;
	// lwz r9,60(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// add r7,r11,r10
	ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r9,2
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 2, ctx.xer);
	// lwz r11,424(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 424);
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// extsh r9,r6
	ctx.r9.s64 = ctx.r6.s16;
	// bgt cr6,0x822c7e20
	if (ctx.cr6.gt) goto loc_822C7E20;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lhz r6,-2(r7)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r7.u32 + -2);
	// sth r6,-2(r10)
	PPC_STORE_U16(ctx.r10.u32 + -2, ctx.r6.u16);
	// lwz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lhzx r4,r5,r9
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r9.u32);
	// sth r4,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, ctx.r4.u16);
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// sth r28,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, ctx.r28.u16);
	// sth r21,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r21.u16);
	// b 0x822c7e34
	goto loc_822C7E34;
loc_822C7E14:
	// lis r19,-32768
	ctx.r19.s64 = -2147483648;
	// ori r19,r19,16389
	ctx.r19.u64 = ctx.r19.u64 | 16389;
	// b 0x822c7e94
	goto loc_822C7E94;
loc_822C7E20:
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lhz r9,-2(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// sth r9,-2(r11)
	PPC_STORE_U16(ctx.r11.u32 + -2, ctx.r9.u16);
loc_822C7E34:
	// addi r11,r8,1
	ctx.r11.s64 = ctx.r8.s64 + 1;
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x822c7dc0
	if (ctx.cr6.lt) goto loc_822C7DC0;
loc_822C7E4C:
	// stw r21,52(r29)
	PPC_STORE_U32(ctx.r29.u32 + 52, ctx.r21.u32);
	// stw r16,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r16.u32);
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x822c7e6c
	if (!ctx.cr6.gt) goto loc_822C7E6C;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822ee350
	ctx.lr = 0x822C7E6C;
	sub_822EE350(ctx, base);
loc_822C7E6C:
	// lhz r11,16(r29)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r29.u32 + 16);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bgt cr6,0x822c7e94
	if (ctx.cr6.gt) goto loc_822C7E94;
loc_822C7E7C:
	// mr r19,r17
	ctx.r19.u64 = ctx.r17.u64;
	// b 0x822c7e94
	goto loc_822C7E94;
loc_822C7E84:
	// stw r17,32(r29)
	PPC_STORE_U32(ctx.r29.u32 + 32, ctx.r17.u32);
	// b 0x822c7e94
	goto loc_822C7E94;
loc_822C7E8C:
	// lis r19,-32761
	ctx.r19.s64 = -2147024896;
	// ori r19,r19,87
	ctx.r19.u64 = ctx.r19.u64 | 87;
loc_822C7E94:
	// cmplwi cr6,r18,0
	ctx.cr6.compare<uint32_t>(ctx.r18.u32, 0, ctx.xer);
	// beq cr6,0x822c7ec0
	if (ctx.cr6.eq) goto loc_822C7EC0;
	// lhz r11,0(r18)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r18.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c7ec0
	if (ctx.cr6.eq) goto loc_822C7EC0;
	// stw r15,692(r29)
	PPC_STORE_U32(ctx.r29.u32 + 692, ctx.r15.u32);
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// lhz r11,0(r18)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r18.u32 + 0);
	// stw r11,700(r29)
	PPC_STORE_U32(ctx.r29.u32 + 700, ctx.r11.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8233e484
	__restgprlr_15(ctx, base);
	return;
loc_822C7EC0:
	// stw r16,692(r29)
	PPC_STORE_U32(ctx.r29.u32 + 692, ctx.r16.u32);
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8233e484
	__restgprlr_15(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C7ED0"))) PPC_WEAK_FUNC(sub_822C7ED0);
PPC_FUNC_IMPL(__imp__sub_822C7ED0) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// stw r4,328(r3)
	PPC_STORE_U32(ctx.r3.u32 + 328, ctx.r4.u32);
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// stw r5,332(r3)
	PPC_STORE_U32(ctx.r3.u32 + 332, ctx.r5.u32);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x822c7f04
	if (!ctx.cr6.gt) goto loc_822C7F04;
loc_822C7EE8:
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x822c7efc
	if (!ctx.cr6.lt) goto loc_822C7EFC;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_822C7EFC:
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bgt 0x822c7ee8
	if (ctx.cr0.gt) goto loc_822C7EE8;
loc_822C7F04:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// divw r10,r4,r11
	ctx.r10.s32 = ctx.r4.s32 / ctx.r11.s32;
	// divw r9,r5,r11
	ctx.r9.s32 = ctx.r5.s32 / ctx.r11.s32;
	// stw r10,328(r3)
	PPC_STORE_U32(ctx.r3.u32 + 328, ctx.r10.u32);
	// stw r9,332(r3)
	PPC_STORE_U32(ctx.r3.u32 + 332, ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822C7F20"))) PPC_WEAK_FUNC(sub_822C7F20);
PPC_FUNC_IMPL(__imp__sub_822C7F20) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r10,316(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 316);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x822c7f44
	if (ctx.cr6.eq) goto loc_822C7F44;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x822c7f44
	if (!ctx.cr6.gt) goto loc_822C7F44;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// blr 
	return;
loc_822C7F44:
	// lwz r10,324(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 324);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blelr cr6
	if (!ctx.cr6.gt) return;
	// lwz r10,332(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 332);
	// lwz r9,340(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 340);
	// mullw r8,r10,r3
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// lwz r7,328(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 328);
	// subf r6,r9,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r9.s64;
	// divw r11,r6,r7
	ctx.r11.s32 = ctx.r6.s32 / ctx.r7.s32;
	// addi r3,r11,1
	ctx.r3.s64 = ctx.r11.s64 + 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822C7F78"))) PPC_WEAK_FUNC(sub_822C7F78);
PPC_FUNC_IMPL(__imp__sub_822C7F78) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e43c
	ctx.lr = 0x822C7F80;
	__restfpr_17(ctx, base);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r19,r3
	ctx.r19.u64 = ctx.r3.u64;
	// lwz r28,360(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 360);
	// mr r21,r4
	ctx.r21.u64 = ctx.r4.u64;
	// lhz r11,0(r5)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// mr r18,r5
	ctx.r18.u64 = ctx.r5.u64;
	// lwz r10,88(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// divwu r9,r6,r10
	ctx.r9.u32 = ctx.r6.u32 / ctx.r10.u32;
	// divwu r8,r9,r28
	ctx.r8.u32 = ctx.r9.u32 / ctx.r28.u32;
	// rlwinm r7,r8,31,1,31
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// cmplw cr6,r7,r11
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x822c7fc4
	if (!ctx.cr6.lt) goto loc_822C7FC4;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
loc_822C7FC4:
	// li r25,0
	ctx.r25.s64 = 0;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x822c8128
	if (!ctx.cr6.gt) goto loc_822C8128;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// li r22,0
	ctx.r22.s64 = 0;
	// mullw r23,r10,r28
	ctx.r23.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r28.s32);
	// mullw r24,r11,r28
	ctx.r24.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r28.s32);
loc_822C7FE8:
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r10,524(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// mullw r11,r24,r4
	ctx.r11.s64 = int64_t(ctx.r24.s32) * int64_t(ctx.r4.s32);
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// add r27,r11,r21
	ctx.r27.u64 = ctx.r11.u64 + ctx.r21.u64;
	// mullw r11,r23,r4
	ctx.r11.s64 = int64_t(ctx.r23.s32) * int64_t(ctx.r4.s32);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// add r29,r11,r21
	ctx.r29.u64 = ctx.r11.u64 + ctx.r21.u64;
	// bctrl 
	ctx.lr = 0x822C8014;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,88(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// add r8,r25,r28
	ctx.r8.u64 = ctx.r25.u64 + ctx.r28.u64;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mullw r11,r8,r9
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// add r26,r11,r21
	ctx.r26.u64 = ctx.r11.u64 + ctx.r21.u64;
	// mr r20,r3
	ctx.r20.u64 = ctx.r3.u64;
	// cmplw cr6,r29,r26
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r26.u32, ctx.xer);
	// ble cr6,0x822c80b8
	if (!ctx.cr6.gt) goto loc_822C80B8;
loc_822C8034:
	// lwz r11,520(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 520);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C8050;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r10,524(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// mr r17,r30
	ctx.r17.u64 = ctx.r30.u64;
	// mullw r9,r4,r28
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r28.s32);
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// subf r27,r9,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r9.s64;
	// subf r29,r9,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r9.s64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bctrl 
	ctx.lr = 0x822C807C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r8,520(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 520);
	// srawi r10,r17,1
	ctx.xer.ca = (ctx.r17.s32 < 0) & ((ctx.r17.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r17.s32 >> 1;
	// srawi r11,r3,1
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r3.s32 >> 1;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C80A4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r7,88(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mullw r6,r28,r7
	ctx.r6.s64 = int64_t(ctx.r28.s32) * int64_t(ctx.r7.s32);
	// subf r29,r6,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r6.s64;
	// cmplw cr6,r29,r26
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r26.u32, ctx.xer);
	// bgt cr6,0x822c8034
	if (ctx.cr6.gt) goto loc_822C8034;
loc_822C80B8:
	// lwz r11,520(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 520);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C80D4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r10,344(r19)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r19.u32 + 344);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r9,88(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// lwz r8,520(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 520);
	// mullw r7,r28,r9
	ctx.r7.s64 = int64_t(ctx.r28.s32) * int64_t(ctx.r9.s32);
	// lwzx r4,r22,r10
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r10.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// srawi r11,r4,1
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r4.s32 >> 1;
	// srawi r10,r30,1
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r30.s32 >> 1;
	// subf r4,r7,r29
	ctx.r4.s64 = ctx.r29.s64 - ctx.r7.s64;
	// add r3,r11,r10
	ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x822C8108;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r3,344(r19)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r19.u32 + 344);
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// addi r24,r24,1
	ctx.r24.s64 = ctx.r24.s64 + 1;
	// addi r23,r23,1
	ctx.r23.s64 = ctx.r23.s64 + 1;
	// cmpw cr6,r25,r28
	ctx.cr6.compare<int32_t>(ctx.r25.s32, ctx.r28.s32, ctx.xer);
	// stwx r20,r22,r3
	PPC_STORE_U32(ctx.r22.u32 + ctx.r3.u32, ctx.r20.u32);
	// addi r22,r22,4
	ctx.r22.s64 = ctx.r22.s64 + 4;
	// blt cr6,0x822c7fe8
	if (ctx.cr6.lt) goto loc_822C7FE8;
loc_822C8128:
	// lhz r11,0(r18)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r18.u32 + 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r10,r11,1,16,30
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFE;
	// sth r10,0(r18)
	PPC_STORE_U16(ctx.r18.u32 + 0, ctx.r10.u16);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C8140"))) PPC_WEAK_FUNC(sub_822C8140);
PPC_FUNC_IMPL(__imp__sub_822C8140) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x822C8148;
	__restfpr_14(ctx, base);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r24,332(r3)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 332);
	// mr r21,r3
	ctx.r21.u64 = ctx.r3.u64;
	// lhz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// mr r20,r4
	ctx.r20.u64 = ctx.r4.u64;
	// lwz r11,340(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// mr r17,r5
	ctx.r17.u64 = ctx.r5.u64;
	// mullw r10,r10,r24
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r24.s32);
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r30,360(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 360);
	// lwz r15,328(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 328);
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x822c81f4
	if (!ctx.cr6.lt) goto loc_822C81F4;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// stw r11,340(r3)
	PPC_STORE_U32(ctx.r3.u32 + 340, ctx.r11.u32);
	// lhz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822c81e0
	if (ctx.cr6.eq) goto loc_822C81E0;
	// li r29,0
	ctx.r29.s64 = 0;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x822c81e0
	if (!ctx.cr6.gt) goto loc_822C81E0;
	// li r28,0
	ctx.r28.s64 = 0;
loc_822C81A0:
	// lhz r11,0(r17)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r17.u32 + 0);
	// mr r3,r20
	ctx.r3.u64 = ctx.r20.u64;
	// lwz r10,524(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// addi r9,r11,-1
	ctx.r9.s64 = ctx.r11.s64 + -1;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mullw r11,r9,r30
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r30.s32);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// add r6,r11,r29
	ctx.r6.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bctrl 
	ctx.lr = 0x822C81C8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r8,344(r21)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r21.u32 + 344);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// cmpw cr6,r29,r30
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r30.s32, ctx.xer);
	// stwx r3,r8,r28
	PPC_STORE_U32(ctx.r8.u32 + ctx.r28.u32, ctx.r3.u32);
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// blt cr6,0x822c81a0
	if (ctx.cr6.lt) goto loc_822C81A0;
loc_822C81E0:
	// li r11,0
	ctx.r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// sth r11,0(r17)
	PPC_STORE_U16(ctx.r17.u32 + 0, ctx.r11.u16);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
loc_822C81F4:
	// lwz r9,88(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r11.s64;
	// divw r26,r8,r15
	ctx.r26.s32 = ctx.r8.s32 / ctx.r15.s32;
	// divw r7,r6,r9
	ctx.r7.s32 = ctx.r6.s32 / ctx.r9.s32;
	// addi r14,r26,1
	ctx.r14.s64 = ctx.r26.s64 + 1;
	// divw r6,r7,r30
	ctx.r6.s32 = ctx.r7.s32 / ctx.r30.s32;
	// cmpw cr6,r6,r14
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r14.s32, ctx.xer);
	// bge cr6,0x822c8224
	if (!ctx.cr6.lt) goto loc_822C8224;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
loc_822C8224:
	// mullw r10,r26,r15
	ctx.r10.s64 = int64_t(ctx.r26.s32) * int64_t(ctx.r15.s32);
	// add r16,r10,r11
	ctx.r16.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// divw r27,r16,r24
	ctx.r27.s32 = ctx.r16.s32 / ctx.r24.s32;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x822c8280
	if (!ctx.cr6.gt) goto loc_822C8280;
	// li r28,0
	ctx.r28.s64 = 0;
loc_822C8240:
	// lhz r11,0(r17)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r17.u32 + 0);
	// mr r3,r20
	ctx.r3.u64 = ctx.r20.u64;
	// lwz r10,524(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// addi r9,r11,-1
	ctx.r9.s64 = ctx.r11.s64 + -1;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mullw r11,r9,r30
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r30.s32);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// add r6,r11,r29
	ctx.r6.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bctrl 
	ctx.lr = 0x822C8268;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r8,348(r21)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r21.u32 + 348);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// cmpw cr6,r29,r30
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r30.s32, ctx.xer);
	// stwx r3,r28,r8
	PPC_STORE_U32(ctx.r28.u32 + ctx.r8.u32, ctx.r3.u32);
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// blt cr6,0x822c8240
	if (ctx.cr6.lt) goto loc_822C8240;
loc_822C8280:
	// lwz r11,88(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// cmpw cr6,r27,r26
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r26.s32, ctx.xer);
	// mullw r10,r11,r27
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r27.s32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r30.s32);
	// add r25,r10,r20
	ctx.r25.u64 = ctx.r10.u64 + ctx.r20.u64;
	// mr r10,r27
	ctx.r10.u64 = ctx.r27.u64;
	// bgt cr6,0x822c82a0
	if (ctx.cr6.gt) goto loc_822C82A0;
	// mr r10,r26
	ctx.r10.u64 = ctx.r26.u64;
loc_822C82A0:
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r30.s32);
	// add r22,r10,r20
	ctx.r22.u64 = ctx.r10.u64 + ctx.r20.u64;
	// li r18,0
	ctx.r18.s64 = 0;
	// cmpw cr6,r27,r26
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r26.s32, ctx.xer);
	// ble cr6,0x822c82bc
	if (!ctx.cr6.gt) goto loc_822C82BC;
	// subf r18,r26,r27
	ctx.r18.s64 = ctx.r27.s64 - ctx.r26.s64;
loc_822C82BC:
	// mullw r11,r11,r30
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r30.s32);
	// add r19,r11,r20
	ctx.r19.u64 = ctx.r11.u64 + ctx.r20.u64;
	// mullw r11,r27,r24
	ctx.r11.s64 = int64_t(ctx.r27.s32) * int64_t(ctx.r24.s32);
	// subf r27,r11,r16
	ctx.r27.s64 = ctx.r16.s64 - ctx.r11.s64;
	// cmplw cr6,r25,r19
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, ctx.r19.u32, ctx.xer);
	// blt cr6,0x822c83ac
	if (ctx.cr6.lt) goto loc_822C83AC;
loc_822C82D4:
	// li r29,0
	ctx.r29.s64 = 0;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x822c836c
	if (!ctx.cr6.gt) goto loc_822C836C;
	// subf r23,r27,r24
	ctx.r23.s64 = ctx.r24.s64 - ctx.r27.s64;
	// neg r28,r30
	ctx.r28.s64 = -ctx.r30.s64;
loc_822C82E8:
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// beq cr6,0x822c8314
	if (ctx.cr6.eq) goto loc_822C8314;
	// lwz r11,524(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C830C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// b 0x822c8318
	goto loc_822C8318;
loc_822C8314:
	// li r26,0
	ctx.r26.s64 = 0;
loc_822C8318:
	// lwz r11,524(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C8334;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r10,520(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 520);
	// mullw r11,r3,r23
	ctx.r11.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r23.s32);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// mullw r10,r26,r27
	ctx.r10.s64 = int64_t(ctx.r26.s32) * int64_t(ctx.r27.s32);
	// add r9,r11,r10
	ctx.r9.u64 = ctx.r11.u64 + ctx.r10.u64;
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// divw r3,r9,r24
	ctx.r3.s32 = ctx.r9.s32 / ctx.r24.s32;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// bctrl 
	ctx.lr = 0x822C835C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// cmpw cr6,r29,r30
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r30.s32, ctx.xer);
	// blt cr6,0x822c82e8
	if (ctx.cr6.lt) goto loc_822C82E8;
loc_822C836C:
	// subf. r27,r15,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r15.s64;
	ctx.cr0.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// bgt 0x822c8398
	if (ctx.cr0.gt) goto loc_822C8398;
	// subf r11,r24,r27
	ctx.r11.s64 = ctx.r27.s64 - ctx.r24.s64;
	// lwz r10,88(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// divw r8,r9,r24
	ctx.r8.s32 = ctx.r9.s32 / ctx.r24.s32;
	// mullw r7,r10,r8
	ctx.r7.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// mullw r6,r8,r24
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r24.s32);
	// mullw r11,r7,r30
	ctx.r11.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r30.s32);
	// subf r27,r6,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r6.s64;
	// add r25,r11,r25
	ctx.r25.u64 = ctx.r11.u64 + ctx.r25.u64;
loc_822C8398:
	// lwz r11,88(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// cmplw cr6,r25,r19
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, ctx.r19.u32, ctx.xer);
	// mullw r10,r11,r30
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r30.s32);
	// subf r22,r10,r22
	ctx.r22.s64 = ctx.r22.s64 - ctx.r10.s64;
	// bge cr6,0x822c82d4
	if (!ctx.cr6.lt) goto loc_822C82D4;
loc_822C83AC:
	// lwz r11,340(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 340);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x822c8430
	if (!ctx.cr6.gt) goto loc_822C8430;
	// cmpw cr6,r11,r24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r24.s32, ctx.xer);
	// bge cr6,0x822c8430
	if (!ctx.cr6.lt) goto loc_822C8430;
	// li r29,0
	ctx.r29.s64 = 0;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x822c8458
	if (!ctx.cr6.gt) goto loc_822C8458;
	// subf r26,r27,r24
	ctx.r26.s64 = ctx.r24.s64 - ctx.r27.s64;
	// li r28,0
	ctx.r28.s64 = 0;
loc_822C83D4:
	// lwz r11,524(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r3,r20
	ctx.r3.u64 = ctx.r20.u64;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C83F0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,344(r21)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r21.u32 + 344);
	// mullw r10,r3,r27
	ctx.r10.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r27.s32);
	// lwz r8,520(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 520);
	// lwzx r7,r9,r28
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r28.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// mullw r11,r7,r26
	ctx.r11.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r26.s32);
	// add r3,r11,r10
	ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// divw r3,r3,r24
	ctx.r3.s32 = ctx.r3.s32 / ctx.r24.s32;
	// bctrl 
	ctx.lr = 0x822C8420;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// cmpw cr6,r29,r30
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r30.s32, ctx.xer);
	// blt cr6,0x822c83d4
	if (ctx.cr6.lt) goto loc_822C83D4;
loc_822C8430:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x822c8458
	if (!ctx.cr6.gt) goto loc_822C8458;
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822C8440:
	// lwz r10,348(r21)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r21.u32 + 348);
	// lwz r9,344(r21)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r21.u32 + 344);
	// lwzx r8,r11,r10
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// stwx r8,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, ctx.r8.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822c8440
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822C8440;
loc_822C8458:
	// lhz r11,0(r17)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r17.u32 + 0);
	// cmpwi cr6,r18,0
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 0, ctx.xer);
	// mullw r10,r11,r24
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r24.s32);
	// subf r11,r10,r16
	ctx.r11.s64 = ctx.r16.s64 - ctx.r10.s64;
	// add r9,r11,r15
	ctx.r9.u64 = ctx.r11.u64 + ctx.r15.u64;
	// stw r9,340(r21)
	PPC_STORE_U32(ctx.r21.u32 + 340, ctx.r9.u32);
	// beq cr6,0x822c8494
	if (ctx.cr6.eq) goto loc_822C8494;
	// lwz r10,88(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mr r3,r20
	ctx.r3.u64 = ctx.r20.u64;
	// mullw r9,r10,r18
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r18.s32);
	// mullw r11,r9,r30
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r30.s32);
	// mullw r8,r14,r10
	ctx.r8.s64 = int64_t(ctx.r14.s32) * int64_t(ctx.r10.s32);
	// mullw r5,r8,r30
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r30.s32);
	// add r4,r11,r20
	ctx.r4.u64 = ctx.r11.u64 + ctx.r20.u64;
	// bl 0x8233e4e0
	ctx.lr = 0x822C8494;
	sub_8233E4E0(ctx, base);
loc_822C8494:
	// sth r14,0(r17)
	PPC_STORE_U16(ctx.r17.u32 + 0, ctx.r14.u16);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C84A4"))) PPC_WEAK_FUNC(sub_822C84A4);
PPC_FUNC_IMPL(__imp__sub_822C84A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C84A8"))) PPC_WEAK_FUNC(sub_822C84A8);
PPC_FUNC_IMPL(__imp__sub_822C84A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e444
	ctx.lr = 0x822C84B0;
	__restfpr_19(ctx, base);
	// addi r12,r1,-112
	ctx.r12.s64 = ctx.r1.s64 + -112;
	// bl 0x8233fa34
	ctx.lr = 0x822C84B8;
	sub_8233FA34(ctx, base);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lwz r10,352(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 352);
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// lwz r19,360(r3)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r3.u32 + 360);
	// mr r27,r6
	ctx.r27.u64 = ctx.r6.u64;
	// lwz r31,384(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 384);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lhz r20,34(r11)
	ctx.r20.u64 = PPC_LOAD_U16(ctx.r11.u32 + 34);
	// beq cr6,0x822c8ae4
	if (ctx.cr6.eq) goto loc_822C8AE4;
	// lwz r11,424(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 424);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c84f4
	if (ctx.cr6.eq) goto loc_822C84F4;
	// li r19,6
	ctx.r19.s64 = 6;
loc_822C84F4:
	// cmpwi cr6,r20,6
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 6, ctx.xer);
	// bne cr6,0x822c8724
	if (!ctx.cr6.eq) goto loc_822C8724;
	// cmpwi cr6,r19,2
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 2, ctx.xer);
	// bne cr6,0x822c8724
	if (!ctx.cr6.eq) goto loc_822C8724;
	// lwz r10,372(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lfs f0,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,12(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,16(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,20(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	ctx.f3.f64 = double(temp.f32);
	// blt cr6,0x822c86a4
	if (ctx.cr6.lt) goto loc_822C86A4;
	// addi r10,r5,-3
	ctx.r10.s64 = ctx.r5.s64 + -3;
loc_822C8550:
	// lfs f2,16(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 16);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f2,f10
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// lfs f31,20(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 20);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f2,f2,f4
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// lfs f30,12(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,8(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,4(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,0(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f1,f31,f9,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f9.f64), float(ctx.f1.f64)));
	// fmadds f2,f31,f3,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f3.f64), float(ctx.f2.f64)));
	// fmadds f1,f30,f11,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f11.f64), float(ctx.f1.f64)));
	// fmadds f2,f30,f5,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f5.f64), float(ctx.f2.f64)));
	// fmadds f1,f29,f12,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f12.f64), float(ctx.f1.f64)));
	// fmadds f2,f29,f6,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f6.f64), float(ctx.f2.f64)));
	// fmadds f1,f28,f13,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f13.f64), float(ctx.f1.f64)));
	// fmadds f2,f28,f7,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f7.f64), float(ctx.f2.f64)));
	// fmadds f1,f27,f0,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f0.f64), float(ctx.f1.f64)));
	// stfs f1,0(r27)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// fmadds f2,f27,f8,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f8.f64), float(ctx.f2.f64)));
	// stfs f2,4(r27)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r27.u32 + 4, temp.u32);
	// lfs f1,44(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 44);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,36(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 36);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,32(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 32);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,40(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 40);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f29,f30,f10
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// fmuls f30,f30,f4
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// lfs f28,24(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 24);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f29,f1,f9,f29
	ctx.f29.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f9.f64), float(ctx.f29.f64)));
	// lfs f27,28(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 28);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f1,f1,f3,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f3.f64), float(ctx.f30.f64)));
	// fmadds f30,f2,f11,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f11.f64), float(ctx.f29.f64)));
	// fmadds f2,f2,f5,f1
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f5.f64), float(ctx.f1.f64)));
	// fmadds f1,f31,f12,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f12.f64), float(ctx.f30.f64)));
	// fmadds f2,f31,f6,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f6.f64), float(ctx.f2.f64)));
	// fmadds f1,f27,f13,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f13.f64), float(ctx.f1.f64)));
	// fmadds f2,f27,f7,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f7.f64), float(ctx.f2.f64)));
	// fmadds f1,f28,f0,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f0.f64), float(ctx.f1.f64)));
	// stfs f1,8(r27)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r27.u32 + 8, temp.u32);
	// fmadds f2,f28,f8,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f8.f64), float(ctx.f2.f64)));
	// stfs f2,12(r27)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r27.u32 + 12, temp.u32);
	// lfs f1,68(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 68);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,60(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 60);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,56(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 56);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,64(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 64);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f29,f30,f10
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// fmuls f30,f30,f4
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// lfs f28,48(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 48);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f29,f1,f9,f29
	ctx.f29.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f9.f64), float(ctx.f29.f64)));
	// lfs f27,52(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 52);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f1,f1,f3,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f3.f64), float(ctx.f30.f64)));
	// fmadds f30,f2,f11,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f11.f64), float(ctx.f29.f64)));
	// fmadds f2,f2,f5,f1
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f5.f64), float(ctx.f1.f64)));
	// fmadds f1,f31,f12,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f12.f64), float(ctx.f30.f64)));
	// fmadds f2,f31,f6,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f6.f64), float(ctx.f2.f64)));
	// fmadds f1,f27,f13,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f13.f64), float(ctx.f1.f64)));
	// fmadds f2,f27,f7,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f7.f64), float(ctx.f2.f64)));
	// fmadds f1,f28,f0,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f0.f64), float(ctx.f1.f64)));
	// stfs f1,16(r27)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16, temp.u32);
	// fmadds f2,f28,f8,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f8.f64), float(ctx.f2.f64)));
	// stfs f2,20(r27)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r27.u32 + 20, temp.u32);
	// lfs f1,92(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 92);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,84(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,80(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 80);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,88(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 88);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f29,f30,f10
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// fmuls f30,f30,f4
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// lfs f28,72(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 72);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f29,f1,f9,f29
	ctx.f29.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f9.f64), float(ctx.f29.f64)));
	// lfs f27,76(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 76);
	ctx.f27.f64 = double(temp.f32);
	// addi r29,r29,96
	ctx.r29.s64 = ctx.r29.s64 + 96;
	// fmadds f1,f1,f3,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f3.f64), float(ctx.f30.f64)));
	// fmadds f30,f2,f11,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f11.f64), float(ctx.f29.f64)));
	// fmadds f2,f2,f5,f1
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f5.f64), float(ctx.f1.f64)));
	// fmadds f1,f31,f12,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f12.f64), float(ctx.f30.f64)));
	// fmadds f2,f31,f6,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f6.f64), float(ctx.f2.f64)));
	// fmadds f1,f27,f13,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f13.f64), float(ctx.f1.f64)));
	// fmadds f2,f27,f7,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f7.f64), float(ctx.f2.f64)));
	// fmadds f1,f28,f0,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f0.f64), float(ctx.f1.f64)));
	// stfs f1,24(r27)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r27.u32 + 24, temp.u32);
	// fmadds f2,f28,f8,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f8.f64), float(ctx.f2.f64)));
	// stfs f2,28(r27)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r27.u32 + 28, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r27,r27,32
	ctx.r27.s64 = ctx.r27.s64 + 32;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x822c8550
	if (ctx.cr6.lt) goto loc_822C8550;
loc_822C86A4:
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// bge cr6,0x822c8ae4
	if (!ctx.cr6.lt) goto loc_822C8AE4;
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r11.s64;
	// addi r11,r29,-4
	ctx.r11.s64 = ctx.r29.s64 + -4;
	// addi r10,r27,-4
	ctx.r10.s64 = ctx.r27.s64 + -4;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822C86BC:
	// lfs f2,20(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f2,f10
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// lfs f31,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f30,f2,f4
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// lfs f29,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f27.f64 = double(temp.f32);
	// lfsu f2,24(r11)
	ea = 24 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f2.f64 = double(temp.f32);
	// fmadds f1,f2,f9,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f9.f64), float(ctx.f1.f64)));
	// fmadds f2,f2,f3,f30
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f3.f64), float(ctx.f30.f64)));
	// fmadds f1,f27,f11,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f11.f64), float(ctx.f1.f64)));
	// fmadds f2,f27,f5,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f5.f64), float(ctx.f2.f64)));
	// fmadds f1,f28,f12,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f12.f64), float(ctx.f1.f64)));
	// fmadds f2,f28,f6,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f6.f64), float(ctx.f2.f64)));
	// fmadds f1,f29,f13,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f13.f64), float(ctx.f1.f64)));
	// fmadds f2,f29,f7,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f7.f64), float(ctx.f2.f64)));
	// fmadds f1,f31,f0,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f0.f64), float(ctx.f1.f64)));
	// stfs f1,4(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// fmadds f2,f31,f8,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f8.f64), float(ctx.f2.f64)));
	// stfsu f2,8(r10)
	temp.f32 = float(ctx.f2.f64);
	ea = 8 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822c86bc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822C86BC;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// addi r12,r1,-112
	ctx.r12.s64 = ctx.r1.s64 + -112;
	// bl 0x8233fa80
	ctx.lr = 0x822C8720;
	__savefpr_27(ctx, base);
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
loc_822C8724:
	// cmpw cr6,r20,r19
	ctx.cr6.compare<int32_t>(ctx.r20.s32, ctx.r19.s32, ctx.xer);
	// blt cr6,0x822c8904
	if (ctx.cr6.lt) goto loc_822C8904;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// ble cr6,0x822c8ae4
	if (!ctx.cr6.gt) goto loc_822C8AE4;
	// neg r11,r19
	ctx.r11.s64 = -ctx.r19.s64;
	// neg r10,r20
	ctx.r10.s64 = -ctx.r20.s64;
	// rlwinm r23,r20,2,0,29
	ctx.r23.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r24,r19,2,0,29
	ctx.r24.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r22,r11,2,0,29
	ctx.r22.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r21,r10,2,0,29
	ctx.r21.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r26,r6,r31
	ctx.r26.s64 = ctx.r31.s64 - ctx.r6.s64;
	// subfic r28,r4,-8
	ctx.xer.ca = ctx.r4.u32 <= 4294967288;
	ctx.r28.s64 = -8 - ctx.r4.s64;
	// mr r25,r5
	ctx.r25.u64 = ctx.r5.u64;
loc_822C8758:
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822C8768;
	sub_8233EAF0(ctx, base);
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// ble cr6,0x822c8868
	if (!ctx.cr6.gt) goto loc_822C8868;
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
loc_822C8778:
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r20,4
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 4, ctx.xer);
	// blt cr6,0x822c8824
	if (ctx.cr6.lt) goto loc_822C8824;
	// addi r5,r20,-3
	ctx.r5.s64 = ctx.r20.s64 + -3;
	// li r9,8
	ctx.r9.s64 = 8;
	// addi r10,r29,8
	ctx.r10.s64 = ctx.r29.s64 + 8;
	// addi r4,r28,12
	ctx.r4.s64 = ctx.r28.s64 + 12;
loc_822C8794:
	// lwz r6,372(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// add r7,r28,r10
	ctx.r7.u64 = ctx.r28.u64 + ctx.r10.u64;
	// lfs f0,-8(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lfsx f13,r11,r31
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	ctx.f13.f64 = double(temp.f32);
	// cmpw cr6,r8,r5
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, ctx.xer);
	// lwzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// lfsx f12,r6,r7
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f12,f0,f13
	ctx.f11.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfsx f11,r11,r31
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// lwz r6,372(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// lfs f10,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f10.f64 = double(temp.f32);
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lwzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// lfs f8,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f7,f8,f10,f11
	ctx.f7.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f10.f64), float(ctx.f11.f64)));
	// stfsx f7,r11,r31
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// lwz r6,372(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// lfs f6,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fmr f5,f7
	ctx.f5.f64 = ctx.f7.f64;
	// lwzx r7,r6,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// lfsx f4,r7,r9
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f3,f4,f6,f7
	ctx.f3.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f6.f64), float(ctx.f7.f64)));
	// stfsx f3,r11,r31
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// lwz r6,372(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// fmr f1,f3
	ctx.f1.f64 = ctx.f3.f64;
	// lfs f2,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// lwzx r7,r6,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// lfsx f0,r7,r4
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r4.u32);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f13,f0,f2,f3
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f2.f64), float(ctx.f3.f64)));
	// stfsx f13,r11,r31
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// blt cr6,0x822c8794
	if (ctx.cr6.lt) goto loc_822C8794;
loc_822C8824:
	// cmpw cr6,r8,r20
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r20.s32, ctx.xer);
	// bge cr6,0x822c885c
	if (!ctx.cr6.lt) goto loc_822C885C;
	// subf r9,r8,r20
	ctx.r9.s64 = ctx.r20.s64 - ctx.r8.s64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822C8838:
	// lwz r9,372(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// lfsx f0,r10,r29
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r29.u32);
	ctx.f0.f64 = double(temp.f32);
	// lfsx f13,r11,r31
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	ctx.f13.f64 = double(temp.f32);
	// lwzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// lfsx f12,r8,r10
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fmadds f11,f12,f0,f13
	ctx.f11.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfsx f11,r11,r31
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// bdnz 0x822c8838
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822C8838;
loc_822C885C:
	// addic. r3,r3,-1
	ctx.xer.ca = ctx.r3.u32 > 0;
	ctx.r3.s64 = ctx.r3.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bne 0x822c8778
	if (!ctx.cr0.eq) goto loc_822C8778;
loc_822C8868:
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r19,4
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 4, ctx.xer);
	// blt cr6,0x822c88b0
	if (ctx.cr6.lt) goto loc_822C88B0;
	// addi r8,r19,-3
	ctx.r8.s64 = ctx.r19.s64 + -3;
	// addi r10,r31,-4
	ctx.r10.s64 = ctx.r31.s64 + -4;
	// addi r11,r27,4
	ctx.r11.s64 = ctx.r27.s64 + 4;
loc_822C8880:
	// lfs f0,4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stfs f0,-4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lfsx f13,r26,r11
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r11.u32);
	ctx.f13.f64 = double(temp.f32);
	// cmpw cr6,r9,r8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, ctx.xer);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfs f12,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfsu f0,16(r10)
	ea = 16 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// blt cr6,0x822c8880
	if (ctx.cr6.lt) goto loc_822C8880;
loc_822C88B0:
	// cmpw cr6,r9,r19
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r19.s32, ctx.xer);
	// bge cr6,0x822c88d8
	if (!ctx.cr6.lt) goto loc_822C88D8;
	// subf r10,r9,r19
	ctx.r10.s64 = ctx.r19.s64 - ctx.r9.s64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822C88C8:
	// lfsx f0,r26,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822c88c8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822C88C8;
loc_822C88D8:
	// addic. r25,r25,-1
	ctx.xer.ca = ctx.r25.u32 > 0;
	ctx.r25.s64 = ctx.r25.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// add r29,r23,r29
	ctx.r29.u64 = ctx.r23.u64 + ctx.r29.u64;
	// add r28,r28,r21
	ctx.r28.u64 = ctx.r28.u64 + ctx.r21.u64;
	// add r27,r24,r27
	ctx.r27.u64 = ctx.r24.u64 + ctx.r27.u64;
	// add r26,r26,r22
	ctx.r26.u64 = ctx.r26.u64 + ctx.r22.u64;
	// bne 0x822c8758
	if (!ctx.cr0.eq) goto loc_822C8758;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// addi r12,r1,-112
	ctx.r12.s64 = ctx.r1.s64 + -112;
	// bl 0x8233fa80
	ctx.lr = 0x822C8900;
	__savefpr_27(ctx, base);
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
loc_822C8904:
	// addi r11,r5,-1
	ctx.r11.s64 = ctx.r5.s64 + -1;
	// mullw r10,r11,r20
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r20.s32);
	// mullw r9,r11,r19
	ctx.r9.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r19.s32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r10,r4
	ctx.r28.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r26,r9,r6
	ctx.r26.u64 = ctx.r9.u64 + ctx.r6.u64;
	// mr r25,r11
	ctx.r25.u64 = ctx.r11.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// blt cr6,0x822c8ae4
	if (ctx.cr6.lt) goto loc_822C8AE4;
	// neg r11,r19
	ctx.r11.s64 = -ctx.r19.s64;
	// neg r10,r20
	ctx.r10.s64 = -ctx.r20.s64;
	// rlwinm r23,r20,2,0,29
	ctx.r23.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r24,r19,2,0,29
	ctx.r24.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r22,r11,2,0,29
	ctx.r22.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r21,r10,2,0,29
	ctx.r21.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r27,r26,r31
	ctx.r27.s64 = ctx.r31.s64 - ctx.r26.s64;
	// subfic r29,r28,-8
	ctx.xer.ca = ctx.r28.u32 <= 4294967288;
	ctx.r29.s64 = -8 - ctx.r28.s64;
loc_822C894C:
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822C895C;
	sub_8233EAF0(ctx, base);
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// ble cr6,0x822c8a5c
	if (!ctx.cr6.gt) goto loc_822C8A5C;
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
loc_822C896C:
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r20,4
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 4, ctx.xer);
	// blt cr6,0x822c8a18
	if (ctx.cr6.lt) goto loc_822C8A18;
	// addi r5,r20,-3
	ctx.r5.s64 = ctx.r20.s64 + -3;
	// li r9,8
	ctx.r9.s64 = 8;
	// addi r10,r28,8
	ctx.r10.s64 = ctx.r28.s64 + 8;
	// addi r4,r29,12
	ctx.r4.s64 = ctx.r29.s64 + 12;
loc_822C8988:
	// lwz r6,372(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// add r7,r29,r10
	ctx.r7.u64 = ctx.r29.u64 + ctx.r10.u64;
	// lfs f0,-8(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lfsx f13,r11,r31
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	ctx.f13.f64 = double(temp.f32);
	// cmpw cr6,r8,r5
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, ctx.xer);
	// lwzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// lfsx f12,r6,r7
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f12,f0,f13
	ctx.f11.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfsx f11,r11,r31
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lwz r6,372(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// lfs f10,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f10.f64 = double(temp.f32);
	// lwzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// lfs f8,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f7,f8,f10,f11
	ctx.f7.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f10.f64), float(ctx.f11.f64)));
	// stfsx f7,r11,r31
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// lwz r6,372(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// fmr f5,f7
	ctx.f5.f64 = ctx.f7.f64;
	// lfs f6,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lwzx r7,r6,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// lfsx f4,r7,r9
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f3,f4,f6,f7
	ctx.f3.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f6.f64), float(ctx.f7.f64)));
	// stfsx f3,r11,r31
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// lwz r6,372(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// fmr f2,f3
	ctx.f2.f64 = ctx.f3.f64;
	// lfs f1,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f1.f64 = double(temp.f32);
	// lwzx r7,r6,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// lfsx f0,r7,r10
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f13,f0,f1,f3
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f1.f64), float(ctx.f3.f64)));
	// stfsx f13,r11,r31
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// blt cr6,0x822c8988
	if (ctx.cr6.lt) goto loc_822C8988;
loc_822C8A18:
	// cmpw cr6,r8,r20
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r20.s32, ctx.xer);
	// bge cr6,0x822c8a50
	if (!ctx.cr6.lt) goto loc_822C8A50;
	// subf r9,r8,r20
	ctx.r9.s64 = ctx.r20.s64 - ctx.r8.s64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822C8A2C:
	// lwz r9,372(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 372);
	// lfsx f0,r10,r28
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r28.u32);
	ctx.f0.f64 = double(temp.f32);
	// lfsx f13,r11,r31
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	ctx.f13.f64 = double(temp.f32);
	// lwzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// lfsx f12,r8,r10
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fmadds f11,f12,f0,f13
	ctx.f11.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfsx f11,r11,r31
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// bdnz 0x822c8a2c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822C8A2C;
loc_822C8A50:
	// addic. r3,r3,-1
	ctx.xer.ca = ctx.r3.u32 > 0;
	ctx.r3.s64 = ctx.r3.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bne 0x822c896c
	if (!ctx.cr0.eq) goto loc_822C896C;
loc_822C8A5C:
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r19,4
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 4, ctx.xer);
	// blt cr6,0x822c8aa4
	if (ctx.cr6.lt) goto loc_822C8AA4;
	// addi r8,r19,-3
	ctx.r8.s64 = ctx.r19.s64 + -3;
	// addi r10,r31,-4
	ctx.r10.s64 = ctx.r31.s64 + -4;
	// addi r11,r26,4
	ctx.r11.s64 = ctx.r26.s64 + 4;
loc_822C8A74:
	// lfs f0,4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stfs f0,-4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lfsx f13,r27,r11
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + ctx.r11.u32);
	ctx.f13.f64 = double(temp.f32);
	// cmpw cr6,r9,r8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, ctx.xer);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfs f12,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfsu f0,16(r10)
	ea = 16 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// blt cr6,0x822c8a74
	if (ctx.cr6.lt) goto loc_822C8A74;
loc_822C8AA4:
	// cmpw cr6,r9,r19
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r19.s32, ctx.xer);
	// bge cr6,0x822c8acc
	if (!ctx.cr6.lt) goto loc_822C8ACC;
	// subf r10,r9,r19
	ctx.r10.s64 = ctx.r19.s64 - ctx.r9.s64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r26
	ctx.r11.u64 = ctx.r11.u64 + ctx.r26.u64;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822C8ABC:
	// lfsx f0,r27,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822c8abc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822C8ABC;
loc_822C8ACC:
	// addic. r25,r25,-1
	ctx.xer.ca = ctx.r25.u32 > 0;
	ctx.r25.s64 = ctx.r25.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// subf r28,r23,r28
	ctx.r28.s64 = ctx.r28.s64 - ctx.r23.s64;
	// subf r29,r21,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r21.s64;
	// subf r26,r24,r26
	ctx.r26.s64 = ctx.r26.s64 - ctx.r24.s64;
	// subf r27,r22,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r22.s64;
	// bge 0x822c894c
	if (!ctx.cr0.lt) goto loc_822C894C;
loc_822C8AE4:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// addi r12,r1,-112
	ctx.r12.s64 = ctx.r1.s64 + -112;
	// bl 0x8233fa80
	ctx.lr = 0x822C8AF4;
	__savefpr_27(ctx, base);
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C8AF8"))) PPC_WEAK_FUNC(sub_822C8AF8);
PPC_FUNC_IMPL(__imp__sub_822C8AF8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e444
	ctx.lr = 0x822C8B00;
	__restfpr_19(ctx, base);
	// addi r12,r1,-112
	ctx.r12.s64 = ctx.r1.s64 + -112;
	// bl 0x8233fa00
	ctx.lr = 0x822C8B08;
	sub_8233FA00(ctx, base);
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r21,r3
	ctx.r21.u64 = ctx.r3.u64;
	// lwz r11,352(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 352);
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// lwz r20,360(r3)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r3.u32 + 360);
	// mr r23,r6
	ctx.r23.u64 = ctx.r6.u64;
	// lwz r27,384(r3)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 384);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lhz r25,34(r31)
	ctx.r25.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// beq cr6,0x822c90d8
	if (ctx.cr6.eq) goto loc_822C90D8;
	// lwz r11,424(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 424);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c8b44
	if (ctx.cr6.eq) goto loc_822C8B44;
	// li r20,6
	ctx.r20.s64 = 6;
loc_822C8B44:
	// cmpwi cr6,r25,6
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 6, ctx.xer);
	// bne cr6,0x822c8dfc
	if (!ctx.cr6.eq) goto loc_822C8DFC;
	// cmpwi cr6,r20,2
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 2, ctx.xer);
	// bne cr6,0x822c8dfc
	if (!ctx.cr6.eq) goto loc_822C8DFC;
	// lwz r11,372(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 372);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lfs f29,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f18.f64 = double(temp.f32);
	// ble cr6,0x822c90d8
	if (!ctx.cr6.gt) goto loc_822C90D8;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// lfs f0,5268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5268);
	ctx.f0.f64 = double(temp.f32);
	// lfs f30,-28948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f30.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
loc_822C8BB0:
	// lwz r11,524(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C8BCC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// extsw r10,r3
	ctx.r10.s64 = ctx.r3.s32;
	// lwz r9,524(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// li r6,1
	ctx.r6.s64 = 1;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// frsp f31,f13
	ctx.f31.f64 = double(float(ctx.f13.f64));
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x822C8BFC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// extsw r8,r3
	ctx.r8.s64 = ctx.r3.s32;
	// lwz r7,524(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// li r6,2
	ctx.r6.s64 = 2;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// lfd f12,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// frsp f17,f11
	ctx.f17.f64 = double(float(ctx.f11.f64));
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x822C8C2C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// extsw r4,r3
	ctx.r4.s64 = ctx.r3.s32;
	// lwz r11,524(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// li r6,3
	ctx.r6.s64 = 3;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// std r4,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r4.u64);
	// lfd f10,104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// frsp f16,f9
	ctx.f16.f64 = double(float(ctx.f9.f64));
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C8C5C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// extsw r10,r3
	ctx.r10.s64 = ctx.r3.s32;
	// lwz r9,524(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// li r6,4
	ctx.r6.s64 = 4;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// std r10,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r10.u64);
	// lfd f8,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// frsp f15,f7
	ctx.f15.f64 = double(float(ctx.f7.f64));
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x822C8C8C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// extsw r8,r3
	ctx.r8.s64 = ctx.r3.s32;
	// lwz r7,524(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// li r6,5
	ctx.r6.s64 = 5;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// std r8,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r8.u64);
	// lfd f6,120(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// frsp f14,f5
	ctx.f14.f64 = double(float(ctx.f5.f64));
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x822C8CBC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// extsw r6,r3
	ctx.r6.s64 = ctx.r3.s32;
	// fmuls f4,f14,f25
	ctx.fpscr.disableFlushMode();
	ctx.f4.f64 = double(float(ctx.f14.f64 * ctx.f25.f64));
	// fmuls f3,f14,f19
	ctx.f3.f64 = double(float(ctx.f14.f64 * ctx.f19.f64));
	// std r6,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r6.u64);
	// lfd f2,128(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f0,f1
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// fmadds f13,f0,f24,f4
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f24.f64), float(ctx.f4.f64)));
	// fmadds f12,f0,f18,f3
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f18.f64), float(ctx.f3.f64)));
	// fmadds f11,f15,f26,f13
	ctx.f11.f64 = double(std::fma(float(ctx.f15.f64), float(ctx.f26.f64), float(ctx.f13.f64)));
	// fmadds f10,f15,f20,f12
	ctx.f10.f64 = double(std::fma(float(ctx.f15.f64), float(ctx.f20.f64), float(ctx.f12.f64)));
	// fmadds f9,f16,f27,f11
	ctx.f9.f64 = double(std::fma(float(ctx.f16.f64), float(ctx.f27.f64), float(ctx.f11.f64)));
	// fmadds f8,f17,f28,f9
	ctx.f8.f64 = double(std::fma(float(ctx.f17.f64), float(ctx.f28.f64), float(ctx.f9.f64)));
	// fmadds f0,f31,f29,f8
	ctx.f0.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f29.f64), float(ctx.f8.f64)));
	// fmadds f7,f16,f21,f10
	ctx.f7.f64 = double(std::fma(float(ctx.f16.f64), float(ctx.f21.f64), float(ctx.f10.f64)));
	// fcmpu cr6,f0,f30
	ctx.cr6.compare(ctx.f0.f64, ctx.f30.f64);
	// fmadds f6,f17,f22,f7
	ctx.f6.f64 = double(std::fma(float(ctx.f17.f64), float(ctx.f22.f64), float(ctx.f7.f64)));
	// lfs f17,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f31,f31,f23,f6
	ctx.f31.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f23.f64), float(ctx.f6.f64)));
	// bge cr6,0x822c8d20
	if (!ctx.cr6.lt) goto loc_822C8D20;
	// fsubs f0,f0,f17
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f17.f64);
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.f13.u64);
	// lwz r3,140(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// b 0x822c8d30
	goto loc_822C8D30;
loc_822C8D20:
	// fadds f0,f0,f17
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f17.f64));
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.f13.u64);
	// lwz r3,140(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
loc_822C8D30:
	// lwz r11,112(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x822c8d48
	if (ctx.cr6.lt) goto loc_822C8D48;
	// lwz r11,116(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x822c8d4c
	if (!ctx.cr6.gt) goto loc_822C8D4C;
loc_822C8D48:
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
loc_822C8D4C:
	// lwz r11,520(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 520);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C8D64;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// fcmpu cr6,f31,f30
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f31.f64, ctx.f30.f64);
	// bge cr6,0x822c8d80
	if (!ctx.cr6.lt) goto loc_822C8D80;
	// fsubs f0,f31,f17
	ctx.f0.f64 = static_cast<float>(ctx.f31.f64 - ctx.f17.f64);
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.f13.u64);
	// lwz r3,140(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// b 0x822c8d90
	goto loc_822C8D90;
loc_822C8D80:
	// fadds f0,f31,f17
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f31.f64 + ctx.f17.f64));
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.f13.u64);
	// lwz r3,140(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
loc_822C8D90:
	// lwz r11,112(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x822c8da8
	if (ctx.cr6.lt) goto loc_822C8DA8;
	// lwz r11,116(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x822c8dac
	if (!ctx.cr6.gt) goto loc_822C8DAC;
loc_822C8DA8:
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
loc_822C8DAC:
	// lwz r11,520(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 520);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C8DC4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,88(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// addic. r30,r30,-1
	ctx.xer.ca = ctx.r30.u32 > 0;
	ctx.r30.s64 = ctx.r30.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// add r23,r9,r23
	ctx.r23.u64 = ctx.r9.u64 + ctx.r23.u64;
	// rlwinm r11,r10,1,0,30
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r26,r11,r26
	ctx.r26.u64 = ctx.r11.u64 + ctx.r26.u64;
	// bne 0x822c8bb0
	if (!ctx.cr0.eq) goto loc_822C8BB0;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// addi r12,r1,-112
	ctx.r12.s64 = ctx.r1.s64 + -112;
	// bl 0x8233fa4c
	ctx.lr = 0x822C8DF8;
	__savefpr_14(ctx, base);
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
loc_822C8DFC:
	// cmpw cr6,r25,r20
	ctx.cr6.compare<int32_t>(ctx.r25.s32, ctx.r20.s32, ctx.xer);
	// blt cr6,0x822c8f6c
	if (ctx.cr6.lt) goto loc_822C8F6C;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// ble cr6,0x822c90d8
	if (!ctx.cr6.gt) goto loc_822C90D8;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// rlwinm r19,r20,2,0,29
	ctx.r19.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r22,r5
	ctx.r22.u64 = ctx.r5.u64;
	// lfs f31,5268(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5268);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,-28948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f30.f64 = double(temp.f32);
loc_822C8E24:
	// mr r5,r19
	ctx.r5.u64 = ctx.r19.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822C8E34;
	sub_8233EAF0(ctx, base);
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x822c8eb8
	if (!ctx.cr6.gt) goto loc_822C8EB8;
	// li r28,0
	ctx.r28.s64 = 0;
	// mr r24,r20
	ctx.r24.u64 = ctx.r20.u64;
loc_822C8E44:
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// ble cr6,0x822c8eac
	if (!ctx.cr6.gt) goto loc_822C8EAC;
	// li r29,0
	ctx.r29.s64 = 0;
loc_822C8E54:
	// lwz r11,524(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C8E70;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// extsw r10,r3
	ctx.r10.s64 = ctx.r3.s32;
	// lwz r9,372(r21)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r21.u32 + 372);
	// lfsx f0,r28,r27
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r27.u32);
	ctx.f0.f64 = double(temp.f32);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// std r10,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r10.u64);
	// lfd f13,136(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// cmpw cr6,r30,r25
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r25.s32, ctx.xer);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// lwzx r8,r28,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r9.u32);
	// lfsx f10,r8,r29
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r29.u32);
	ctx.f10.f64 = double(temp.f32);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// fmadds f9,f11,f10,f0
	ctx.f9.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f10.f64), float(ctx.f0.f64)));
	// stfsx f9,r28,r27
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r28.u32 + ctx.r27.u32, temp.u32);
	// blt cr6,0x822c8e54
	if (ctx.cr6.lt) goto loc_822C8E54;
loc_822C8EAC:
	// addic. r24,r24,-1
	ctx.xer.ca = ctx.r24.u32 > 0;
	ctx.r24.s64 = ctx.r24.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// bne 0x822c8e44
	if (!ctx.cr0.eq) goto loc_822C8E44;
loc_822C8EB8:
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x822c8f3c
	if (!ctx.cr6.gt) goto loc_822C8F3C;
	// mr r29,r27
	ctx.r29.u64 = ctx.r27.u64;
loc_822C8EC8:
	// lfs f0,0(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f30
	ctx.cr6.compare(ctx.f0.f64, ctx.f30.f64);
	// bge cr6,0x822c8ee8
	if (!ctx.cr6.lt) goto loc_822C8EE8;
	// fsubs f0,f0,f31
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f31.f64);
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.f13.u64);
	// lwz r3,132(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// b 0x822c8ef8
	goto loc_822C8EF8;
loc_822C8EE8:
	// fadds f0,f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f31.f64));
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.f13.u64);
	// lwz r3,132(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
loc_822C8EF8:
	// lwz r11,112(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x822c8f10
	if (ctx.cr6.lt) goto loc_822C8F10;
	// lwz r11,116(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x822c8f14
	if (!ctx.cr6.gt) goto loc_822C8F14;
loc_822C8F10:
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
loc_822C8F14:
	// lwz r11,520(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 520);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C8F2C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// cmpw cr6,r30,r20
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r20.s32, ctx.xer);
	// blt cr6,0x822c8ec8
	if (ctx.cr6.lt) goto loc_822C8EC8;
loc_822C8F3C:
	// lwz r10,88(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// addic. r22,r22,-1
	ctx.xer.ca = ctx.r22.u32 > 0;
	ctx.r22.s64 = ctx.r22.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// mullw r11,r10,r25
	ctx.r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r25.s32);
	// mullw r10,r10,r20
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r20.s32);
	// add r26,r11,r26
	ctx.r26.u64 = ctx.r11.u64 + ctx.r26.u64;
	// add r23,r10,r23
	ctx.r23.u64 = ctx.r10.u64 + ctx.r23.u64;
	// bne 0x822c8e24
	if (!ctx.cr0.eq) goto loc_822C8E24;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// addi r12,r1,-112
	ctx.r12.s64 = ctx.r1.s64 + -112;
	// bl 0x8233fa4c
	ctx.lr = 0x822C8F68;
	__savefpr_14(ctx, base);
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
loc_822C8F6C:
	// lwz r11,88(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// addi r22,r5,-1
	ctx.r22.s64 = ctx.r5.s64 + -1;
	// mullw r10,r22,r11
	ctx.r10.s64 = int64_t(ctx.r22.s32) * int64_t(ctx.r11.s32);
	// mullw r11,r10,r25
	ctx.r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r25.s32);
	// mullw r10,r10,r20
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r20.s32);
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// add r26,r11,r26
	ctx.r26.u64 = ctx.r11.u64 + ctx.r26.u64;
	// add r23,r10,r23
	ctx.r23.u64 = ctx.r10.u64 + ctx.r23.u64;
	// blt cr6,0x822c90d8
	if (ctx.cr6.lt) goto loc_822C90D8;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// rlwinm r19,r20,2,0,29
	ctx.r19.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f31,5268(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5268);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,-28948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f30.f64 = double(temp.f32);
loc_822C8FA4:
	// mr r5,r19
	ctx.r5.u64 = ctx.r19.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822C8FB4;
	sub_8233EAF0(ctx, base);
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x822c9038
	if (!ctx.cr6.gt) goto loc_822C9038;
	// li r28,0
	ctx.r28.s64 = 0;
	// mr r24,r20
	ctx.r24.u64 = ctx.r20.u64;
loc_822C8FC4:
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// ble cr6,0x822c902c
	if (!ctx.cr6.gt) goto loc_822C902C;
	// li r29,0
	ctx.r29.s64 = 0;
loc_822C8FD4:
	// lwz r11,524(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 524);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lhz r5,110(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C8FF0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// extsw r10,r3
	ctx.r10.s64 = ctx.r3.s32;
	// lwz r9,372(r21)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r21.u32 + 372);
	// lfsx f0,r28,r27
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r27.u32);
	ctx.f0.f64 = double(temp.f32);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// std r10,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r10.u64);
	// lfd f12,136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// cmpw cr6,r30,r25
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r25.s32, ctx.xer);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// lwzx r8,r28,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r9.u32);
	// lfsx f13,r8,r29
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r29.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// fmadds f9,f10,f13,f0
	ctx.f9.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f13.f64), float(ctx.f0.f64)));
	// stfsx f9,r28,r27
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r28.u32 + ctx.r27.u32, temp.u32);
	// blt cr6,0x822c8fd4
	if (ctx.cr6.lt) goto loc_822C8FD4;
loc_822C902C:
	// addic. r24,r24,-1
	ctx.xer.ca = ctx.r24.u32 > 0;
	ctx.r24.s64 = ctx.r24.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// bne 0x822c8fc4
	if (!ctx.cr0.eq) goto loc_822C8FC4;
loc_822C9038:
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x822c90bc
	if (!ctx.cr6.gt) goto loc_822C90BC;
	// mr r29,r27
	ctx.r29.u64 = ctx.r27.u64;
loc_822C9048:
	// lfs f0,0(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f30
	ctx.cr6.compare(ctx.f0.f64, ctx.f30.f64);
	// bge cr6,0x822c9068
	if (!ctx.cr6.lt) goto loc_822C9068;
	// fsubs f0,f0,f31
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f31.f64);
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.f13.u64);
	// lwz r3,132(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// b 0x822c9078
	goto loc_822C9078;
loc_822C9068:
	// fadds f0,f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f31.f64));
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.f13.u64);
	// lwz r3,132(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
loc_822C9078:
	// lwz r11,112(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x822c9090
	if (ctx.cr6.lt) goto loc_822C9090;
	// lwz r11,116(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x822c9094
	if (!ctx.cr6.gt) goto loc_822C9094;
loc_822C9090:
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
loc_822C9094:
	// lwz r11,520(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 520);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C90AC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// cmpw cr6,r30,r20
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r20.s32, ctx.xer);
	// blt cr6,0x822c9048
	if (ctx.cr6.lt) goto loc_822C9048;
loc_822C90BC:
	// lwz r11,88(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// addic. r22,r22,-1
	ctx.xer.ca = ctx.r22.u32 > 0;
	ctx.r22.s64 = ctx.r22.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// mullw r10,r11,r25
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r25.s32);
	// mullw r9,r11,r20
	ctx.r9.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r20.s32);
	// subf r26,r10,r26
	ctx.r26.s64 = ctx.r26.s64 - ctx.r10.s64;
	// subf r23,r9,r23
	ctx.r23.s64 = ctx.r23.s64 - ctx.r9.s64;
	// bge 0x822c8fa4
	if (!ctx.cr0.lt) goto loc_822C8FA4;
loc_822C90D8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// addi r12,r1,-112
	ctx.r12.s64 = ctx.r1.s64 + -112;
	// bl 0x8233fa4c
	ctx.lr = 0x822C90E8;
	__savefpr_14(ctx, base);
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C90EC"))) PPC_WEAK_FUNC(sub_822C90EC);
PPC_FUNC_IMPL(__imp__sub_822C90EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C90F0"))) PPC_WEAK_FUNC(sub_822C90F0);
PPC_FUNC_IMPL(__imp__sub_822C90F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e440
	ctx.lr = 0x822C90F8;
	__restfpr_18(ctx, base);
	// stfd f30,-136(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f30.u64);
	// stfd f31,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.f31.u64);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r30,0(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// lwz r11,352(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 352);
	// mr r23,r4
	ctx.r23.u64 = ctx.r4.u64;
	// lwz r26,360(r3)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 360);
	// mr r18,r5
	ctx.r18.u64 = ctx.r5.u64;
	// lhz r21,0(r5)
	ctx.r21.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// mr r27,r6
	ctx.r27.u64 = ctx.r6.u64;
	// lwz r25,428(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 428);
	// li r19,0
	ctx.r19.s64 = 0;
	// lhz r24,34(r30)
	ctx.r24.u64 = PPC_LOAD_U16(ctx.r30.u32 + 34);
	// li r29,0
	ctx.r29.s64 = 0;
	// li r22,0
	ctx.r22.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c9154
	if (!ctx.cr6.eq) goto loc_822C9154;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f30,-136(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// lfd f31,-128(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// b 0x8233e490
	__restgprlr_18(ctx, base);
	return;
loc_822C9154:
	// lwz r11,424(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 424);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c9164
	if (ctx.cr6.eq) goto loc_822C9164;
	// li r24,6
	ctx.r24.s64 = 6;
loc_822C9164:
	// li r20,0
	ctx.r20.s64 = 0;
	// cmpwi cr6,r21,0
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 0, ctx.xer);
	// ble cr6,0x822c92d8
	if (!ctx.cr6.gt) goto loc_822C92D8;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// lfs f31,5268(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5268);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,-28948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f30.f64 = double(temp.f32);
loc_822C9180:
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// ble cr6,0x822c91e0
	if (!ctx.cr6.gt) goto loc_822C91E0;
loc_822C918C:
	// lwz r11,524(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 524);
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// lhz r5,110(r30)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r30.u32 + 110);
	// lwz r4,88(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 88);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C91A8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// extsw r10,r3
	ctx.r10.s64 = ctx.r3.s32;
	// lwz r9,432(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 432);
	// lwz r8,148(r25)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r25.u32 + 148);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// mullw r11,r31,r9
	ctx.r11.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r9.s32);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// add r7,r11,r29
	ctx.r7.u64 = ctx.r11.u64 + ctx.r29.u64;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r31,r24
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r24.s32, ctx.xer);
	// stfsx f12,r6,r8
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r6.u32 + ctx.r8.u32, temp.u32);
	// blt cr6,0x822c918c
	if (ctx.cr6.lt) goto loc_822C918C;
loc_822C91E0:
	// lwz r11,88(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 88);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// lwz r10,432(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 432);
	// mullw r11,r11,r24
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r24.s32);
	// add r23,r11,r23
	ctx.r23.u64 = ctx.r11.u64 + ctx.r23.u64;
	// cmpw cr6,r29,r10
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r10.s32, ctx.xer);
	// beq cr6,0x822c9208
	if (ctx.cr6.eq) goto loc_822C9208;
	// addi r11,r21,-1
	ctx.r11.s64 = ctx.r21.s64 + -1;
	// cmpw cr6,r20,r11
	ctx.cr6.compare<int32_t>(ctx.r20.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x822c92cc
	if (!ctx.cr6.eq) goto loc_822C92CC;
loc_822C9208:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,428(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 428);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x822ebf48
	ctx.lr = 0x822C9218;
	sub_822EBF48(ctx, base);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r19,r3
	ctx.r19.u64 = ctx.r3.u64;
	// cmpw cr6,r10,r29
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r29.s32, ctx.xer);
	// bgt cr6,0x822c92f0
	if (ctx.cr6.gt) goto loc_822C92F0;
	// li r29,0
	ctx.r29.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822c92c4
	if (!ctx.cr6.gt) goto loc_822C92C4;
loc_822C9234:
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// ble cr6,0x822c92ac
	if (!ctx.cr6.gt) goto loc_822C92AC;
loc_822C9240:
	// lwz r11,432(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 432);
	// lwz r10,148(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 148);
	// mullw r11,r31,r11
	ctx.r11.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r11.s32);
	// add r9,r11,r29
	ctx.r9.u64 = ctx.r11.u64 + ctx.r29.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r8,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f30
	ctx.cr6.compare(ctx.f0.f64, ctx.f30.f64);
	// bge cr6,0x822c9274
	if (!ctx.cr6.lt) goto loc_822C9274;
	// fsubs f0,f0,f31
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f31.f64);
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f13.u64);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// b 0x822c9284
	goto loc_822C9284;
loc_822C9274:
	// fadds f0,f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f31.f64));
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f13.u64);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
loc_822C9284:
	// lwz r11,520(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 520);
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C929C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// cmpw cr6,r31,r26
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r26.s32, ctx.xer);
	// blt cr6,0x822c9240
	if (ctx.cr6.lt) goto loc_822C9240;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_822C92AC:
	// lwz r11,88(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 88);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// mullw r11,r11,r26
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r26.s32);
	// add r27,r11,r27
	ctx.r27.u64 = ctx.r11.u64 + ctx.r27.u64;
	// cmpw cr6,r29,r10
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x822c9234
	if (ctx.cr6.lt) goto loc_822C9234;
loc_822C92C4:
	// add r22,r10,r22
	ctx.r22.u64 = ctx.r10.u64 + ctx.r22.u64;
	// li r29,0
	ctx.r29.s64 = 0;
loc_822C92CC:
	// addi r20,r20,1
	ctx.r20.s64 = ctx.r20.s64 + 1;
	// cmpw cr6,r20,r21
	ctx.cr6.compare<int32_t>(ctx.r20.s32, ctx.r21.s32, ctx.xer);
	// blt cr6,0x822c9180
	if (ctx.cr6.lt) goto loc_822C9180;
loc_822C92D8:
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// sth r22,0(r18)
	PPC_STORE_U16(ctx.r18.u32 + 0, ctx.r22.u16);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f30,-136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// lfd f31,-128(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// b 0x8233e490
	__restgprlr_18(ctx, base);
	return;
loc_822C92F0:
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f30,-136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// lfd f31,-128(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// b 0x8233e490
	__restgprlr_18(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C9308"))) PPC_WEAK_FUNC(sub_822C9308);
PPC_FUNC_IMPL(__imp__sub_822C9308) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x822C9310;
	__restfpr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bgt cr6,0x822c9330
	if (ctx.cr6.gt) goto loc_822C9330;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
loc_822C9330:
	// lwz r11,572(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 572);
	// li r29,0
	ctx.r29.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x822c9394
	if (!ctx.cr6.gt) goto loc_822C9394;
	// li r30,0
	ctx.r30.s64 = 0;
loc_822C9344:
	// lwz r11,576(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 576);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x822c9380
	if (!ctx.cr6.eq) goto loc_822C9380;
	// lwz r9,564(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 564);
	// lwz r8,560(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 560);
	// lwz r7,148(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 148);
	// lhz r6,34(r31)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r4,140(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// lwz r3,136(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	// bl 0x822ed2d0
	ctx.lr = 0x822C9378;
	sub_822ED2D0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c9394
	if (ctx.cr6.lt) goto loc_822C9394;
loc_822C9380:
	// lwz r11,572(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 572);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// addi r30,r30,152
	ctx.r30.s64 = ctx.r30.s64 + 152;
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x822c9344
	if (ctx.cr6.lt) goto loc_822C9344;
loc_822C9394:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C939C"))) PPC_WEAK_FUNC(sub_822C939C);
PPC_FUNC_IMPL(__imp__sub_822C939C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C93A0"))) PPC_WEAK_FUNC(sub_822C93A0);
PPC_FUNC_IMPL(__imp__sub_822C93A0) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 60);
	// cmpwi cr6,r10,2
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 2, ctx.xer);
	// bgt cr6,0x822c93bc
	if (ctx.cr6.gt) goto loc_822C93BC;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, ctx.r11.u32);
	// blr 
	return;
loc_822C93BC:
	// lwz r11,512(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 512);
	// li r10,4
	ctx.r10.s64 = 4;
	// stw r10,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, ctx.r10.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__sub_822C93D8"))) PPC_WEAK_FUNC(sub_822C93D8);
PPC_FUNC_IMPL(__imp__sub_822C93D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e440
	ctx.lr = 0x822C93E0;
	__restfpr_18(ctx, base);
	// stfd f29,-144(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f29.u64);
	// stfd f30,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f30.u64);
	// stfd f31,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.f31.u64);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// li r25,0
	ctx.r25.s64 = 0;
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// mr r26,r25
	ctx.r26.u64 = ctx.r25.u64;
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// beq cr6,0x822c9b34
	if (ctx.cr6.eq) goto loc_822C9B34;
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// li r21,1
	ctx.r21.s64 = 1;
	// li r22,4
	ctx.r22.s64 = 4;
	// li r19,2
	ctx.r19.s64 = 2;
	// lfs f30,-28948(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f30.f64 = double(temp.f32);
	// li r24,6
	ctx.r24.s64 = 6;
	// lfs f29,5260(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5260);
	ctx.f29.f64 = double(temp.f32);
	// li r18,5
	ctx.r18.s64 = 5;
	// li r20,8
	ctx.r20.s64 = 8;
	// li r23,-1
	ctx.r23.s64 = -1;
loc_822C9438:
	// lwz r11,36(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bgt cr6,0x822c9b28
	if (ctx.cr6.gt) goto loc_822C9B28;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bdzf 4*cr6+eq,0x822c94f0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822C94F0;
	// bdzf 4*cr6+eq,0x822c9518
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822C9518;
	// bdzf 4*cr6+eq,0x822c9b28
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822C9B28;
	// bdzf 4*cr6+eq,0x822c95cc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822C95CC;
	// bne cr6,0x822c9690
	if (!ctx.cr6.eq) goto loc_822C9690;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x822f0c60
	ctx.lr = 0x822C9468;
	sub_822F0C60(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c9b34
	if (ctx.cr6.lt) goto loc_822C9B34;
	// lwz r11,132(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// bne cr6,0x822c94c8
	if (!ctx.cr6.eq) goto loc_822C94C8;
	// stw r21,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r21.u32);
	// sth r25,202(r11)
	PPC_STORE_U16(ctx.r11.u32 + 202, ctx.r25.u16);
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// sth r25,150(r28)
	PPC_STORE_U16(ctx.r28.u32 + 150, ctx.r25.u16);
	// lwz r9,60(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	// cmpwi cr6,r9,2
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 2, ctx.xer);
	// bgt cr6,0x822c94a8
	if (ctx.cr6.gt) goto loc_822C94A8;
	// stw r25,56(r28)
	PPC_STORE_U32(ctx.r28.u32 + 56, ctx.r25.u32);
	// b 0x822c9b28
	goto loc_822C9B28;
loc_822C94A8:
	// lwz r11,512(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 512);
	// stw r22,56(r28)
	PPC_STORE_U32(ctx.r28.u32 + 56, ctx.r22.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c9b28
	if (ctx.cr6.eq) goto loc_822C9B28;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C94C4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// b 0x822c9b28
	goto loc_822C9B28;
loc_822C94C8:
	// stw r19,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r19.u32);
	// sth r25,150(r28)
	PPC_STORE_U16(ctx.r28.u32 + 150, ctx.r25.u16);
	// stb r25,145(r28)
	PPC_STORE_U8(ctx.r28.u32 + 145, ctx.r25.u8);
	// stw r24,72(r28)
	PPC_STORE_U32(ctx.r28.u32 + 72, ctx.r24.u32);
	// sth r25,148(r28)
	PPC_STORE_U16(ctx.r28.u32 + 148, ctx.r25.u16);
	// sth r25,202(r11)
	PPC_STORE_U16(ctx.r11.u32 + 202, ctx.r25.u16);
	// stw r25,76(r28)
	PPC_STORE_U32(ctx.r28.u32 + 76, ctx.r25.u32);
	// stw r25,200(r28)
	PPC_STORE_U32(ctx.r28.u32 + 200, ctx.r25.u32);
	// stw r25,208(r28)
	PPC_STORE_U32(ctx.r28.u32 + 208, ctx.r25.u32);
	// b 0x822c9b28
	goto loc_822C9B28;
loc_822C94F0:
	// lwz r11,508(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 508);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lwz r4,320(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C9504;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c9b34
	if (ctx.cr6.lt) goto loc_822C9B34;
	// stw r18,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r18.u32);
	// b 0x822c9b28
	goto loc_822C9B28;
loc_822C9518:
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// lhz r10,150(r28)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r28.u32 + 150);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// bge cr6,0x822c95c4
	if (!ctx.cr6.lt) goto loc_822C95C4;
loc_822C9530:
	// lhz r11,150(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 150);
	// lwz r10,584(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// lwz r11,320(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r7,r8,r10
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r10.u32);
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// mulli r10,r6,1776
	ctx.r10.s64 = ctx.r6.s64 * 1776;
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r4,40(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 40);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// beq cr6,0x822c9578
	if (ctx.cr6.eq) goto loc_822C9578;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// bl 0x822f2890
	ctx.lr = 0x822C956C;
	sub_822F2890(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c9b34
	if (ctx.cr6.lt) goto loc_822C9B34;
loc_822C9578:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// stb r25,145(r28)
	PPC_STORE_U8(ctx.r28.u32 + 145, ctx.r25.u8);
	// stw r24,72(r28)
	PPC_STORE_U32(ctx.r28.u32 + 72, ctx.r24.u32);
	// sth r25,148(r28)
	PPC_STORE_U16(ctx.r28.u32 + 148, ctx.r25.u16);
	// sth r25,202(r11)
	PPC_STORE_U16(ctx.r11.u32 + 202, ctx.r25.u16);
	// stw r25,76(r28)
	PPC_STORE_U32(ctx.r28.u32 + 76, ctx.r25.u32);
	// stw r25,200(r28)
	PPC_STORE_U32(ctx.r28.u32 + 200, ctx.r25.u32);
	// stw r25,208(r28)
	PPC_STORE_U32(ctx.r28.u32 + 208, ctx.r25.u32);
	// lhz r10,150(r28)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r28.u32 + 150);
	// extsh r11,r10
	ctx.r11.s64 = ctx.r10.s16;
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// clrlwi r6,r8,16
	ctx.r6.u64 = ctx.r8.u32 & 0xFFFF;
	// sth r8,150(r28)
	PPC_STORE_U16(ctx.r28.u32 + 150, ctx.r8.u16);
	// lhz r7,580(r31)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r4,r7
	ctx.r4.s64 = ctx.r7.s16;
	// extsh r5,r6
	ctx.r5.s64 = ctx.r6.s16;
	// cmpw cr6,r5,r4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r4.s32, ctx.xer);
	// blt cr6,0x822c9530
	if (ctx.cr6.lt) goto loc_822C9530;
loc_822C95C4:
	// stw r22,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r22.u32);
	// b 0x822c9b28
	goto loc_822C9B28;
loc_822C95CC:
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822c9b24
	if (!ctx.cr6.gt) goto loc_822C9B24;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// rlwinm r10,r25,1,0,30
	ctx.r10.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C95E4:
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// lwz r11,320(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lhzx r8,r10,r9
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// mulli r10,r7,1776
	ctx.r10.s64 = ctx.r7.s64 * 1776;
	// add r30,r10,r11
	ctx.r30.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r6,40(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x822c9650
	if (ctx.cr6.eq) goto loc_822C9650;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x822f2e38
	ctx.lr = 0x822C9618;
	sub_822F2E38(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c9b34
	if (ctx.cr6.lt) goto loc_822C9B34;
	// stw r25,48(r30)
	PPC_STORE_U32(ctx.r30.u32 + 48, ctx.r25.u32);
	// lwz r11,72(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// bne cr6,0x822c966c
	if (!ctx.cr6.eq) goto loc_822C966C;
	// lhz r11,120(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// li r6,3
	ctx.r6.s64 = 3;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r3,56(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// extsh r4,r11
	ctx.r4.s64 = ctx.r11.s16;
	// bl 0x822ea358
	ctx.lr = 0x822C964C;
	sub_822EA358(ctx, base);
	// b 0x822c966c
	goto loc_822C966C;
loc_822C9650:
	// lhz r11,120(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,56(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233eaf0
	ctx.lr = 0x822C9668;
	sub_8233EAF0(ctx, base);
	// stw r25,48(r30)
	PPC_STORE_U32(ctx.r30.u32 + 48, ctx.r25.u32);
loc_822C966C:
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// addi r11,r29,1
	ctx.r11.s64 = ctx.r29.s64 + 1;
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// mr r29,r9
	ctx.r29.u64 = ctx.r9.u64;
	// cmpw cr6,r9,r8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, ctx.xer);
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822c95e4
	if (ctx.cr6.lt) goto loc_822C95E4;
	// b 0x822c9b24
	goto loc_822C9B24;
loc_822C9690:
	// lwz r11,72(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// bne cr6,0x822c9700
	if (!ctx.cr6.eq) goto loc_822C9700;
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c9700
	if (ctx.cr6.eq) goto loc_822C9700;
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// rlwinm r10,r25,1,0,30
	ctx.r10.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C96B0:
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// lwz r11,320(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lhzx r8,r10,r9
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// mulli r10,r7,1776
	ctx.r10.s64 = ctx.r7.s64 * 1776;
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r6,40(r5)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + 40);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x822c96e4
	if (ctx.cr6.eq) goto loc_822C96E4;
	// lwz r6,72(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// lwz r4,36(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 36);
	// lwz r3,0(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// bl 0x822ea358
	ctx.lr = 0x822C96E4;
	sub_822EA358(ctx, base);
loc_822C96E4:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// mr r30,r9
	ctx.r30.u64 = ctx.r9.u64;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822c96b0
	if (ctx.cr6.lt) goto loc_822C96B0;
loc_822C9700:
	// lwz r3,296(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 296);
	// bl 0x822ef640
	ctx.lr = 0x822C9708;
	sub_822EF640(ctx, base);
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// stfs f1,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c97dc
	if (ctx.cr6.eq) goto loc_822C97DC;
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// rlwinm r10,r25,1,0,30
	ctx.r10.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C9720:
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// lwz r11,320(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lhzx r8,r10,r9
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// mulli r10,r7,1776
	ctx.r10.s64 = ctx.r7.s64 * 1776;
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r6,40(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x822c9768
	if (ctx.cr6.eq) goto loc_822C9768;
	// lwz r11,476(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 476);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,52(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C9758;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c9b34
	if (ctx.cr6.lt) goto loc_822C9B34;
	// b 0x822c97c0
	goto loc_822C97C0;
loc_822C9768:
	// lwz r11,460(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 460);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c978c
	if (ctx.cr6.eq) goto loc_822C978C;
	// lwz r11,456(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 456);
	// lhz r10,118(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 118);
	// extsh r7,r11
	ctx.r7.s64 = ctx.r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// sraw r11,r8,r7
	temp.u32 = ctx.r7.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	ctx.r11.s64 = ctx.r8.s32 >> temp.u32;
	// b 0x822c97b0
	goto loc_822C97B0;
loc_822C978C:
	// lwz r11,448(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lhz r11,118(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 118);
	// beq cr6,0x822c97ac
	if (ctx.cr6.eq) goto loc_822C97AC;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 456);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// slw r11,r9,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// b 0x822c97b0
	goto loc_822C97B0;
loc_822C97AC:
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
loc_822C97B0:
	// lwz r3,56(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8233eaf0
	ctx.lr = 0x822C97C0;
	sub_8233EAF0(ctx, base);
loc_822C97C0:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// mr r30,r9
	ctx.r30.u64 = ctx.r9.u64;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822c9720
	if (ctx.cr6.lt) goto loc_822C9720;
loc_822C97DC:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x822c9308
	ctx.lr = 0x822C97E4;
	sub_822C9308(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c9b34
	if (ctx.cr6.lt) goto loc_822C9B34;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822efaa0
	ctx.lr = 0x822C97FC;
	sub_822EFAA0(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822efaa0
	ctx.lr = 0x822C9808;
	sub_822EFAA0(ctx, base);
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// mr r27,r21
	ctx.r27.u64 = ctx.r21.u64;
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822c9870
	if (!ctx.cr6.gt) goto loc_822C9870;
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
	// lwz r8,320(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// rlwinm r10,r25,1,0,30
	ctx.r10.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C982C:
	// lhzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// mulli r10,r7,1776
	ctx.r10.s64 = ctx.r7.s64 * 1776;
	// add r6,r10,r8
	ctx.r6.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r5,40(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 40);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bne cr6,0x822c986c
	if (!ctx.cr6.eq) goto loc_822C986C;
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// extsh r7,r11
	ctx.r7.s64 = ctx.r11.s16;
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// mr r11,r7
	ctx.r11.u64 = ctx.r7.u64;
	// cmpw cr6,r7,r6
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r6.s32, ctx.xer);
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822c982c
	if (ctx.cr6.lt) goto loc_822C982C;
	// b 0x822c9870
	goto loc_822C9870;
loc_822C986C:
	// mr r27,r25
	ctx.r27.u64 = ctx.r25.u64;
loc_822C9870:
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x822c992c
	if (!ctx.cr6.gt) goto loc_822C992C;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// bne cr6,0x822c992c
	if (!ctx.cr6.eq) goto loc_822C992C;
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c992c
	if (ctx.cr6.eq) goto loc_822C992C;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// rlwinm r10,r25,1,0,30
	ctx.r10.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C9898:
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// lhz r8,108(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 108);
	// lwz r11,320(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// lhzx r6,r10,r9
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// extsh r5,r6
	ctx.r5.s64 = ctx.r6.s16;
	// mulli r10,r5,1776
	ctx.r10.s64 = ctx.r5.s64 * 1776;
	// add r30,r10,r11
	ctx.r30.u64 = ctx.r10.u64 + ctx.r11.u64;
	// cmpw cr6,r5,r7
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r7.s32, ctx.xer);
	// bne cr6,0x822c98e8
	if (!ctx.cr6.eq) goto loc_822C98E8;
	// lhz r11,120(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r9,312(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 312);
	// extsh r8,r11
	ctx.r8.s64 = ctx.r11.s16;
	// lwz r11,56(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233eaf0
	ctx.lr = 0x822C98E8;
	sub_8233EAF0(ctx, base);
loc_822C98E8:
	// sth r23,202(r31)
	PPC_STORE_U16(ctx.r31.u32 + 202, ctx.r23.u16);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r11,424(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 424);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// bl 0x822ec548
	ctx.lr = 0x822C9904;
	sub_822EC548(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822c9b34
	if (ctx.cr6.lt) goto loc_822C9B34;
	// addi r11,r29,1
	ctx.r11.s64 = ctx.r29.s64 + 1;
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// mr r29,r9
	ctx.r29.u64 = ctx.r9.u64;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822c9898
	if (ctx.cr6.lt) goto loc_822C9898;
loc_822C992C:
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c99a8
	if (!ctx.cr6.eq) goto loc_822C99A8;
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c99a8
	if (ctx.cr6.eq) goto loc_822C99A8;
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// rlwinm r11,r25,1,0,30
	ctx.r11.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C994C:
	// lwz r8,584(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lwz r7,472(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 472);
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r6,r11,r8
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r8.u32);
	// extsh r5,r6
	ctx.r5.s64 = ctx.r6.s16;
	// mulli r11,r5,1776
	ctx.r11.s64 = ctx.r5.s64 * 1776;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lhz r3,120(r11)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r11.u32 + 120);
	// lwz r11,56(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 56);
	// extsh r9,r3
	ctx.r9.s64 = ctx.r3.s16;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
	// subf r8,r7,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r5,r8,2,0,29
	ctx.r5.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233eaf0
	ctx.lr = 0x822C998C;
	sub_8233EAF0(ctx, base);
	// addi r7,r30,1
	ctx.r7.s64 = ctx.r30.s64 + 1;
	// lhz r5,34(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
	// rlwinm r11,r6,1,0,30
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r6,r5
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, ctx.xer);
	// blt cr6,0x822c994c
	if (ctx.cr6.lt) goto loc_822C994C;
loc_822C99A8:
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// beq cr6,0x822c99bc
	if (ctx.cr6.eq) goto loc_822C99BC;
	// lwz r11,784(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 784);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c9b24
	if (ctx.cr6.eq) goto loc_822C9B24;
loc_822C99BC:
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822c9b24
	if (ctx.cr6.eq) goto loc_822C9B24;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// rlwinm r11,r25,1,0,30
	ctx.r11.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C99D0:
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lwz r8,60(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r8,1
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 1, ctx.xer);
	// lhzx r7,r11,r9
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r9.u32);
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// mulli r11,r6,1776
	ctx.r11.s64 = ctx.r6.s64 * 1776;
	// add r30,r11,r10
	ctx.r30.u64 = ctx.r11.u64 + ctx.r10.u64;
	// bne cr6,0x822c99fc
	if (!ctx.cr6.eq) goto loc_822C99FC;
	// lfs f31,300(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	ctx.f31.f64 = double(temp.f32);
	// b 0x822c9a18
	goto loc_822C9A18;
loc_822C99FC:
	// lhz r11,118(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 118);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fdivs f31,f29,f12
	ctx.f31.f64 = double(float(ctx.f29.f64 / ctx.f12.f64));
loc_822C9A18:
	// lwz r11,320(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 320);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c9a6c
	if (ctx.cr6.eq) goto loc_822C9A6C;
	// lhz r11,118(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 118);
	// lwz r10,332(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 332);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// lwz r7,328(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 328);
	// lwz r8,56(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// srawi r6,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r9.s32 >> 1;
	// addze r11,r6
	temp.s64 = ctx.r6.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r11.s64 = temp.s64;
	// mullw r5,r10,r11
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// divw r4,r5,r7
	ctx.r4.s32 = ctx.r5.s32 / ctx.r7.s32;
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// subf. r10,r4,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r4.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// add r11,r9,r8
	ctx.r11.u64 = ctx.r9.u64 + ctx.r8.u64;
	// ble 0x822c9a6c
	if (!ctx.cr0.gt) goto loc_822C9A6C;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822C9A5C:
	// stfs f30,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfsu f30,4(r11)
	temp.f32 = float(ctx.f30.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822c9a5c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822C9A5C;
loc_822C9A6C:
	// lwz r11,40(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822c9a84
	if (!ctx.cr6.eq) goto loc_822C9A84;
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x822c9b08
	if (!ctx.cr6.gt) goto loc_822C9B08;
loc_822C9A84:
	// lhz r11,120(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// lhz r10,118(r30)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r30.u32 + 118);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// ble cr6,0x822c9ac0
	if (!ctx.cr6.gt) goto loc_822C9AC0;
	// clrlwi r9,r10,16
	ctx.r9.u64 = ctx.r10.u32 & 0xFFFF;
	// lwz r10,56(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// li r4,0
	ctx.r4.s64 = 0;
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// subf r7,r8,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r8.s64;
	// rlwinm r11,r8,2,0,29
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r10
	ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822C9AC0;
	sub_8233EAF0(ctx, base);
loc_822C9AC0:
	// lwz r9,464(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 464);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r7,496(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 496);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f31.f64;
	// lwz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// lhz r6,114(r30)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r30.u32 + 114);
	// lhz r4,120(r30)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r10,r6
	ctx.r10.s64 = ctx.r6.s16;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// lwz r9,440(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 440);
	// lwz r8,532(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 532);
	// lwz r7,516(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 516);
	// lwz r3,56(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// bctrl 
	ctx.lr = 0x822C9B08;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_822C9B08:
	// addi r11,r29,1
	ctx.r11.s64 = ctx.r29.s64 + 1;
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// mr r29,r9
	ctx.r29.u64 = ctx.r9.u64;
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// rlwinm r11,r9,1,0,30
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822c99d0
	if (ctx.cr6.lt) goto loc_822C99D0;
loc_822C9B24:
	// stw r20,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r20.u32);
loc_822C9B28:
	// lwz r11,36(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// bne cr6,0x822c9438
	if (!ctx.cr6.eq) goto loc_822C9438;
loc_822C9B34:
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f29,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lfd f30,-136(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// lfd f31,-128(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// b 0x8233e490
	__restgprlr_18(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822C9B4C"))) PPC_WEAK_FUNC(sub_822C9B4C);
PPC_FUNC_IMPL(__imp__sub_822C9B4C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C9B50"))) PPC_WEAK_FUNC(sub_822C9B50);
PPC_FUNC_IMPL(__imp__sub_822C9B50) {
	PPC_FUNC_PROLOGUE();
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// bne cr6,0x822c9b80
	if (!ctx.cr6.eq) goto loc_822C9B80;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,216(r3)
	PPC_STORE_U32(ctx.r3.u32 + 216, ctx.r11.u32);
loc_822C9B70:
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_822C9B80:
	// lhz r11,580(r9)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + 580);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822c9b70
	if (!ctx.cr6.gt) goto loc_822C9B70;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
loc_822C9B9C:
	// lwz r8,584(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 584);
	// lwz r11,320(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 320);
	// lwz r31,256(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 256);
	// lhzx r7,r10,r8
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// extsh r10,r7
	ctx.r10.s64 = ctx.r7.s16;
	// mulli r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 * 1776;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r8,424(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// lhz r10,114(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 114);
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// lwz r30,8(r8)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,12(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lhzx r7,r7,r30
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + ctx.r30.u32);
	// lhz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r7,r31
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r31.s32, ctx.xer);
	// blt cr6,0x822c9c0c
	if (ctx.cr6.lt) goto loc_822C9C0C;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// stw r3,216(r5)
	PPC_STORE_U32(ctx.r5.u32 + 216, ctx.r3.u32);
	// lwz r7,424(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// addi r11,r10,1
	ctx.r11.s64 = ctx.r10.s64 + 1;
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r4,r7,r10
	PPC_STORE_U16(ctx.r7.u32 + ctx.r10.u32, ctx.r4.u16);
	// b 0x822c9c80
	goto loc_822C9C80;
loc_822C9C0C:
	// lwz r8,424(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,8(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// sth r4,2(r8)
	PPC_STORE_U16(ctx.r8.u32 + 2, ctx.r4.u16);
	// lwz r7,424(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// lwz r8,8(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// lwz r31,12(r7)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lhzx r8,r8,r10
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r10.u32);
	// lhz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// sth r8,0(r31)
	PPC_STORE_U16(ctx.r31.u32 + 0, ctx.r8.u16);
	// lwz r8,424(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// lwz r7,256(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 256);
	// lwz r31,12(r8)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r8,8(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lhz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// lhz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r8,r7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, ctx.xer);
	// bgt cr6,0x822c9cb0
	if (ctx.cr6.gt) goto loc_822C9CB0;
	// lwz r10,424(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 424);
	// lhz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r7,r11,1
	ctx.r7.s64 = ctx.r11.s64 + 1;
	// sth r7,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r7.u16);
loc_822C9C80:
	// lhz r10,580(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 580);
	// addi r11,r6,1
	ctx.r11.s64 = ctx.r6.s64 + 1;
	// extsh r8,r11
	ctx.r8.s64 = ctx.r11.s16;
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// cmpw cr6,r8,r7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, ctx.xer);
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822c9b9c
	if (ctx.cr6.lt) goto loc_822C9B9C;
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_822C9CB0:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822C9CC4"))) PPC_WEAK_FUNC(sub_822C9CC4);
PPC_FUNC_IMPL(__imp__sub_822C9CC4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822C9CC8"))) PPC_WEAK_FUNC(sub_822C9CC8);
PPC_FUNC_IMPL(__imp__sub_822C9CC8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e43c
	ctx.lr = 0x822C9CD0;
	__restfpr_17(ctx, base);
	// stfd f30,-144(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f30.u64);
	// stfd f31,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f31.u64);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lwz r17,56(r5)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r5.u32 + 56);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r20,r4
	ctx.r20.u64 = ctx.r4.u64;
	// mr r25,r5
	ctx.r25.u64 = ctx.r5.u64;
	// addi r19,r11,-17328
	ctx.r19.s64 = ctx.r11.s64 + -17328;
	// dcbt r0,r19
	// li r11,128
	ctx.r11.s64 = 128;
	// dcbt r11,r19
	// lwz r10,224(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822c9d1c
	if (!ctx.cr6.gt) goto loc_822C9D1C;
	// lhz r11,118(r5)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x822c9d34
	if (ctx.cr6.gt) goto loc_822C9D34;
loc_822C9D1C:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f30,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lfd f31,-136(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
loc_822C9D34:
	// rlwinm r9,r10,12,0,19
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFFFF000;
	// li r24,0
	ctx.r24.s64 = 0;
	// divw r23,r9,r11
	ctx.r23.s32 = ctx.r9.s32 / ctx.r11.s32;
	// cmplwi cr6,r23,1
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 1, ctx.xer);
	// ble cr6,0x822c9d58
	if (!ctx.cr6.gt) goto loc_822C9D58;
loc_822C9D48:
	// addi r24,r24,1
	ctx.r24.s64 = ctx.r24.s64 + 1;
	// srw r11,r23,r24
	ctx.r11.u64 = ctx.r24.u8 & 0x20 ? 0 : (ctx.r23.u32 >> (ctx.r24.u8 & 0x3F));
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bgt cr6,0x822c9d48
	if (ctx.cr6.gt) goto loc_822C9D48;
loc_822C9D58:
	// lwz r9,256(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 256);
	// li r11,0
	ctx.r11.s64 = 0;
	// divw r10,r9,r10
	ctx.r10.s32 = ctx.r9.s32 / ctx.r10.s32;
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// ble cr6,0x822c9d7c
	if (!ctx.cr6.gt) goto loc_822C9D7C;
loc_822C9D6C:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 1, ctx.xer);
	// bgt cr6,0x822c9d6c
	if (ctx.cr6.gt) goto loc_822C9D6C;
loc_822C9D7C:
	// lwz r9,344(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 344);
	// mulli r11,r11,116
	ctx.r11.s64 = ctx.r11.s64 * 116;
	// add r27,r11,r9
	ctx.r27.u64 = ctx.r11.u64 + ctx.r9.u64;
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// ble cr6,0x822c9da4
	if (!ctx.cr6.gt) goto loc_822C9DA4;
loc_822C9D94:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 1, ctx.xer);
	// bgt cr6,0x822c9d94
	if (ctx.cr6.gt) goto loc_822C9D94;
loc_822C9DA4:
	// lhz r9,202(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,36(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 36);
	// addi r10,r27,4
	ctx.r10.s64 = ctx.r27.s64 + 4;
	// extsh r6,r9
	ctx.r6.s64 = ctx.r9.s16;
	// lwz r7,340(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 340);
	// addi r3,r11,-1
	ctx.r3.s64 = ctx.r11.s64 + -1;
	// lwz r5,4(r27)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	// mullw r4,r6,r23
	ctx.r4.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r23.s32);
	// lwzx r21,r7,r8
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
	// srawi r11,r4,12
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFF) != 0);
	ctx.r11.s64 = ctx.r4.s32 >> 12;
	// li r30,0
	ctx.r30.s64 = 0;
	// extsh r18,r3
	ctx.r18.s64 = ctx.r3.s16;
	// li r29,-1
	ctx.r29.s64 = -1;
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// blt cr6,0x822c9e04
	if (ctx.cr6.lt) goto loc_822C9E04;
	// clrlwi r11,r9,16
	ctx.r11.u64 = ctx.r9.u32 & 0xFFFF;
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// mullw r8,r9,r23
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r23.s32);
	// srawi r9,r8,12
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFF) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 12;
loc_822C9DF4:
	// lwzu r11,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r11.u64 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x822c9df4
	if (!ctx.cr6.lt) goto loc_822C9DF4;
loc_822C9E04:
	// lwz r11,484(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 484);
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// mr r3,r20
	ctx.r3.u64 = ctx.r20.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822C9E18;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822ca178
	if (ctx.cr6.lt) goto loc_822CA178;
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// extsh r22,r18
	ctx.r22.s64 = ctx.r18.s16;
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lfs f30,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f30.f64 = double(temp.f32);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
	// sth r10,202(r31)
	PPC_STORE_U16(ctx.r31.u32 + 202, ctx.r10.u16);
	// cmpw cr6,r10,r22
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r22.s32, ctx.xer);
	// bge cr6,0x822ca024
	if (!ctx.cr6.lt) goto loc_822CA024;
	// li r26,1
	ctx.r26.s64 = 1;
loc_822C9E58:
	// cmpw cr6,r30,r21
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822ca024
	if (!ctx.cr6.lt) goto loc_822CA024;
	// addi r9,r30,1
	ctx.r9.s64 = ctx.r30.s64 + 1;
	// mullw r8,r11,r23
	ctx.r8.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r23.s32);
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r7,r8,12
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 12;
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmpw cr6,r7,r6
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r6.s32, ctx.xer);
	// blt cr6,0x822c9ea0
	if (ctx.cr6.lt) goto loc_822C9EA0;
	// lhz r9,202(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// mullw r7,r8,r23
	ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r23.s32);
	// srawi r8,r7,12
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFF) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 12;
loc_822C9E90:
	// lwzu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// bge cr6,0x822c9e90
	if (!ctx.cr6.lt) goto loc_822C9E90;
loc_822C9EA0:
	// cmpw cr6,r30,r21
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822ca024
	if (!ctx.cr6.lt) goto loc_822CA024;
	// extsh r11,r10
	ctx.r11.s64 = ctx.r10.s16;
	// extsh r10,r29
	ctx.r10.s64 = ctx.r29.s16;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// bne cr6,0x822c9eec
	if (!ctx.cr6.eq) goto loc_822C9EEC;
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f31,f12,f30
	ctx.f31.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// bl 0x822f30e8
	ctx.lr = 0x822C9EE4;
	sub_822F30E8(ctx, base);
	// fmr f30,f1
	ctx.fpscr.disableFlushMode();
	ctx.f30.f64 = ctx.f1.f64;
	// b 0x822c9f14
	goto loc_822C9F14;
loc_822C9EEC:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822f30e8
	ctx.lr = 0x822C9EF4;
	sub_822F30E8(ctx, base);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// fmr f30,f1
	ctx.fpscr.disableFlushMode();
	ctx.f30.f64 = ctx.f1.f64;
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f0,88(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f31,f12,f1
	ctx.f31.f64 = double(float(ctx.f12.f64 * ctx.f1.f64));
loc_822C9F14:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// cmpwi cr6,r24,12
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 12, ctx.xer);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// ble cr6,0x822c9f44
	if (!ctx.cr6.gt) goto loc_822C9F44;
	// addi r10,r24,-13
	ctx.r10.s64 = ctx.r24.s64 + -13;
	// addi r8,r24,-12
	ctx.r8.s64 = ctx.r24.s64 + -12;
	// slw r11,r26,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r26.u32 << (ctx.r10.u8 & 0x3F));
	// lwzx r10,r9,r27
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r27.u32);
	// add r7,r11,r10
	ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
	// sraw r6,r7,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r7.s32 < 0) & (((ctx.r7.s32 >> temp.u32) << temp.u32) != ctx.r7.s32);
	ctx.r6.s64 = ctx.r7.s32 >> temp.u32;
	// extsh r11,r6
	ctx.r11.s64 = ctx.r6.s16;
	// b 0x822c9f54
	goto loc_822C9F54;
loc_822C9F44:
	// lwzx r8,r9,r27
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r27.u32);
	// subfic r10,r24,12
	ctx.xer.ca = ctx.r24.u32 <= 12;
	ctx.r10.s64 = 12 - ctx.r24.s64;
	// slw r7,r8,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
	// extsh r11,r7
	ctx.r11.s64 = ctx.r7.s16;
loc_822C9F54:
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// extsh r29,r11
	ctx.r29.s64 = ctx.r11.s16;
	// mr r10,r29
	ctx.r10.u64 = ctx.r29.u64;
	// cmpw cr6,r29,r22
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r22.s32, ctx.xer);
	// ble cr6,0x822c9f6c
	if (!ctx.cr6.gt) goto loc_822C9F6C;
	// mr r29,r18
	ctx.r29.u64 = ctx.r18.u64;
loc_822C9F6C:
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822c9f7c
	if (ctx.cr6.eq) goto loc_822C9F7C;
	// fneg f31,f31
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = ctx.f31.u64 ^ 0x8000000000000000;
loc_822C9F7C:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// mr r3,r20
	ctx.r3.u64 = ctx.r20.u64;
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f31,r9,r17
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r17.u32, temp.u32);
	// lwz r8,484(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 484);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x822C9FA0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822ca178
	if (ctx.cr6.lt) goto loc_822CA178;
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// extsh r9,r29
	ctx.r9.s64 = ctx.r29.s16;
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// extsh r10,r8
	ctx.r10.s64 = ctx.r8.s16;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
	// sth r10,202(r31)
	PPC_STORE_U16(ctx.r31.u32 + 202, ctx.r10.u16);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// bge cr6,0x822ca010
	if (!ctx.cr6.lt) goto loc_822CA010;
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r11,64
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 64, ctx.xer);
	// bge cr6,0x822c9ff4
	if (!ctx.cr6.lt) goto loc_822C9FF4;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r11,r19
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r19.u32);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f0,f30
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f30.f64));
	// b 0x822c9f6c
	goto loc_822C9F6C;
loc_822C9FF4:
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f31,f12,f30
	ctx.f31.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// b 0x822c9f6c
	goto loc_822C9F6C;
loc_822CA010:
	// lhz r9,202(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// cmpw cr6,r8,r22
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r22.s32, ctx.xer);
	// blt cr6,0x822c9e58
	if (ctx.cr6.lt) goto loc_822C9E58;
loc_822CA024:
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// cmpw cr6,r9,r22
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r22.s32, ctx.xer);
	// bne cr6,0x822ca0d0
	if (!ctx.cr6.eq) goto loc_822CA0D0;
	// extsh r11,r29
	ctx.r11.s64 = ctx.r29.s16;
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x822ca094
	if (ctx.cr6.lt) goto loc_822CA094;
	// addi r10,r30,-1
	ctx.r10.s64 = ctx.r30.s64 + -1;
	// cmpw cr6,r10,r21
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822ca078
	if (!ctx.cr6.lt) goto loc_822CA078;
	// rlwinm r11,r30,2,0,29
	ctx.r11.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
loc_822CA050:
	// mullw r8,r9,r23
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r23.s32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// srawi r6,r8,12
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFF) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 12;
	// cmpw cr6,r6,r7
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r7.s32, ctx.xer);
	// blt cr6,0x822ca078
	if (ctx.cr6.lt) goto loc_822CA078;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmpw cr6,r10,r21
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r21.s32, ctx.xer);
	// blt cr6,0x822ca050
	if (ctx.cr6.lt) goto loc_822CA050;
loc_822CA078:
	// addi r5,r30,-1
	ctx.r5.s64 = ctx.r30.s64 + -1;
	// cmpw cr6,r5,r21
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r21.s32, ctx.xer);
	// bgt cr6,0x822ca094
	if (ctx.cr6.gt) goto loc_822CA094;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822f30e8
	ctx.lr = 0x822CA090;
	sub_822F30E8(ctx, base);
	// fmr f30,f1
	ctx.fpscr.disableFlushMode();
	ctx.f30.f64 = ctx.f1.f64;
loc_822CA094:
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// extsw r9,r11
	ctx.r9.s64 = ctx.r11.s32;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f0,f12,f30
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// beq cr6,0x822ca0c0
	if (ctx.cr6.eq) goto loc_822CA0C0;
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
loc_822CA0C0:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f0,r9,r17
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r17.u32, temp.u32);
loc_822CA0D0:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lhz r10,118(r25)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r25.u32 + 118);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// cmpw cr6,r9,r8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, ctx.xer);
	// bgt cr6,0x822c9d1c
	if (ctx.cr6.gt) goto loc_822C9D1C;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x822ca104
	if (!ctx.cr6.gt) goto loc_822CA104;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r17
	ctx.r3.u64 = ctx.r17.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822CA104;
	sub_8233EAF0(ctx, base);
loc_822CA104:
	// lhz r11,120(r25)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r25.u32 + 120);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,472(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 472);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r10,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r10.s64;
	// add r3,r11,r17
	ctx.r3.u64 = ctx.r11.u64 + ctx.r17.u64;
	// rlwinm r5,r8,2,0,29
	ctx.r5.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233eaf0
	ctx.lr = 0x822CA128;
	sub_8233EAF0(ctx, base);
	// lhz r7,202(r31)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// cmpw cr6,r6,r22
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r22.s32, ctx.xer);
	// bne cr6,0x822ca158
	if (!ctx.cr6.eq) goto loc_822CA158;
	// mr r11,r18
	ctx.r11.u64 = ctx.r18.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// sth r11,490(r25)
	PPC_STORE_U16(ctx.r25.u32 + 490, ctx.r11.u16);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f30,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lfd f31,-136(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
loc_822CA158:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// sth r9,490(r25)
	PPC_STORE_U16(ctx.r25.u32 + 490, ctx.r9.u16);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f30,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lfd f31,-136(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
loc_822CA178:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f30,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lfd f31,-136(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CA18C"))) PPC_WEAK_FUNC(sub_822CA18C);
PPC_FUNC_IMPL(__imp__sub_822CA18C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CA190"))) PPC_WEAK_FUNC(sub_822CA190);
PPC_FUNC_IMPL(__imp__sub_822CA190) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822CA198;
	__restfpr_27(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lhz r11,118(r5)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r27,56(r5)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r5.u32 + 56);
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822ca2fc
	if (!ctx.cr6.gt) goto loc_822CA2FC;
	// lwz r10,484(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 484);
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// lwz r11,268(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r30,r11,-1
	ctx.r30.s64 = ctx.r11.s64 + -1;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x822CA1DC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822ca304
	if (ctx.cr6.lt) goto loc_822CA304;
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// extsh r11,r10
	ctx.r11.s64 = ctx.r10.s16;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// sth r11,202(r31)
	PPC_STORE_U16(ctx.r31.u32 + 202, ctx.r11.u16);
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// bge cr6,0x822ca2a4
	if (!ctx.cr6.lt) goto loc_822CA2A4;
loc_822CA20C:
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// extsw r9,r11
	ctx.r9.s64 = ctx.r11.s32;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_822CA21C:
	// fcfid f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(ctx.f0.s64);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// frsp f0,f13
	ctx.f0.f64 = double(float(ctx.f13.f64));
	// beq cr6,0x822ca234
	if (ctx.cr6.eq) goto loc_822CA234;
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
loc_822CA234:
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfsx f0,r11,r27
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r27.u32, temp.u32);
	// lwz r10,484(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 484);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x822CA250;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822ca304
	if (ctx.cr6.lt) goto loc_822CA304;
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// extsh r11,r10
	ctx.r11.s64 = ctx.r10.s16;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// sth r11,202(r31)
	PPC_STORE_U16(ctx.r31.u32 + 202, ctx.r11.u16);
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// bge cr6,0x822ca294
	if (!ctx.cr6.lt) goto loc_822CA294;
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// extsw r9,r11
	ctx.r9.s64 = ctx.r11.s32;
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x822ca21c
	goto loc_822CA21C;
loc_822CA294:
	// lhz r9,202(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// cmpw cr6,r8,r30
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r30.s32, ctx.xer);
	// blt cr6,0x822ca20c
	if (ctx.cr6.lt) goto loc_822CA20C;
loc_822CA2A4:
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// bne cr6,0x822ca2e8
	if (!ctx.cr6.eq) goto loc_822CA2E8;
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// extsw r9,r11
	ctx.r9.s64 = ctx.r11.s32;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f0,f13
	ctx.f0.f64 = double(float(ctx.f13.f64));
	// beq cr6,0x822ca2d8
	if (ctx.cr6.eq) goto loc_822CA2D8;
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
loc_822CA2D8:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f0,r9,r27
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r27.u32, temp.u32);
loc_822CA2E8:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lwz r10,268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// ble cr6,0x822ca304
	if (!ctx.cr6.gt) goto loc_822CA304;
loc_822CA2FC:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
loc_822CA304:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpw cr6,r10,r30
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r30.s32, ctx.xer);
	// bne cr6,0x822ca324
	if (!ctx.cr6.eq) goto loc_822CA324;
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// sth r11,490(r29)
	PPC_STORE_U16(ctx.r29.u32 + 490, ctx.r11.u16);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822CA324:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// sth r9,490(r29)
	PPC_STORE_U16(ctx.r29.u32 + 490, ctx.r9.u16);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CA33C"))) PPC_WEAK_FUNC(sub_822CA33C);
PPC_FUNC_IMPL(__imp__sub_822CA33C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CA340"))) PPC_WEAK_FUNC(sub_822CA340);
PPC_FUNC_IMPL(__imp__sub_822CA340) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e460
	ctx.lr = 0x822CA348;
	__restfpr_26(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lhz r11,118(r5)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r28,56(r5)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r5.u32 + 56);
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// li r26,0
	ctx.r26.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x822ca4e0
	if (!ctx.cr6.gt) goto loc_822CA4E0;
	// lwz r9,456(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 456);
	// lwz r10,268(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// addi r26,r10,-1
	ctx.r26.s64 = ctx.r10.s64 + -1;
	// sraw r30,r11,r7
	temp.u32 = ctx.r7.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r11.s32 < 0) & (((ctx.r11.s32 >> temp.u32) << temp.u32) != ctx.r11.s32);
	ctx.r30.s64 = ctx.r11.s32 >> temp.u32;
	// cmpw cr6,r30,r26
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r26.s32, ctx.xer);
	// blt cr6,0x822ca390
	if (ctx.cr6.lt) goto loc_822CA390;
	// mr r30,r26
	ctx.r30.u64 = ctx.r26.u64;
loc_822CA390:
	// lwz r11,484(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 484);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822CA3A4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822ca4e8
	if (ctx.cr6.lt) goto loc_822CA4E8;
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// extsh r11,r10
	ctx.r11.s64 = ctx.r10.s16;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// sth r11,202(r31)
	PPC_STORE_U16(ctx.r31.u32 + 202, ctx.r11.u16);
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// bge cr6,0x822ca46c
	if (!ctx.cr6.lt) goto loc_822CA46C;
loc_822CA3D4:
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// extsw r9,r11
	ctx.r9.s64 = ctx.r11.s32;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_822CA3E4:
	// fcfid f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(ctx.f0.s64);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// frsp f0,f13
	ctx.f0.f64 = double(float(ctx.f13.f64));
	// beq cr6,0x822ca3fc
	if (ctx.cr6.eq) goto loc_822CA3FC;
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
loc_822CA3FC:
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// stfsx f0,r11,r28
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r28.u32, temp.u32);
	// lwz r10,484(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 484);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x822CA418;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822ca4e8
	if (ctx.cr6.lt) goto loc_822CA4E8;
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// extsh r11,r10
	ctx.r11.s64 = ctx.r10.s16;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// sth r11,202(r31)
	PPC_STORE_U16(ctx.r31.u32 + 202, ctx.r11.u16);
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// bge cr6,0x822ca45c
	if (!ctx.cr6.lt) goto loc_822CA45C;
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// extsw r9,r11
	ctx.r9.s64 = ctx.r11.s32;
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x822ca3e4
	goto loc_822CA3E4;
loc_822CA45C:
	// lhz r9,202(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// cmpw cr6,r8,r30
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r30.s32, ctx.xer);
	// blt cr6,0x822ca3d4
	if (ctx.cr6.lt) goto loc_822CA3D4;
loc_822CA46C:
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// cmpw cr6,r11,r26
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r26.s32, ctx.xer);
	// bge cr6,0x822ca4cc
	if (!ctx.cr6.lt) goto loc_822CA4CC;
loc_822CA478:
	// lwz r11,484(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 484);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822CA48C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822ca4e8
	if (ctx.cr6.lt) goto loc_822CA4E8;
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// sth r9,202(r31)
	PPC_STORE_U16(ctx.r31.u32 + 202, ctx.r9.u16);
	// cmpw cr6,r9,r26
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r26.s32, ctx.xer);
	// blt cr6,0x822ca478
	if (ctx.cr6.lt) goto loc_822CA478;
	// clrlwi r11,r9,16
	ctx.r11.u64 = ctx.r9.u32 & 0xFFFF;
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpw cr6,r10,r26
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r26.s32, ctx.xer);
	// blt cr6,0x822ca478
	if (ctx.cr6.lt) goto loc_822CA478;
loc_822CA4CC:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lwz r10,268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// ble cr6,0x822ca4e8
	if (!ctx.cr6.gt) goto loc_822CA4E8;
loc_822CA4E0:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
loc_822CA4E8:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpw cr6,r10,r26
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r26.s32, ctx.xer);
	// bne cr6,0x822ca508
	if (!ctx.cr6.eq) goto loc_822CA508;
	// addi r11,r26,1
	ctx.r11.s64 = ctx.r26.s64 + 1;
	// sth r11,490(r29)
	PPC_STORE_U16(ctx.r29.u32 + 490, ctx.r11.u16);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
loc_822CA508:
	// lhz r11,202(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 202);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// sth r9,490(r29)
	PPC_STORE_U16(ctx.r29.u32 + 490, ctx.r9.u16);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CA520"))) PPC_WEAK_FUNC(sub_822CA520);
PPC_FUNC_IMPL(__imp__sub_822CA520) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e438
	ctx.lr = 0x822CA528;
	__restfpr_16(ctx, base);
	// stfd f29,-160(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.f29.u64);
	// stfd f30,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.f30.u64);
	// stfd f31,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f31.u64);
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// li r25,0
	ctx.r25.s64 = 0;
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r26,r25
	ctx.r26.u64 = ctx.r25.u64;
	// mr r17,r25
	ctx.r17.u64 = ctx.r25.u64;
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// beq cr6,0x822cae6c
	if (ctx.cr6.eq) goto loc_822CAE6C;
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// lwz r30,96(r1)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32764
	ctx.r9.s64 = -2147221504;
	// li r22,1
	ctx.r22.s64 = 1;
	// li r23,4
	ctx.r23.s64 = 4;
	// lfs f31,-28948(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// li r18,2
	ctx.r18.s64 = 2;
	// lfs f29,5260(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5260);
	ctx.f29.f64 = double(temp.f32);
	// li r24,6
	ctx.r24.s64 = 6;
	// ori r20,r9,4
	ctx.r20.u64 = ctx.r9.u64 | 4;
	// li r16,5
	ctx.r16.s64 = 5;
	// li r19,8
	ctx.r19.s64 = 8;
	// li r21,-1
	ctx.r21.s64 = -1;
loc_822CA590:
	// lwz r11,36(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 36);
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bgt cr6,0x822cae3c
	if (ctx.cr6.gt) goto loc_822CAE3C;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bdzf 4*cr6+eq,0x822ca728
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822CA728;
	// bdzf 4*cr6+eq,0x822ca918
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822CA918;
	// bdzf 4*cr6+eq,0x822cae3c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822CAE3C;
	// bdzf 4*cr6+eq,0x822cae24
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822CAE24;
	// bne cr6,0x822caa8c
	if (!ctx.cr6.eq) goto loc_822CAA8C;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822f0c60
	ctx.lr = 0x822CA5C0;
	sub_822F0C60(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cae48
	if (ctx.cr6.lt) goto loc_822CAE48;
	// lwz r11,132(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ca698
	if (!ctx.cr6.eq) goto loc_822CA698;
	// lwz r3,296(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 296);
	// bl 0x822ef640
	ctx.lr = 0x822CA5E0;
	sub_822EF640(ctx, base);
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// stfs f1,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822ca648
	if (!ctx.cr6.gt) goto loc_822CA648;
	// mr r28,r25
	ctx.r28.u64 = ctx.r25.u64;
	// rlwinm r11,r25,1,0,30
	ctx.r11.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822CA5FC:
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lwz r8,472(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 472);
	// rlwinm r5,r8,2,0,29
	ctx.r5.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lhzx r7,r11,r9
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r9.u32);
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// mulli r11,r6,1776
	ctx.r11.s64 = ctx.r6.s64 * 1776;
	// add r30,r11,r10
	ctx.r30.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r3,56(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// bl 0x8233eaf0
	ctx.lr = 0x822CA628;
	sub_8233EAF0(ctx, base);
	// lhz r3,580(r31)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// addi r5,r28,1
	ctx.r5.s64 = ctx.r28.s64 + 1;
	// extsh r4,r5
	ctx.r4.s64 = ctx.r5.s16;
	// extsh r10,r3
	ctx.r10.s64 = ctx.r3.s16;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r4,r10
	ctx.cr6.compare<int32_t>(ctx.r4.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x822ca5fc
	if (ctx.cr6.lt) goto loc_822CA5FC;
loc_822CA648:
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// stw r22,36(r29)
	PPC_STORE_U32(ctx.r29.u32 + 36, ctx.r22.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// sth r11,202(r10)
	PPC_STORE_U16(ctx.r10.u32 + 202, ctx.r11.u16);
	// lwz r8,0(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// sth r25,150(r29)
	PPC_STORE_U16(ctx.r29.u32 + 150, ctx.r25.u16);
	// lwz r7,60(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 60);
	// cmpwi cr6,r7,2
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 2, ctx.xer);
	// bgt cr6,0x822ca678
	if (ctx.cr6.gt) goto loc_822CA678;
	// stw r25,56(r29)
	PPC_STORE_U32(ctx.r29.u32 + 56, ctx.r25.u32);
	// b 0x822cae3c
	goto loc_822CAE3C;
loc_822CA678:
	// lwz r11,512(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 512);
	// stw r23,56(r29)
	PPC_STORE_U32(ctx.r29.u32 + 56, ctx.r23.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cae3c
	if (ctx.cr6.eq) goto loc_822CAE3C;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822CA694;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// b 0x822cae3c
	goto loc_822CAE3C;
loc_822CA698:
	// lwz r10,268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// sth r10,730(r31)
	PPC_STORE_U16(ctx.r31.u32 + 730, ctx.r10.u16);
	// bne cr6,0x822ca6cc
	if (!ctx.cr6.eq) goto loc_822CA6CC;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x822ca6cc
	if (ctx.cr6.eq) goto loc_822CA6CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,320(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// bl 0x822ed500
	ctx.lr = 0x822CA6C0;
	sub_822ED500(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cae48
	if (ctx.cr6.lt) goto loc_822CAE48;
loc_822CA6CC:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822ca6fc
	if (!ctx.cr6.eq) goto loc_822CA6FC;
	// lwz r11,192(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822ca6fc
	if (!ctx.cr6.eq) goto loc_822CA6FC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,320(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// bl 0x822ed500
	ctx.lr = 0x822CA6F0;
	sub_822ED500(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cae48
	if (ctx.cr6.lt) goto loc_822CAE48;
loc_822CA6FC:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// stw r18,36(r29)
	PPC_STORE_U32(ctx.r29.u32 + 36, ctx.r18.u32);
	// sth r25,150(r29)
	PPC_STORE_U16(ctx.r29.u32 + 150, ctx.r25.u16);
	// stb r25,145(r29)
	PPC_STORE_U8(ctx.r29.u32 + 145, ctx.r25.u8);
	// stw r24,72(r29)
	PPC_STORE_U32(ctx.r29.u32 + 72, ctx.r24.u32);
	// sth r25,148(r29)
	PPC_STORE_U16(ctx.r29.u32 + 148, ctx.r25.u16);
	// sth r25,202(r11)
	PPC_STORE_U16(ctx.r11.u32 + 202, ctx.r25.u16);
	// stw r25,76(r29)
	PPC_STORE_U32(ctx.r29.u32 + 76, ctx.r25.u32);
	// stw r25,200(r29)
	PPC_STORE_U32(ctx.r29.u32 + 200, ctx.r25.u32);
	// stw r25,208(r29)
	PPC_STORE_U32(ctx.r29.u32 + 208, ctx.r25.u32);
	// b 0x822cae3c
	goto loc_822CAE3C;
loc_822CA728:
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// lhz r10,150(r29)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r29.u32 + 150);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// bge cr6,0x822ca8bc
	if (!ctx.cr6.lt) goto loc_822CA8BC;
loc_822CA740:
	// lhz r10,150(r29)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r29.u32 + 150);
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r6,r7,r9
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r7.u32 + ctx.r9.u32);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// mulli r9,r6,1776
	ctx.r9.s64 = ctx.r6.s64 * 1776;
	// add r30,r9,r10
	ctx.r30.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r10,36(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 36);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// ble cr6,0x822ca788
	if (!ctx.cr6.gt) goto loc_822CA788;
loc_822CA778:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 1, ctx.xer);
	// bgt cr6,0x822ca778
	if (ctx.cr6.gt) goto loc_822CA778;
loc_822CA788:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// sth r11,312(r29)
	PPC_STORE_U16(ctx.r29.u32 + 312, ctx.r11.u16);
	// lwz r9,40(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x822ca810
	if (ctx.cr6.eq) goto loc_822CA810;
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// blt cr6,0x822ca7fc
	if (ctx.cr6.lt) goto loc_822CA7FC;
	// lwz r11,444(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 444);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ca7c8
	if (!ctx.cr6.eq) goto loc_822CA7C8;
	// bl 0x822ca190
	ctx.lr = 0x822CA7C4;
	sub_822CA190(ctx, base);
	// b 0x822ca7cc
	goto loc_822CA7CC;
loc_822CA7C8:
	// bl 0x822ca340
	ctx.lr = 0x822CA7CC;
	sub_822CA340(ctx, base);
loc_822CA7CC:
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmplw cr6,r3,r20
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r20.u32, ctx.xer);
	// bne cr6,0x822ca808
	if (!ctx.cr6.eq) goto loc_822CA808;
	// addi r3,r29,224
	ctx.r3.s64 = ctx.r29.s64 + 224;
	// bl 0x822ed990
	ctx.lr = 0x822CA7E0;
	sub_822ED990(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x822cae48
	if (ctx.cr6.eq) goto loc_822CAE48;
	// lwz r11,704(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cae48
	if (ctx.cr6.eq) goto loc_822CAE48;
	// mr r17,r22
	ctx.r17.u64 = ctx.r22.u64;
	// b 0x822ca810
	goto loc_822CA810;
loc_822CA7FC:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c9cc8
	ctx.lr = 0x822CA804;
	sub_822C9CC8(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
loc_822CA808:
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// blt cr6,0x822cae48
	if (ctx.cr6.lt) goto loc_822CAE48;
loc_822CA810:
	// lhz r11,490(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 490);
	// lhz r10,730(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 730);
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x822ca82c
	if (!ctx.cr6.gt) goto loc_822CA82C;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822CA82C:
	// lwz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// sth r11,730(r31)
	PPC_STORE_U16(ctx.r31.u32 + 730, ctx.r11.u16);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x822ca850
	if (!ctx.cr6.eq) goto loc_822CA850;
	// lwz r10,264(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 264);
	// addi r11,r29,224
	ctx.r11.s64 = ctx.r29.s64 + 224;
	// clrlwi r9,r10,29
	ctx.r9.u64 = ctx.r10.u32 & 0x7;
	// subf r8,r9,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r9.s64;
	// stw r8,264(r29)
	PPC_STORE_U32(ctx.r29.u32 + 264, ctx.r8.u32);
loc_822CA850:
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// sth r11,202(r31)
	PPC_STORE_U16(ctx.r31.u32 + 202, ctx.r11.u16);
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r8,60(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 60);
	// cmpwi cr6,r8,2
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 2, ctx.xer);
	// bgt cr6,0x822ca874
	if (ctx.cr6.gt) goto loc_822CA874;
	// stw r25,56(r29)
	PPC_STORE_U32(ctx.r29.u32 + 56, ctx.r25.u32);
	// b 0x822ca890
	goto loc_822CA890;
loc_822CA874:
	// lwz r11,512(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 512);
	// stw r23,56(r29)
	PPC_STORE_U32(ctx.r29.u32 + 56, ctx.r23.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822ca890
	if (ctx.cr6.eq) goto loc_822CA890;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822CA890;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_822CA890:
	// lhz r11,150(r29)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r29.u32 + 150);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// clrlwi r6,r9,16
	ctx.r6.u64 = ctx.r9.u32 & 0xFFFF;
	// sth r9,150(r29)
	PPC_STORE_U16(ctx.r29.u32 + 150, ctx.r9.u16);
	// lhz r8,580(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// extsh r5,r6
	ctx.r5.s64 = ctx.r6.s16;
	// cmpw cr6,r5,r7
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r7.s32, ctx.xer);
	// blt cr6,0x822ca740
	if (ctx.cr6.lt) goto loc_822CA740;
loc_822CA8BC:
	// lwz r11,444(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 444);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ca910
	if (ctx.cr6.eq) goto loc_822CA910;
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x822ca910
	if (!ctx.cr6.gt) goto loc_822CA910;
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lhz r11,730(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 730);
	// lhz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// mulli r9,r7,1776
	ctx.r9.s64 = ctx.r7.s64 * 1776;
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lhz r5,118(r6)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r6.u32 + 118);
	// extsh r4,r5
	ctx.r4.s64 = ctx.r5.s16;
	// srawi r3,r4,1
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r4.s32 >> 1;
	// addze r10,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r10.s64 = temp.s64;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x822ca90c
	if (ctx.cr6.lt) goto loc_822CA90C;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822CA90C:
	// sth r11,730(r31)
	PPC_STORE_U16(ctx.r31.u32 + 730, ctx.r11.u16);
loc_822CA910:
	// stw r16,36(r29)
	PPC_STORE_U32(ctx.r29.u32 + 36, ctx.r16.u32);
	// b 0x822cae3c
	goto loc_822CAE3C;
loc_822CA918:
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// lhz r10,150(r29)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r29.u32 + 150);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// bge cr6,0x822caa84
	if (!ctx.cr6.lt) goto loc_822CAA84;
loc_822CA930:
	// lhz r11,150(r29)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r29.u32 + 150);
	// lwz r10,584(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// lwz r11,320(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r7,r8,r10
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r10.u32);
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// mulli r10,r6,1776
	ctx.r10.s64 = ctx.r6.s64 * 1776;
	// add r30,r10,r11
	ctx.r30.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r5,40(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// beq cr6,0x822ca97c
	if (ctx.cr6.eq) goto loc_822CA97C;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x822f2890
	ctx.lr = 0x822CA970;
	sub_822F2890(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cae48
	if (ctx.cr6.lt) goto loc_822CAE48;
loc_822CA97C:
	// lwz r11,40(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ca9e4
	if (ctx.cr6.eq) goto loc_822CA9E4;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x822f2e38
	ctx.lr = 0x822CA998;
	sub_822F2E38(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cae48
	if (ctx.cr6.lt) goto loc_822CAE48;
	// stw r25,48(r30)
	PPC_STORE_U32(ctx.r30.u32 + 48, ctx.r25.u32);
	// lwz r11,72(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// bne cr6,0x822caa14
	if (!ctx.cr6.eq) goto loc_822CAA14;
	// lwz r11,460(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 460);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ca9c8
	if (ctx.cr6.eq) goto loc_822CA9C8;
	// lwz r3,328(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 328);
	// b 0x822ca9cc
	goto loc_822CA9CC;
loc_822CA9C8:
	// lwz r3,56(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
loc_822CA9CC:
	// lhz r11,120(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// li r6,3
	ctx.r6.s64 = 3;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// extsh r4,r11
	ctx.r4.s64 = ctx.r11.s16;
	// bl 0x822ea358
	ctx.lr = 0x822CA9E0;
	sub_822EA358(ctx, base);
	// b 0x822caa14
	goto loc_822CAA14;
loc_822CA9E4:
	// lwz r11,460(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 460);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ca9f8
	if (ctx.cr6.eq) goto loc_822CA9F8;
	// lwz r3,328(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 328);
	// b 0x822ca9fc
	goto loc_822CA9FC;
loc_822CA9F8:
	// lwz r3,56(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
loc_822CA9FC:
	// lhz r11,120(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// li r4,0
	ctx.r4.s64 = 0;
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233eaf0
	ctx.lr = 0x822CAA10;
	sub_8233EAF0(ctx, base);
	// stw r25,48(r30)
	PPC_STORE_U32(ctx.r30.u32 + 48, ctx.r25.u32);
loc_822CAA14:
	// lwz r11,460(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 460);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822caa38
	if (ctx.cr6.eq) goto loc_822CAA38;
	// lhz r11,120(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// lwz r4,328(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 328);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// lwz r3,56(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233e4e0
	ctx.lr = 0x822CAA38;
	sub_8233E4E0(ctx, base);
loc_822CAA38:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// stb r25,145(r29)
	PPC_STORE_U8(ctx.r29.u32 + 145, ctx.r25.u8);
	// stw r24,72(r29)
	PPC_STORE_U32(ctx.r29.u32 + 72, ctx.r24.u32);
	// sth r25,148(r29)
	PPC_STORE_U16(ctx.r29.u32 + 148, ctx.r25.u16);
	// sth r25,202(r11)
	PPC_STORE_U16(ctx.r11.u32 + 202, ctx.r25.u16);
	// stw r25,76(r29)
	PPC_STORE_U32(ctx.r29.u32 + 76, ctx.r25.u32);
	// stw r25,200(r29)
	PPC_STORE_U32(ctx.r29.u32 + 200, ctx.r25.u32);
	// stw r25,208(r29)
	PPC_STORE_U32(ctx.r29.u32 + 208, ctx.r25.u32);
	// lhz r10,150(r29)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r29.u32 + 150);
	// extsh r11,r10
	ctx.r11.s64 = ctx.r10.s16;
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// clrlwi r6,r8,16
	ctx.r6.u64 = ctx.r8.u32 & 0xFFFF;
	// sth r8,150(r29)
	PPC_STORE_U16(ctx.r29.u32 + 150, ctx.r8.u16);
	// lhz r7,580(r31)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r4,r7
	ctx.r4.s64 = ctx.r7.s16;
	// extsh r5,r6
	ctx.r5.s64 = ctx.r6.s16;
	// cmpw cr6,r5,r4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r4.s32, ctx.xer);
	// blt cr6,0x822ca930
	if (ctx.cr6.lt) goto loc_822CA930;
loc_822CAA84:
	// stw r23,36(r29)
	PPC_STORE_U32(ctx.r29.u32 + 36, ctx.r23.u32);
	// b 0x822cae3c
	goto loc_822CAE3C;
loc_822CAA8C:
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822cab30
	if (!ctx.cr6.gt) goto loc_822CAB30;
	// mr r28,r25
	ctx.r28.u64 = ctx.r25.u64;
	// rlwinm r10,r25,1,0,30
	ctx.r10.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822CAAA4:
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// lwz r11,320(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lhzx r8,r10,r9
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// mulli r10,r7,1776
	ctx.r10.s64 = ctx.r7.s64 * 1776;
	// add r30,r10,r11
	ctx.r30.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r6,40(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x822caaf4
	if (ctx.cr6.eq) goto loc_822CAAF4;
	// stw r25,48(r30)
	PPC_STORE_U32(ctx.r30.u32 + 48, ctx.r25.u32);
	// lwz r11,72(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// bne cr6,0x822cab10
	if (!ctx.cr6.eq) goto loc_822CAB10;
	// lhz r11,118(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 118);
	// li r6,3
	ctx.r6.s64 = 3;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r3,56(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// extsh r4,r11
	ctx.r4.s64 = ctx.r11.s16;
	// bl 0x822ea358
	ctx.lr = 0x822CAAF0;
	sub_822EA358(ctx, base);
	// b 0x822cab10
	goto loc_822CAB10;
loc_822CAAF4:
	// lhz r11,120(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,56(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233eaf0
	ctx.lr = 0x822CAB0C;
	sub_8233EAF0(ctx, base);
	// stw r25,48(r30)
	PPC_STORE_U32(ctx.r30.u32 + 48, ctx.r25.u32);
loc_822CAB10:
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// mr r28,r9
	ctx.r28.u64 = ctx.r9.u64;
	// cmpw cr6,r9,r8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, ctx.xer);
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822caaa4
	if (ctx.cr6.lt) goto loc_822CAAA4;
loc_822CAB30:
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// mr r27,r22
	ctx.r27.u64 = ctx.r22.u64;
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822cab98
	if (!ctx.cr6.gt) goto loc_822CAB98;
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
	// lwz r8,320(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// rlwinm r10,r25,1,0,30
	ctx.r10.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822CAB54:
	// lhzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r9.u32);
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// mulli r10,r7,1776
	ctx.r10.s64 = ctx.r7.s64 * 1776;
	// add r6,r10,r8
	ctx.r6.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r5,40(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 40);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bne cr6,0x822cab94
	if (!ctx.cr6.eq) goto loc_822CAB94;
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// extsh r7,r11
	ctx.r7.s64 = ctx.r11.s16;
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// mr r11,r7
	ctx.r11.u64 = ctx.r7.u64;
	// cmpw cr6,r7,r6
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r6.s32, ctx.xer);
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822cab54
	if (ctx.cr6.lt) goto loc_822CAB54;
	// b 0x822cab98
	goto loc_822CAB98;
loc_822CAB94:
	// mr r27,r25
	ctx.r27.u64 = ctx.r25.u64;
loc_822CAB98:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822c9308
	ctx.lr = 0x822CABA0;
	sub_822C9308(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cae48
	if (ctx.cr6.lt) goto loc_822CAE48;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822efaa0
	ctx.lr = 0x822CABB8;
	sub_822EFAA0(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822efaa0
	ctx.lr = 0x822CABC4;
	sub_822EFAA0(ctx, base);
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x822cac8c
	if (!ctx.cr6.gt) goto loc_822CAC8C;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// bne cr6,0x822cac94
	if (!ctx.cr6.eq) goto loc_822CAC94;
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822caca0
	if (!ctx.cr6.gt) goto loc_822CACA0;
	// mr r28,r25
	ctx.r28.u64 = ctx.r25.u64;
	// rlwinm r11,r25,1,0,30
	ctx.r11.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822CABF0:
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// lhz r8,108(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 108);
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// lhzx r6,r11,r9
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r9.u32);
	// extsh r5,r6
	ctx.r5.s64 = ctx.r6.s16;
	// mulli r11,r5,1776
	ctx.r11.s64 = ctx.r5.s64 * 1776;
	// add r30,r11,r10
	ctx.r30.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmpw cr6,r5,r7
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r7.s32, ctx.xer);
	// bne cr6,0x822cac40
	if (!ctx.cr6.eq) goto loc_822CAC40;
	// lhz r11,120(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r9,312(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 312);
	// extsh r8,r11
	ctx.r8.s64 = ctx.r11.s16;
	// lwz r10,56(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r3,r11,r10
	ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233eaf0
	ctx.lr = 0x822CAC40;
	sub_8233EAF0(ctx, base);
loc_822CAC40:
	// sth r21,202(r31)
	PPC_STORE_U16(ctx.r31.u32 + 202, ctx.r21.u16);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r11,424(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 424);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// bl 0x822ec548
	ctx.lr = 0x822CAC5C;
	sub_822EC548(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cae48
	if (ctx.cr6.lt) goto loc_822CAE48;
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// mr r28,r9
	ctx.r28.u64 = ctx.r9.u64;
	// cmpw cr6,r9,r8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, ctx.xer);
	// rlwinm r11,r9,1,0,30
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822cabf0
	if (ctx.cr6.lt) goto loc_822CABF0;
	// b 0x822caca0
	goto loc_822CACA0;
loc_822CAC8C:
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// beq cr6,0x822caca0
	if (ctx.cr6.eq) goto loc_822CACA0;
loc_822CAC94:
	// lwz r11,784(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 784);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cae10
	if (ctx.cr6.eq) goto loc_822CAE10;
loc_822CACA0:
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822cacb4
	if (!ctx.cr6.eq) goto loc_822CACB4;
	// lfs f30,300(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	ctx.f30.f64 = double(temp.f32);
	// b 0x822cacd0
	goto loc_822CACD0;
loc_822CACB4:
	// lhz r11,118(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 118);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fdivs f30,f29,f12
	ctx.f30.f64 = double(float(ctx.f29.f64 / ctx.f12.f64));
loc_822CACD0:
	// lhz r11,580(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822cae10
	if (!ctx.cr6.gt) goto loc_822CAE10;
	// mr r28,r25
	ctx.r28.u64 = ctx.r25.u64;
	// rlwinm r11,r25,1,0,30
	ctx.r11.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
loc_822CACE8:
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 584);
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 320);
	// lwz r8,320(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 320);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// lhzx r7,r11,r9
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r9.u32);
	// extsh r6,r7
	ctx.r6.s64 = ctx.r7.s16;
	// mulli r11,r6,1776
	ctx.r11.s64 = ctx.r6.s64 * 1776;
	// add r30,r11,r10
	ctx.r30.u64 = ctx.r11.u64 + ctx.r10.u64;
	// beq cr6,0x822cad54
	if (ctx.cr6.eq) goto loc_822CAD54;
	// lhz r11,118(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 118);
	// lwz r10,332(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 332);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// lwz r7,328(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 328);
	// lwz r8,56(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// srawi r6,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r9.s32 >> 1;
	// addze r11,r6
	temp.s64 = ctx.r6.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r11.s64 = temp.s64;
	// mullw r5,r10,r11
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// divw r4,r5,r7
	ctx.r4.s32 = ctx.r5.s32 / ctx.r7.s32;
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// subf. r10,r4,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r4.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// add r11,r9,r8
	ctx.r11.u64 = ctx.r9.u64 + ctx.r8.u64;
	// ble 0x822cad54
	if (!ctx.cr0.gt) goto loc_822CAD54;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822CAD44:
	// stfs f31,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfsu f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cad44
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CAD44;
loc_822CAD54:
	// lwz r11,40(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822cad6c
	if (!ctx.cr6.eq) goto loc_822CAD6C;
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x822cadf0
	if (!ctx.cr6.gt) goto loc_822CADF0;
loc_822CAD6C:
	// lhz r11,120(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// lhz r10,118(r30)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r30.u32 + 118);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// ble cr6,0x822cada8
	if (!ctx.cr6.gt) goto loc_822CADA8;
	// clrlwi r9,r10,16
	ctx.r9.u64 = ctx.r10.u32 & 0xFFFF;
	// lwz r10,56(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// li r4,0
	ctx.r4.s64 = 0;
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// subf r7,r8,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r8.s64;
	// rlwinm r11,r8,2,0,29
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r10
	ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822CADA8;
	sub_8233EAF0(ctx, base);
loc_822CADA8:
	// lwz r9,464(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 464);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r7,496(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 496);
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f30.f64;
	// lwz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// lhz r6,114(r30)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r30.u32 + 114);
	// lhz r4,120(r30)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r30.u32 + 120);
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r10,r6
	ctx.r10.s64 = ctx.r6.s16;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// lwz r9,440(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 440);
	// lwz r8,532(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 532);
	// lwz r7,516(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 516);
	// lwz r3,56(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// bctrl 
	ctx.lr = 0x822CADF0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_822CADF0:
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 580);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// mr r28,r9
	ctx.r28.u64 = ctx.r9.u64;
	// cmpw cr6,r9,r8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, ctx.xer);
	// rlwinm r11,r9,1,0,30
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// blt cr6,0x822cace8
	if (ctx.cr6.lt) goto loc_822CACE8;
loc_822CAE10:
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x822cae30
	if (!ctx.cr6.gt) goto loc_822CAE30;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// bne cr6,0x822cae30
	if (!ctx.cr6.eq) goto loc_822CAE30;
loc_822CAE24:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82364238
	ctx.lr = 0x822CAE2C;
	sub_82364238(ctx, base);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
loc_822CAE30:
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// blt cr6,0x822cae48
	if (ctx.cr6.lt) goto loc_822CAE48;
	// stw r19,36(r29)
	PPC_STORE_U32(ctx.r29.u32 + 36, ctx.r19.u32);
loc_822CAE3C:
	// lwz r11,36(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 36);
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// bne cr6,0x822ca590
	if (!ctx.cr6.eq) goto loc_822CA590;
loc_822CAE48:
	// cmpwi cr6,r17,0
	ctx.cr6.compare<int32_t>(ctx.r17.s32, 0, ctx.xer);
	// beq cr6,0x822cae6c
	if (ctx.cr6.eq) goto loc_822CAE6C;
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,4
	ctx.r3.u64 = ctx.r3.u64 | 4;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f29,-160(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// lfd f30,-152(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// lfd f31,-144(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// b 0x8233e488
	__restgprlr_16(ctx, base);
	return;
loc_822CAE6C:
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f29,-160(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// lfd f30,-152(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// lfd f31,-144(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// b 0x8233e488
	__restgprlr_16(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CAE84"))) PPC_WEAK_FUNC(sub_822CAE84);
PPC_FUNC_IMPL(__imp__sub_822CAE84) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CAE88"))) PPC_WEAK_FUNC(sub_822CAE88);
PPC_FUNC_IMPL(__imp__sub_822CAE88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lhz r11,580(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 580);
	// li r4,0
	ctx.r4.s64 = 0;
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x822caf60
	if (!ctx.cr6.gt) goto loc_822CAF60;
	// li r5,0
	ctx.r5.s64 = 0;
loc_822CAEA8:
	// lwz r11,584(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 584);
	// lwz r10,320(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// lwz r9,176(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 176);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// lhzx r8,r5,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r11.u32);
	// extsh r6,r8
	ctx.r6.s64 = ctx.r8.s16;
	// mulli r11,r6,1776
	ctx.r11.s64 = ctx.r6.s64 * 1776;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// bne cr6,0x822caf48
	if (!ctx.cr6.eq) goto loc_822CAF48;
	// lwz r10,460(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 460);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x822caee8
	if (ctx.cr6.eq) goto loc_822CAEE8;
	// lwz r10,256(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
	// lwz r9,456(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 456);
	// sraw r10,r10,r9
	temp.u32 = ctx.r9.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// b 0x822caf00
	goto loc_822CAF00;
loc_822CAEE8:
	// lwz r10,448(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 448);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lwz r10,256(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
	// beq cr6,0x822caf00
	if (ctx.cr6.eq) goto loc_822CAF00;
	// lwz r9,456(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 456);
	// slw r10,r10,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r9.u8 & 0x3F));
loc_822CAF00:
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r31,116(r11)
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r11.u32 + 116);
	// lwz r7,140(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// add r30,r10,r9
	ctx.r30.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r8,324(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 324);
	// extsh r9,r31
	ctx.r9.s64 = ctx.r31.s16;
	// srawi r31,r30,1
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x1) != 0);
	ctx.r31.s64 = ctx.r30.s32 >> 1;
	// add r30,r7,r9
	ctx.r30.u64 = ctx.r7.u64 + ctx.r9.u64;
	// addze r31,r31
	temp.s64 = ctx.r31.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r31.u32;
	ctx.r31.s64 = temp.s64;
	// srawi r7,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 1;
	// sth r30,116(r11)
	PPC_STORE_U16(ctx.r11.u32 + 116, ctx.r30.u16);
	// mullw r10,r31,r6
	ctx.r10.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r6.s32);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r6,r10,r9
	ctx.r6.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r10,56(r11)
	PPC_STORE_U32(ctx.r11.u32 + 56, ctx.r10.u32);
	// stw r10,144(r11)
	PPC_STORE_U32(ctx.r11.u32 + 144, ctx.r10.u32);
loc_822CAF48:
	// lhz r11,580(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 580);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpw cr6,r4,r10
	ctx.cr6.compare<int32_t>(ctx.r4.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x822caea8
	if (ctx.cr6.lt) goto loc_822CAEA8;
loc_822CAF60:
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CAF70"))) PPC_WEAK_FUNC(sub_822CAF70);
PPC_FUNC_IMPL(__imp__sub_822CAF70) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x822caf98
	if (ctx.cr6.eq) goto loc_822CAF98;
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x822caf98
	if (!ctx.cr6.eq) goto loc_822CAF98;
	// lis r10,-32211
	ctx.r10.s64 = -2110980096;
	// addi r9,r10,-23264
	ctx.r9.s64 = ctx.r10.s64 + -23264;
	// b 0x822cafa0
	goto loc_822CAFA0;
loc_822CAF98:
	// lis r10,-32211
	ctx.r10.s64 = -2110980096;
	// addi r9,r10,-27688
	ctx.r9.s64 = ctx.r10.s64 + -27688;
loc_822CAFA0:
	// stw r9,504(r3)
	PPC_STORE_U32(ctx.r3.u32 + 504, ctx.r9.u32);
	// lhz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 34);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822cb068
	if (ctx.cr6.eq) goto loc_822CB068;
	// li r7,0
	ctx.r7.s64 = 0;
loc_822CAFB4:
	// lwz r9,320(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 320);
	// mulli r10,r7,1776
	ctx.r10.s64 = ctx.r7.s64 * 1776;
	// lwz r8,280(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// bne cr6,0x822cb030
	if (!ctx.cr6.eq) goto loc_822CB030;
	// lwz r10,448(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 448);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x822cafe8
	if (ctx.cr6.eq) goto loc_822CAFE8;
	// lwz r10,456(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 456);
	// lwz r8,256(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	// slw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
	// b 0x822cafec
	goto loc_822CAFEC;
loc_822CAFE8:
	// lwz r10,256(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 256);
loc_822CAFEC:
	// mullw r10,r7,r10
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// lwz r8,436(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 436);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r8,52(r9)
	PPC_STORE_U32(ctx.r9.u32 + 52, ctx.r8.u32);
	// lwz r6,448(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 448);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// lwz r10,256(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	// beq cr6,0x822cb018
	if (ctx.cr6.eq) goto loc_822CB018;
	// lwz r8,456(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 456);
	// slw r10,r10,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r8.u8 & 0x3F));
loc_822CB018:
	// mullw r10,r7,r10
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// lwz r8,436(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 436);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r8,148(r9)
	PPC_STORE_U32(ctx.r9.u32 + 148, ctx.r8.u32);
	// b 0x822cb050
	goto loc_822CB050;
loc_822CB030:
	// lwz r8,320(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 320);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r6,52(r9)
	PPC_STORE_U32(ctx.r9.u32 + 52, ctx.r6.u32);
	// lwz r8,320(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 320);
	// add r5,r8,r10
	ctx.r5.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r4,4(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// stw r4,148(r9)
	PPC_STORE_U32(ctx.r9.u32 + 148, ctx.r4.u32);
loc_822CB050:
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
	// lhz r9,34(r11)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r11.u32 + 34);
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x822cafb4
	if (ctx.cr6.lt) goto loc_822CAFB4;
loc_822CB068:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CB070"))) PPC_WEAK_FUNC(sub_822CB070);
PPC_FUNC_IMPL(__imp__sub_822CB070) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
loc_822CB07C:
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// blt cr6,0x822cb160
	if (ctx.cr6.lt) goto loc_822CB160;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r4,12
	ctx.r10.s64 = ctx.r4.s64 + 12;
	// addi r11,r3,4
	ctx.r11.s64 = ctx.r3.s64 + 4;
	// subf r9,r3,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r3.s64;
loc_822CB09C:
	// lfs f0,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// ble cr6,0x822cb0c8
	if (!ctx.cr6.gt) goto loc_822CB0C8;
	// lwz r31,-12(r10)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	// stfs f13,-4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lwzx r30,r9,r11
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// stw r30,-12(r10)
	PPC_STORE_U32(ctx.r10.u32 + -12, ctx.r30.u32);
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stwx r31,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, ctx.r31.u32);
loc_822CB0C8:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// ble cr6,0x822cb0f4
	if (!ctx.cr6.gt) goto loc_822CB0F4;
	// lwzx r31,r9,r11
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r30,-4(r10)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// stwx r30,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, ctx.r30.u32);
	// stfs f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r31,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r31.u32);
loc_822CB0F4:
	// lfs f0,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// ble cr6,0x822cb120
	if (!ctx.cr6.gt) goto loc_822CB120;
	// lwz r31,-4(r10)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lwz r30,0(r10)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// li r8,1
	ctx.r8.s64 = 1;
	// stw r30,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r30.u32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stw r31,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r31.u32);
loc_822CB120:
	// lfs f0,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// ble cr6,0x822cb14c
	if (!ctx.cr6.gt) goto loc_822CB14C;
	// lwz r31,0(r10)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r30,4(r10)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r30.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r31,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r31.u32);
loc_822CB14C:
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmpw cr6,r7,r6
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r6.s32, ctx.xer);
	// blt cr6,0x822cb09c
	if (ctx.cr6.lt) goto loc_822CB09C;
loc_822CB160:
	// cmpw cr6,r7,r5
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r5.s32, ctx.xer);
	// bge cr6,0x822cb1bc
	if (!ctx.cr6.lt) goto loc_822CB1BC;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r7,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r7.s64;
	// add r9,r11,r3
	ctx.r9.u64 = ctx.r11.u64 + ctx.r3.u64;
	// add r10,r11,r4
	ctx.r10.u64 = ctx.r11.u64 + ctx.r4.u64;
	// addi r11,r9,4
	ctx.r11.s64 = ctx.r9.s64 + 4;
	// subf r9,r3,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r3.s64;
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_822CB184:
	// lfs f0,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// ble cr6,0x822cb1b0
	if (!ctx.cr6.gt) goto loc_822CB1B0;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stfs f13,-4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwzx r6,r11,r9
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stwx r7,r11,r9
	PPC_STORE_U32(ctx.r11.u32 + ctx.r9.u32, ctx.r7.u32);
loc_822CB1B0:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x822cb184
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CB184;
loc_822CB1BC:
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// bne cr6,0x822cb07c
	if (!ctx.cr6.eq) goto loc_822CB07C;
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CB1D0"))) PPC_WEAK_FUNC(sub_822CB1D0);
PPC_FUNC_IMPL(__imp__sub_822CB1D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e454
	ctx.lr = 0x822CB1D8;
	__restfpr_23(ctx, base);
	// addi r12,r1,-80
	ctx.r12.s64 = ctx.r1.s64 + -80;
	// bl 0x8233fa28
	ctx.lr = 0x822CB1E0;
	sub_8233FA28(ctx, base);
	// stwu r1,-976(r1)
	ea = -976 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// mr r25,r5
	ctx.r25.u64 = ctx.r5.u64;
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
	// lfs f0,-17904(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -17904);
	ctx.f0.f64 = double(temp.f32);
	// mr r24,r7
	ctx.r24.u64 = ctx.r7.u64;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// lfs f13,-17908(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -17908);
	ctx.f13.f64 = double(temp.f32);
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// lfs f31,-28948(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// lis r3,-32255
	ctx.r3.s64 = -2113863680;
	// lfs f12,-17916(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -17916);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lfs f11,-17920(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -17920);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,-17924(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -17924);
	ctx.f10.f64 = double(temp.f32);
	// li r23,0
	ctx.r23.s64 = 0;
	// lfs f9,11152(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 11152);
	ctx.f9.f64 = double(temp.f32);
	// cmpwi cr6,r26,1
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 1, ctx.xer);
	// lfs f8,-17928(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + -17928);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,-17932(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + -17932);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,-17936(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -17936);
	ctx.f6.f64 = double(temp.f32);
	// stfs f13,100(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f31,104(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f31,108(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// stfs f12,112(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f11,116(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f10,120(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f9,124(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// stfs f8,128(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// stfs f7,132(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// stfs f6,136(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// blt cr6,0x822cb288
	if (ctx.cr6.lt) goto loc_822CB288;
	// cmpwi cr6,r25,1
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 1, ctx.xer);
	// bge cr6,0x822cb2a0
	if (!ctx.cr6.lt) goto loc_822CB2A0;
loc_822CB288:
	// lis r23,-32764
	ctx.r23.s64 = -2147221504;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// addi r1,r1,976
	ctx.r1.s64 = ctx.r1.s64 + 976;
	// addi r12,r1,-80
	ctx.r12.s64 = ctx.r1.s64 + -80;
	// bl 0x8233fa74
	ctx.lr = 0x822CB29C;
	__savefpr_24(ctx, base);
	// b 0x8233e4a4
	__restgprlr_23(ctx, base);
	return;
loc_822CB2A0:
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// ble cr6,0x822cb324
	if (!ctx.cr6.gt) goto loc_822CB324;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
loc_822CB2B0:
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r26,4
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 4, ctx.xer);
	// blt cr6,0x822cb2f4
	if (ctx.cr6.lt) goto loc_822CB2F4;
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r6,r26,-3
	ctx.r6.s64 = ctx.r26.s64 + -3;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CB2C8:
	// addi r8,r11,12
	ctx.r8.s64 = ctx.r11.s64 + 12;
	// stfsx f31,r10,r11
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
	// add r3,r10,r8
	ctx.r3.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmpw cr6,r9,r6
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, ctx.xer);
	// stfs f31,4(r7)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// stfs f31,-4(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + -4, temp.u32);
	// stfsx f31,r10,r8
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, temp.u32);
	// blt cr6,0x822cb2c8
	if (ctx.cr6.lt) goto loc_822CB2C8;
loc_822CB2F4:
	// cmpw cr6,r9,r26
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r26.s32, ctx.xer);
	// bge cr6,0x822cb318
	if (!ctx.cr6.lt) goto loc_822CB318;
	// subf r8,r9,r26
	ctx.r8.s64 = ctx.r26.s64 - ctx.r9.s64;
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822CB30C:
	// stfsx f31,r10,r11
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cb30c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CB30C;
loc_822CB318:
	// addic. r4,r4,-1
	ctx.xer.ca = ctx.r4.u32 > 0;
	ctx.r4.s64 = ctx.r4.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne 0x822cb2b0
	if (!ctx.cr0.eq) goto loc_822CB2B0;
loc_822CB324:
	// cmpwi cr6,r26,5
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 5, ctx.xer);
	// bne cr6,0x822cb344
	if (!ctx.cr6.eq) goto loc_822CB344;
	// cmpwi cr6,r25,5
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 5, ctx.xer);
	// bne cr6,0x822cb344
	if (!ctx.cr6.eq) goto loc_822CB344;
	// cmplwi cr6,r31,1543
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1543, ctx.xer);
	// bne cr6,0x822cb5b4
	if (!ctx.cr6.eq) goto loc_822CB5B4;
	// cmplwi cr6,r30,55
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 55, ctx.xer);
	// beq cr6,0x822cb5c4
	if (ctx.cr6.eq) goto loc_822CB5C4;
loc_822CB344:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f1,9024(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r11.u32 + 9024);
	// bl 0x8233d410
	ctx.lr = 0x822CB350;
	sub_8233D410(ctx, base);
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// li r11,1
	ctx.r11.s64 = 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// lfs f0,5272(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5272);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f26,f13,f0
	ctx.f26.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// ble cr6,0x822cb3d4
	if (!ctx.cr6.gt) goto loc_822CB3D4;
	// li r8,0
	ctx.r8.s64 = 0;
loc_822CB378:
	// addi r7,r1,576
	ctx.r7.s64 = ctx.r1.s64 + 576;
	// and r6,r11,r31
	ctx.r6.u64 = ctx.r11.u64 & ctx.r31.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// stwx r9,r8,r7
	PPC_STORE_U32(ctx.r8.u32 + ctx.r7.u32, ctx.r9.u32);
	// bne cr6,0x822cb3a8
	if (!ctx.cr6.eq) goto loc_822CB3A8;
loc_822CB38C:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r10,10
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 10, ctx.xer);
	// bgt cr6,0x822cb778
	if (ctx.cr6.gt) goto loc_822CB778;
	// and r7,r11,r31
	ctx.r7.u64 = ctx.r11.u64 & ctx.r31.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822cb38c
	if (ctx.cr6.eq) goto loc_822CB38C;
loc_822CB3A8:
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,704
	ctx.r5.s64 = ctx.r1.s64 + 704;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lfsx f0,r7,r6
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r6.u32);
	ctx.f0.f64 = double(temp.f32);
	// cmpw cr6,r9,r26
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r26.s32, ctx.xer);
	// stfsx f0,r8,r5
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + ctx.r5.u32, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// blt cr6,0x822cb378
	if (ctx.cr6.lt) goto loc_822CB378;
loc_822CB3D4:
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// addi r4,r1,576
	ctx.r4.s64 = ctx.r1.s64 + 576;
	// addi r3,r1,704
	ctx.r3.s64 = ctx.r1.s64 + 704;
	// bl 0x822cb070
	ctx.lr = 0x822CB3E4;
	sub_822CB070(ctx, base);
	// li r11,1
	ctx.r11.s64 = 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// ble cr6,0x822cb458
	if (!ctx.cr6.gt) goto loc_822CB458;
	// li r8,0
	ctx.r8.s64 = 0;
loc_822CB3FC:
	// addi r7,r1,448
	ctx.r7.s64 = ctx.r1.s64 + 448;
	// and r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 & ctx.r30.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// stwx r9,r8,r7
	PPC_STORE_U32(ctx.r8.u32 + ctx.r7.u32, ctx.r9.u32);
	// bne cr6,0x822cb42c
	if (!ctx.cr6.eq) goto loc_822CB42C;
loc_822CB410:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r10,10
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 10, ctx.xer);
	// bgt cr6,0x822cb778
	if (ctx.cr6.gt) goto loc_822CB778;
	// and r7,r11,r30
	ctx.r7.u64 = ctx.r11.u64 & ctx.r30.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822cb410
	if (ctx.cr6.eq) goto loc_822CB410;
loc_822CB42C:
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lfsx f0,r7,r6
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r6.u32);
	ctx.f0.f64 = double(temp.f32);
	// cmpw cr6,r9,r25
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r25.s32, ctx.xer);
	// stfsx f0,r8,r5
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + ctx.r5.u32, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// blt cr6,0x822cb3fc
	if (ctx.cr6.lt) goto loc_822CB3FC;
loc_822CB458:
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// addi r4,r1,448
	ctx.r4.s64 = ctx.r1.s64 + 448;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// bl 0x822cb070
	ctx.lr = 0x822CB468;
	sub_822CB070(ctx, base);
	// addi r27,r25,-1
	ctx.r27.s64 = ctx.r25.s64 + -1;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r27,4
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 4, ctx.xer);
	// blt cr6,0x822cb4e8
	if (ctx.cr6.lt) goto loc_822CB4E8;
	// addi r9,r27,-3
	ctx.r9.s64 = ctx.r27.s64 + -3;
	// li r11,0
	ctx.r11.s64 = 0;
	// addi r8,r1,196
	ctx.r8.s64 = ctx.r1.s64 + 196;
	// addi r7,r1,192
	ctx.r7.s64 = ctx.r1.s64 + 192;
	// addi r6,r1,320
	ctx.r6.s64 = ctx.r1.s64 + 320;
	// addi r5,r1,200
	ctx.r5.s64 = ctx.r1.s64 + 200;
loc_822CB490:
	// addi r4,r1,204
	ctx.r4.s64 = ctx.r1.s64 + 204;
	// lfsx f0,r11,r8
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r8.u32);
	ctx.f0.f64 = double(temp.f32);
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// lfsx f13,r11,r5
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r5.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r31,r1,324
	ctx.r31.s64 = ctx.r1.s64 + 324;
	// lfsx f12,r11,r7
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r7.u32);
	ctx.f12.f64 = double(temp.f32);
	// addi r30,r1,328
	ctx.r30.s64 = ctx.r1.s64 + 328;
	// fsubs f11,f0,f12
	ctx.f11.f64 = static_cast<float>(ctx.f0.f64 - ctx.f12.f64);
	// addi r29,r1,332
	ctx.r29.s64 = ctx.r1.s64 + 332;
	// fsubs f10,f13,f0
	ctx.f10.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// lfsx f9,r11,r4
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r4.u32);
	ctx.f9.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lfsx f8,r11,r3
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
	ctx.f8.f64 = double(temp.f32);
	// fsubs f7,f9,f13
	ctx.f7.f64 = static_cast<float>(ctx.f9.f64 - ctx.f13.f64);
	// fsubs f6,f8,f9
	ctx.f6.f64 = static_cast<float>(ctx.f8.f64 - ctx.f9.f64);
	// stfsx f11,r11,r6
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r6.u32, temp.u32);
	// stfsx f10,r11,r31
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, temp.u32);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// stfsx f7,r11,r30
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r30.u32, temp.u32);
	// stfsx f6,r11,r29
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r29.u32, temp.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// blt cr6,0x822cb490
	if (ctx.cr6.lt) goto loc_822CB490;
loc_822CB4E8:
	// cmpw cr6,r10,r27
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r27.s32, ctx.xer);
	// bge cr6,0x822cb520
	if (!ctx.cr6.lt) goto loc_822CB520;
	// subf r9,r10,r27
	ctx.r9.s64 = ctx.r27.s64 - ctx.r10.s64;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822CB4FC:
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// addi r9,r1,320
	ctx.r9.s64 = ctx.r1.s64 + 320;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lfs f0,4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f0,f13
	ctx.f12.f64 = static_cast<float>(ctx.f0.f64 - ctx.f13.f64);
	// stfsx f12,r11,r9
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r9.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cb4fc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CB4FC;
loc_822CB520:
	// rlwinm r11,r25,2,0,29
	ctx.r11.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f25,192(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f25.f64 = double(temp.f32);
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// addi r9,r1,320
	ctx.r9.s64 = ctx.r1.s64 + 320;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// add r7,r11,r9
	ctx.r7.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// lfs f0,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// fsubs f13,f25,f0
	ctx.f13.f64 = static_cast<float>(ctx.f25.f64 - ctx.f0.f64);
	// lfs f27,-17964(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -17964);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,5268(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 5268);
	ctx.f28.f64 = double(temp.f32);
	// lfs f24,5256(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 5256);
	ctx.f24.f64 = double(temp.f32);
	// fadds f12,f13,f27
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f27.f64));
	// stfs f12,-4(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + -4, temp.u32);
	// ble cr6,0x822cb6e4
	if (!ctx.cr6.gt) goto loc_822CB6E4;
	// li r29,0
	ctx.r29.s64 = 0;
	// mr r28,r26
	ctx.r28.u64 = ctx.r26.u64;
	// addi r11,r1,704
	ctx.r11.s64 = ctx.r1.s64 + 704;
loc_822CB574:
	// lfsx f0,r29,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// li r11,0
	ctx.r11.s64 = 0;
	// fcmpu cr6,f0,f25
	ctx.cr6.compare(ctx.f0.f64, ctx.f25.f64);
	// ble cr6,0x822cb5a8
	if (!ctx.cr6.gt) goto loc_822CB5A8;
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
loc_822CB588:
	// cmpw cr6,r11,r25
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r25.s32, ctx.xer);
	// bge cr6,0x822cb5a0
	if (!ctx.cr6.lt) goto loc_822CB5A0;
	// lfsu f13,4(r10)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bgt cr6,0x822cb588
	if (ctx.cr6.gt) goto loc_822CB588;
loc_822CB5A0:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822cb608
	if (!ctx.cr6.eq) goto loc_822CB608;
loc_822CB5A8:
	// mr r10,r27
	ctx.r10.u64 = ctx.r27.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// b 0x822cb624
	goto loc_822CB624;
loc_822CB5B4:
	// cmplwi cr6,r31,55
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 55, ctx.xer);
	// bne cr6,0x822cb344
	if (!ctx.cr6.eq) goto loc_822CB344;
	// cmplwi cr6,r30,1543
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1543, ctx.xer);
	// bne cr6,0x822cb344
	if (!ctx.cr6.eq) goto loc_822CB344;
loc_822CB5C4:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lwz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// lwz r9,4(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4);
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r8,8(r24)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8);
	// lwz r7,12(r24)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r24.u32 + 12);
	// lwz r6,16(r24)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r24.u32 + 16);
	// lfs f0,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f0,4(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// stfs f0,8(r8)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// stfs f0,12(r7)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 12, temp.u32);
	// stfs f0,16(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 16, temp.u32);
	// addi r1,r1,976
	ctx.r1.s64 = ctx.r1.s64 + 976;
	// addi r12,r1,-80
	ctx.r12.s64 = ctx.r1.s64 + -80;
	// bl 0x8233fa74
	ctx.lr = 0x822CB604;
	__savefpr_24(ctx, base);
	// b 0x8233e4a4
	__restgprlr_23(ctx, base);
	return;
loc_822CB608:
	// cmpw cr6,r11,r27
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r27.s32, ctx.xer);
	// ble cr6,0x822cb61c
	if (!ctx.cr6.gt) goto loc_822CB61C;
	// mr r10,r27
	ctx.r10.u64 = ctx.r27.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// b 0x822cb624
	goto loc_822CB624;
loc_822CB61C:
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
loc_822CB624:
	// rlwinm r30,r10,2,0,29
	ctx.r30.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r1,192
	ctx.r11.s64 = ctx.r1.s64 + 192;
	// lfsx f13,r30,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r11.u32);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f0,f0,f13
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f13.f64);
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// bge cr6,0x822cb648
	if (!ctx.cr6.lt) goto loc_822CB648;
loc_822CB63C:
	// fadds f0,f0,f27
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f27.f64));
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// blt cr6,0x822cb63c
	if (ctx.cr6.lt) goto loc_822CB63C;
loc_822CB648:
	// addi r11,r1,320
	ctx.r11.s64 = ctx.r1.s64 + 320;
	// lfsx f13,r30,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r11.u32);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// fmuls f11,f12,f26
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f26.f64));
	// fmuls f29,f11,f28
	ctx.f29.f64 = double(float(ctx.f11.f64 * ctx.f28.f64));
	// fmr f1,f29
	ctx.f1.f64 = ctx.f29.f64;
	// bl 0x8233c950
	ctx.lr = 0x822CB664;
	sub_8233C950(ctx, base);
	// frsp f30,f1
	ctx.fpscr.disableFlushMode();
	ctx.f30.f64 = double(float(ctx.f1.f64));
	// fcmpu cr6,f30,f31
	ctx.cr6.compare(ctx.f30.f64, ctx.f31.f64);
	// bge cr6,0x822cb674
	if (!ctx.cr6.lt) goto loc_822CB674;
	// fmr f30,f31
	ctx.f30.f64 = ctx.f31.f64;
loc_822CB674:
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f29.f64;
	// bl 0x8233c870
	ctx.lr = 0x822CB67C;
	sub_8233C870(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// bge cr6,0x822cb68c
	if (!ctx.cr6.lt) goto loc_822CB68C;
	// fmr f0,f31
	ctx.f0.f64 = ctx.f31.f64;
loc_822CB68C:
	// cmpwi cr6,r25,1
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 1, ctx.xer);
	// bne cr6,0x822cb69c
	if (!ctx.cr6.eq) goto loc_822CB69C;
	// fmr f0,f24
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f24.f64;
	// fmr f30,f24
	ctx.f30.f64 = ctx.f24.f64;
loc_822CB69C:
	// addi r11,r1,448
	ctx.r11.s64 = ctx.r1.s64 + 448;
	// rlwinm r10,r31,2,0,29
	ctx.r10.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,448
	ctx.r9.s64 = ctx.r1.s64 + 448;
	// addi r8,r1,576
	ctx.r8.s64 = ctx.r1.s64 + 576;
	// addic. r28,r28,-1
	ctx.xer.ca = ctx.r28.u32 > 0;
	ctx.r28.s64 = ctx.r28.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// lwzx r7,r30,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r11.u32);
	// addi r11,r1,704
	ctx.r11.s64 = ctx.r1.s64 + 704;
	// lwzx r6,r10,r9
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r4,r29,r8
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + ctx.r8.u32);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// rlwinm r3,r6,2,0,29
	ctx.r3.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r5,r24
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r24.u32);
	// lwzx r8,r3,r24
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r24.u32);
	// stfsx f30,r9,r10
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, temp.u32);
	// stfsx f0,r8,r10
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, temp.u32);
	// bne 0x822cb574
	if (!ctx.cr0.eq) goto loc_822CB574;
loc_822CB6E4:
	// fmr f11,f31
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = ctx.f31.f64;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// ble cr6,0x822cb778
	if (!ctx.cr6.gt) goto loc_822CB778;
	// mr r6,r24
	ctx.r6.u64 = ctx.r24.u64;
	// mtctr r25
	ctx.ctr.u64 = ctx.r25.u64;
loc_822CB6F8:
	// fmr f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f31.f64;
	// li r10,0
	ctx.r10.s64 = 0;
	// fmr f13,f31
	ctx.f13.f64 = ctx.f31.f64;
	// cmpwi cr6,r26,2
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 2, ctx.xer);
	// fmr f12,f31
	ctx.f12.f64 = ctx.f31.f64;
	// blt cr6,0x822cb740
	if (ctx.cr6.lt) goto loc_822CB740;
	// lwz r9,0(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// addi r7,r26,-1
	ctx.r7.s64 = ctx.r26.s64 + -1;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CB71C:
	// add r8,r9,r11
	ctx.r8.u64 = ctx.r9.u64 + ctx.r11.u64;
	// lfsx f10,r9,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f10.f64 = double(temp.f32);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// fadds f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// cmpw cr6,r10,r7
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, ctx.xer);
	// lfs f9,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// fadds f13,f9,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f13.f64));
	// blt cr6,0x822cb71c
	if (ctx.cr6.lt) goto loc_822CB71C;
loc_822CB740:
	// cmpw cr6,r10,r26
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r26.s32, ctx.xer);
	// bge cr6,0x822cb754
	if (!ctx.cr6.lt) goto loc_822CB754;
	// lwz r11,0(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f12,r10,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
loc_822CB754:
	// fadds f0,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// fadds f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// fcmpu cr6,f11,f0
	ctx.cr6.compare(ctx.f11.f64, ctx.f0.f64);
	// bge cr6,0x822cb768
	if (!ctx.cr6.lt) goto loc_822CB768;
	// fmr f11,f0
	ctx.f11.f64 = ctx.f0.f64;
loc_822CB768:
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// bdnz 0x822cb6f8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CB6F8;
	// fcmpu cr6,f11,f31
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f11.f64, ctx.f31.f64);
	// bgt cr6,0x822cb794
	if (ctx.cr6.gt) goto loc_822CB794;
loc_822CB778:
	// lis r23,-32768
	ctx.r23.s64 = -2147483648;
	// ori r23,r23,16389
	ctx.r23.u64 = ctx.r23.u64 | 16389;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// addi r1,r1,976
	ctx.r1.s64 = ctx.r1.s64 + 976;
	// addi r12,r1,-80
	ctx.r12.s64 = ctx.r1.s64 + -80;
	// bl 0x8233fa74
	ctx.lr = 0x822CB790;
	__savefpr_24(ctx, base);
	// b 0x8233e4a4
	__restgprlr_23(ctx, base);
	return;
loc_822CB794:
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// mr r31,r25
	ctx.r31.u64 = ctx.r25.u64;
	// lfs f12,-17952(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -17952);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,-17956(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -17956);
	ctx.f13.f64 = double(temp.f32);
loc_822CB7AC:
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r26,4
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 4, ctx.xer);
	// blt cr6,0x822cb8b8
	if (ctx.cr6.lt) goto loc_822CB8B8;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// fdivs f0,f24,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f24.f64 / ctx.f11.f64));
	// addi r4,r26,-3
	ctx.r4.s64 = ctx.r26.s64 + -3;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CB7C8:
	// add r8,r11,r10
	ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lfsx f10,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f0,f10
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// addi r9,r11,12
	ctx.r9.s64 = ctx.r11.s64 + 12;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// add r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmpw cr6,r6,r4
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r4.s32, ctx.xer);
	// lfs f7,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// addi r5,r7,-4
	ctx.r5.s64 = ctx.r7.s64 + -4;
	// fmuls f4,f7,f0
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfsx f8,r10,r9
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f6,f8,f0
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f5,-4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -4);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f5,f0
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmadds f2,f9,f13,f28
	ctx.f2.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f13.f64), float(ctx.f28.f64)));
	// fmadds f10,f4,f13,f28
	ctx.f10.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f13.f64), float(ctx.f28.f64)));
	// fmadds f1,f6,f13,f28
	ctx.f1.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f13.f64), float(ctx.f28.f64)));
	// fmadds f9,f3,f13,f28
	ctx.f9.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f13.f64), float(ctx.f28.f64)));
	// fctiwz f8,f2
	ctx.f8.u64 = uint64_t(int32_t(std::trunc(ctx.f2.f64)));
	// stfd f8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f8.u64);
	// lwz r5,92(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// extsw r5,r5
	ctx.r5.s64 = ctx.r5.s32;
	// fctiwz f7,f10
	ctx.f7.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// lwz r30,92(r1)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// fctiwz f6,f9
	ctx.f6.u64 = uint64_t(int32_t(std::trunc(ctx.f9.f64)));
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// lwz r29,84(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// fctiwz f5,f1
	ctx.f5.u64 = uint64_t(int32_t(std::trunc(ctx.f1.f64)));
	// std r5,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r5.u64);
	// stfd f5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f5.u64);
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r5,r5
	ctx.r5.s64 = ctx.r5.s32;
	// std r5,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r5.u64);
	// extsw r5,r30
	ctx.r5.s64 = ctx.r30.s32;
	// lfd f3,176(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// extsw r29,r29
	ctx.r29.s64 = ctx.r29.s32;
	// std r5,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, ctx.r5.u64);
	// lfd f1,152(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// lfd f4,160(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// std r29,168(r1)
	PPC_STORE_U64(ctx.r1.u32 + 168, ctx.r29.u64);
	// lfd f2,168(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 168);
	// fcfid f10,f4
	ctx.f10.f64 = double(ctx.f4.s64);
	// fcfid f8,f2
	ctx.f8.f64 = double(ctx.f2.s64);
	// fcfid f9,f3
	ctx.f9.f64 = double(ctx.f3.s64);
	// fcfid f7,f1
	ctx.f7.f64 = double(ctx.f1.s64);
	// frsp f6,f10
	ctx.f6.f64 = double(float(ctx.f10.f64));
	// frsp f4,f8
	ctx.f4.f64 = double(float(ctx.f8.f64));
	// frsp f5,f9
	ctx.f5.f64 = double(float(ctx.f9.f64));
	// frsp f3,f7
	ctx.f3.f64 = double(float(ctx.f7.f64));
	// fmuls f2,f6,f12
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// stfsx f2,r11,r10
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, temp.u32);
	// fmuls f10,f4,f12
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stfs f10,-4(r7)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r7.u32 + -4, temp.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// fmuls f1,f5,f12
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfsx f1,r10,r9
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, temp.u32);
	// fmuls f9,f3,f12
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// stfs f9,4(r8)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// blt cr6,0x822cb7c8
	if (ctx.cr6.lt) goto loc_822CB7C8;
loc_822CB8B8:
	// cmpw cr6,r6,r26
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r26.s32, ctx.xer);
	// bge cr6,0x822cb910
	if (!ctx.cr6.lt) goto loc_822CB910;
	// subf r9,r6,r26
	ctx.r9.s64 = ctx.r26.s64 - ctx.r6.s64;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// fdivs f0,f24,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f24.f64 / ctx.f11.f64));
	// rlwinm r11,r6,2,0,29
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822CB8D4:
	// lfsx f10,r10,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f0,f10
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// fmadds f8,f9,f13,f28
	ctx.f8.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f13.f64), float(ctx.f28.f64)));
	// fctiwz f7,f8
	ctx.f7.u64 = uint64_t(int32_t(std::trunc(ctx.f8.f64)));
	// stfd f7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f7.u64);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// std r8,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r8.u64);
	// lfd f6,144(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fmuls f3,f4,f12
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stfsx f3,r10,r11
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cb8d4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CB8D4;
loc_822CB910:
	// addic. r31,r31,-1
	ctx.xer.ca = ctx.r31.u32 > 0;
	ctx.r31.s64 = ctx.r31.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// bne 0x822cb7ac
	if (!ctx.cr0.eq) goto loc_822CB7AC;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// addi r1,r1,976
	ctx.r1.s64 = ctx.r1.s64 + 976;
	// addi r12,r1,-80
	ctx.r12.s64 = ctx.r1.s64 + -80;
	// bl 0x8233fa74
	ctx.lr = 0x822CB92C;
	__savefpr_24(ctx, base);
	// b 0x8233e4a4
	__restgprlr_23(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CB930"))) PPC_WEAK_FUNC(sub_822CB930);
PPC_FUNC_IMPL(__imp__sub_822CB930) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e43c
	ctx.lr = 0x822CB938;
	__restfpr_17(ctx, base);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r10,-1
	ctx.r10.s64 = -1;
	// mr r21,r3
	ctx.r21.u64 = ctx.r3.u64;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// mr r22,r5
	ctx.r22.u64 = ctx.r5.u64;
	// sth r10,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r10.u16);
	// mr r24,r6
	ctx.r24.u64 = ctx.r6.u64;
	// mr r20,r7
	ctx.r20.u64 = ctx.r7.u64;
	// mr r23,r8
	ctx.r23.u64 = ctx.r8.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822cb99c
	if (ctx.cr6.eq) goto loc_822CB99C;
	// cmpwi cr6,r3,32
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 32, ctx.xer);
	// bgt cr6,0x822cb99c
	if (ctx.cr6.gt) goto loc_822CB99C;
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// blt cr6,0x822cb99c
	if (ctx.cr6.lt) goto loc_822CB99C;
	// cmpwi cr6,r5,32
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 32, ctx.xer);
	// bgt cr6,0x822cb99c
	if (ctx.cr6.gt) goto loc_822CB99C;
	// cmpwi cr6,r5,1
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 1, ctx.xer);
	// blt cr6,0x822cb99c
	if (ctx.cr6.lt) goto loc_822CB99C;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x822cb99c
	if (ctx.cr6.eq) goto loc_822CB99C;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x822cb9ac
	if (!ctx.cr6.eq) goto loc_822CB9AC;
loc_822CB99C:
	// lis r28,-32764
	ctx.r28.s64 = -2147221504;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
loc_822CB9AC:
	// rlwinm r10,r25,0,0,20
	ctx.r10.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 0) & 0xFFFFF800;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822cb99c
	if (!ctx.cr6.eq) goto loc_822CB99C;
	// rlwinm r10,r24,0,0,20
	ctx.r10.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 0) & 0xFFFFF800;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822cb99c
	if (!ctx.cr6.eq) goto loc_822CB99C;
	// li r8,8
	ctx.r8.s64 = 8;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
loc_822CB9E8:
	// and r31,r11,r25
	ctx.r31.u64 = ctx.r11.u64 & ctx.r25.u64;
	// and r30,r11,r24
	ctx.r30.u64 = ctx.r11.u64 & ctx.r24.u64;
	// addic r29,r31,-1
	ctx.xer.ca = ctx.r31.u32 > 0;
	ctx.r29.s64 = ctx.r31.s64 + -1;
	// rlwinm r28,r11,1,0,30
	ctx.r28.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subfe r11,r29,r31
	temp.u8 = (~ctx.r29.u32 + ctx.r31.u32 < ~ctx.r29.u32) | (~ctx.r29.u32 + ctx.r31.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r29.u64 + ctx.r31.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r31,r30,-1
	ctx.xer.ca = ctx.r30.u32 > 0;
	ctx.r31.s64 = ctx.r30.s64 + -1;
	// and r29,r28,r25
	ctx.r29.u64 = ctx.r28.u64 & ctx.r25.u64;
	// subfe r31,r31,r30
	temp.u8 = (~ctx.r31.u32 + ctx.r30.u32 < ~ctx.r31.u32) | (~ctx.r31.u32 + ctx.r30.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r31.u64 = ~ctx.r31.u64 + ctx.r30.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r30,r29,-1
	ctx.xer.ca = ctx.r29.u32 > 0;
	ctx.r30.s64 = ctx.r29.s64 + -1;
	// and r27,r28,r24
	ctx.r27.u64 = ctx.r28.u64 & ctx.r24.u64;
	// rlwinm r28,r28,1,0,30
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 1) & 0xFFFFFFFE;
	// subfe r30,r30,r29
	temp.u8 = (~ctx.r30.u32 + ctx.r29.u32 < ~ctx.r30.u32) | (~ctx.r30.u32 + ctx.r29.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r30.u64 = ~ctx.r30.u64 + ctx.r29.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r29,r27,-1
	ctx.xer.ca = ctx.r27.u32 > 0;
	ctx.r29.s64 = ctx.r27.s64 + -1;
	// and r26,r28,r25
	ctx.r26.u64 = ctx.r28.u64 & ctx.r25.u64;
	// subfe r29,r29,r27
	temp.u8 = (~ctx.r29.u32 + ctx.r27.u32 < ~ctx.r29.u32) | (~ctx.r29.u32 + ctx.r27.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r29.u64 = ~ctx.r29.u64 + ctx.r27.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r27,r26,-1
	ctx.xer.ca = ctx.r26.u32 > 0;
	ctx.r27.s64 = ctx.r26.s64 + -1;
	// and r19,r28,r24
	ctx.r19.u64 = ctx.r28.u64 & ctx.r24.u64;
	// rlwinm r18,r28,1,0,30
	ctx.r18.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 1) & 0xFFFFFFFE;
	// subfe r28,r27,r26
	temp.u8 = (~ctx.r27.u32 + ctx.r26.u32 < ~ctx.r27.u32) | (~ctx.r27.u32 + ctx.r26.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r28.u64 = ~ctx.r27.u64 + ctx.r26.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r27,r19,-1
	ctx.xer.ca = ctx.r19.u32 > 0;
	ctx.r27.s64 = ctx.r19.s64 + -1;
	// and r26,r18,r25
	ctx.r26.u64 = ctx.r18.u64 & ctx.r25.u64;
	// subfe r27,r27,r19
	temp.u8 = (~ctx.r27.u32 + ctx.r19.u32 < ~ctx.r27.u32) | (~ctx.r27.u32 + ctx.r19.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r27.u64 = ~ctx.r27.u64 + ctx.r19.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r19,r26,-1
	ctx.xer.ca = ctx.r26.u32 > 0;
	ctx.r19.s64 = ctx.r26.s64 + -1;
	// and r17,r18,r24
	ctx.r17.u64 = ctx.r18.u64 & ctx.r24.u64;
	// subfe r26,r19,r26
	temp.u8 = (~ctx.r19.u32 + ctx.r26.u32 < ~ctx.r19.u32) | (~ctx.r19.u32 + ctx.r26.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r26.u64 = ~ctx.r19.u64 + ctx.r26.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r19,r17,-1
	ctx.xer.ca = ctx.r17.u32 > 0;
	ctx.r19.s64 = ctx.r17.s64 + -1;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// subfe r11,r19,r17
	temp.u8 = (~ctx.r19.u32 + ctx.r17.u32 < ~ctx.r19.u32) | (~ctx.r19.u32 + ctx.r17.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r19.u64 + ctx.r17.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// add r9,r31,r9
	ctx.r9.u64 = ctx.r31.u64 + ctx.r9.u64;
	// add r3,r11,r3
	ctx.r3.u64 = ctx.r11.u64 + ctx.r3.u64;
	// add r8,r30,r8
	ctx.r8.u64 = ctx.r30.u64 + ctx.r8.u64;
	// add r7,r29,r7
	ctx.r7.u64 = ctx.r29.u64 + ctx.r7.u64;
	// add r6,r28,r6
	ctx.r6.u64 = ctx.r28.u64 + ctx.r6.u64;
	// add r5,r27,r5
	ctx.r5.u64 = ctx.r27.u64 + ctx.r5.u64;
	// add r4,r26,r4
	ctx.r4.u64 = ctx.r26.u64 + ctx.r4.u64;
	// rlwinm r11,r18,1,0,30
	ctx.r11.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 1) & 0xFFFFFFFE;
	// bdnz 0x822cb9e8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CB9E8;
	// add r11,r8,r6
	ctx.r11.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r8,r7,r5
	ctx.r8.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// cmpw cr6,r11,r21
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r21.s32, ctx.xer);
	// bne cr6,0x822cbaa4
	if (!ctx.cr6.eq) goto loc_822CBAA4;
	// cmpw cr6,r10,r22
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r22.s32, ctx.xer);
	// beq cr6,0x822cbab8
	if (ctx.cr6.eq) goto loc_822CBAB8;
loc_822CBAA4:
	// lis r28,-32761
	ctx.r28.s64 = -2147024896;
	// ori r28,r28,87
	ctx.r28.u64 = ctx.r28.u64 | 87;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
loc_822CBAB8:
	// rlwinm r11,r25,0,28,28
	ctx.r11.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 0) & 0x8;
	// mr r3,r21
	ctx.r3.u64 = ctx.r21.u64;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cbad4
	if (ctx.cr6.eq) goto loc_822CBAD4;
	// rlwinm r4,r25,0,29,27
	ctx.r4.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF7;
	// addi r3,r21,-1
	ctx.r3.s64 = ctx.r21.s64 + -1;
loc_822CBAD4:
	// rlwinm r11,r24,0,28,28
	ctx.r11.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 0) & 0x8;
	// mr r5,r22
	ctx.r5.u64 = ctx.r22.u64;
	// mr r6,r24
	ctx.r6.u64 = ctx.r24.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cbaf0
	if (ctx.cr6.eq) goto loc_822CBAF0;
	// rlwinm r6,r24,0,29,27
	ctx.r6.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF7;
	// addi r5,r22,-1
	ctx.r5.s64 = ctx.r22.s64 + -1;
loc_822CBAF0:
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// blt cr6,0x822cb99c
	if (ctx.cr6.lt) goto loc_822CB99C;
	// cmpwi cr6,r5,1
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 1, ctx.xer);
	// blt cr6,0x822cb99c
	if (ctx.cr6.lt) goto loc_822CB99C;
	// mr r7,r23
	ctx.r7.u64 = ctx.r23.u64;
	// bl 0x822cb1d0
	ctx.lr = 0x822CBB08;
	sub_822CB1D0(ctx, base);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cc1b4
	if (ctx.cr6.lt) goto loc_822CC1B4;
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// ble cr6,0x822cbba0
	if (!ctx.cr6.gt) goto loc_822CBBA0;
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// mr r5,r20
	ctx.r5.u64 = ctx.r20.u64;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// lfs f0,-28948(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
loc_822CBB2C:
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r21,4
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 4, ctx.xer);
	// blt cr6,0x822cbb70
	if (ctx.cr6.lt) goto loc_822CBB70;
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r6,r21,-3
	ctx.r6.s64 = ctx.r21.s64 + -3;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CBB44:
	// addi r8,r11,12
	ctx.r8.s64 = ctx.r11.s64 + 12;
	// stfsx f0,r10,r11
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
	// add r3,r10,r8
	ctx.r3.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmpw cr6,r9,r6
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, ctx.xer);
	// stfs f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// stfs f0,-4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + -4, temp.u32);
	// stfsx f0,r10,r8
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, temp.u32);
	// blt cr6,0x822cbb44
	if (ctx.cr6.lt) goto loc_822CBB44;
loc_822CBB70:
	// cmpw cr6,r9,r21
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822cbb94
	if (!ctx.cr6.lt) goto loc_822CBB94;
	// subf r8,r9,r21
	ctx.r8.s64 = ctx.r21.s64 - ctx.r9.s64;
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822CBB88:
	// stfsx f0,r10,r11
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cbb88
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CBB88;
loc_822CBB94:
	// addic. r4,r4,-1
	ctx.xer.ca = ctx.r4.u32 > 0;
	ctx.r4.s64 = ctx.r4.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne 0x822cbb2c
	if (!ctx.cr0.eq) goto loc_822CBB2C;
loc_822CBBA0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x822ecd38
	ctx.lr = 0x822CBBB0;
	sub_822ECD38(ctx, base);
	// addi r5,r1,82
	ctx.r5.s64 = ctx.r1.s64 + 82;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x822ecd38
	ctx.lr = 0x822CBBC0;
	sub_822ECD38(ctx, base);
	// lhz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// lhz r9,82(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// extsh r10,r11
	ctx.r10.s64 = ctx.r11.s16;
	// cmpwi cr6,r10,-1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, -1, ctx.xer);
	// beq cr6,0x822cbe20
	if (ctx.cr6.eq) goto loc_822CBE20;
	// extsh r11,r9
	ctx.r11.s64 = ctx.r9.s16;
	// cmpwi cr6,r11,-1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -1, ctx.xer);
	// beq cr6,0x822cbe18
	if (ctx.cr6.eq) goto loc_822CBE18;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r31,0
	ctx.r31.s64 = 0;
	// lwzx r7,r11,r20
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r20.u32);
	// lfs f0,5256(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r7,r8
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + ctx.r8.u32, temp.u32);
	// lhz r6,82(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// extsh r11,r6
	ctx.r11.s64 = ctx.r6.s16;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x822cbd00
	if (!ctx.cr6.gt) goto loc_822CBD00;
	// li r5,0
	ctx.r5.s64 = 0;
loc_822CBC10:
	// lhz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// li r10,0
	ctx.r10.s64 = 0;
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x822cbc50
	if (!ctx.cr6.gt) goto loc_822CBC50;
	// lwzx r8,r5,r20
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CBC2C:
	// lwzx r9,r5,r23
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lfsx f0,r9,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r8,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + ctx.r11.u32, temp.u32);
	// lhz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x822cbc2c
	if (ctx.cr6.lt) goto loc_822CBC2C;
loc_822CBC50:
	// addi r10,r9,1
	ctx.r10.s64 = ctx.r9.s64 + 1;
	// cmpw cr6,r10,r21
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822cbce8
	if (!ctx.cr6.lt) goto loc_822CBCE8;
	// subf r11,r10,r21
	ctx.r11.s64 = ctx.r21.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// blt cr6,0x822cbcb8
	if (ctx.cr6.lt) goto loc_822CBCB8;
	// lwzx r9,r5,r20
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// addi r3,r21,-3
	ctx.r3.s64 = ctx.r21.s64 + -3;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_822CBC74:
	// lwzx r7,r5,r23
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	// add r4,r9,r11
	ctx.r4.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r8,r11,8
	ctx.r8.s64 = ctx.r11.s64 + 8;
	// add r6,r7,r11
	ctx.r6.u64 = ctx.r7.u64 + ctx.r11.u64;
	// add r30,r9,r8
	ctx.r30.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r10,r3
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r3.s32, ctx.xer);
	// lfs f0,-4(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r9,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, temp.u32);
	// lfsx f13,r7,r11
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r11.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f12,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfsx f12,r9,r8
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, temp.u32);
	// lfsx f11,r8,r7
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,4(r30)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r30.u32 + 4, temp.u32);
	// blt cr6,0x822cbc74
	if (ctx.cr6.lt) goto loc_822CBC74;
loc_822CBCB8:
	// cmpw cr6,r10,r21
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822cbce8
	if (!ctx.cr6.lt) goto loc_822CBCE8;
	// subf r8,r10,r21
	ctx.r8.s64 = ctx.r21.s64 - ctx.r10.s64;
	// lwzx r9,r5,r20
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822CBCD0:
	// lwzx r10,r5,r23
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lfs f0,-4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r9,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cbcd0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CBCD0;
loc_822CBCE8:
	// lhz r11,82(r1)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// cmpw cr6,r31,r11
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x822cbc10
	if (ctx.cr6.lt) goto loc_822CBC10;
loc_822CBD00:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r11,r22
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r22.s32, ctx.xer);
	// bge cr6,0x822cc1b4
	if (!ctx.cr6.lt) goto loc_822CC1B4;
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r30,r11,r22
	ctx.r30.s64 = ctx.r22.s64 - ctx.r11.s64;
loc_822CBD14:
	// lhz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// li r10,0
	ctx.r10.s64 = 0;
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x822cbd5c
	if (!ctx.cr6.gt) goto loc_822CBD5C;
	// lwzx r7,r6,r20
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r20.u32);
	// li r11,0
	ctx.r11.s64 = 0;
	// add r8,r6,r23
	ctx.r8.u64 = ctx.r6.u64 + ctx.r23.u64;
loc_822CBD34:
	// lwz r9,-4(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r8,r6,r23
	ctx.r8.u64 = ctx.r6.u64 + ctx.r23.u64;
	// lfsx f0,r9,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r7,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + ctx.r11.u32, temp.u32);
	// lhz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r9,r5
	ctx.r9.s64 = ctx.r5.s16;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x822cbd34
	if (ctx.cr6.lt) goto loc_822CBD34;
loc_822CBD5C:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpw cr6,r9,r21
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822cbe00
	if (!ctx.cr6.lt) goto loc_822CBE00;
	// subf r11,r9,r21
	ctx.r11.s64 = ctx.r21.s64 - ctx.r9.s64;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// blt cr6,0x822cbdcc
	if (ctx.cr6.lt) goto loc_822CBDCC;
	// lwzx r10,r6,r20
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r20.u32);
	// addi r31,r21,-3
	ctx.r31.s64 = ctx.r21.s64 + -3;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r6,r23
	ctx.r4.u64 = ctx.r6.u64 + ctx.r23.u64;
loc_822CBD84:
	// lwz r7,-4(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + -4);
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r8,r11,8
	ctx.r8.s64 = ctx.r11.s64 + 8;
	// add r5,r11,r7
	ctx.r5.u64 = ctx.r11.u64 + ctx.r7.u64;
	// add r29,r10,r8
	ctx.r29.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// add r4,r6,r23
	ctx.r4.u64 = ctx.r6.u64 + ctx.r23.u64;
	// cmpw cr6,r9,r31
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r31.s32, ctx.xer);
	// lfs f0,-4(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r10,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// lfsx f13,r11,r7
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r7.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stfs f13,4(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// lfs f12,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfsx f12,r10,r8
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, temp.u32);
	// lfsx f11,r8,r7
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,4(r29)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r29.u32 + 4, temp.u32);
	// blt cr6,0x822cbd84
	if (ctx.cr6.lt) goto loc_822CBD84;
loc_822CBDCC:
	// cmpw cr6,r9,r21
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822cbe00
	if (!ctx.cr6.lt) goto loc_822CBE00;
	// subf r10,r9,r21
	ctx.r10.s64 = ctx.r21.s64 - ctx.r9.s64;
	// lwzx r8,r6,r20
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r20.u32);
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822CBDE4:
	// add r10,r6,r23
	ctx.r10.u64 = ctx.r6.u64 + ctx.r23.u64;
	// lwz r10,-4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lfs f0,-4(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r8,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cbde4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CBDE4;
loc_822CBE00:
	// addic. r30,r30,-1
	ctx.xer.ca = ctx.r30.u32 > 0;
	ctx.r30.s64 = ctx.r30.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// bne 0x822cbd14
	if (!ctx.cr0.eq) goto loc_822CBD14;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
loc_822CBE18:
	// cmpwi cr6,r10,-1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, -1, ctx.xer);
	// bne cr6,0x822cbfa4
	if (!ctx.cr6.eq) goto loc_822CBFA4;
loc_822CBE20:
	// extsh r11,r9
	ctx.r11.s64 = ctx.r9.s16;
	// cmpwi cr6,r11,-1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -1, ctx.xer);
	// beq cr6,0x822cbf9c
	if (ctx.cr6.eq) goto loc_822CBF9C;
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x822cbedc
	if (!ctx.cr6.gt) goto loc_822CBEDC;
	// li r5,0
	ctx.r5.s64 = 0;
loc_822CBE3C:
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r21,4
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 4, ctx.xer);
	// blt cr6,0x822cbe98
	if (ctx.cr6.lt) goto loc_822CBE98;
	// lwzx r9,r5,r20
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// addi r31,r21,-3
	ctx.r31.s64 = ctx.r21.s64 + -3;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CBE54:
	// lwzx r10,r5,r23
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	// add r4,r9,r11
	ctx.r4.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r8,r11,12
	ctx.r8.s64 = ctx.r11.s64 + 12;
	// add r3,r11,r10
	ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r6,r8,-4
	ctx.r6.s64 = ctx.r8.s64 + -4;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r9,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// cmpw cr6,r7,r31
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r31.s32, ctx.xer);
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfsx f12,r6,r10
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r10.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfsx f12,r9,r6
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, temp.u32);
	// lfsx f11,r8,r10
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfsx f11,r9,r8
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, temp.u32);
	// blt cr6,0x822cbe54
	if (ctx.cr6.lt) goto loc_822CBE54;
loc_822CBE98:
	// cmpw cr6,r7,r21
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822cbec4
	if (!ctx.cr6.lt) goto loc_822CBEC4;
	// subf r9,r7,r21
	ctx.r9.s64 = ctx.r21.s64 - ctx.r7.s64;
	// lwzx r10,r5,r20
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822CBEB0:
	// lwzx r9,r5,r23
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	// lfsx f0,r9,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r10,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cbeb0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CBEB0;
loc_822CBEC4:
	// lhz r11,82(r1)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x822cbe3c
	if (ctx.cr6.lt) goto loc_822CBE3C;
loc_822CBEDC:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r11,r22
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r22.s32, ctx.xer);
	// bge cr6,0x822cc1b4
	if (!ctx.cr6.lt) goto loc_822CC1B4;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r11,r22
	ctx.r29.s64 = ctx.r22.s64 - ctx.r11.s64;
loc_822CBEF0:
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r21,4
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 4, ctx.xer);
	// blt cr6,0x822cbf54
	if (ctx.cr6.lt) goto loc_822CBF54;
	// lwzx r9,r5,r20
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// addi r30,r21,-3
	ctx.r30.s64 = ctx.r21.s64 + -3;
	// li r11,0
	ctx.r11.s64 = 0;
	// add r4,r5,r23
	ctx.r4.u64 = ctx.r5.u64 + ctx.r23.u64;
loc_822CBF0C:
	// lwz r10,-4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + -4);
	// add r3,r11,r9
	ctx.r3.u64 = ctx.r11.u64 + ctx.r9.u64;
	// addi r8,r11,12
	ctx.r8.s64 = ctx.r11.s64 + 12;
	// add r31,r11,r10
	ctx.r31.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r6,r8,-4
	ctx.r6.s64 = ctx.r8.s64 + -4;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f0.f64 = double(temp.f32);
	// add r4,r5,r23
	ctx.r4.u64 = ctx.r5.u64 + ctx.r23.u64;
	// stfsx f0,r11,r9
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r9.u32, temp.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmpw cr6,r7,r30
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r30.s32, ctx.xer);
	// lfs f13,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// lfsx f12,r10,r6
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfsx f12,r9,r6
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, temp.u32);
	// lfsx f11,r10,r8
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfsx f11,r9,r8
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, temp.u32);
	// blt cr6,0x822cbf0c
	if (ctx.cr6.lt) goto loc_822CBF0C;
loc_822CBF54:
	// cmpw cr6,r7,r21
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822cbf84
	if (!ctx.cr6.lt) goto loc_822CBF84;
	// subf r9,r7,r21
	ctx.r9.s64 = ctx.r21.s64 - ctx.r7.s64;
	// lwzx r10,r5,r20
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822CBF6C:
	// add r9,r5,r23
	ctx.r9.u64 = ctx.r5.u64 + ctx.r23.u64;
	// lwz r8,-4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// lfsx f0,r8,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r10,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cbf6c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CBF6C;
loc_822CBF84:
	// addic. r29,r29,-1
	ctx.xer.ca = ctx.r29.u32 > 0;
	ctx.r29.s64 = ctx.r29.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne 0x822cbef0
	if (!ctx.cr0.eq) goto loc_822CBEF0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
loc_822CBF9C:
	// cmpwi cr6,r10,-1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, -1, ctx.xer);
	// beq cr6,0x822cc110
	if (ctx.cr6.eq) goto loc_822CC110;
loc_822CBFA4:
	// extsh r11,r9
	ctx.r11.s64 = ctx.r9.s16;
	// cmpwi cr6,r11,-1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -1, ctx.xer);
	// bne cr6,0x822cc110
	if (!ctx.cr6.eq) goto loc_822CC110;
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// ble cr6,0x822cc1b4
	if (!ctx.cr6.gt) goto loc_822CC1B4;
	// extsw r11,r22
	ctx.r11.s64 = ctx.r22.s32;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r31,r22
	ctx.r31.u64 = ctx.r22.u64;
	// lfs f12,2104(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2104);
	ctx.f12.f64 = double(temp.f32);
	// lfd f0,88(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f0,f13
	ctx.f0.f64 = double(float(ctx.f13.f64));
	// fadds f13,f0,f12
	ctx.f13.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// fdivs f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f13.f64));
loc_822CBFE4:
	// lhz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// li r10,0
	ctx.r10.s64 = 0;
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x822cc02c
	if (!ctx.cr6.gt) goto loc_822CC02C;
	// lwzx r8,r5,r20
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CC000:
	// lwzx r9,r5,r23
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lfsx f11,r9,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fdivs f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 / ctx.f13.f64));
	// stfsx f9,r8,r11
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r8.u32 + ctx.r11.u32, temp.u32);
	// lhz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x822cc000
	if (ctx.cr6.lt) goto loc_822CC000;
loc_822CC02C:
	// lwzx r10,r5,r20
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f12,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, temp.u32);
	// lhz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r11,r9
	ctx.r11.s64 = ctx.r9.s16;
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r9,r21
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822cc0f8
	if (!ctx.cr6.lt) goto loc_822CC0F8;
	// subf r11,r9,r21
	ctx.r11.s64 = ctx.r21.s64 - ctx.r9.s64;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// blt cr6,0x822cc0c4
	if (ctx.cr6.lt) goto loc_822CC0C4;
	// addi r3,r21,-3
	ctx.r3.s64 = ctx.r21.s64 + -3;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_822CC060:
	// lwzx r7,r5,r23
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r8,r11,8
	ctx.r8.s64 = ctx.r11.s64 + 8;
	// add r6,r7,r11
	ctx.r6.u64 = ctx.r7.u64 + ctx.r11.u64;
	// add r30,r10,r8
	ctx.r30.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmpw cr6,r9,r3
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r3.s32, ctx.xer);
	// lfs f11,-4(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -4);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fdivs f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 / ctx.f13.f64));
	// stfsx f9,r10,r11
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// lfsx f8,r7,r11
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r11.u32);
	ctx.f8.f64 = double(temp.f32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// fmuls f7,f0,f8
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// fdivs f6,f7,f13
	ctx.f6.f64 = double(float(ctx.f7.f64 / ctx.f13.f64));
	// stfs f6,4(r4)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f5,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fdivs f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 / ctx.f13.f64));
	// stfsx f3,r10,r8
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, temp.u32);
	// lfsx f2,r7,r8
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fdivs f11,f1,f13
	ctx.f11.f64 = double(float(ctx.f1.f64 / ctx.f13.f64));
	// stfs f11,4(r30)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r30.u32 + 4, temp.u32);
	// blt cr6,0x822cc060
	if (ctx.cr6.lt) goto loc_822CC060;
loc_822CC0C4:
	// cmpw cr6,r9,r21
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822cc0f8
	if (!ctx.cr6.lt) goto loc_822CC0F8;
	// subf r8,r9,r21
	ctx.r8.s64 = ctx.r21.s64 - ctx.r9.s64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822CC0D8:
	// lwzx r9,r5,r23
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// lfs f11,-4(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fdivs f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 / ctx.f13.f64));
	// stfsx f9,r10,r11
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cc0d8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CC0D8;
loc_822CC0F8:
	// addic. r31,r31,-1
	ctx.xer.ca = ctx.r31.u32 > 0;
	ctx.r31.s64 = ctx.r31.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne 0x822cbfe4
	if (!ctx.cr0.eq) goto loc_822CBFE4;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
loc_822CC110:
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// ble cr6,0x822cc1b4
	if (!ctx.cr6.gt) goto loc_822CC1B4;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r30,r22
	ctx.r30.u64 = ctx.r22.u64;
loc_822CC120:
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r21,4
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 4, ctx.xer);
	// blt cr6,0x822cc17c
	if (ctx.cr6.lt) goto loc_822CC17C;
	// lwzx r9,r5,r20
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// addi r31,r21,-3
	ctx.r31.s64 = ctx.r21.s64 + -3;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CC138:
	// lwzx r10,r5,r23
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	// add r4,r9,r11
	ctx.r4.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r8,r11,12
	ctx.r8.s64 = ctx.r11.s64 + 12;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r6,r8,-4
	ctx.r6.s64 = ctx.r8.s64 + -4;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lfsx f0,r10,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r9,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmpw cr6,r7,r31
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r31.s32, ctx.xer);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfsx f12,r10,r6
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfsx f12,r9,r6
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, temp.u32);
	// lfsx f11,r10,r8
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfsx f11,r9,r8
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, temp.u32);
	// blt cr6,0x822cc138
	if (ctx.cr6.lt) goto loc_822CC138;
loc_822CC17C:
	// cmpw cr6,r7,r21
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r21.s32, ctx.xer);
	// bge cr6,0x822cc1a8
	if (!ctx.cr6.lt) goto loc_822CC1A8;
	// subf r9,r7,r21
	ctx.r9.s64 = ctx.r21.s64 - ctx.r7.s64;
	// lwzx r10,r5,r20
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822CC194:
	// lwzx r9,r5,r23
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	// lfsx f0,r9,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r10,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cc194
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CC194;
loc_822CC1A8:
	// addic. r30,r30,-1
	ctx.xer.ca = ctx.r30.u32 > 0;
	ctx.r30.s64 = ctx.r30.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne 0x822cc120
	if (!ctx.cr0.eq) goto loc_822CC120;
loc_822CC1B4:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8233e48c
	__restgprlr_17(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CC1C0"))) PPC_WEAK_FUNC(sub_822CC1C0);
PPC_FUNC_IMPL(__imp__sub_822CC1C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e44c
	ctx.lr = 0x822CC1C8;
	__restfpr_21(ctx, base);
	// stfd f29,-120(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.f29.u64);
	// stfd f30,-112(r1)
	PPC_STORE_U64(ctx.r1.u32 + -112, ctx.f30.u64);
	// stfd f31,-104(r1)
	PPC_STORE_U64(ctx.r1.u32 + -104, ctx.f31.u64);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r21,r4
	ctx.r21.u64 = ctx.r4.u64;
	// mr r22,r5
	ctx.r22.u64 = ctx.r5.u64;
	// mr r26,r6
	ctx.r26.u64 = ctx.r6.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// mr r25,r8
	ctx.r25.u64 = ctx.r8.u64;
	// li r23,0
	ctx.r23.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cc540
	if (ctx.cr6.eq) goto loc_822CC540;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cc540
	if (ctx.cr6.eq) goto loc_822CC540;
	// lwz r11,372(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 372);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cc264
	if (ctx.cr6.eq) goto loc_822CC264;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// ble cr6,0x822cc250
	if (!ctx.cr6.gt) goto loc_822CC250;
	// mr r30,r23
	ctx.r30.u64 = ctx.r23.u64;
	// mr r29,r7
	ctx.r29.u64 = ctx.r7.u64;
loc_822CC224:
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// lwzx r10,r30,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r11.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822cc244
	if (ctx.cr6.eq) goto loc_822CC244;
	// rotlwi r3,r10,0
	ctx.r3.u64 = rotl32(ctx.r10.u32, 0);
	// bl 0x822e8ab0
	ctx.lr = 0x822CC23C;
	sub_822E8AB0(ctx, base);
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// stwx r23,r30,r11
	PPC_STORE_U32(ctx.r30.u32 + ctx.r11.u32, ctx.r23.u32);
loc_822CC244:
	// addic. r29,r29,-1
	ctx.xer.ca = ctx.r29.u32 > 0;
	ctx.r29.s64 = ctx.r29.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// bne 0x822cc224
	if (!ctx.cr0.eq) goto loc_822CC224;
loc_822CC250:
	// lwz r3,372(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cc264
	if (ctx.cr6.eq) goto loc_822CC264;
	// bl 0x822e8ab0
	ctx.lr = 0x822CC260;
	sub_822E8AB0(ctx, base);
	// stw r23,372(r31)
	PPC_STORE_U32(ctx.r31.u32 + 372, ctx.r23.u32);
loc_822CC264:
	// lwz r11,376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 376);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cc2c0
	if (ctx.cr6.eq) goto loc_822CC2C0;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// ble cr6,0x822cc2ac
	if (!ctx.cr6.gt) goto loc_822CC2AC;
	// mr r30,r23
	ctx.r30.u64 = ctx.r23.u64;
	// mr r29,r27
	ctx.r29.u64 = ctx.r27.u64;
loc_822CC280:
	// lwz r11,376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 376);
	// lwzx r10,r30,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r11.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822cc2a0
	if (ctx.cr6.eq) goto loc_822CC2A0;
	// rotlwi r3,r10,0
	ctx.r3.u64 = rotl32(ctx.r10.u32, 0);
	// bl 0x822e8ab0
	ctx.lr = 0x822CC298;
	sub_822E8AB0(ctx, base);
	// lwz r11,376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 376);
	// stwx r23,r30,r11
	PPC_STORE_U32(ctx.r30.u32 + ctx.r11.u32, ctx.r23.u32);
loc_822CC2A0:
	// addic. r29,r29,-1
	ctx.xer.ca = ctx.r29.u32 > 0;
	ctx.r29.s64 = ctx.r29.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// bne 0x822cc280
	if (!ctx.cr0.eq) goto loc_822CC280;
loc_822CC2AC:
	// lwz r3,376(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 376);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cc2c0
	if (ctx.cr6.eq) goto loc_822CC2C0;
	// bl 0x822e8ab0
	ctx.lr = 0x822CC2BC;
	sub_822E8AB0(ctx, base);
	// stw r23,376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 376, ctx.r23.u32);
loc_822CC2C0:
	// rlwinm r24,r27,2,0,29
	ctx.r24.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x822e8aa0
	ctx.lr = 0x822CC2CC;
	sub_822E8AA0(ctx, base);
	// stw r3,372(r31)
	PPC_STORE_U32(ctx.r31.u32 + 372, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822cc2e4
	if (!ctx.cr6.eq) goto loc_822CC2E4;
loc_822CC2D8:
	// lis r23,-32761
	ctx.r23.s64 = -2147024896;
	// ori r23,r23,14
	ctx.r23.u64 = ctx.r23.u64 | 14;
	// b 0x822cc548
	goto loc_822CC548;
loc_822CC2E4:
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8233eaf0
	ctx.lr = 0x822CC2F0;
	sub_8233EAF0(ctx, base);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// ble cr6,0x822cc344
	if (!ctx.cr6.gt) goto loc_822CC344;
	// rlwinm r28,r22,2,0,29
	ctx.r28.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r30,r23
	ctx.r30.u64 = ctx.r23.u64;
loc_822CC304:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x822e8aa0
	ctx.lr = 0x822CC30C;
	sub_822E8AA0(ctx, base);
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// stwx r3,r30,r11
	PPC_STORE_U32(ctx.r30.u32 + ctx.r11.u32, ctx.r3.u32);
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// lwzx r10,r30,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r11.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822cc2d8
	if (ctx.cr6.eq) goto loc_822CC2D8;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// rotlwi r3,r10,0
	ctx.r3.u64 = rotl32(ctx.r10.u32, 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8233eaf0
	ctx.lr = 0x822CC334;
	sub_8233EAF0(ctx, base);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// cmpw cr6,r29,r27
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r27.s32, ctx.xer);
	// blt cr6,0x822cc304
	if (ctx.cr6.lt) goto loc_822CC304;
loc_822CC344:
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x822e8aa0
	ctx.lr = 0x822CC34C;
	sub_822E8AA0(ctx, base);
	// stw r3,376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 376, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cc2d8
	if (ctx.cr6.eq) goto loc_822CC2D8;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8233eaf0
	ctx.lr = 0x822CC364;
	sub_8233EAF0(ctx, base);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// ble cr6,0x822cc3b8
	if (!ctx.cr6.gt) goto loc_822CC3B8;
	// rlwinm r28,r22,2,0,29
	ctx.r28.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r30,r23
	ctx.r30.u64 = ctx.r23.u64;
loc_822CC378:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x822e8aa0
	ctx.lr = 0x822CC380;
	sub_822E8AA0(ctx, base);
	// lwz r11,376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 376);
	// stwx r3,r30,r11
	PPC_STORE_U32(ctx.r30.u32 + ctx.r11.u32, ctx.r3.u32);
	// lwz r11,376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 376);
	// lwzx r10,r30,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r11.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822cc2d8
	if (ctx.cr6.eq) goto loc_822CC2D8;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// rotlwi r3,r10,0
	ctx.r3.u64 = rotl32(ctx.r10.u32, 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8233eaf0
	ctx.lr = 0x822CC3A8;
	sub_8233EAF0(ctx, base);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// cmpw cr6,r29,r27
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r27.s32, ctx.xer);
	// blt cr6,0x822cc378
	if (ctx.cr6.lt) goto loc_822CC378;
loc_822CC3B8:
	// cmplwi cr6,r21,0
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 0, ctx.xer);
	// beq cr6,0x822cc468
	if (ctx.cr6.eq) goto loc_822CC468;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// ble cr6,0x822cc548
	if (!ctx.cr6.gt) goto loc_822CC548;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// mr r28,r23
	ctx.r28.u64 = ctx.r23.u64;
	// mr r25,r27
	ctx.r25.u64 = ctx.r27.u64;
	// lfd f30,-17064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r11.u32 + -17064);
	// lis r26,-32768
	ctx.r26.s64 = -2147483648;
	// lfd f31,-17072(r10)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r10.u32 + -17072);
	// lfs f29,-28948(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28948);
	ctx.f29.f64 = double(temp.f32);
loc_822CC3EC:
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// ble cr6,0x822cc458
	if (!ctx.cr6.gt) goto loc_822CC458;
	// mr r30,r23
	ctx.r30.u64 = ctx.r23.u64;
	// add r29,r28,r21
	ctx.r29.u64 = ctx.r28.u64 + ctx.r21.u64;
	// mr r27,r22
	ctx.r27.u64 = ctx.r22.u64;
loc_822CC400:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// cmpw cr6,r11,r26
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r26.s32, ctx.xer);
	// bne cr6,0x822cc41c
	if (!ctx.cr6.eq) goto loc_822CC41C;
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// lwzx r10,r28,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r11.u32);
	// stfsx f29,r10,r30
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r30.u32, temp.u32);
	// b 0x822cc448
	goto loc_822CC448;
loc_822CC41C:
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f30.f64;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f0,80(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// fmul f2,f13,f31
	ctx.f2.f64 = ctx.f13.f64 * ctx.f31.f64;
	// bl 0x8233c318
	ctx.lr = 0x822CC438;
	sub_8233C318(ctx, base);
	// lwz r10,372(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// frsp f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// lwzx r9,r28,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r10.u32);
	// stfsx f12,r9,r30
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r30.u32, temp.u32);
loc_822CC448:
	// addic. r27,r27,-1
	ctx.xer.ca = ctx.r27.u32 > 0;
	ctx.r27.s64 = ctx.r27.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// add r29,r29,r24
	ctx.r29.u64 = ctx.r29.u64 + ctx.r24.u64;
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// bne 0x822cc400
	if (!ctx.cr0.eq) goto loc_822CC400;
loc_822CC458:
	// addic. r25,r25,-1
	ctx.xer.ca = ctx.r25.u32 > 0;
	ctx.r25.s64 = ctx.r25.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// bne 0x822cc3ec
	if (!ctx.cr0.eq) goto loc_822CC3EC;
	// b 0x822cc548
	goto loc_822CC548;
loc_822CC468:
	// lwz r11,352(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 352);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cc498
	if (ctx.cr6.eq) goto loc_822CC498;
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// lwz r8,376(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 376);
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// lwz r7,372(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r22
	ctx.r3.u64 = ctx.r22.u64;
	// bl 0x822cb930
	ctx.lr = 0x822CC490;
	sub_822CB930(ctx, base);
	// mr r23,r3
	ctx.r23.u64 = ctx.r3.u64;
	// b 0x822cc548
	goto loc_822CC548;
loc_822CC498:
	// cmpw cr6,r27,r22
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r22.s32, ctx.xer);
	// beq cr6,0x822cc4a8
	if (ctx.cr6.eq) goto loc_822CC4A8;
	// lis r23,-32764
	ctx.r23.s64 = -2147221504;
	// b 0x822cc548
	goto loc_822CC548;
loc_822CC4A8:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// mr r9,r23
	ctx.r9.u64 = ctx.r23.u64;
	// cmpwi cr6,r27,4
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 4, ctx.xer);
	// lfs f0,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// blt cr6,0x822cc514
	if (ctx.cr6.lt) goto loc_822CC514;
	// addi r6,r27,-3
	ctx.r6.s64 = ctx.r27.s64 + -3;
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
loc_822CC4C4:
	// lwz r7,372(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// addi r10,r11,12
	ctx.r10.s64 = ctx.r11.s64 + 12;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r8,r10,-4
	ctx.r8.s64 = ctx.r10.s64 + -4;
	// cmpw cr6,r9,r6
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, ctx.xer);
	// lwzx r5,r11,r7
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r7.u32);
	// stfsx f0,r5,r11
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, temp.u32);
	// lwz r7,372(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// add r4,r11,r7
	ctx.r4.u64 = ctx.r11.u64 + ctx.r7.u64;
	// lwz r7,4(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// add r3,r7,r11
	ctx.r3.u64 = ctx.r7.u64 + ctx.r11.u64;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// lwz r7,372(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// lwzx r5,r8,r7
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	// stfsx f0,r5,r8
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r8.u32, temp.u32);
	// lwz r4,372(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// lwzx r3,r10,r4
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r4.u32);
	// stfsx f0,r3,r10
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + ctx.r10.u32, temp.u32);
	// blt cr6,0x822cc4c4
	if (ctx.cr6.lt) goto loc_822CC4C4;
loc_822CC514:
	// cmpw cr6,r9,r27
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r27.s32, ctx.xer);
	// bge cr6,0x822cc548
	if (!ctx.cr6.lt) goto loc_822CC548;
	// subf r10,r9,r27
	ctx.r10.s64 = ctx.r27.s64 - ctx.r9.s64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822CC528:
	// lwz r10,372(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// stfsx f0,r9,r11
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cc528
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CC528;
	// b 0x822cc548
	goto loc_822CC548;
loc_822CC540:
	// lis r23,-32761
	ctx.r23.s64 = -2147024896;
	// ori r23,r23,87
	ctx.r23.u64 = ctx.r23.u64 | 87;
loc_822CC548:
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lfd f29,-120(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// lfd f30,-112(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// lfd f31,-104(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// b 0x8233e49c
	__restgprlr_21(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CC560"))) PPC_WEAK_FUNC(sub_822CC560);
PPC_FUNC_IMPL(__imp__sub_822CC560) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e454
	ctx.lr = 0x822CC568;
	__restfpr_23(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,356(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 356);
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// lwz r30,0(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// lwz r28,360(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 360);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cc600
	if (ctx.cr6.eq) goto loc_822CC600;
	// lhz r11,110(r30)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r30.u32 + 110);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// addi r25,r11,-16
	ctx.r25.s64 = ctx.r11.s64 + -16;
	// ble cr6,0x822cc600
	if (!ctx.cr6.gt) goto loc_822CC600;
	// rlwinm r23,r28,1,0,30
	ctx.r23.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
loc_822CC5A0:
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x822cc5e8
	if (!ctx.cr6.gt) goto loc_822CC5E8;
	// addi r29,r24,-2
	ctx.r29.s64 = ctx.r24.s64 + -2;
loc_822CC5B0:
	// lwz r11,524(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 524);
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// lhz r5,110(r30)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r30.u32 + 110);
	// lwz r4,88(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 88);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822CC5CC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// sraw r10,r3,r25
	temp.u32 = ctx.r25.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r3.s32 < 0) & (((ctx.r3.s32 >> temp.u32) << temp.u32) != ctx.r3.s32);
	ctx.r10.s64 = ctx.r3.s32 >> temp.u32;
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lhz r9,82(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// cmpw cr6,r31,r28
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r28.s32, ctx.xer);
	// sthu r9,2(r29)
	ea = 2 + ctx.r29.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r29.u32 = ea;
	// blt cr6,0x822cc5b0
	if (ctx.cr6.lt) goto loc_822CC5B0;
loc_822CC5E8:
	// lwz r11,88(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 88);
	// addic. r26,r26,-1
	ctx.xer.ca = ctx.r26.u32 > 0;
	ctx.r26.s64 = ctx.r26.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// add r24,r23,r24
	ctx.r24.u64 = ctx.r23.u64 + ctx.r24.u64;
	// mullw r11,r28,r11
	ctx.r11.s64 = int64_t(ctx.r28.s32) * int64_t(ctx.r11.s32);
	// add r27,r11,r27
	ctx.r27.u64 = ctx.r11.u64 + ctx.r27.u64;
	// bne 0x822cc5a0
	if (!ctx.cr0.eq) goto loc_822CC5A0;
loc_822CC600:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8233e4a4
	__restgprlr_23(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CC60C"))) PPC_WEAK_FUNC(sub_822CC60C);
PPC_FUNC_IMPL(__imp__sub_822CC60C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CC610"))) PPC_WEAK_FUNC(sub_822CC610);
PPC_FUNC_IMPL(__imp__sub_822CC610) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,444(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 444);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cc6ac
	if (ctx.cr6.eq) goto loc_822CC6AC;
	// lhz r11,118(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 118);
	// lwz r7,8(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// extsh r8,r11
	ctx.r8.s64 = ctx.r11.s16;
	// lwz r6,432(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 432);
	// lwz r5,428(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 428);
	// lwz r9,304(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 304);
	// lwz r4,4(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// bl 0x822ecf10
	ctx.lr = 0x822CC658;
	sub_822ECF10(ctx, base);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// lwz r7,304(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 304);
	// cmpwi cr6,r7,1
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 1, ctx.xer);
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// ble cr6,0x822cc698
	if (!ctx.cr6.gt) goto loc_822CC698;
	// li r10,1
	ctx.r10.s64 = 1;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_822CC674:
	// lwzx r11,r11,r8
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r8.u32);
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// ble cr6,0x822cc684
	if (!ctx.cr6.gt) goto loc_822CC684;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
loc_822CC684:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r7
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, ctx.xer);
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// blt cr6,0x822cc674
	if (ctx.cr6.lt) goto loc_822CC674;
loc_822CC698:
	// lwz r11,424(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 424);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r9,64(r31)
	PPC_STORE_U32(ctx.r31.u32 + 64, ctx.r9.u32);
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// stb r10,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r10.u8);
loc_822CC6AC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CC6C8"))) PPC_WEAK_FUNC(sub_822CC6C8);
PPC_FUNC_IMPL(__imp__sub_822CC6C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e460
	ctx.lr = 0x822CC6D0;
	__restfpr_26(ctx, base);
	// stfd f30,-72(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -72, ctx.f30.u64);
	// stfd f31,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.f31.u64);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lhz r29,34(r8)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r8.u32 + 34);
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// ble cr6,0x822cc71c
	if (!ctx.cr6.gt) goto loc_822CC71C;
	// mtctr r29
	ctx.ctr.u64 = ctx.r29.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CC6FC:
	// lwz r9,320(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 320);
	// lwz r7,388(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// add r6,r9,r11
	ctx.r6.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r11,r11,1776
	ctx.r11.s64 = ctx.r11.s64 + 1776;
	// lwz r4,60(r6)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 60);
	// stwx r4,r7,r10
	PPC_STORE_U32(ctx.r7.u32 + ctx.r10.u32, ctx.r4.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x822cc6fc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CC6FC;
loc_822CC71C:
	// lwz r11,440(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 440);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cca14
	if (ctx.cr6.eq) goto loc_822CCA14;
	// clrlwi r26,r5,16
	ctx.r26.u64 = ctx.r5.u32 & 0xFFFF;
	// li r27,0
	ctx.r27.s64 = 0;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// ble cr6,0x822cca14
	if (!ctx.cr6.gt) goto loc_822CCA14;
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// mullw r28,r29,r29
	ctx.r28.s64 = int64_t(ctx.r29.s32) * int64_t(ctx.r29.s32);
	// lfs f31,-28948(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,5256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f30.f64 = double(temp.f32);
loc_822CC74C:
	// lwz r11,460(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 460);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cc880
	if (ctx.cr6.eq) goto loc_822CC880;
	// extsw r10,r27
	ctx.r10.s64 = ctx.r27.s32;
	// extsw r11,r26
	ctx.r11.s64 = ctx.r26.s32;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f13,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f0,80(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f0
	ctx.f11.f64 = double(ctx.f0.s64);
	// li r8,0
	ctx.r8.s64 = 0;
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// cmpwi cr6,r28,4
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 4, ctx.xer);
	// frsp f9,f11
	ctx.f9.f64 = double(float(ctx.f11.f64));
	// frsp f10,f12
	ctx.f10.f64 = double(float(ctx.f12.f64));
	// fdivs f0,f10,f9
	ctx.f0.f64 = double(float(ctx.f10.f64 / ctx.f9.f64));
	// fsubs f13,f30,f0
	ctx.f13.f64 = static_cast<float>(ctx.f30.f64 - ctx.f0.f64);
	// blt cr6,0x822cc840
	if (ctx.cr6.lt) goto loc_822CC840;
	// addi r5,r28,-3
	ctx.r5.s64 = ctx.r28.s64 + -3;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CC79C:
	// lwz r7,464(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 464);
	// addi r10,r11,12
	ctx.r10.s64 = ctx.r11.s64 + 12;
	// lwz r6,448(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lwz r4,468(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 468);
	// addi r9,r10,-4
	ctx.r9.s64 = ctx.r10.s64 + -4;
	// cmpw cr6,r8,r5
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, ctx.xer);
	// lfsx f12,r7,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfsx f10,r11,r6
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r6.u32);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f9,f10,f0,f11
	ctx.f9.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// stfsx f9,r4,r11
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r11.u32, temp.u32);
	// lwz r6,448(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// lwz r7,464(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 464);
	// add r3,r7,r11
	ctx.r3.u64 = ctx.r7.u64 + ctx.r11.u64;
	// add r7,r11,r6
	ctx.r7.u64 = ctx.r11.u64 + ctx.r6.u64;
	// lwz r6,468(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 468);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfs f6,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f5,f6,f0,f7
	ctx.f5.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f0.f64), float(ctx.f7.f64)));
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stfs f5,4(r6)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// lwz r4,448(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 464);
	// lwz r7,468(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 468);
	// lfsx f4,r9,r4
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r4.u32);
	ctx.f4.f64 = double(temp.f32);
	// lfsx f3,r9,r3
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r3.u32);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmadds f1,f4,f0,f2
	ctx.f1.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f0.f64), float(ctx.f2.f64)));
	// stfsx f1,r9,r7
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, temp.u32);
	// lwz r6,448(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// lwz r4,464(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 464);
	// lwz r3,468(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 468);
	// lfsx f12,r10,r6
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfsx f11,r10,r4
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r4.u32);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmadds f9,f12,f0,f10
	ctx.f9.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f10.f64)));
	// stfsx f9,r10,r3
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r3.u32, temp.u32);
	// blt cr6,0x822cc79c
	if (ctx.cr6.lt) goto loc_822CC79C;
loc_822CC840:
	// cmpw cr6,r8,r28
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r28.s32, ctx.xer);
	// bge cr6,0x822cc890
	if (!ctx.cr6.lt) goto loc_822CC890;
	// subf r10,r8,r28
	ctx.r10.s64 = ctx.r28.s64 - ctx.r8.s64;
	// rlwinm r11,r8,2,0,29
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822CC854:
	// lwz r10,464(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 464);
	// lwz r9,448(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// lwz r8,468(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 468);
	// lfsx f12,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfsx f10,r11,r9
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f9,f10,f0,f11
	ctx.f9.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// stfsx f9,r11,r8
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r8.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cc854
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CC854;
	// b 0x822cc890
	goto loc_822CC890;
loc_822CC880:
	// rlwinm r5,r28,2,0,29
	ctx.r5.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,448(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// lwz r3,468(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 468);
	// bl 0x8233e4e0
	ctx.lr = 0x822CC890;
	sub_8233E4E0(ctx, base);
loc_822CC890:
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r29,4
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 4, ctx.xer);
	// blt cr6,0x822cc914
	if (ctx.cr6.lt) goto loc_822CC914;
	// addi r5,r29,-3
	ctx.r5.s64 = ctx.r29.s64 + -3;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CC8A4:
	// lwz r7,388(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// addi r10,r11,12
	ctx.r10.s64 = ctx.r11.s64 + 12;
	// lwz r6,384(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 384);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r8,r10,-4
	ctx.r8.s64 = ctx.r10.s64 + -4;
	// cmpw cr6,r9,r5
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r5.s32, ctx.xer);
	// lwzx r4,r11,r7
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r7.u32);
	// lfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r11,r6
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r6.u32, temp.u32);
	// lwz r7,384(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 384);
	// lwz r6,388(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// add r3,r11,r6
	ctx.r3.u64 = ctx.r11.u64 + ctx.r6.u64;
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// add r4,r11,r7
	ctx.r4.u64 = ctx.r11.u64 + ctx.r7.u64;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// lfs f13,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lwz r3,388(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// lwzx r7,r3,r8
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r8.u32);
	// lwz r6,384(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 384);
	// lfs f12,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// stfsx f12,r6,r8
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r6.u32 + ctx.r8.u32, temp.u32);
	// lwz r3,388(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// lwzx r8,r3,r10
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r10.u32);
	// lwz r4,384(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 384);
	// lfs f11,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// stfsx f11,r4,r10
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r10.u32, temp.u32);
	// blt cr6,0x822cc8a4
	if (ctx.cr6.lt) goto loc_822CC8A4;
loc_822CC914:
	// cmpw cr6,r9,r29
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r29.s32, ctx.xer);
	// bge cr6,0x822cc944
	if (!ctx.cr6.lt) goto loc_822CC944;
	// subf r10,r9,r29
	ctx.r10.s64 = ctx.r29.s64 - ctx.r9.s64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822CC928:
	// lwz r10,388(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// lwz r9,384(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 384);
	// lwzx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lfs f0,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r9,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cc928
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CC928;
loc_822CC944:
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// ble cr6,0x822cca08
	if (!ctx.cr6.gt) goto loc_822CCA08;
	// mtctr r29
	ctx.ctr.u64 = ctx.r29.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r30,r29,2,0,29
	ctx.r30.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
loc_822CC95C:
	// lwz r11,468(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 468);
	// fmr f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f31.f64;
	// fmr f13,f31
	ctx.f13.f64 = ctx.f31.f64;
	// li r10,0
	ctx.r10.s64 = 0;
	// fmr f12,f31
	ctx.f12.f64 = ctx.f31.f64;
	// add r8,r11,r4
	ctx.r8.u64 = ctx.r11.u64 + ctx.r4.u64;
	// cmpwi cr6,r29,2
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 2, ctx.xer);
	// blt cr6,0x822cc9b8
	if (ctx.cr6.lt) goto loc_822CC9B8;
	// lwz r9,384(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 384);
	// addi r5,r29,-1
	ctx.r5.s64 = ctx.r29.s64 + -1;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822CC988:
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + ctx.r11.u64;
	// lfsx f11,r11,r8
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r8.u32);
	ctx.f11.f64 = double(temp.f32);
	// add r6,r11,r8
	ctx.r6.u64 = ctx.r11.u64 + ctx.r8.u64;
	// lfsx f10,r9,r11
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f10.f64 = double(temp.f32);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// fmadds f0,f11,f10,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f10.f64), float(ctx.f0.f64)));
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// cmpw cr6,r10,r5
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, ctx.xer);
	// lfs f9,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f13,f8,f9,f13
	ctx.f13.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f9.f64), float(ctx.f13.f64)));
	// blt cr6,0x822cc988
	if (ctx.cr6.lt) goto loc_822CC988;
loc_822CC9B8:
	// cmpw cr6,r10,r29
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r29.s32, ctx.xer);
	// bge cr6,0x822cc9d4
	if (!ctx.cr6.lt) goto loc_822CC9D4;
	// lwz r11,384(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 384);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f12,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfsx f11,r10,r8
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
loc_822CC9D4:
	// lwz r11,388(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// fadds f0,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// add r4,r30,r4
	ctx.r4.u64 = ctx.r30.u64 + ctx.r4.u64;
	// lwzx r10,r11,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
	// fadds f13,f0,f12
	ctx.f13.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lwz r11,388(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// bdnz 0x822cc95c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CC95C;
loc_822CCA08:
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// cmpw cr6,r27,r26
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r26.s32, ctx.xer);
	// blt cr6,0x822cc74c
	if (ctx.cr6.lt) goto loc_822CC74C;
loc_822CCA14:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lfd f30,-72(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// lfd f31,-64(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CCA28"))) PPC_WEAK_FUNC(sub_822CCA28);
PPC_FUNC_IMPL(__imp__sub_822CCA28) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822CCA30;
	__restfpr_28(ctx, base);
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// beq cr6,0x822ccb58
	if (ctx.cr6.eq) goto loc_822CCB58;
	// lwz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// blt cr6,0x822ccb58
	if (ctx.cr6.lt) goto loc_822CCB58;
	// lwz r11,472(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 472);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ccb58
	if (!ctx.cr6.eq) goto loc_822CCB58;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f3348
	ctx.lr = 0x822CCA68;
	sub_822F3348(ctx, base);
	// lwz r10,488(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 488);
	// lwz r9,484(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 484);
	// li r29,3
	ctx.r29.s64 = 3;
	// li r28,1
	ctx.r28.s64 = 1;
	// lwz r11,336(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 336);
	// stw r31,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r31.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r29.u32);
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r29.u32);
	// stw r28,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r28.u32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// lwz r10,452(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 452);
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// blt cr6,0x822ccaa8
	if (ctx.cr6.lt) goto loc_822CCAA8;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r11.u32);
loc_822CCAA8:
	// lhz r11,34(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 34);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r11.u32);
	// lwz r10,88(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// lhz r9,110(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 110);
	// stw r9,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r9.u32);
	// bl 0x822f3290
	ctx.lr = 0x822CCAC8;
	sub_822F3290(ctx, base);
	// lwz r8,624(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 624);
	// lwz r7,496(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 496);
	// lwz r6,492(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 492);
	// stw r28,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r28.u32);
	// stw r29,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r29.u32);
	// stw r8,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r8.u32);
	// stw r7,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r7.u32);
	// stw r6,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r6.u32);
	// stw r29,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r29.u32);
	// stw r28,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r28.u32);
	// lwz r3,568(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 568);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822ccb10
	if (ctx.cr6.eq) goto loc_822CCB10;
	// bl 0x822f3498
	ctx.lr = 0x822CCB00;
	sub_822F3498(ctx, base);
	// lwz r3,568(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 568);
	// bl 0x822e8ab0
	ctx.lr = 0x822CCB08;
	sub_822E8AB0(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,568(r31)
	PPC_STORE_U32(ctx.r31.u32 + 568, ctx.r11.u32);
loc_822CCB10:
	// li r3,304
	ctx.r3.s64 = 304;
	// bl 0x822e8aa0
	ctx.lr = 0x822CCB18;
	sub_822E8AA0(ctx, base);
	// stw r3,568(r31)
	PPC_STORE_U32(ctx.r31.u32 + 568, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822ccb34
	if (!ctx.cr6.eq) goto loc_822CCB34;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_822CCB34:
	// bl 0x822f3390
	ctx.lr = 0x822CCB38;
	sub_822F3390(ctx, base);
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,568(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 568);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x822f3af8
	ctx.lr = 0x822CCB4C;
	sub_822F3AF8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822ccb58
	if (ctx.cr6.lt) goto loc_822CCB58;
	// stw r28,472(r30)
	PPC_STORE_U32(ctx.r30.u32 + 472, ctx.r28.u32);
loc_822CCB58:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CCB60"))) PPC_WEAK_FUNC(sub_822CCB60);
PPC_FUNC_IMPL(__imp__sub_822CCB60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e44c
	ctx.lr = 0x822CCB68;
	__restfpr_21(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r23,0
	ctx.r23.s64 = 0;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// mr r21,r5
	ctx.r21.u64 = ctx.r5.u64;
	// mr r22,r6
	ctx.r22.u64 = ctx.r6.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// mr r30,r23
	ctx.r30.u64 = ctx.r23.u64;
	// beq cr6,0x822ccdb4
	if (ctx.cr6.eq) goto loc_822CCDB4;
	// lwz r24,0(r3)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x822ccdb4
	if (ctx.cr6.eq) goto loc_822CCDB4;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x822ccdb4
	if (ctx.cr6.eq) goto loc_822CCDB4;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x822ccdb4
	if (ctx.cr6.eq) goto loc_822CCDB4;
	// lwz r11,692(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 692);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x822ccbc0
	if (ctx.cr6.eq) goto loc_822CCBC0;
	// lis r30,-32764
	ctx.r30.s64 = -2147221504;
	// ori r30,r30,10
	ctx.r30.u64 = ctx.r30.u64 | 10;
	// b 0x822ccdbc
	goto loc_822CCDBC;
loc_822CCBC0:
	// stw r23,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r23.u32);
	// stw r23,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r23.u32);
	// lwz r11,72(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 72);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x822ccdc8
	if (ctx.cr6.eq) goto loc_822CCDC8;
	// lwz r11,820(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 820);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822ccbe4
	if (!ctx.cr6.eq) goto loc_822CCBE4;
	// stw r23,696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 696, ctx.r23.u32);
loc_822CCBE4:
	// lwz r11,696(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 696);
	// lis r10,-32764
	ctx.r10.s64 = -2147221504;
	// lis r9,-32764
	ctx.r9.s64 = -2147221504;
	// ori r27,r10,2
	ctx.r27.u64 = ctx.r10.u64 | 2;
	// ori r25,r9,4
	ctx.r25.u64 = ctx.r9.u64 | 4;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cccc4
	if (ctx.cr6.eq) goto loc_822CCCC4;
	// lwz r11,704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 704);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ccc24
	if (ctx.cr6.eq) goto loc_822CCC24;
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// addi r3,r31,224
	ctx.r3.s64 = ctx.r31.s64 + 224;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822ccc24
	if (ctx.cr6.eq) goto loc_822CCC24;
	// bl 0x822ed5e8
	ctx.lr = 0x822CCC24;
	sub_822ED5E8(ctx, base);
loc_822CCC24:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c7248
	ctx.lr = 0x822CCC2C;
	sub_822C7248(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplw cr6,r3,r27
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r27.u32, ctx.xer);
	// bne cr6,0x822ccc68
	if (!ctx.cr6.eq) goto loc_822CCC68;
	// lis r11,15
	ctx.r11.s64 = 983040;
	// ori r28,r11,16960
	ctx.r28.u64 = ctx.r11.u64 | 16960;
loc_822CCC40:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c6f48
	ctx.lr = 0x822CCC48;
	sub_822C6F48(ctx, base);
	// cmpw cr6,r29,r28
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r28.s32, ctx.xer);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// bgt cr6,0x822ccc70
	if (ctx.cr6.gt) goto loc_822CCC70;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c7248
	ctx.lr = 0x822CCC5C;
	sub_822C7248(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplw cr6,r3,r27
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r27.u32, ctx.xer);
	// beq cr6,0x822ccc40
	if (ctx.cr6.eq) goto loc_822CCC40;
loc_822CCC68:
	// cmplw cr6,r30,r25
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r25.u32, ctx.xer);
	// bne cr6,0x822cccb8
	if (!ctx.cr6.eq) goto loc_822CCCB8;
loc_822CCC70:
	// lwz r11,300(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ccc98
	if (ctx.cr6.eq) goto loc_822CCC98;
	// lwz r11,704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ccc98
	if (!ctx.cr6.eq) goto loc_822CCC98;
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
	// stw r23,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r23.u32);
	// mr r30,r23
	ctx.r30.u64 = ctx.r23.u64;
	// b 0x822ccdc8
	goto loc_822CCDC8;
loc_822CCC98:
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// mr r30,r23
	ctx.r30.u64 = ctx.r23.u64;
	// cntlzw r10,r11
	ctx.r10.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r11,r9,1
	ctx.r11.u64 = ctx.r9.u64 ^ 1;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r11.u32);
	// b 0x822ccdc8
	goto loc_822CCDC8;
loc_822CCCB8:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x822ccdbc
	if (ctx.cr6.lt) goto loc_822CCDBC;
	// stw r23,696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 696, ctx.r23.u32);
loc_822CCCC4:
	// sth r23,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r23.u16);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c75f8
	ctx.lr = 0x822CCCD8;
	sub_822C75F8(ctx, base);
	// lhz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// stw r4,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r4.u32);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r9,336(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// lwz r8,452(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 452);
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// beq cr6,0x822ccd04
	if (ctx.cr6.eq) goto loc_822CCD04;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c7f20
	ctx.lr = 0x822CCD00;
	sub_822C7F20(ctx, base);
	// stw r3,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r3.u32);
loc_822CCD04:
	// cmplw cr6,r30,r27
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r27.u32, ctx.xer);
	// bne cr6,0x822ccd1c
	if (!ctx.cr6.eq) goto loc_822CCD1C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c6f48
	ctx.lr = 0x822CCD14;
	sub_822C6F48(ctx, base);
	// mr r30,r23
	ctx.r30.u64 = ctx.r23.u64;
	// b 0x822ccdc8
	goto loc_822CCDC8;
loc_822CCD1C:
	// cmpwi cr6,r30,4
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 4, ctx.xer);
	// bne cr6,0x822ccd70
	if (!ctx.cr6.eq) goto loc_822CCD70;
	// li r11,1
	ctx.r11.s64 = 1;
	// li r10,6
	ctx.r10.s64 = 6;
	// stw r11,696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 696, ctx.r11.u32);
	// stw r10,72(r24)
	PPC_STORE_U32(ctx.r24.u32 + 72, ctx.r10.u32);
	// lwz r9,704(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 704);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x822ccdc8
	if (ctx.cr6.eq) goto loc_822CCDC8;
	// cmplwi cr6,r22,0
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, 0, ctx.xer);
	// beq cr6,0x822ccdc8
	if (ctx.cr6.eq) goto loc_822CCDC8;
	// addi r3,r31,224
	ctx.r3.s64 = ctx.r31.s64 + 224;
	// bl 0x822ed9e0
	ctx.lr = 0x822CCD50;
	sub_822ED9E0(ctx, base);
	// lwz r11,252(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 252);
	// lwz r10,244(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 244);
	// srawi r9,r3,3
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r3.s32 >> 3;
	// addze r8,r9
	temp.s64 = ctx.r9.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r8.s64 = temp.s64;
	// subf r7,r10,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r10.s64;
	// subf r6,r8,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r8.s64;
	// stw r6,0(r22)
	PPC_STORE_U32(ctx.r22.u32 + 0, ctx.r6.u32);
	// b 0x822ccdc8
	goto loc_822CCDC8;
loc_822CCD70:
	// cmplw cr6,r30,r25
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r25.u32, ctx.xer);
	// bne cr6,0x822ccda0
	if (!ctx.cr6.eq) goto loc_822CCDA0;
	// lwz r11,300(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ccc98
	if (ctx.cr6.eq) goto loc_822CCC98;
	// lwz r11,704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ccc98
	if (!ctx.cr6.eq) goto loc_822CCC98;
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
	// stw r23,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r23.u32);
	// mr r30,r23
	ctx.r30.u64 = ctx.r23.u64;
	// b 0x822ccdc8
	goto loc_822CCDC8;
loc_822CCDA0:
	// li r11,7
	ctx.r11.s64 = 7;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r11,72(r24)
	PPC_STORE_U32(ctx.r24.u32 + 72, ctx.r11.u32);
	// blt cr6,0x822ccdbc
	if (ctx.cr6.lt) goto loc_822CCDBC;
	// b 0x822ccdc8
	goto loc_822CCDC8;
loc_822CCDB4:
	// lis r30,-32761
	ctx.r30.s64 = -2147024896;
	// ori r30,r30,87
	ctx.r30.u64 = ctx.r30.u64 | 87;
loc_822CCDBC:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x822cce30
	if (ctx.cr6.eq) goto loc_822CCE30;
	// stw r23,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r23.u32);
loc_822CCDC8:
	// lwz r11,704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ccde4
	if (!ctx.cr6.eq) goto loc_822CCDE4;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r10,820(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 820);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x822cce14
	if (ctx.cr6.eq) goto loc_822CCE14;
loc_822CCDE4:
	// lwz r11,696(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 696);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cce14
	if (ctx.cr6.eq) goto loc_822CCE14;
	// lwz r11,692(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 692);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x822cce14
	if (!ctx.cr6.eq) goto loc_822CCE14;
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// cntlzw r10,r11
	ctx.r10.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r11,r9,1
	ctx.r11.u64 = ctx.r9.u64 ^ 1;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// stw r8,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r8.u32);
loc_822CCE14:
	// cmplwi cr6,r21,0
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 0, ctx.xer);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// beq cr6,0x822cce34
	if (ctx.cr6.eq) goto loc_822CCE34;
	// lwz r11,692(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 692);
	// stw r11,0(r21)
	PPC_STORE_U32(ctx.r21.u32 + 0, ctx.r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8233e49c
	__restgprlr_21(ctx, base);
	return;
loc_822CCE30:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
loc_822CCE34:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8233e49c
	__restgprlr_21(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CCE3C"))) PPC_WEAK_FUNC(sub_822CCE3C);
PPC_FUNC_IMPL(__imp__sub_822CCE3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CCE40"))) PPC_WEAK_FUNC(sub_822CCE40);
PPC_FUNC_IMPL(__imp__sub_822CCE40) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822CCE48;
	__restfpr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lhz r11,500(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 500);
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// extsh r28,r11
	ctx.r28.s64 = ctx.r11.s16;
	// bne cr6,0x822cce78
	if (!ctx.cr6.eq) goto loc_822CCE78;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822CCE78:
	// lwz r11,440(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 440);
	// lwz r30,0(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lhz r29,0(r27)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r27.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ccea0
	if (ctx.cr6.eq) goto loc_822CCEA0;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cc6c8
	ctx.lr = 0x822CCE98;
	sub_822CC6C8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822ccefc
	if (ctx.cr6.lt) goto loc_822CCEFC;
loc_822CCEA0:
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cca28
	ctx.lr = 0x822CCEAC;
	sub_822CCA28(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822ccefc
	if (ctx.cr6.lt) goto loc_822CCEFC;
	// lwz r11,472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 472);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822ccefc
	if (!ctx.cr6.eq) goto loc_822CCEFC;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x822ccefc
	if (ctx.cr6.eq) goto loc_822CCEFC;
	// lwz r11,176(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 176);
	// clrlwi r7,r29,16
	ctx.r7.u64 = ctx.r29.u32 & 0xFFFF;
	// lbz r5,201(r30)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r30.u32 + 201);
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// lwz r3,568(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 568);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// bne cr6,0x822ccef8
	if (!ctx.cr6.eq) goto loc_822CCEF8;
	// bl 0x822f4cf8
	ctx.lr = 0x822CCEEC;
	sub_822F4CF8(ctx, base);
	// sth r29,0(r27)
	PPC_STORE_U16(ctx.r27.u32 + 0, ctx.r29.u16);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822CCEF8:
	// bl 0x822f47a8
	ctx.lr = 0x822CCEFC;
	sub_822F47A8(ctx, base);
loc_822CCEFC:
	// sth r29,0(r27)
	PPC_STORE_U16(ctx.r27.u32 + 0, ctx.r29.u16);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CCF08"))) PPC_WEAK_FUNC(sub_822CCF08);
PPC_FUNC_IMPL(__imp__sub_822CCF08) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822CCF10;
	__restfpr_27(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r29,0
	ctx.r29.s64 = 0;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x822ccf3c
	if (ctx.cr6.eq) goto loc_822CCF3C;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// bne cr6,0x822ccf48
	if (!ctx.cr6.eq) goto loc_822CCF48;
loc_822CCF3C:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// b 0x822cd088
	goto loc_822CD088;
loc_822CCF48:
	// lwz r10,368(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 368);
	// lwz r9,360(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// lhz r29,0(r27)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r27.u32 + 0);
	// mullw r8,r10,r9
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// sth r29,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r29.u16);
	// mullw r7,r8,r29
	ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r29.s32);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// cmpw cr6,r7,r28
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r28.s32, ctx.xer);
	// bgt cr6,0x822cd09c
	if (ctx.cr6.gt) goto loc_822CD09C;
	// cmpw cr6,r29,r28
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r28.s32, ctx.xer);
	// bgt cr6,0x822cd09c
	if (ctx.cr6.gt) goto loc_822CD09C;
	// lwz r10,352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 352);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x822ccff4
	if (!ctx.cr6.eq) goto loc_822CCFF4;
	// lwz r10,420(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 420);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x822ccf9c
	if (ctx.cr6.eq) goto loc_822CCF9C;
	// lwz r10,424(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 424);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x822ccfc8
	if (!ctx.cr6.eq) goto loc_822CCFC8;
loc_822CCF9C:
	// lwz r11,100(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 100);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bne cr6,0x822ccfbc
	if (!ctx.cr6.eq) goto loc_822CCFBC;
	// bl 0x822c8af8
	ctx.lr = 0x822CCFB8;
	sub_822C8AF8(ctx, base);
	// b 0x822ccfc0
	goto loc_822CCFC0;
loc_822CCFBC:
	// bl 0x822c84a8
	ctx.lr = 0x822CCFC0;
	sub_822C84A8(ctx, base);
loc_822CCFC0:
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cd088
	if (ctx.cr6.lt) goto loc_822CD088;
loc_822CCFC8:
	// lwz r11,420(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 420);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822ccff4
	if (!ctx.cr6.eq) goto loc_822CCFF4;
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c90f0
	ctx.lr = 0x822CCFE8;
	sub_822C90F0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cd084
	if (ctx.cr6.lt) goto loc_822CD084;
	// lhz r29,80(r1)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
loc_822CCFF4:
	// lwz r11,316(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 316);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cd02c
	if (ctx.cr6.eq) goto loc_822CD02C;
	// clrlwi r11,r29,16
	ctx.r11.u64 = ctx.r29.u32 & 0xFFFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cd02c
	if (ctx.cr6.eq) goto loc_822CD02C;
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c7f78
	ctx.lr = 0x822CD020;
	sub_822C7F78(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cd084
	if (ctx.cr6.lt) goto loc_822CD084;
	// lhz r29,80(r1)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
loc_822CD02C:
	// lwz r11,324(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 324);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cd064
	if (ctx.cr6.eq) goto loc_822CD064;
	// clrlwi r11,r29,16
	ctx.r11.u64 = ctx.r29.u32 & 0xFFFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cd064
	if (ctx.cr6.eq) goto loc_822CD064;
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c8140
	ctx.lr = 0x822CD058;
	sub_822C8140(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cd084
	if (ctx.cr6.lt) goto loc_822CD084;
	// lhz r29,80(r1)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
loc_822CD064:
	// lwz r11,356(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 356);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822cd088
	if (!ctx.cr6.eq) goto loc_822CD088;
	// clrlwi r5,r29,16
	ctx.r5.u64 = ctx.r29.u32 & 0xFFFF;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cc560
	ctx.lr = 0x822CD080;
	sub_822CC560(ctx, base);
	// b 0x822cd088
	goto loc_822CD088;
loc_822CD084:
	// lhz r29,80(r1)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
loc_822CD088:
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// beq cr6,0x822cd0a4
	if (ctx.cr6.eq) goto loc_822CD0A4;
	// sth r29,0(r27)
	PPC_STORE_U16(ctx.r27.u32 + 0, ctx.r29.u16);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822CD09C:
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
loc_822CD0A4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CD0AC"))) PPC_WEAK_FUNC(sub_822CD0AC);
PPC_FUNC_IMPL(__imp__sub_822CD0AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CD0B0"))) PPC_WEAK_FUNC(sub_822CD0B0);
PPC_FUNC_IMPL(__imp__sub_822CD0B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e444
	ctx.lr = 0x822CD0B8;
	__restfpr_19(ctx, base);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r26,0
	ctx.r26.s64 = 0;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r20,r5
	ctx.r20.u64 = ctx.r5.u64;
	// mr r27,r6
	ctx.r27.u64 = ctx.r6.u64;
	// mr r24,r7
	ctx.r24.u64 = ctx.r7.u64;
	// mr r23,r8
	ctx.r23.u64 = ctx.r8.u64;
	// mr r21,r9
	ctx.r21.u64 = ctx.r9.u64;
	// mr r19,r10
	ctx.r19.u64 = ctx.r10.u64;
	// mr r28,r7
	ctx.r28.u64 = ctx.r7.u64;
	// mr r11,r26
	ctx.r11.u64 = ctx.r26.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cd348
	if (ctx.cr6.eq) goto loc_822CD348;
	// lwz r29,0(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x822cd348
	if (ctx.cr6.eq) goto loc_822CD348;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x822cd348
	if (ctx.cr6.eq) goto loc_822CD348;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x822cd110
	if (ctx.cr6.eq) goto loc_822CD110;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x822cd348
	if (ctx.cr6.eq) goto loc_822CD348;
loc_822CD110:
	// lwz r10,692(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 692);
	// cmpwi cr6,r10,3
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 3, ctx.xer);
	// beq cr6,0x822cd128
	if (ctx.cr6.eq) goto loc_822CD128;
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,10
	ctx.r3.u64 = ctx.r3.u64 | 10;
	// b 0x822cd358
	goto loc_822CD358;
loc_822CD128:
	// lwz r11,292(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r22,700(r31)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r31.u32 + 700);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// sth r4,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r4.u16);
	// beq cr6,0x822cd15c
	if (ctx.cr6.eq) goto loc_822CD15C;
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// rlwinm r8,r9,0,22,22
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x200;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x822cd158
	if (ctx.cr6.eq) goto loc_822CD158;
	// stw r10,412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 412, ctx.r10.u32);
	// b 0x822cd15c
	goto loc_822CD15C;
loc_822CD158:
	// stw r26,412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 412, ctx.r26.u32);
loc_822CD15C:
	// lwz r9,416(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 416);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x822cd16c
	if (ctx.cr6.eq) goto loc_822CD16C;
	// stw r10,412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 412, ctx.r10.u32);
loc_822CD16C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cd180
	if (ctx.cr6.eq) goto loc_822CD180;
	// lhz r11,24(r11)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + 24);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// b 0x822cd184
	goto loc_822CD184;
loc_822CD180:
	// mr r11,r26
	ctx.r11.u64 = ctx.r26.u64;
loc_822CD184:
	// lwz r30,192(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// extsh r25,r11
	ctx.r25.s64 = ctx.r11.s16;
	// stw r26,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r26.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// lhz r11,34(r29)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r29.u32 + 34);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// rotlwi r5,r11,2
	ctx.r5.u64 = rotl32(ctx.r11.u32, 2);
	// bl 0x8233eaf0
	ctx.lr = 0x822CD1A4;
	sub_8233EAF0(ctx, base);
	// lhz r8,34(r29)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r29.u32 + 34);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// ble cr6,0x822cd1ec
	if (!ctx.cr6.gt) goto loc_822CD1EC;
	// mr r11,r26
	ctx.r11.u64 = ctx.r26.u64;
	// rlwinm r9,r26,2,0,29
	ctx.r9.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
loc_822CD1BC:
	// lwz r10,88(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 88);
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// add r7,r11,r27
	ctx.r7.u64 = ctx.r11.u64 + ctx.r27.u64;
	// extsh r6,r8
	ctx.r6.s64 = ctx.r8.s16;
	// stwx r7,r9,r30
	PPC_STORE_U32(ctx.r9.u32 + ctx.r30.u32, ctx.r7.u32);
	// lhz r8,34(r29)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r29.u32 + 34);
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r6,r8
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822cd1bc
	if (ctx.cr6.lt) goto loc_822CD1BC;
loc_822CD1EC:
	// lwz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x822cd218
	if (ctx.cr6.eq) goto loc_822CD218;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x822cd218
	if (ctx.cr6.eq) goto loc_822CD218;
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// beq cr6,0x822cd218
	if (ctx.cr6.eq) goto loc_822CD218;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// mr r11,r26
	ctx.r11.u64 = ctx.r26.u64;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// b 0x822cd358
	goto loc_822CD358;
loc_822CD218:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r9,336(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// lwz r7,452(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 452);
	// cmpw cr6,r7,r9
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r9.s32, ctx.xer);
	// beq cr6,0x822cd24c
	if (ctx.cr6.eq) goto loc_822CD24C;
	// lwz r11,328(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 328);
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,332(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 332);
	// mullw r7,r11,r24
	ctx.r7.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r24.s32);
	// divwu r11,r7,r9
	ctx.r11.u32 = ctx.r7.u32 / ctx.r9.u32;
	// cmplw cr6,r24,r11
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822cd24c
	if (ctx.cr6.lt) goto loc_822CD24C;
	// mr r28,r11
	ctx.r28.u64 = ctx.r11.u64;
loc_822CD24C:
	// lwz r11,424(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 424);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822cd268
	if (!ctx.cr6.eq) goto loc_822CD268;
	// mullw r11,r10,r28
	ctx.r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r28.s32);
	// li r10,6
	ctx.r10.s64 = 6;
	// divwu r11,r11,r10
	ctx.r11.u32 = ctx.r11.u32 / ctx.r10.u32;
	// b 0x822cd274
	goto loc_822CD274;
loc_822CD268:
	// lwz r11,360(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// mullw r10,r10,r28
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r28.s32);
	// divwu r11,r10,r11
	ctx.r11.u32 = ctx.r10.u32 / ctx.r11.u32;
loc_822CD274:
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822cd280
	if (ctx.cr6.lt) goto loc_822CD280;
	// mr r28,r11
	ctx.r28.u64 = ctx.r11.u64;
loc_822CD280:
	// mr r7,r28
	ctx.r7.u64 = ctx.r28.u64;
	// lwz r9,708(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 708);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822eb2f0
	ctx.lr = 0x822CD29C;
	sub_822EB2F0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cd354
	if (ctx.cr6.lt) goto loc_822CD354;
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
	// lhz r30,80(r1)
	ctx.r30.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// mr r6,r24
	ctx.r6.u64 = ctx.r24.u64;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822ccf08
	ctx.lr = 0x822CD2C0;
	sub_822CCF08(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cd354
	if (ctx.cr6.lt) goto loc_822CD354;
	// lhz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// cmplwi cr6,r23,0
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 0, ctx.xer);
	// beq cr6,0x822cd2ec
	if (ctx.cr6.eq) goto loc_822CD2EC;
	// lwz r10,368(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 368);
	// clrlwi r9,r11,16
	ctx.r9.u64 = ctx.r11.u32 & 0xFFFF;
	// lwz r8,360(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// mullw r7,r10,r8
	ctx.r7.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// mullw r6,r7,r9
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// stw r6,0(r23)
	PPC_STORE_U32(ctx.r23.u32 + 0, ctx.r6.u32);
loc_822CD2EC:
	// cmplwi cr6,r21,0
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 0, ctx.xer);
	// beq cr6,0x822cd314
	if (ctx.cr6.eq) goto loc_822CD314;
	// lis r10,152
	ctx.r10.s64 = 9961472;
	// ld r9,184(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 184);
	// lwz r8,336(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// ori r7,r10,38528
	ctx.r7.u64 = ctx.r10.u64 | 38528;
	// extsw r6,r8
	ctx.r6.s64 = ctx.r8.s32;
	// mulld r5,r9,r7
	ctx.r5.s64 = ctx.r9.s64 * ctx.r7.s64;
	// divd r4,r5,r6
	ctx.r4.s64 = ctx.r5.s64 / ctx.r6.s64;
	// std r4,0(r21)
	PPC_STORE_U64(ctx.r21.u32 + 0, ctx.r4.u64);
loc_822CD314:
	// clrlwi r8,r30,16
	ctx.r8.u64 = ctx.r30.u32 & 0xFFFF;
	// ld r9,184(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 184);
	// clrldi r10,r11,48
	ctx.r10.u64 = ctx.r11.u64 & 0xFFFF;
	// subf r7,r8,r22
	ctx.r7.s64 = ctx.r22.s64 - ctx.r8.s64;
	// add r6,r10,r9
	ctx.r6.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subfic r5,r7,0
	ctx.xer.ca = ctx.r7.u32 <= 0;
	ctx.r5.s64 = 0 - ctx.r7.s64;
	// stw r7,700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 700, ctx.r7.u32);
	// std r6,184(r31)
	PPC_STORE_U64(ctx.r31.u32 + 184, ctx.r6.u64);
	// subfe r4,r5,r5
	temp.u8 = (~ctx.r5.u32 + ctx.r5.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r5.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r4.u64 = ~ctx.r5.u64 + ctx.r5.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// stw r10,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r10.u32);
	// b 0x822cd358
	goto loc_822CD358;
loc_822CD348:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// b 0x822cd358
	goto loc_822CD358;
loc_822CD354:
	// lhz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
loc_822CD358:
	// cmplwi cr6,r20,0
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, 0, ctx.xer);
	// beq cr6,0x822cd368
	if (ctx.cr6.eq) goto loc_822CD368;
	// clrlwi r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	// stw r11,0(r20)
	PPC_STORE_U32(ctx.r20.u32 + 0, ctx.r11.u32);
loc_822CD368:
	// lwz r11,704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cd3a4
	if (ctx.cr6.eq) goto loc_822CD3A4;
	// lwz r11,696(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 696);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cd3a4
	if (ctx.cr6.eq) goto loc_822CD3A4;
	// lwz r11,692(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 692);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x822cd3a4
	if (!ctx.cr6.eq) goto loc_822CD3A4;
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// cntlzw r10,r11
	ctx.r10.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r11,r9,1
	ctx.r11.u64 = ctx.r9.u64 ^ 1;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// stw r8,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r8.u32);
loc_822CD3A4:
	// cmplwi cr6,r19,0
	ctx.cr6.compare<uint32_t>(ctx.r19.u32, 0, ctx.xer);
	// beq cr6,0x822cd3b4
	if (ctx.cr6.eq) goto loc_822CD3B4;
	// lwz r11,692(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 692);
	// stw r11,0(r19)
	PPC_STORE_U32(ctx.r19.u32 + 0, ctx.r11.u32);
loc_822CD3B4:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CD3BC"))) PPC_WEAK_FUNC(sub_822CD3BC);
PPC_FUNC_IMPL(__imp__sub_822CD3BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CD3C0"))) PPC_WEAK_FUNC(sub_822CD3C0);
PPC_FUNC_IMPL(__imp__sub_822CD3C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lwz r17,-30396(r24)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r24.u32 + -30396);
	// lwz r16,-13560(r6)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r6.u32 + -13560);
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r31,r1,-128
	ctx.r31.s64 = ctx.r1.s64 + -128;
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// nop 
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// lwz r11,220(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r30,212(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r30.u32);
	// bl 0x822cd0b0
	ctx.lr = 0x822CD400;
	sub_822CD0B0(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x822cd410
	// ERROR 822CD410
	return;
}

__attribute__((alias("__imp__sub_822CD3C8"))) PPC_WEAK_FUNC(sub_822CD3C8);
PPC_FUNC_IMPL(__imp__sub_822CD3C8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r31,r1,-128
	ctx.r31.s64 = ctx.r1.s64 + -128;
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// nop 
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// lwz r11,220(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r30,212(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r30.u32);
	// bl 0x822cd0b0
	ctx.lr = 0x822CD400;
	sub_822CD0B0(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x822cd410
	goto loc_822CD410;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
loc_822CD410:
	// addi r1,r31,128
	ctx.r1.s64 = ctx.r31.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CD408"))) PPC_WEAK_FUNC(sub_822CD408);
PPC_FUNC_IMPL(__imp__sub_822CD408) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// addi r1,r31,128
	ctx.r1.s64 = ctx.r31.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CD428"))) PPC_WEAK_FUNC(sub_822CD428);
PPC_FUNC_IMPL(__imp__sub_822CD428) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-16384
	ctx.r11.s64 = -1073741824;
	// ori r10,r11,5
	ctx.r10.u64 = ctx.r11.u64 | 5;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// subf r7,r8,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r8.s64;
	// cntlzw r6,r7
	ctx.r6.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// rlwinm r3,r6,27,31,31
	ctx.r3.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CD44C"))) PPC_WEAK_FUNC(sub_822CD44C);
PPC_FUNC_IMPL(__imp__sub_822CD44C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CD450"))) PPC_WEAK_FUNC(sub_822CD450);
PPC_FUNC_IMPL(__imp__sub_822CD450) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x822CD458;
	__restfpr_14(ctx, base);
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r24,0
	ctx.r24.s64 = 0;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// mr r23,r5
	ctx.r23.u64 = ctx.r5.u64;
	// mr r27,r6
	ctx.r27.u64 = ctx.r6.u64;
	// mr r18,r7
	ctx.r18.u64 = ctx.r7.u64;
	// mr r21,r8
	ctx.r21.u64 = ctx.r8.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// mr r26,r24
	ctx.r26.u64 = ctx.r24.u64;
	// beq cr6,0x822cdd78
	if (ctx.cr6.eq) goto loc_822CDD78;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822cdd78
	if (ctx.cr6.eq) goto loc_822CDD78;
	// lhz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// mr r19,r24
	ctx.r19.u64 = ctx.r24.u64;
	// addi r11,r11,-352
	ctx.r11.s64 = ctx.r11.s64 + -352;
	// cmplwi cr6,r11,7
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 7, ctx.xer);
	// bgt cr6,0x822cd83c
	if (ctx.cr6.gt) goto loc_822CD83C;
	// li r20,1
	ctx.r20.s64 = 1;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bdzf 4*cr6+eq,0x822cd540
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822CD540;
	// bdzf 4*cr6+eq,0x822cd548
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822CD548;
	// bdzf 4*cr6+eq,0x822cd550
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822CD550;
	// bdzf 4*cr6+eq,0x822cd83c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822CD83C;
	// bdzf 4*cr6+eq,0x822cd55c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822CD55C;
	// bdzf 4*cr6+eq,0x822cd56c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822CD56C;
	// bne cr6,0x822cd57c
	if (!ctx.cr6.eq) goto loc_822CD57C;
	// mr r30,r20
	ctx.r30.u64 = ctx.r20.u64;
loc_822CD4D0:
	// mr r6,r21
	ctx.r6.u64 = ctx.r21.u64;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x822c6a68
	ctx.lr = 0x822CD4E4;
	sub_822C6A68(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c6798
	ctx.lr = 0x822CD4F8;
	sub_822C6798(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
	// bl 0x822ea7f8
	ctx.lr = 0x822CD508;
	sub_822EA7F8(ctx, base);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// stw r3,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
	// lis r11,-32209
	ctx.r11.s64 = -2110849024;
	// lis r10,-32212
	ctx.r10.s64 = -2111045632;
	// addi r9,r11,32712
	ctx.r9.s64 = ctx.r11.s64 + 32712;
	// addi r8,r10,28840
	ctx.r8.s64 = ctx.r10.s64 + 28840;
	// stw r9,484(r3)
	PPC_STORE_U32(ctx.r3.u32 + 484, ctx.r9.u32);
	// stw r8,712(r31)
	PPC_STORE_U32(ctx.r31.u32 + 712, ctx.r8.u32);
loc_822CD530:
	// cmplwi cr6,r21,0
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 0, ctx.xer);
	// beq cr6,0x822cd590
	if (ctx.cr6.eq) goto loc_822CD590;
	// lwz r11,12(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 12);
	// b 0x822cd594
	goto loc_822CD594;
loc_822CD540:
	// li r30,2
	ctx.r30.s64 = 2;
	// b 0x822cd4d0
	goto loc_822CD4D0;
loc_822CD548:
	// li r30,3
	ctx.r30.s64 = 3;
	// b 0x822cd4d0
	goto loc_822CD4D0;
loc_822CD550:
	// li r30,3
	ctx.r30.s64 = 3;
	// mr r19,r20
	ctx.r19.u64 = ctx.r20.u64;
	// b 0x822cd4d0
	goto loc_822CD4D0;
loc_822CD55C:
	// li r20,1
	ctx.r20.s64 = 1;
	// li r30,2
	ctx.r30.s64 = 2;
	// mr r26,r20
	ctx.r26.u64 = ctx.r20.u64;
	// b 0x822cd530
	goto loc_822CD530;
loc_822CD56C:
	// li r20,1
	ctx.r20.s64 = 1;
	// li r30,3
	ctx.r30.s64 = 3;
	// mr r26,r20
	ctx.r26.u64 = ctx.r20.u64;
	// b 0x822cd530
	goto loc_822CD530;
loc_822CD57C:
	// li r20,1
	ctx.r20.s64 = 1;
	// li r30,3
	ctx.r30.s64 = 3;
	// mr r26,r20
	ctx.r26.u64 = ctx.r20.u64;
	// mr r19,r20
	ctx.r19.u64 = ctx.r20.u64;
	// b 0x822cd530
	goto loc_822CD530;
loc_822CD590:
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
loc_822CD594:
	// stw r11,704(r31)
	PPC_STORE_U32(ctx.r31.u32 + 704, ctx.r11.u32);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// stw r11,708(r31)
	PPC_STORE_U32(ctx.r31.u32 + 708, ctx.r11.u32);
	// lhz r6,20(r25)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r25.u32 + 20);
	// lwz r3,4(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + 4);
	// lwz r11,8(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8);
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x822f7ad8
	ctx.lr = 0x822CD5B4;
	sub_822F7AD8(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// ble cr6,0x822cd83c
	if (!ctx.cr6.gt) goto loc_822CD83C;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// addi r11,r31,476
	ctx.r11.s64 = ctx.r31.s64 + 476;
	// beq cr6,0x822cd5ec
	if (ctx.cr6.eq) goto loc_822CD5EC;
	// li r8,7
	ctx.r8.s64 = 7;
	// addi r10,r27,-4
	ctx.r10.s64 = ctx.r27.s64 + -4;
	// addi r9,r11,-4
	ctx.r9.s64 = ctx.r11.s64 + -4;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822CD5DC:
	// lwzu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r8.u64 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	// stwu r8,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r9.u32 = ea;
	// bdnz 0x822cd5dc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CD5DC;
	// b 0x822cd604
	goto loc_822CD604;
loc_822CD5EC:
	// li r9,7
	ctx.r9.s64 = 7;
	// mr r8,r24
	ctx.r8.u64 = ctx.r24.u64;
	// addi r10,r11,-4
	ctx.r10.s64 = ctx.r11.s64 + -4;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822CD5FC:
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822cd5fc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CD5FC;
loc_822CD604:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// lwz r27,480(r31)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r31.u32 + 480);
	// rlwinm r8,r10,0,22,22
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x200;
	// mr r22,r10
	ctx.r22.u64 = ctx.r10.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x822cd624
	if (ctx.cr6.eq) goto loc_822CD624;
	// stw r20,412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 412, ctx.r20.u32);
loc_822CD624:
	// lhz r8,500(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 500);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x822cd644
	if (ctx.cr6.eq) goto loc_822CD644;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// ori r8,r10,128
	ctx.r8.u64 = ctx.r10.u64 | 128;
	// ori r22,r9,128
	ctx.r22.u64 = ctx.r9.u64 | 128;
	// sth r8,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r8.u16);
loc_822CD644:
	// lhz r10,14(r25)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r25.u32 + 14);
	// lwz r9,16(r23)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r23.u32 + 16);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// addi r8,r8,7
	ctx.r8.s64 = ctx.r8.s64 + 7;
	// rlwinm r8,r8,29,3,31
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 29) & 0x1FFFFFFF;
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// bgt cr6,0x822cd664
	if (ctx.cr6.gt) goto loc_822CD664;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
loc_822CD664:
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// lwz r29,20(r23)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r23.u32 + 20);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r17,0(r23)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r23.u32 + 0);
	// lhz r16,22(r25)
	ctx.r16.u64 = PPC_LOAD_U16(ctx.r25.u32 + 22);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lhz r15,20(r25)
	ctx.r15.u64 = PPC_LOAD_U16(ctx.r25.u32 + 20);
	// lhz r26,12(r25)
	ctx.r26.u64 = PPC_LOAD_U16(ctx.r25.u32 + 12);
	// lwz r30,8(r25)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8);
	// lwz r14,16(r25)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16);
	// lhz r8,2(r25)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r25.u32 + 2);
	// lwz r7,4(r25)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r25.u32 + 4);
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r11.u32);
	// stw r29,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r29.u32);
	// stw r17,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r17.u32);
	// sth r16,118(r1)
	PPC_STORE_U16(ctx.r1.u32 + 118, ctx.r16.u16);
	// sth r15,110(r1)
	PPC_STORE_U16(ctx.r1.u32 + 110, ctx.r15.u16);
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r26.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r30.u32);
	// stw r14,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r14.u32);
	// bl 0x822eb240
	ctx.lr = 0x822CD6B8;
	sub_822EB240(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c6438
	ctx.lr = 0x822CD6CC;
	sub_822C6438(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,32
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 32, ctx.xer);
	// bgt cr6,0x822cd83c
	if (ctx.cr6.gt) goto loc_822CD83C;
	// lhz r11,34(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// mulli r3,r11,1776
	ctx.r3.s64 = ctx.r11.s64 * 1776;
	// bl 0x822e8aa0
	ctx.lr = 0x822CD6E4;
	sub_822E8AA0(ctx, base);
	// stw r3,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
	// lhz r11,34(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// mulli r5,r11,1776
	ctx.r5.s64 = ctx.r11.s64 * 1776;
	// bl 0x8233eaf0
	ctx.lr = 0x822CD700;
	sub_8233EAF0(ctx, base);
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x822e9128
	ctx.lr = 0x822CD70C;
	sub_822E9128(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// bl 0x822c68a8
	ctx.lr = 0x822CD724;
	sub_822C68A8(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// bl 0x822e96d0
	ctx.lr = 0x822CD73C;
	sub_822E96D0(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,320(r28)
	PPC_STORE_U32(ctx.r28.u32 + 320, ctx.r11.u32);
	// bl 0x822caf70
	ctx.lr = 0x822CD758;
	sub_822CAF70(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
	// lwz r11,588(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 588);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cd78c
	if (ctx.cr6.eq) goto loc_822CD78C;
	// lis r11,-32209
	ctx.r11.s64 = -2110849024;
	// stw r24,516(r31)
	PPC_STORE_U32(ctx.r31.u32 + 516, ctx.r24.u32);
	// lis r10,-32209
	ctx.r10.s64 = -2110849024;
	// addi r9,r11,31408
	ctx.r9.s64 = ctx.r11.s64 + 31408;
	// addi r8,r10,31152
	ctx.r8.s64 = ctx.r10.s64 + 31152;
	// stw r9,512(r31)
	PPC_STORE_U32(ctx.r31.u32 + 512, ctx.r9.u32);
	// stw r8,484(r28)
	PPC_STORE_U32(ctx.r28.u32 + 484, ctx.r8.u32);
loc_822CD78C:
	// stw r24,420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 420, ctx.r24.u32);
	// stw r24,352(r31)
	PPC_STORE_U32(ctx.r31.u32 + 352, ctx.r24.u32);
	// lwz r11,4(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 4);
	// stw r11,360(r31)
	PPC_STORE_U32(ctx.r31.u32 + 360, ctx.r11.u32);
	// lwz r7,360(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// lwz r11,8(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8);
	// stw r11,364(r31)
	PPC_STORE_U32(ctx.r31.u32 + 364, ctx.r11.u32);
	// lhz r10,34(r28)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// cmpw cr6,r10,r7
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, ctx.xer);
	// bne cr6,0x822cd7c0
	if (!ctx.cr6.eq) goto loc_822CD7C0;
	// lwz r10,104(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 104);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x822cd7d0
	if (ctx.cr6.eq) goto loc_822CD7D0;
loc_822CD7C0:
	// lwz r11,60(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x822cd83c
	if (!ctx.cr6.gt) goto loc_822CD83C;
	// stw r20,352(r31)
	PPC_STORE_U32(ctx.r31.u32 + 352, ctx.r20.u32);
loc_822CD7D0:
	// lwz r11,352(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 352);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822cd8f4
	if (!ctx.cr6.eq) goto loc_822CD8F4;
	// rlwinm r11,r22,0,23,23
	ctx.r11.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 0) & 0x100;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cd7ec
	if (ctx.cr6.eq) goto loc_822CD7EC;
	// stw r20,420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 420, ctx.r20.u32);
loc_822CD7EC:
	// lwz r11,420(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 420);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822cd80c
	if (!ctx.cr6.eq) goto loc_822CD80C;
	// cmpwi cr6,r7,2
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 2, ctx.xer);
	// bne cr6,0x822cd80c
	if (!ctx.cr6.eq) goto loc_822CD80C;
	// lhz r11,34(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bgt cr6,0x822cd810
	if (ctx.cr6.gt) goto loc_822CD810;
loc_822CD80C:
	// stw r24,420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 420, ctx.r24.u32);
loc_822CD810:
	// lwz r11,420(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 420);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822cd84c
	if (!ctx.cr6.eq) goto loc_822CD84C;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// lwz r8,364(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 364);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r6,104(r28)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r28.u32 + 104);
	// lhz r5,34(r28)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// bl 0x822cc1c0
	ctx.lr = 0x822CD834;
	sub_822CC1C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x822cd8f4
	if (ctx.cr6.eq) goto loc_822CD8F4;
loc_822CD83C:
	// lis r29,-32764
	ctx.r29.s64 = -2147221504;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
loc_822CD84C:
	// stw r24,424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 424, ctx.r24.u32);
	// lhz r5,34(r28)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// cmplwi cr6,r5,6
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 6, ctx.xer);
	// bne cr6,0x822cd868
	if (!ctx.cr6.eq) goto loc_822CD868;
	// lwz r11,104(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 104);
	// cmplwi cr6,r11,63
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 63, ctx.xer);
	// beq cr6,0x822cd890
	if (ctx.cr6.eq) goto loc_822CD890;
loc_822CD868:
	// li r8,63
	ctx.r8.s64 = 63;
	// lwz r6,104(r28)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r28.u32 + 104);
	// li r7,6
	ctx.r7.s64 = 6;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cc1c0
	ctx.lr = 0x822CD880;
	sub_822CC1C0(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
	// stw r20,424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 424, ctx.r20.u32);
loc_822CD890:
	// li r3,168
	ctx.r3.s64 = 168;
	// bl 0x822e8aa0
	ctx.lr = 0x822CD898;
	sub_822E8AA0(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,428(r31)
	PPC_STORE_U32(ctx.r31.u32 + 428, ctx.r3.u32);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
	// bl 0x822eb798
	ctx.lr = 0x822CD8A8;
	sub_822EB798(ctx, base);
	// li r11,1000
	ctx.r11.s64 = 1000;
	// li r7,1000
	ctx.r7.s64 = 1000;
	// lwz r3,428(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 428);
	// stw r11,432(r31)
	PPC_STORE_U32(ctx.r31.u32 + 432, ctx.r11.u32);
	// li r4,40
	ctx.r4.s64 = 40;
	// lwz r5,452(r28)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r28.u32 + 452);
	// lhz r11,110(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 110);
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// slw r11,r20,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r20.u32 << (ctx.r10.u8 & 0x3F));
	// addi r9,r11,-1
	ctx.r9.s64 = ctx.r11.s64 + -1;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// std r8,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r8.u64);
	// lfd f0,144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f1,f13
	ctx.f1.f64 = double(float(ctx.f13.f64));
	// bl 0x822eb8a8
	ctx.lr = 0x822CD8E8;
	sub_822EB8A8(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
loc_822CD8F4:
	// stw r24,448(r31)
	PPC_STORE_U32(ctx.r31.u32 + 448, ctx.r24.u32);
	// stw r24,468(r31)
	PPC_STORE_U32(ctx.r31.u32 + 468, ctx.r24.u32);
	// stw r24,464(r31)
	PPC_STORE_U32(ctx.r31.u32 + 464, ctx.r24.u32);
	// stw r24,460(r31)
	PPC_STORE_U32(ctx.r31.u32 + 460, ctx.r24.u32);
	// stw r24,440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 440, ctx.r24.u32);
	// lwz r11,60(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 60);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x822cd968
	if (!ctx.cr6.gt) goto loc_822CD968;
	// lhz r11,34(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// mullw r10,r11,r11
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r11.s32);
	// rlwinm r3,r10,2,0,29
	ctx.r3.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x822e8aa0
	ctx.lr = 0x822CD924;
	sub_822E8AA0(ctx, base);
	// stw r3,448(r31)
	PPC_STORE_U32(ctx.r31.u32 + 448, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
	// lhz r11,34(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// mullw r10,r11,r11
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r11.s32);
	// rlwinm r3,r10,2,0,29
	ctx.r3.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x822e8aa0
	ctx.lr = 0x822CD940;
	sub_822E8AA0(ctx, base);
	// stw r3,464(r31)
	PPC_STORE_U32(ctx.r31.u32 + 464, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
	// lhz r11,34(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// mullw r10,r11,r11
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r11.s32);
	// rlwinm r3,r10,2,0,29
	ctx.r3.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x822e8aa0
	ctx.lr = 0x822CD95C;
	sub_822E8AA0(ctx, base);
	// stw r3,468(r31)
	PPC_STORE_U32(ctx.r31.u32 + 468, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
loc_822CD968:
	// lhz r11,34(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// lwz r10,360(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bgt cr6,0x822cd97c
	if (ctx.cr6.gt) goto loc_822CD97C;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822CD97C:
	// lwz r10,424(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 424);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x822cd994
	if (ctx.cr6.eq) goto loc_822CD994;
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bgt cr6,0x822cd994
	if (ctx.cr6.gt) goto loc_822CD994;
	// li r11,6
	ctx.r11.s64 = 6;
loc_822CD994:
	// rlwinm r30,r11,2,0,29
	ctx.r30.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x822e8aa0
	ctx.lr = 0x822CD9A0;
	sub_822E8AA0(ctx, base);
	// stw r3,380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 380, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
	// stw r3,384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 384, ctx.r3.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x822e8aa0
	ctx.lr = 0x822CD9B8;
	sub_822E8AA0(ctx, base);
	// stw r3,388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 388, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
	// stw r3,392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 392, ctx.r3.u32);
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c6c90
	ctx.lr = 0x822CD9D4;
	sub_822C6C90(ctx, base);
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 0);
	// stw r24,316(r31)
	PPC_STORE_U32(ctx.r31.u32 + 316, ctx.r24.u32);
	// rotlwi r5,r11,0
	ctx.r5.u64 = rotl32(ctx.r11.u32, 0);
	// stw r24,324(r31)
	PPC_STORE_U32(ctx.r31.u32 + 324, ctx.r24.u32);
	// stw r24,320(r31)
	PPC_STORE_U32(ctx.r31.u32 + 320, ctx.r24.u32);
	// stw r11,336(r31)
	PPC_STORE_U32(ctx.r31.u32 + 336, ctx.r11.u32);
	// lwz r11,452(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 452);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r5,r10
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r10.s32, ctx.xer);
	// bne cr6,0x822cda04
	if (!ctx.cr6.eq) goto loc_822CDA04;
	// stw r20,316(r31)
	PPC_STORE_U32(ctx.r31.u32 + 316, ctx.r20.u32);
	// b 0x822cda20
	goto loc_822CDA20;
loc_822CDA04:
	// cmpw cr6,r5,r11
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x822cda20
	if (ctx.cr6.eq) goto loc_822CDA20;
	// stw r20,324(r31)
	PPC_STORE_U32(ctx.r31.u32 + 324, ctx.r20.u32);
	// lwz r11,452(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 452);
	// cmpw cr6,r5,r11
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x822cda20
	if (!ctx.cr6.lt) goto loc_822CDA20;
	// stw r20,320(r31)
	PPC_STORE_U32(ctx.r31.u32 + 320, ctx.r20.u32);
loc_822CDA20:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,452(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 452);
	// bl 0x822c7ed0
	ctx.lr = 0x822CDA2C;
	sub_822C7ED0(ctx, base);
	// lwz r11,332(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 332);
	// lwz r10,328(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 328);
	// cmpwi cr6,r10,10000
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 10000, ctx.xer);
	// stw r11,340(r31)
	PPC_STORE_U32(ctx.r31.u32 + 340, ctx.r11.u32);
	// bge cr6,0x822cd83c
	if (!ctx.cr6.lt) goto loc_822CD83C;
	// cmpwi cr6,r11,10000
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 10000, ctx.xer);
	// bge cr6,0x822cd83c
	if (!ctx.cr6.lt) goto loc_822CD83C;
	// lwz r11,316(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 316);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822cda60
	if (!ctx.cr6.eq) goto loc_822CDA60;
	// lwz r11,324(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 324);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cdac0
	if (ctx.cr6.eq) goto loc_822CDAC0;
loc_822CDA60:
	// lwz r11,360(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x822e8aa0
	ctx.lr = 0x822CDA6C;
	sub_822E8AA0(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,344(r31)
	PPC_STORE_U32(ctx.r31.u32 + 344, ctx.r3.u32);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
	// lwz r11,360(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// mr r10,r24
	ctx.r10.u64 = ctx.r24.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x822cdaa8
	if (!ctx.cr6.gt) goto loc_822CDAA8;
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
loc_822CDA8C:
	// lwz r9,344(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 344);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r24,r11,r9
	PPC_STORE_U32(ctx.r11.u32 + ctx.r9.u32, ctx.r24.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lwz r8,360(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// cmpw cr6,r10,r8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822cda8c
	if (ctx.cr6.lt) goto loc_822CDA8C;
loc_822CDAA8:
	// lwz r11,360(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x822e8aa0
	ctx.lr = 0x822CDAB4;
	sub_822E8AA0(ctx, base);
	// stw r3,348(r31)
	PPC_STORE_U32(ctx.r31.u32 + 348, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
loc_822CDAC0:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// rlwinm r10,r22,0,24,24
	ctx.r10.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 0) & 0x80;
	// addi r9,r11,-12736
	ctx.r9.s64 = ctx.r11.s64 + -12736;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stw r9,492(r28)
	PPC_STORE_U32(ctx.r28.u32 + 492, ctx.r9.u32);
	// stw r24,472(r31)
	PPC_STORE_U32(ctx.r31.u32 + 472, ctx.r24.u32);
	// beq cr6,0x822cdaf8
	if (ctx.cr6.eq) goto loc_822CDAF8;
	// lhz r11,500(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 500);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// extsh r4,r11
	ctx.r4.s64 = ctx.r11.s16;
	// bl 0x822cca28
	ctx.lr = 0x822CDAEC;
	sub_822CCA28(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
loc_822CDAF8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822c6d08
	ctx.lr = 0x822CDB00;
	sub_822C6D08(ctx, base);
	// lhz r11,2(r25)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r25.u32 + 2);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x822cdb18
	if (!ctx.cr6.eq) goto loc_822CDB18;
	// lis r11,-32209
	ctx.r11.s64 = -2110849024;
	// addi r10,r11,22624
	ctx.r10.s64 = ctx.r11.s64 + 22624;
	// b 0x822cdb20
	goto loc_822CDB20;
loc_822CDB18:
	// lis r11,-32209
	ctx.r11.s64 = -2110849024;
	// addi r10,r11,22752
	ctx.r10.s64 = ctx.r11.s64 + 22752;
loc_822CDB20:
	// stw r10,508(r31)
	PPC_STORE_U32(ctx.r31.u32 + 508, ctx.r10.u32);
	// addi r25,r31,224
	ctx.r25.s64 = ctx.r31.s64 + 224;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x822edbb0
	ctx.lr = 0x822CDB34;
	sub_822EDBB0(ctx, base);
	// cmplwi cr6,r21,0
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 0, ctx.xer);
	// beq cr6,0x822cdb4c
	if (ctx.cr6.eq) goto loc_822CDB4C;
	// lwz r11,4(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 4);
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// lwz r10,8(r21)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r21.u32 + 8);
	// stw r10,228(r31)
	PPC_STORE_U32(ctx.r31.u32 + 228, ctx.r10.u32);
loc_822CDB4C:
	// lwz r11,288(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 288);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// bne cr6,0x822cdb84
	if (!ctx.cr6.eq) goto loc_822CDB84;
	// lwz r11,320(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 320);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// addi r8,r10,12000
	ctx.r8.s64 = ctx.r10.s64 + 12000;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// addi r6,r9,30480
	ctx.r6.s64 = ctx.r9.s64 + 30480;
	// stw r8,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r8.u32);
	// addi r5,r7,31432
	ctx.r5.s64 = ctx.r7.s64 + 31432;
	// lwz r4,320(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 320);
	// stw r6,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r6.u32);
	// b 0x822cdbd8
	goto loc_822CDBD8;
loc_822CDB84:
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x822cdba8
	if (!ctx.cr6.eq) goto loc_822CDBA8;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// lis r6,-32254
	ctx.r6.s64 = -2113798144;
	// addi r8,r10,25144
	ctx.r8.s64 = ctx.r10.s64 + 25144;
	// addi r7,r9,-21776
	ctx.r7.s64 = ctx.r9.s64 + -21776;
	// addi r5,r6,-20448
	ctx.r5.s64 = ctx.r6.s64 + -20448;
	// b 0x822cdbc8
	goto loc_822CDBC8;
loc_822CDBA8:
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x822cdd78
	if (!ctx.cr6.eq) goto loc_822CDD78;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// lis r6,-32254
	ctx.r6.s64 = -2113798144;
	// addi r8,r10,13744
	ctx.r8.s64 = ctx.r10.s64 + 13744;
	// addi r7,r9,-31408
	ctx.r7.s64 = ctx.r9.s64 + -31408;
	// addi r5,r6,-28736
	ctx.r5.s64 = ctx.r6.s64 + -28736;
loc_822CDBC8:
	// lwz r11,320(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 320);
	// stw r8,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r8.u32);
	// lwz r4,320(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 320);
	// stw r7,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r7.u32);
loc_822CDBD8:
	// lwz r3,320(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 320);
	// li r11,-2
	ctx.r11.s64 = -2;
	// li r10,3
	ctx.r10.s64 = 3;
	// cmplwi cr6,r21,0
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 0, ctx.xer);
	// stw r5,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, ctx.r5.u32);
	// stw r11,4(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4, ctx.r11.u32);
	// stw r10,72(r28)
	PPC_STORE_U32(ctx.r28.u32 + 72, ctx.r10.u32);
	// beq cr6,0x822cdc00
	if (ctx.cr6.eq) goto loc_822CDC00;
	// bl 0x82364238
	ctx.lr = 0x822CDBFC;
	sub_82364238(ctx, base);
	// stw r3,0(r21)
	PPC_STORE_U32(ctx.r21.u32 + 0, ctx.r3.u32);
loc_822CDC00:
	// stw r24,116(r31)
	PPC_STORE_U32(ctx.r31.u32 + 116, ctx.r24.u32);
	// addi r27,r31,120
	ctx.r27.s64 = ctx.r31.s64 + 120;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// lhz r4,34(r28)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// bl 0x822ecfa0
	ctx.lr = 0x822CDC14;
	sub_822ECFA0(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822cdd80
	if (ctx.cr6.lt) goto loc_822CDD80;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r26,r24
	ctx.r26.u64 = ctx.r24.u64;
	// lhz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 34);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822cdca0
	if (ctx.cr6.eq) goto loc_822CDCA0;
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
loc_822CDC38:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// add r11,r30,r11
	ctx.r11.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r24,144(r11)
	PPC_STORE_U32(ctx.r11.u32 + 144, ctx.r24.u32);
	// lhz r10,34(r28)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// mullw r9,r10,r10
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r10.s32);
	// rlwinm r3,r9,2,0,29
	ctx.r3.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x822e8aa0
	ctx.lr = 0x822CDC54;
	sub_822E8AA0(ctx, base);
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// add r8,r30,r11
	ctx.r8.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r3,148(r8)
	PPC_STORE_U32(ctx.r8.u32 + 148, ctx.r3.u32);
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// add r7,r30,r11
	ctx.r7.u64 = ctx.r30.u64 + ctx.r11.u64;
	// lwz r3,148(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 148);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
	// lhz r11,34(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r10,r11,r11
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r11.s32);
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233eaf0
	ctx.lr = 0x822CDC88;
	sub_8233EAF0(ctx, base);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// addi r30,r30,152
	ctx.r30.s64 = ctx.r30.s64 + 152;
	// lhz r8,34(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 34);
	// cmpw cr6,r26,r8
	ctx.cr6.compare<int32_t>(ctx.r26.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822cdc38
	if (ctx.cr6.lt) goto loc_822CDC38;
loc_822CDCA0:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// lwz r10,60(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 60);
	// stw r24,572(r28)
	PPC_STORE_U32(ctx.r28.u32 + 572, ctx.r24.u32);
	// cmpwi cr6,r10,3
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 3, ctx.xer);
	// stw r11,576(r28)
	PPC_STORE_U32(ctx.r28.u32 + 576, ctx.r11.u32);
	// blt cr6,0x822cdcc8
	if (ctx.cr6.lt) goto loc_822CDCC8;
	// lwz r11,64(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 64);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x822cdcd0
	if (!ctx.cr6.eq) goto loc_822CDCD0;
loc_822CDCC8:
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// beq cr6,0x822cdcec
	if (ctx.cr6.eq) goto loc_822CDCEC;
loc_822CDCD0:
	// stw r20,176(r28)
	PPC_STORE_U32(ctx.r28.u32 + 176, ctx.r20.u32);
	// b 0x822cdcf0
	goto loc_822CDCF0;
loc_822CDCD8:
	// lis r29,-32761
	ctx.r29.s64 = -2147024896;
	// ori r29,r29,14
	ctx.r29.u64 = ctx.r29.u64 | 14;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
loc_822CDCEC:
	// stw r24,176(r28)
	PPC_STORE_U32(ctx.r28.u32 + 176, ctx.r24.u32);
loc_822CDCF0:
	// stw r24,124(r28)
	PPC_STORE_U32(ctx.r28.u32 + 124, ctx.r24.u32);
	// stw r20,732(r28)
	PPC_STORE_U32(ctx.r28.u32 + 732, ctx.r20.u32);
	// lhz r11,34(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// rotlwi r3,r11,2
	ctx.r3.u64 = rotl32(ctx.r11.u32, 2);
	// bl 0x822e8aa0
	ctx.lr = 0x822CDD04;
	sub_822E8AA0(ctx, base);
	// stw r3,192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 192, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cdcd8
	if (ctx.cr6.eq) goto loc_822CDCD8;
	// lhz r11,34(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// rotlwi r5,r11,2
	ctx.r5.u64 = rotl32(ctx.r11.u32, 2);
	// bl 0x8233eaf0
	ctx.lr = 0x822CDD20;
	sub_8233EAF0(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x8234f138
	ctx.lr = 0x822CDD30;
	sub_8234F138(ctx, base);
	// lwz r9,0(r25)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r20,696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 696, ctx.r20.u32);
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// stw r24,300(r31)
	PPC_STORE_U32(ctx.r31.u32 + 300, ctx.r24.u32);
	// rldicr r11,r10,63,63
	ctx.r11.u64 = rotl64(ctx.r10.u64, 63) & 0xFFFFFFFFFFFFFFFF;
	// stw r24,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r24.u32);
	// rlwinm r7,r8,27,31,31
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// sth r24,154(r31)
	PPC_STORE_U16(ctx.r31.u32 + 154, ctx.r24.u16);
	// std r11,168(r31)
	PPC_STORE_U64(ctx.r31.u32 + 168, ctx.r11.u64);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// std r11,176(r31)
	PPC_STORE_U64(ctx.r31.u32 + 176, ctx.r11.u64);
	// xori r11,r7,1
	ctx.r11.u64 = ctx.r7.u64 ^ 1;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 692, ctx.r11.u32);
	// stw r11,0(r18)
	PPC_STORE_U32(ctx.r18.u32 + 0, ctx.r11.u32);
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
loc_822CDD78:
	// lis r29,-32761
	ctx.r29.s64 = -2147024896;
	// ori r29,r29,87
	ctx.r29.u64 = ctx.r29.u64 | 87;
loc_822CDD80:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CDD8C"))) PPC_WEAK_FUNC(sub_822CDD8C);
PPC_FUNC_IMPL(__imp__sub_822CDD8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CDD90"))) PPC_WEAK_FUNC(sub_822CDD90);
PPC_FUNC_IMPL(__imp__sub_822CDD90) {
	PPC_FUNC_PROLOGUE();
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// beq cr6,0x822cddc0
	if (ctx.cr6.eq) goto loc_822CDDC0;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// addi r11,r11,-12048
	ctx.r11.s64 = ctx.r11.s64 + -12048;
	// beq cr6,0x822cddb4
	if (ctx.cr6.eq) goto loc_822CDDB4;
	// li r9,11
	ctx.r9.s64 = 11;
	// addi r8,r11,8768
	ctx.r8.s64 = ctx.r11.s64 + 8768;
	// b 0x822cddc8
	goto loc_822CDDC8;
loc_822CDDB4:
	// li r9,22
	ctx.r9.s64 = 22;
	// addi r8,r11,6104
	ctx.r8.s64 = ctx.r11.s64 + 6104;
	// b 0x822cddc8
	goto loc_822CDDC8;
loc_822CDDC0:
	// li r9,30
	ctx.r9.s64 = 30;
	// addi r8,r11,-12048
	ctx.r8.s64 = ctx.r11.s64 + -12048;
loc_822CDDC8:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x822cde28
	if (ctx.cr6.eq) goto loc_822CDE28;
	// mr r11,r8
	ctx.r11.u64 = ctx.r8.u64;
loc_822CDDD8:
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r5,r6
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r6.u32, ctx.xer);
	// bne cr6,0x822cddf0
	if (!ctx.cr6.eq) goto loc_822CDDF0;
	// lhz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r11.u32 + 4);
	// cmplw cr6,r4,r7
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r7.u32, ctx.xer);
	// beq cr6,0x822cde08
	if (ctx.cr6.eq) goto loc_822CDE08;
loc_822CDDF0:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// blt cr6,0x822cddd8
	if (ctx.cr6.lt) goto loc_822CDDD8;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_822CDE08:
	// rlwinm r11,r10,4,0,27
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cde28
	if (ctx.cr6.eq) goto loc_822CDE28;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
loc_822CDE28:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CDE30"))) PPC_WEAK_FUNC(sub_822CDE30);
PPC_FUNC_IMPL(__imp__sub_822CDE30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// addic r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	// li r11,1
	ctx.r11.s64 = 1;
	// subfe r31,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r31.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// li r10,0
	ctx.r10.s64 = 0;
	// and r11,r31,r11
	ctx.r11.u64 = ctx.r31.u64 & ctx.r11.u64;
	// cmplwi cr6,r8,3
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 3, ctx.xer);
	// bge cr6,0x822cdefc
	if (!ctx.cr6.lt) goto loc_822CDEFC;
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// li r3,1
	ctx.r3.s64 = 1;
	// beq cr6,0x822cde60
	if (ctx.cr6.eq) goto loc_822CDE60;
	// li r3,32
	ctx.r3.s64 = 32;
loc_822CDE60:
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// beq cr6,0x822cde70
	if (ctx.cr6.eq) goto loc_822CDE70;
	// clrlwi r11,r3,16
	ctx.r11.u64 = ctx.r3.u32 & 0xFFFF;
	// ori r3,r11,2
	ctx.r3.u64 = ctx.r11.u64 | 2;
loc_822CDE70:
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// beq cr6,0x822cdf90
	if (ctx.cr6.eq) goto loc_822CDF90;
	// clrlwi r11,r3,16
	ctx.r11.u64 = ctx.r3.u32 & 0xFFFF;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// ori r10,r11,4
	ctx.r10.u64 = ctx.r11.u64 | 4;
	// li r11,0
	ctx.r11.s64 = 0;
	// bne cr6,0x822cdec4
	if (!ctx.cr6.eq) goto loc_822CDEC4;
	// cmplwi cr6,r6,1
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 1, ctx.xer);
	// ble cr6,0x822cdea4
	if (!ctx.cr6.gt) goto loc_822CDEA4;
loc_822CDE94:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// srw r9,r6,r11
	ctx.r9.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r6.u32 >> (ctx.r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 1, ctx.xer);
	// bgt cr6,0x822cde94
	if (ctx.cr6.gt) goto loc_822CDE94;
loc_822CDEA4:
	// addis r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 65536;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// addi r11,r11,-3
	ctx.r11.s64 = ctx.r11.s64 + -3;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// or r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 | ctx.r10.u64;
	// clrlwi r3,r8,16
	ctx.r3.u64 = ctx.r8.u32 & 0xFFFF;
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_822CDEC4:
	// cmplwi cr6,r6,1
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 1, ctx.xer);
	// ble cr6,0x822cdedc
	if (!ctx.cr6.gt) goto loc_822CDEDC;
loc_822CDECC:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// srw r9,r6,r11
	ctx.r9.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r6.u32 >> (ctx.r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 1, ctx.xer);
	// bgt cr6,0x822cdecc
	if (ctx.cr6.gt) goto loc_822CDECC;
loc_822CDEDC:
	// addis r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 65536;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// or r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 | ctx.r10.u64;
	// clrlwi r3,r8,16
	ctx.r3.u64 = ctx.r8.u32 & 0xFFFF;
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_822CDEFC:
	// bne cr6,0x822cdf8c
	if (!ctx.cr6.eq) goto loc_822CDF8C;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x822cdf20
	if (ctx.cr6.eq) goto loc_822CDF20;
	// lwz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplwi cr6,r9,100
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 100, ctx.xer);
	// bne cr6,0x822cdf20
	if (!ctx.cr6.eq) goto loc_822CDF20;
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x822cdf40
	goto loc_822CDF40;
loc_822CDF20:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cdf2c
	if (ctx.cr6.eq) goto loc_822CDF2C;
	// li r10,64
	ctx.r10.s64 = 64;
loc_822CDF2C:
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cdf40
	if (ctx.cr6.eq) goto loc_822CDF40;
	// clrlwi r11,r10,16
	ctx.r11.u64 = ctx.r10.u32 & 0xFFFF;
	// ori r10,r11,128
	ctx.r10.u64 = ctx.r11.u64 | 128;
loc_822CDF40:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r6,1
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 1, ctx.xer);
	// ble cr6,0x822cdf5c
	if (!ctx.cr6.gt) goto loc_822CDF5C;
loc_822CDF4C:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// srw r9,r6,r11
	ctx.r9.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r6.u32 >> (ctx.r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 1, ctx.xer);
	// bgt cr6,0x822cdf4c
	if (ctx.cr6.gt) goto loc_822CDF4C;
loc_822CDF5C:
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// or r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 | ctx.r10.u64;
	// clrlwi r3,r8,16
	ctx.r3.u64 = ctx.r8.u32 & 0xFFFF;
	// clrlwi r7,r3,31
	ctx.r7.u64 = ctx.r3.u32 & 0x1;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822cdf90
	if (ctx.cr6.eq) goto loc_822CDF90;
	// clrlwi r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	// ori r3,r11,256
	ctx.r3.u64 = ctx.r11.u64 | 256;
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_822CDF8C:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
loc_822CDF90:
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CDF98"))) PPC_WEAK_FUNC(sub_822CDF98);
PPC_FUNC_IMPL(__imp__sub_822CDF98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// cmplwi cr6,r11,128
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 128, ctx.xer);
	// beq cr6,0x822cdfe0
	if (ctx.cr6.eq) goto loc_822CDFE0;
	// lwz r9,0(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x822cdfd8
	if (!ctx.cr6.eq) goto loc_822CDFD8;
	// lwz r9,36(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 36);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x822cdfe0
	if (!ctx.cr6.eq) goto loc_822CDFE0;
loc_822CDFD8:
	// li r4,1
	ctx.r4.s64 = 1;
	// b 0x822cdfe4
	goto loc_822CDFE4;
loc_822CDFE0:
	// li r4,0
	ctx.r4.s64 = 0;
loc_822CDFE4:
	// divwu r9,r11,r10
	ctx.r9.u32 = ctx.r11.u32 / ctx.r10.u32;
	// lwz r31,32(r7)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 32);
	// li r6,32000
	ctx.r6.s64 = 32000;
	// lwz r10,56(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 56);
	// addi r11,r11,-128
	ctx.r11.s64 = ctx.r11.s64 + -128;
	// lwz r30,16(r5)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// subfc r3,r6,r9
	ctx.xer.ca = ctx.r9.u32 >= ctx.r6.u32;
	ctx.r3.s64 = ctx.r9.s64 - ctx.r6.s64;
	// lwz r9,0(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r6,8(r5)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	// subfe r7,r3,r3
	temp.u8 = (~ctx.r3.u32 + ctx.r3.u32 < ~ctx.r3.u32) | (~ctx.r3.u32 + ctx.r3.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r7.u64 = ~ctx.r3.u64 + ctx.r3.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// lwz r3,12(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// addic r5,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r5.s64 = ctx.r11.s64 + -1;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r31.u32);
	// clrlwi r7,r7,31
	ctx.r7.u64 = ctx.r7.u32 & 0x1;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r30.u32);
	// subfe r5,r5,r11
	temp.u8 = (~ctx.r5.u32 + ctx.r11.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r5.u64 = ~ctx.r5.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// bl 0x822cde30
	ctx.lr = 0x822CE028;
	sub_822CDE30(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CE040"))) PPC_WEAK_FUNC(sub_822CE040);
PPC_FUNC_IMPL(__imp__sub_822CE040) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822CE048;
	__restfpr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// mr r29,r7
	ctx.r29.u64 = ctx.r7.u64;
	// cmplwi cr6,r5,128
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 128, ctx.xer);
	// bne cr6,0x822ce074
	if (!ctx.cr6.eq) goto loc_822CE074;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822CE074:
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x822f7ad8
	ctx.lr = 0x822CE088;
	sub_822F7AD8(ctx, base);
	// mullw r11,r3,r28
	ctx.r11.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r28.s32);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// clrldi r9,r31,32
	ctx.r9.u64 = ctx.r31.u64 & 0xFFFFFFFF;
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// mulld r9,r9,r11
	ctx.r9.s64 = ctx.r9.s64 * ctx.r11.s64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmplwi cr6,r29,2
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 2, ctx.xer);
	// addi r8,r9,-1
	ctx.r8.s64 = ctx.r9.s64 + -1;
	// divd r7,r8,r10
	ctx.r7.s64 = ctx.r8.s64 / ctx.r10.s64;
	// rotlwi r10,r7,0
	ctx.r10.u64 = rotl32(ctx.r7.u32, 0);
	// addi r6,r10,7
	ctx.r6.s64 = ctx.r10.s64 + 7;
	// rlwinm r9,r6,29,3,31
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFF;
	// bne cr6,0x822ce134
	if (!ctx.cr6.eq) goto loc_822CE134;
	// cmplwi cr6,r30,44100
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 44100, ctx.xer);
	// bne cr6,0x822ce134
	if (!ctx.cr6.eq) goto loc_822CE134;
	// cmplwi cr6,r27,2
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 2, ctx.xer);
	// bne cr6,0x822ce134
	if (!ctx.cr6.eq) goto loc_822CE134;
	// cmplwi cr6,r31,32000
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 32000, ctx.xer);
	// bne cr6,0x822ce134
	if (!ctx.cr6.eq) goto loc_822CE134;
	// lis r8,0
	ctx.r8.s64 = 0;
	// rlwinm r10,r9,3,0,28
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// ori r8,r8,44100
	ctx.r8.u64 = ctx.r8.u64 | 44100;
	// mulld r7,r10,r8
	ctx.r7.s64 = ctx.r10.s64 * ctx.r8.s64;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// divd r6,r7,r11
	ctx.r6.s64 = ctx.r7.s64 / ctx.r11.s64;
	// rotlwi r7,r6,0
	ctx.r7.u64 = rotl32(ctx.r6.u32, 0);
	// addi r5,r7,7
	ctx.r5.s64 = ctx.r7.s64 + 7;
	// rlwinm r4,r5,0,0,28
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFF8;
	// cmplwi cr6,r4,32000
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 32000, ctx.xer);
	// blt cr6,0x822ce134
	if (ctx.cr6.lt) goto loc_822CE134;
loc_822CE104:
	// addi r10,r10,-8
	ctx.r10.s64 = ctx.r10.s64 + -8;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// clrldi r7,r10,32
	ctx.r7.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// mulld r7,r7,r8
	ctx.r7.s64 = ctx.r7.s64 * ctx.r8.s64;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
	// addi r6,r7,-1
	ctx.r6.s64 = ctx.r7.s64 + -1;
	// divd r5,r6,r11
	ctx.r5.s64 = ctx.r6.s64 / ctx.r11.s64;
	// rotlwi r7,r5,0
	ctx.r7.u64 = rotl32(ctx.r5.u32, 0);
	// addi r4,r7,7
	ctx.r4.s64 = ctx.r7.s64 + 7;
	// rlwinm r3,r4,0,0,28
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFF8;
	// cmplwi cr6,r3,32000
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 32000, ctx.xer);
	// bge cr6,0x822ce104
	if (!ctx.cr6.lt) goto loc_822CE104;
loc_822CE134:
	// clrlwi r3,r9,16
	ctx.r3.u64 = ctx.r9.u32 & 0xFFFF;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CE140"))) PPC_WEAK_FUNC(sub_822CE140);
PPC_FUNC_IMPL(__imp__sub_822CE140) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e460
	ctx.lr = 0x822CE148;
	__restfpr_26(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// mr r26,r8
	ctx.r26.u64 = ctx.r8.u64;
	// bl 0x822f7ad8
	ctx.lr = 0x822CE174;
	sub_822F7AD8(ctx, base);
	// mullw r11,r3,r28
	ctx.r11.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r28.s32);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r8,r26
	ctx.r8.u64 = ctx.r26.u64;
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// clrldi r31,r11,32
	ctx.r31.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// bl 0x822ce040
	ctx.lr = 0x822CE198;
	sub_822CE040(ctx, base);
	// rlwinm r10,r3,3,13,28
	ctx.r10.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0x7FFF8;
	// clrldi r9,r30,32
	ctx.r9.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// mulld r11,r10,r9
	ctx.r11.s64 = ctx.r10.s64 * ctx.r9.s64;
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
	// addi r8,r11,-1
	ctx.r8.s64 = ctx.r11.s64 + -1;
	// divd r7,r8,r31
	ctx.r7.s64 = ctx.r8.s64 / ctx.r31.s64;
	// rotlwi r11,r7,0
	ctx.r11.u64 = rotl32(ctx.r7.u32, 0);
	// addi r6,r11,7
	ctx.r6.s64 = ctx.r11.s64 + 7;
	// rlwinm r3,r6,29,3,31
	ctx.r3.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFF;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CE1C4"))) PPC_WEAK_FUNC(sub_822CE1C4);
PPC_FUNC_IMPL(__imp__sub_822CE1C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CE1C8"))) PPC_WEAK_FUNC(sub_822CE1C8);
PPC_FUNC_IMPL(__imp__sub_822CE1C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822CE1D0;
	__restfpr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r4,0(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// li r11,32000
	ctx.r11.s64 = 32000;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// subfc r10,r11,r4
	ctx.xer.ca = ctx.r4.u32 >= ctx.r11.u32;
	ctx.r10.s64 = ctx.r4.s64 - ctx.r11.s64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// subfe r9,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// rlwinm r11,r9,0,0,30
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFE;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r29,r11,5
	ctx.r29.s64 = ctx.r11.s64 + 5;
	// bl 0x822f7ad8
	ctx.lr = 0x822CE204;
	sub_822F7AD8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822ce214
	if (!ctx.cr6.eq) goto loc_822CE214;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_822CE214:
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// bne cr6,0x822ce240
	if (!ctx.cr6.eq) goto loc_822CE240;
	// lbz r11,7(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 7);
	// mullw r10,r11,r3
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r3.s32);
	// divwu r9,r31,r10
	ctx.r9.u32 = ctx.r31.u32 / ctx.r10.u32;
	// cmplw cr6,r9,r29
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r29.u32, ctx.xer);
	// bge cr6,0x822ce240
	if (!ctx.cr6.lt) goto loc_822CE240;
	// mullw r11,r3,r29
	ctx.r11.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r29.s32);
	// divwu r3,r31,r11
	ctx.r3.u32 = ctx.r31.u32 / ctx.r11.u32;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_822CE240:
	// lbz r3,7(r30)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r30.u32 + 7);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CE24C"))) PPC_WEAK_FUNC(sub_822CE24C);
PPC_FUNC_IMPL(__imp__sub_822CE24C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CE250"))) PPC_WEAK_FUNC(sub_822CE250);
PPC_FUNC_IMPL(__imp__sub_822CE250) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e434
	ctx.lr = 0x822CE258;
	__restfpr_15(ctx, base);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,92(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 92);
	// mr r21,r5
	ctx.r21.u64 = ctx.r5.u64;
	// lwz r28,108(r5)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r5.u32 + 108);
	// mr r17,r3
	ctx.r17.u64 = ctx.r3.u64;
	// lwz r29,112(r5)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 112);
	// mr r16,r4
	ctx.r16.u64 = ctx.r4.u64;
	// lwz r19,4(r7)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// mr r26,r7
	ctx.r26.u64 = ctx.r7.u64;
	// lhz r31,2(r7)
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// li r23,2
	ctx.r23.s64 = 2;
	// lwz r24,28(r7)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// li r22,0
	ctx.r22.s64 = 0;
	// lwz r25,32(r7)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r7.u32 + 32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r20,36(r7)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r7.u32 + 36);
	// lwz r27,40(r7)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r7.u32 + 40);
	// lwz r5,24(r7)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r18,44(r7)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r7.u32 + 44);
	// bne cr6,0x822ce2bc
	if (!ctx.cr6.eq) goto loc_822CE2BC;
	// lwz r11,84(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 84);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ce2bc
	if (!ctx.cr6.eq) goto loc_822CE2BC;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x822ce920
	if (ctx.cr6.eq) goto loc_822CE920;
loc_822CE2BC:
	// li r15,8
	ctx.r15.s64 = 8;
	// cmplwi cr6,r17,2
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 2, ctx.xer);
	// bgt cr6,0x822ce324
	if (ctx.cr6.gt) goto loc_822CE324;
	// lhz r8,20(r26)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r26.u32 + 20);
	// rlwinm r10,r8,0,30,30
	ctx.r10.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x2;
	// mr r11,r8
	ctx.r11.u64 = ctx.r8.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822ce31c
	if (ctx.cr6.eq) goto loc_822CE31C;
	// rlwinm r10,r8,0,29,29
	ctx.r10.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822ce31c
	if (ctx.cr6.eq) goto loc_822CE31C;
	// mr r7,r17
	ctx.r7.u64 = ctx.r17.u64;
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// rlwinm r30,r8,29,30,31
	ctx.r30.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 29) & 0x3;
	// bl 0x822ce140
	ctx.lr = 0x822CE300;
	sub_822CE140(ctx, base);
	// divwu r11,r3,r31
	ctx.r11.u32 = ctx.r3.u32 / ctx.r31.u32;
	// cmplwi cr6,r11,4000
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4000, ctx.xer);
	// blt cr6,0x822ce314
	if (ctx.cr6.lt) goto loc_822CE314;
	// slw r31,r15,r30
	ctx.r31.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r15.u32 << (ctx.r30.u8 & 0x3F));
	// b 0x822ce378
	goto loc_822CE378;
loc_822CE314:
	// slw r31,r23,r30
	ctx.r31.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r23.u32 << (ctx.r30.u8 & 0x3F));
	// b 0x822ce378
	goto loc_822CE378;
loc_822CE31C:
	// li r31,1
	ctx.r31.s64 = 1;
	// b 0x822ce378
	goto loc_822CE378;
loc_822CE324:
	// lhz r11,20(r26)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r26.u32 + 20);
	// li r10,1
	ctx.r10.s64 = 1;
	// rlwinm r9,r11,29,29,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 29) & 0x7;
	// rlwinm r11,r11,0,29,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x6;
	// slw r31,r10,r9
	ctx.r31.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r9.u8 & 0x3F));
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bgt cr6,0x822cea88
	if (ctx.cr6.gt) goto loc_822CEA88;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ce378
	if (ctx.cr6.eq) goto loc_822CE378;
	// bdz 0x822cea88
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CEA88;
	// bdz 0x822ce364
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CE364;
	// bdz 0x822cea88
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CEA88;
	// bdz 0x822ce36c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CE36C;
	// bdz 0x822cea88
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CEA88;
	// b 0x822ce374
	goto loc_822CE374;
loc_822CE364:
	// li r23,1
	ctx.r23.s64 = 1;
	// b 0x822ce378
	goto loc_822CE378;
loc_822CE36C:
	// li r23,4
	ctx.r23.s64 = 4;
	// b 0x822ce378
	goto loc_822CE378;
loc_822CE374:
	// mr r23,r15
	ctx.r23.u64 = ctx.r15.u64;
loc_822CE378:
	// lis r11,-13108
	ctx.r11.s64 = -859045888;
	// lis r10,-21846
	ctx.r10.s64 = -1431699456;
	// ori r5,r11,52429
	ctx.r5.u64 = ctx.r11.u64 | 52429;
	// li r3,5
	ctx.r3.s64 = 5;
	// ori r6,r10,43691
	ctx.r6.u64 = ctx.r10.u64 | 43691;
	// li r4,3
	ctx.r4.s64 = 3;
	// cmplwi cr6,r17,3
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 3, ctx.xer);
	// bne cr6,0x822ce3f8
	if (!ctx.cr6.eq) goto loc_822CE3F8;
	// lwz r11,92(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 92);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ce3f8
	if (ctx.cr6.eq) goto loc_822CE3F8;
	// lwz r11,0(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ce3f8
	if (!ctx.cr6.eq) goto loc_822CE3F8;
	// lwz r11,96(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 96);
	// cmplwi cr6,r11,151
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 151, ctx.xer);
	// bge cr6,0x822ce3d8
	if (!ctx.cr6.lt) goto loc_822CE3D8;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// rlwinm r11,r24,30,2,31
	ctx.r11.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 30) & 0x3FFFFFFF;
	// bne cr6,0x822ce3ec
	if (!ctx.cr6.eq) goto loc_822CE3EC;
loc_822CE3C8:
	// cmplw cr6,r24,r11
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822ce5e8
	if (ctx.cr6.lt) goto loc_822CE5E8;
	// mr r29,r11
	ctx.r29.u64 = ctx.r11.u64;
	// b 0x822ce408
	goto loc_822CE408;
loc_822CE3D8:
	// cmplwi cr6,r11,251
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 251, ctx.xer);
	// bge cr6,0x822ce3f8
	if (!ctx.cr6.lt) goto loc_822CE3F8;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// rlwinm r11,r24,31,1,31
	ctx.r11.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 31) & 0x7FFFFFFF;
	// beq cr6,0x822ce3c8
	if (ctx.cr6.eq) goto loc_822CE3C8;
loc_822CE3EC:
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822ce3f8
	if (ctx.cr6.lt) goto loc_822CE3F8;
	// mr r29,r11
	ctx.r29.u64 = ctx.r11.u64;
loc_822CE3F8:
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x822ce5e8
	if (ctx.cr6.eq) goto loc_822CE5E8;
	// cmplwi cr6,r17,3
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 3, ctx.xer);
	// bne cr6,0x822ce5e8
	if (!ctx.cr6.eq) goto loc_822CE5E8;
loc_822CE408:
	// cmplw cr6,r29,r24
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r24.u32, ctx.xer);
	// beq cr6,0x822ce5e8
	if (ctx.cr6.eq) goto loc_822CE5E8;
	// rlwinm r11,r24,1,0,30
	ctx.r11.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r11,r29
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x822ce554
	if (!ctx.cr6.eq) goto loc_822CE554;
	// cmplwi cr6,r23,1
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 1, ctx.xer);
	// ble cr6,0x822ce554
	if (!ctx.cr6.gt) goto loc_822CE554;
	// lwz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	// li r22,1
	ctx.r22.s64 = 1;
	// rlwinm r23,r23,31,1,31
	ctx.r23.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r24,r11
	ctx.r24.u64 = ctx.r11.u64;
	// rlwinm r18,r18,31,1,31
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 31) & 0x7FFFFFFF;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x822ce44c
	if (!ctx.cr6.eq) goto loc_822CE44C;
	// lwz r11,36(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 36);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ce5e8
	if (!ctx.cr6.eq) goto loc_822CE5E8;
loc_822CE44C:
	// rlwinm r11,r27,31,1,31
	ctx.r11.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 31) & 0x7FFFFFFF;
	// li r27,1
	ctx.r27.s64 = 1;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// blt cr6,0x822ce460
	if (ctx.cr6.lt) goto loc_822CE460;
	// mr r27,r11
	ctx.r27.u64 = ctx.r11.u64;
loc_822CE460:
	// rlwinm r11,r20,31,1,31
	ctx.r11.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 31) & 0x7FFFFFFF;
	// li r20,1
	ctx.r20.s64 = 1;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// blt cr6,0x822ce474
	if (ctx.cr6.lt) goto loc_822CE474;
	// mr r20,r11
	ctx.r20.u64 = ctx.r11.u64;
loc_822CE474:
	// cmplwi cr6,r27,5
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 5, ctx.xer);
	// bgt cr6,0x822ce484
	if (ctx.cr6.gt) goto loc_822CE484;
	// mr r25,r27
	ctx.r25.u64 = ctx.r27.u64;
	// b 0x822ce5e8
	goto loc_822CE5E8;
loc_822CE484:
	// mulhwu r11,r27,r5
	ctx.r11.u64 = (uint64_t(ctx.r27.u32) * uint64_t(ctx.r5.u32)) >> 32;
	// rlwinm r11,r11,30,2,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// subf. r8,r10,r27
	ctx.r8.s64 = ctx.r27.s64 - ctx.r10.s64;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// bne 0x822ce4a4
	if (!ctx.cr0.eq) goto loc_822CE4A4;
	// divwu r25,r27,r3
	ctx.r25.u32 = ctx.r27.u32 / ctx.r3.u32;
	// b 0x822ce5e8
	goto loc_822CE5E8;
loc_822CE4A4:
	// cmplwi cr6,r27,4
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 4, ctx.xer);
	// ble cr6,0x822ce4c0
	if (!ctx.cr6.gt) goto loc_822CE4C0;
	// clrlwi r11,r27,30
	ctx.r11.u64 = ctx.r27.u32 & 0x3;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822ce4c0
	if (!ctx.cr6.eq) goto loc_822CE4C0;
	// rlwinm r25,r27,30,2,31
	ctx.r25.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 30) & 0x3FFFFFFF;
	// b 0x822ce5e8
	goto loc_822CE5E8;
loc_822CE4C0:
	// cmplwi cr6,r27,3
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 3, ctx.xer);
	// ble cr6,0x822ce4e8
	if (!ctx.cr6.gt) goto loc_822CE4E8;
	// mulhwu r11,r27,r6
	ctx.r11.u64 = (uint64_t(ctx.r27.u32) * uint64_t(ctx.r6.u32)) >> 32;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// subf. r9,r10,r27
	ctx.r9.s64 = ctx.r27.s64 - ctx.r10.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne 0x822ce4e8
	if (!ctx.cr0.eq) goto loc_822CE4E8;
	// divwu r25,r27,r4
	ctx.r25.u32 = ctx.r27.u32 / ctx.r4.u32;
	// b 0x822ce5e8
	goto loc_822CE5E8;
loc_822CE4E8:
	// mulhwu r11,r27,r6
	ctx.r11.u64 = (uint64_t(ctx.r27.u32) * uint64_t(ctx.r6.u32)) >> 32;
	// rlwinm r10,r11,31,1,31
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// clrlwi r11,r27,30
	ctx.r11.u64 = ctx.r27.u32 & 0x3;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r10,r27
	ctx.r10.s64 = ctx.r27.s64 - ctx.r10.s64;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822ce524
	if (ctx.cr6.lt) goto loc_822CE524;
	// cmplw cr6,r11,r8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r8.u32, ctx.xer);
	// bgt cr6,0x822ce51c
	if (ctx.cr6.gt) goto loc_822CE51C;
	// subf r27,r11,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r11.s64;
	// rlwinm r25,r27,30,2,31
	ctx.r25.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 30) & 0x3FFFFFFF;
	// b 0x822ce540
	goto loc_822CE540;
loc_822CE51C:
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x822ce538
	if (ctx.cr6.gt) goto loc_822CE538;
loc_822CE524:
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// bgt cr6,0x822ce538
	if (ctx.cr6.gt) goto loc_822CE538;
	// subf r27,r10,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r10.s64;
	// divwu r25,r27,r4
	ctx.r25.u32 = ctx.r27.u32 / ctx.r4.u32;
	// b 0x822ce540
	goto loc_822CE540;
loc_822CE538:
	// subf r27,r8,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r8.s64;
	// divwu r25,r27,r3
	ctx.r25.u32 = ctx.r27.u32 / ctx.r3.u32;
loc_822CE540:
	// addi r11,r27,1
	ctx.r11.s64 = ctx.r27.s64 + 1;
	// cmplw cr6,r20,r11
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822ce5e8
	if (ctx.cr6.lt) goto loc_822CE5E8;
	// mr r20,r11
	ctx.r20.u64 = ctx.r11.u64;
	// b 0x822ce5e8
	goto loc_822CE5E8;
loc_822CE554:
	// rlwinm r11,r29,1,0,30
	ctx.r11.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r24,r11
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x822ce5a0
	if (!ctx.cr6.eq) goto loc_822CE5A0;
	// cmplwi cr6,r23,8
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 8, ctx.xer);
	// bge cr6,0x822ce5a0
	if (!ctx.cr6.lt) goto loc_822CE5A0;
	// lwz r11,0(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	// li r22,1
	ctx.r22.s64 = 1;
	// rlwinm r23,r23,1,0,30
	ctx.r23.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r24,r24,31,1,31
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r18,r18,1,0,30
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ce590
	if (!ctx.cr6.eq) goto loc_822CE590;
	// lwz r11,36(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 36);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ce5e8
	if (!ctx.cr6.eq) goto loc_822CE5E8;
loc_822CE590:
	// rlwinm r27,r27,1,0,30
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r20,r20,1,0,30
	ctx.r20.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r25,1,0,30
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x822ce5e8
	goto loc_822CE5E8;
loc_822CE5A0:
	// rlwinm r11,r29,2,0,29
	ctx.r11.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplw cr6,r24,r11
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x822ce5e8
	if (!ctx.cr6.eq) goto loc_822CE5E8;
	// cmplwi cr6,r23,4
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 4, ctx.xer);
	// bge cr6,0x822ce5e8
	if (!ctx.cr6.lt) goto loc_822CE5E8;
	// lwz r11,0(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	// li r22,1
	ctx.r22.s64 = 1;
	// rlwinm r23,r23,2,0,29
	ctx.r23.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r24,r24,30,2,31
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ce5dc
	if (!ctx.cr6.eq) goto loc_822CE5DC;
	// lwz r11,36(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 36);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ce5e8
	if (!ctx.cr6.eq) goto loc_822CE5E8;
loc_822CE5DC:
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r20,r20,2,0,29
	ctx.r20.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
loc_822CE5E8:
	// lwz r11,92(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 92);
	// li r7,1000
	ctx.r7.s64 = 1000;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ce7c8
	if (ctx.cr6.eq) goto loc_822CE7C8;
	// lwz r11,96(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 96);
	// lwz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	// mullw r11,r11,r19
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r19.s32);
	// addi r9,r11,999
	ctx.r9.s64 = ctx.r11.s64 + 999;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// divwu r10,r9,r7
	ctx.r10.u32 = ctx.r9.u32 / ctx.r7.u32;
	// bne cr6,0x822ce760
	if (!ctx.cr6.eq) goto loc_822CE760;
loc_822CE614:
	// rlwinm r11,r24,1,0,30
	ctx.r11.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r24,r11
	ctx.r11.u64 = ctx.r24.u64 + ctx.r11.u64;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x822ce640
	if (!ctx.cr6.gt) goto loc_822CE640;
	// cmplwi cr6,r17,3
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 3, ctx.xer);
	// blt cr6,0x822cea88
	if (ctx.cr6.lt) goto loc_822CEA88;
	// rlwinm r23,r23,1,0,30
	ctx.r23.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r24,r24,31,1,31
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 31) & 0x7FFFFFFF;
	// li r22,1
	ctx.r22.s64 = 1;
	// cmplwi cr6,r23,16
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 16, ctx.xer);
	// blt cr6,0x822ce614
	if (ctx.cr6.lt) goto loc_822CE614;
loc_822CE640:
	// addi r11,r27,2
	ctx.r11.s64 = ctx.r27.s64 + 2;
	// mullw r9,r11,r24
	ctx.r9.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r24.s32);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// bge cr6,0x822ce7c8
	if (!ctx.cr6.lt) goto loc_822CE7C8;
	// divwu r11,r10,r24
	ctx.r11.u32 = ctx.r10.u32 / ctx.r24.u32;
	// li r22,1
	ctx.r22.s64 = 1;
	// clrlwi r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// ble cr6,0x822cea88
	if (!ctx.cr6.gt) goto loc_822CEA88;
	// addi r27,r11,-2
	ctx.r27.s64 = ctx.r11.s64 + -2;
	// cmplwi cr6,r27,1
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 1, ctx.xer);
	// blt cr6,0x822cea88
	if (ctx.cr6.lt) goto loc_822CEA88;
	// addi r11,r27,1
	ctx.r11.s64 = ctx.r27.s64 + 1;
	// cmplw cr6,r20,r11
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822ce680
	if (ctx.cr6.lt) goto loc_822CE680;
	// mr r20,r11
	ctx.r20.u64 = ctx.r11.u64;
loc_822CE680:
	// cmplwi cr6,r27,5
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 5, ctx.xer);
	// bgt cr6,0x822ce690
	if (ctx.cr6.gt) goto loc_822CE690;
	// mr r25,r27
	ctx.r25.u64 = ctx.r27.u64;
	// b 0x822ce7c8
	goto loc_822CE7C8;
loc_822CE690:
	// mulhwu r11,r27,r5
	ctx.r11.u64 = (uint64_t(ctx.r27.u32) * uint64_t(ctx.r5.u32)) >> 32;
	// rlwinm r11,r11,30,2,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// subf. r8,r10,r27
	ctx.r8.s64 = ctx.r27.s64 - ctx.r10.s64;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// bne 0x822ce6b0
	if (!ctx.cr0.eq) goto loc_822CE6B0;
	// divwu r25,r27,r3
	ctx.r25.u32 = ctx.r27.u32 / ctx.r3.u32;
	// b 0x822ce7c8
	goto loc_822CE7C8;
loc_822CE6B0:
	// cmplwi cr6,r27,4
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 4, ctx.xer);
	// ble cr6,0x822ce6cc
	if (!ctx.cr6.gt) goto loc_822CE6CC;
	// clrlwi r11,r27,30
	ctx.r11.u64 = ctx.r27.u32 & 0x3;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822ce6cc
	if (!ctx.cr6.eq) goto loc_822CE6CC;
	// rlwinm r25,r27,30,2,31
	ctx.r25.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 30) & 0x3FFFFFFF;
	// b 0x822ce7c8
	goto loc_822CE7C8;
loc_822CE6CC:
	// cmplwi cr6,r27,3
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 3, ctx.xer);
	// ble cr6,0x822ce6f4
	if (!ctx.cr6.gt) goto loc_822CE6F4;
	// mulhwu r11,r27,r6
	ctx.r11.u64 = (uint64_t(ctx.r27.u32) * uint64_t(ctx.r6.u32)) >> 32;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// subf. r9,r10,r27
	ctx.r9.s64 = ctx.r27.s64 - ctx.r10.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne 0x822ce6f4
	if (!ctx.cr0.eq) goto loc_822CE6F4;
	// divwu r25,r27,r4
	ctx.r25.u32 = ctx.r27.u32 / ctx.r4.u32;
	// b 0x822ce7c8
	goto loc_822CE7C8;
loc_822CE6F4:
	// mulhwu r11,r27,r6
	ctx.r11.u64 = (uint64_t(ctx.r27.u32) * uint64_t(ctx.r6.u32)) >> 32;
	// rlwinm r10,r11,31,1,31
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// clrlwi r11,r27,30
	ctx.r11.u64 = ctx.r27.u32 & 0x3;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r10,r27
	ctx.r10.s64 = ctx.r27.s64 - ctx.r10.s64;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822ce730
	if (ctx.cr6.lt) goto loc_822CE730;
	// cmplw cr6,r11,r8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r8.u32, ctx.xer);
	// bgt cr6,0x822ce728
	if (ctx.cr6.gt) goto loc_822CE728;
	// subf r27,r11,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r11.s64;
	// rlwinm r25,r27,30,2,31
	ctx.r25.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 30) & 0x3FFFFFFF;
	// b 0x822ce74c
	goto loc_822CE74C;
loc_822CE728:
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x822ce744
	if (ctx.cr6.gt) goto loc_822CE744;
loc_822CE730:
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// bgt cr6,0x822ce744
	if (ctx.cr6.gt) goto loc_822CE744;
	// subf r27,r10,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r10.s64;
	// divwu r25,r27,r4
	ctx.r25.u32 = ctx.r27.u32 / ctx.r4.u32;
	// b 0x822ce74c
	goto loc_822CE74C;
loc_822CE744:
	// subf r27,r8,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r8.s64;
	// divwu r25,r27,r3
	ctx.r25.u32 = ctx.r27.u32 / ctx.r3.u32;
loc_822CE74C:
	// addi r11,r27,1
	ctx.r11.s64 = ctx.r27.s64 + 1;
	// cmplw cr6,r20,r11
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822ce7c8
	if (ctx.cr6.lt) goto loc_822CE7C8;
	// mr r20,r11
	ctx.r20.u64 = ctx.r11.u64;
	// b 0x822ce7c8
	goto loc_822CE7C8;
loc_822CE760:
	// lhz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r26.u32 + 0);
loc_822CE764:
	// cmplwi cr6,r9,355
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 355, ctx.xer);
	// beq cr6,0x822ce778
	if (ctx.cr6.eq) goto loc_822CE778;
	// cmplwi cr6,r9,359
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 359, ctx.xer);
	// rlwinm r11,r24,1,0,30
	ctx.r11.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 1) & 0xFFFFFFFE;
	// bne cr6,0x822ce77c
	if (!ctx.cr6.eq) goto loc_822CE77C;
loc_822CE778:
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
loc_822CE77C:
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x822ce7a0
	if (!ctx.cr6.gt) goto loc_822CE7A0;
	// li r22,1
	ctx.r22.s64 = 1;
	// rlwinm r23,r23,1,0,30
	ctx.r23.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r24,r24,31,1,31
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 31) & 0x7FFFFFFF;
	// cmplwi cr6,r17,3
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 3, ctx.xer);
	// blt cr6,0x822cea88
	if (ctx.cr6.lt) goto loc_822CEA88;
	// cmplwi cr6,r23,16
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 16, ctx.xer);
	// blt cr6,0x822ce764
	if (ctx.cr6.lt) goto loc_822CE764;
loc_822CE7A0:
	// mullw r11,r20,r24
	ctx.r11.s64 = int64_t(ctx.r20.s32) * int64_t(ctx.r24.s32);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x822ce7c8
	if (!ctx.cr6.lt) goto loc_822CE7C8;
	// divwu r20,r10,r24
	ctx.r20.u32 = ctx.r10.u32 / ctx.r24.u32;
	// li r22,1
	ctx.r22.s64 = 1;
	// cmplw cr6,r25,r20
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, ctx.r20.u32, ctx.xer);
	// blt cr6,0x822ce7c0
	if (ctx.cr6.lt) goto loc_822CE7C0;
	// mr r25,r20
	ctx.r25.u64 = ctx.r20.u64;
loc_822CE7C0:
	// cmplwi cr6,r20,1
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, 1, ctx.xer);
	// blt cr6,0x822cea88
	if (ctx.cr6.lt) goto loc_822CEA88;
loc_822CE7C8:
	// lwz r11,4(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ce918
	if (!ctx.cr6.eq) goto loc_822CE918;
	// lwz r11,84(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 84);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ce918
	if (ctx.cr6.eq) goto loc_822CE918;
	// lwz r10,88(r21)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r21.u32 + 88);
	// add r11,r18,r25
	ctx.r11.u64 = ctx.r18.u64 + ctx.r25.u64;
	// mullw r10,r10,r19
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r19.s32);
	// addi r9,r10,999
	ctx.r9.s64 = ctx.r10.s64 + 999;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// divwu r10,r9,r7
	ctx.r10.u32 = ctx.r9.u32 / ctx.r7.u32;
	// mullw r8,r11,r24
	ctx.r8.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r24.s32);
	// cmplw cr6,r8,r10
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x822ce918
	if (!ctx.cr6.gt) goto loc_822CE918;
	// divwu r9,r10,r24
	ctx.r9.u32 = ctx.r10.u32 / ctx.r24.u32;
	// cmplwi cr6,r9,5
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 5, ctx.xer);
	// bge cr6,0x822ce820
	if (!ctx.cr6.lt) goto loc_822CE820;
	// cmplwi cr6,r17,2
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 2, ctx.xer);
	// beq cr6,0x822cea88
	if (ctx.cr6.eq) goto loc_822CEA88;
	// cmplwi cr6,r23,8
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 8, ctx.xer);
	// beq cr6,0x822cea88
	if (ctx.cr6.eq) goto loc_822CEA88;
loc_822CE820:
	// cmplwi cr6,r9,6
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 6, ctx.xer);
	// bge cr6,0x822ce844
	if (!ctx.cr6.lt) goto loc_822CE844;
	// cmplwi cr6,r17,3
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 3, ctx.xer);
	// bne cr6,0x822ce844
	if (!ctx.cr6.eq) goto loc_822CE844;
	// cmplwi cr6,r23,8
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 8, ctx.xer);
	// beq cr6,0x822ce844
	if (ctx.cr6.eq) goto loc_822CE844;
	// divwu r9,r15,r23
	ctx.r9.u32 = ctx.r15.u32 / ctx.r23.u32;
	// mr r23,r15
	ctx.r23.u64 = ctx.r15.u64;
	// divwu r24,r24,r9
	ctx.r24.u32 = ctx.r24.u32 / ctx.r9.u32;
loc_822CE844:
	// mullw r8,r11,r24
	ctx.r8.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r24.s32);
	// divwu r9,r10,r24
	ctx.r9.u32 = ctx.r10.u32 / ctx.r24.u32;
	// cmplw cr6,r20,r11
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822ce858
	if (ctx.cr6.lt) goto loc_822CE858;
	// mr r20,r11
	ctx.r20.u64 = ctx.r11.u64;
loc_822CE858:
	// cmplw cr6,r8,r10
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x822ce894
	if (!ctx.cr6.gt) goto loc_822CE894;
	// cmplwi cr6,r9,5
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 5, ctx.xer);
	// blt cr6,0x822cea88
	if (ctx.cr6.lt) goto loc_822CEA88;
	// addi r11,r25,4
	ctx.r11.s64 = ctx.r25.s64 + 4;
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x822ce8e8
	if (!ctx.cr6.gt) goto loc_822CE8E8;
	// subf r11,r25,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r25.s64;
	// addi r11,r11,253
	ctx.r11.s64 = ctx.r11.s64 + 253;
	// clrlwi r18,r11,24
	ctx.r18.u64 = ctx.r11.u32 & 0xFF;
	// add r11,r18,r25
	ctx.r11.u64 = ctx.r18.u64 + ctx.r25.u64;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
loc_822CE888:
	// cmplw cr6,r20,r11
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822ce894
	if (ctx.cr6.lt) goto loc_822CE894;
	// mr r20,r11
	ctx.r20.u64 = ctx.r11.u64;
loc_822CE894:
	// cmplwi cr6,r27,1
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 1, ctx.xer);
	// bne cr6,0x822ce8b4
	if (!ctx.cr6.eq) goto loc_822CE8B4;
	// cmplwi cr6,r17,2
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 2, ctx.xer);
	// bgt cr6,0x822ce8b4
	if (ctx.cr6.gt) goto loc_822CE8B4;
	// lhz r11,20(r26)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r26.u32 + 20);
	// clrlwi r10,r11,16
	ctx.r10.u64 = ctx.r11.u32 & 0xFFFF;
	// rlwinm r10,r10,0,31,28
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF9;
	// sth r10,20(r26)
	PPC_STORE_U16(ctx.r26.u32 + 20, ctx.r10.u16);
loc_822CE8B4:
	// addi r11,r23,-1
	ctx.r11.s64 = ctx.r23.s64 + -1;
	// cmplwi cr6,r11,7
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 7, ctx.xer);
	// bgt cr6,0x822cea88
	if (ctx.cr6.gt) goto loc_822CEA88;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ce92c
	if (ctx.cr6.eq) goto loc_822CE92C;
	// bdz 0x822ce944
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CE944;
	// bdz 0x822cea88
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CEA88;
	// bdz 0x822ce960
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CE960;
	// bdz 0x822cea88
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CEA88;
	// bdz 0x822cea88
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CEA88;
	// bdz 0x822cea88
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822CEA88;
	// b 0x822ce9ac
	goto loc_822CE9AC;
loc_822CE8E8:
	// lwz r11,0(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	// li r18,1
	ctx.r18.s64 = 1;
	// addi r25,r9,-4
	ctx.r25.s64 = ctx.r9.s64 + -4;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ce910
	if (!ctx.cr6.eq) goto loc_822CE910;
	// clrlwi r11,r25,16
	ctx.r11.u64 = ctx.r25.u32 & 0xFFFF;
	// divwu r10,r27,r11
	ctx.r10.u32 = ctx.r27.u32 / ctx.r11.u32;
	// mullw r9,r10,r11
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// subf r8,r9,r27
	ctx.r8.s64 = ctx.r27.s64 - ctx.r9.s64;
	// subf r27,r8,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r8.s64;
loc_822CE910:
	// addi r11,r25,4
	ctx.r11.s64 = ctx.r25.s64 + 4;
	// b 0x822ce888
	goto loc_822CE888;
loc_822CE918:
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// bne cr6,0x822ce894
	if (!ctx.cr6.eq) goto loc_822CE894;
loc_822CE920:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8233e484
	__restgprlr_15(ctx, base);
	return;
loc_822CE92C:
	// lhz r11,20(r26)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r26.u32 + 20);
	// clrlwi r10,r11,16
	ctx.r10.u64 = ctx.r11.u32 & 0xFFFF;
	// rlwinm r10,r10,0,31,28
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF9;
	// ori r9,r10,2
	ctx.r9.u64 = ctx.r10.u64 | 2;
	// sth r9,20(r26)
	PPC_STORE_U16(ctx.r26.u32 + 20, ctx.r9.u16);
	// b 0x822ce9fc
	goto loc_822CE9FC;
loc_822CE944:
	// cmplwi cr6,r17,3
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 3, ctx.xer);
	// bne cr6,0x822ce9fc
	if (!ctx.cr6.eq) goto loc_822CE9FC;
	// lhz r11,20(r26)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r26.u32 + 20);
	// clrlwi r10,r11,16
	ctx.r10.u64 = ctx.r11.u32 & 0xFFFF;
	// rlwinm r10,r10,0,31,28
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF9;
	// sth r10,20(r26)
	PPC_STORE_U16(ctx.r26.u32 + 20, ctx.r10.u16);
	// b 0x822ce9fc
	goto loc_822CE9FC;
loc_822CE960:
	// lhz r11,20(r26)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r26.u32 + 20);
	// cmplwi cr6,r31,8
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 8, ctx.xer);
	// clrlwi r10,r11,16
	ctx.r10.u64 = ctx.r11.u32 & 0xFFFF;
	// rlwinm r10,r10,0,31,28
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF9;
	// ori r11,r10,4
	ctx.r11.u64 = ctx.r10.u64 | 4;
	// sth r11,20(r26)
	PPC_STORE_U16(ctx.r26.u32 + 20, ctx.r11.u16);
	// ble cr6,0x822ce980
	if (!ctx.cr6.gt) goto loc_822CE980;
	// mr r31,r15
	ctx.r31.u64 = ctx.r15.u64;
loc_822CE980:
	// clrlwi r10,r11,16
	ctx.r10.u64 = ctx.r11.u32 & 0xFFFF;
	// li r11,0
	ctx.r11.s64 = 0;
	// rlwinm r10,r10,0,29,25
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFFC7;
	// cmplwi cr6,r31,1
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1, ctx.xer);
	// sth r10,20(r26)
	PPC_STORE_U16(ctx.r26.u32 + 20, ctx.r10.u16);
	// ble cr6,0x822ce9ec
	if (!ctx.cr6.gt) goto loc_822CE9EC;
loc_822CE998:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// srw r9,r31,r11
	ctx.r9.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 1, ctx.xer);
	// bgt cr6,0x822ce998
	if (ctx.cr6.gt) goto loc_822CE998;
	// b 0x822ce9ec
	goto loc_822CE9EC;
loc_822CE9AC:
	// lhz r11,20(r26)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r26.u32 + 20);
	// cmplwi cr6,r31,4
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 4, ctx.xer);
	// ori r11,r11,6
	ctx.r11.u64 = ctx.r11.u64 | 6;
	// sth r11,20(r26)
	PPC_STORE_U16(ctx.r26.u32 + 20, ctx.r11.u16);
	// ble cr6,0x822ce9c4
	if (!ctx.cr6.gt) goto loc_822CE9C4;
	// li r31,4
	ctx.r31.s64 = 4;
loc_822CE9C4:
	// clrlwi r10,r11,16
	ctx.r10.u64 = ctx.r11.u32 & 0xFFFF;
	// li r11,0
	ctx.r11.s64 = 0;
	// rlwinm r10,r10,0,29,25
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFFC7;
	// cmplwi cr6,r31,1
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1, ctx.xer);
	// sth r10,20(r26)
	PPC_STORE_U16(ctx.r26.u32 + 20, ctx.r10.u16);
	// ble cr6,0x822ce9ec
	if (!ctx.cr6.gt) goto loc_822CE9EC;
loc_822CE9DC:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// srw r9,r31,r11
	ctx.r9.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r31.u32 >> (ctx.r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 1, ctx.xer);
	// bgt cr6,0x822ce9dc
	if (ctx.cr6.gt) goto loc_822CE9DC;
loc_822CE9EC:
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// or r9,r11,r10
	ctx.r9.u64 = ctx.r11.u64 | ctx.r10.u64;
	// sth r9,20(r26)
	PPC_STORE_U16(ctx.r26.u32 + 20, ctx.r9.u16);
loc_822CE9FC:
	// stw r24,28(r26)
	PPC_STORE_U32(ctx.r26.u32 + 28, ctx.r24.u32);
	// mr r7,r17
	ctx.r7.u64 = ctx.r17.u64;
	// stw r18,44(r26)
	PPC_STORE_U32(ctx.r26.u32 + 44, ctx.r18.u32);
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// stw r27,40(r26)
	PPC_STORE_U32(ctx.r26.u32 + 40, ctx.r27.u32);
	// stw r25,32(r26)
	PPC_STORE_U32(ctx.r26.u32 + 32, ctx.r25.u32);
	// stw r20,36(r26)
	PPC_STORE_U32(ctx.r26.u32 + 36, ctx.r20.u32);
	// lhz r8,20(r26)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r26.u32 + 20);
	// lwz r5,24(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 24);
	// lwz r4,4(r16)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r16.u32 + 4);
	// lwz r3,0(r16)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	// bl 0x822ce140
	ctx.lr = 0x822CEA2C;
	sub_822CE140(ctx, base);
	// stw r3,8(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8, ctx.r3.u32);
	// mr r7,r17
	ctx.r7.u64 = ctx.r17.u64;
	// lhz r8,20(r26)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r26.u32 + 20);
	// lwz r6,32(r26)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r26.u32 + 32);
	// lwz r5,24(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 24);
	// lwz r4,4(r16)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r16.u32 + 4);
	// lwz r3,0(r16)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	// bl 0x822ce040
	ctx.lr = 0x822CEA4C;
	sub_822CE040(ctx, base);
	// sth r3,12(r26)
	PPC_STORE_U16(ctx.r26.u32 + 12, ctx.r3.u16);
	// lwz r11,28(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 28);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cea74
	if (ctx.cr6.eq) goto loc_822CEA74;
	// lwz r11,28(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 28);
	// cmplwi cr6,r11,4096
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4096, ctx.xer);
	// rlwinm r11,r11,2,16,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFC;
	// bne cr6,0x822cea70
	if (!ctx.cr6.eq) goto loc_822CEA70;
	// li r11,8192
	ctx.r11.s64 = 8192;
loc_822CEA70:
	// sth r11,12(r26)
	PPC_STORE_U16(ctx.r26.u32 + 12, ctx.r11.u16);
loc_822CEA74:
	// lhz r3,0(r26)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r26.u32 + 0);
	// bl 0x822f7cc8
	ctx.lr = 0x822CEA7C;
	sub_822F7CC8(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8233e484
	__restgprlr_15(ctx, base);
	return;
loc_822CEA88:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8233e484
	__restgprlr_15(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CEA94"))) PPC_WEAK_FUNC(sub_822CEA94);
PPC_FUNC_IMPL(__imp__sub_822CEA94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CEA98"))) PPC_WEAK_FUNC(sub_822CEA98);
PPC_FUNC_IMPL(__imp__sub_822CEA98) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e45c
	ctx.lr = 0x822CEAA0;
	__restfpr_25(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// lhz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// bl 0x822f7c70
	ctx.lr = 0x822CEAB4;
	sub_822F7C70(ctx, base);
	// lwz r25,0(r31)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lwz r30,4(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// lhz r7,2(r28)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r28.u32 + 2);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r6,4(r28)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// bl 0x822cdd90
	ctx.lr = 0x822CEAD4;
	sub_822CDD90(ctx, base);
	// li r26,1
	ctx.r26.s64 = 1;
	// li r5,-1
	ctx.r5.s64 = -1;
	// li r27,-1
	ctx.r27.s64 = -1;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// beq cr6,0x822ceafc
	if (ctx.cr6.eq) goto loc_822CEAFC;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x822ceafc
	if (!ctx.cr6.eq) goto loc_822CEAFC;
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// mr r30,r26
	ctx.r30.u64 = ctx.r26.u64;
	// b 0x822ceb08
	goto loc_822CEB08;
loc_822CEAFC:
	// lwz r11,8(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	// li r30,0
	ctx.r30.s64 = 0;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
loc_822CEB08:
	// li r31,0
	ctx.r31.s64 = 0;
	// cmplwi cr6,r29,2
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 2, ctx.xer);
	// beq cr6,0x822ceb24
	if (ctx.cr6.eq) goto loc_822CEB24;
	// cmplwi cr6,r29,3
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 3, ctx.xer);
	// bne cr6,0x822ceb28
	if (!ctx.cr6.eq) goto loc_822CEB28;
	// li r31,16
	ctx.r31.s64 = 16;
	// b 0x822ceb28
	goto loc_822CEB28;
loc_822CEB24:
	// li r31,8
	ctx.r31.s64 = 8;
loc_822CEB28:
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822ceb3c
	if (!ctx.cr6.eq) goto loc_822CEB3C;
loc_822CEB30:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
loc_822CEB3C:
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x822ceb30
	if (ctx.cr6.eq) goto loc_822CEB30;
	// lwz r7,12(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// li r8,0
	ctx.r8.s64 = 0;
loc_822CEB54:
	// add r11,r8,r7
	ctx.r11.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lwz r10,21(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21);
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// rlwimi r3,r26,0,31,29
	ctx.r3.u64 = (rotl32(ctx.r26.u32, 0) & 0xFFFFFFFFFFFFFFFD) | (ctx.r3.u64 & 0x2);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cebd0
	if (ctx.cr6.eq) goto loc_822CEBD0;
	// and r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 & ctx.r31.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822cebd0
	if (ctx.cr6.eq) goto loc_822CEBD0;
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// lhz r3,14(r28)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r28.u32 + 14);
	// cmplw cr6,r10,r3
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r3.u32, ctx.xer);
	// bne cr6,0x822cebd0
	if (!ctx.cr6.eq) goto loc_822CEBD0;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x822ceb98
	if (ctx.cr6.eq) goto loc_822CEB98;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// b 0x822ceb9c
	goto loc_822CEB9C;
loc_822CEB98:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
loc_822CEB9C:
	// subf r11,r10,r4
	ctx.r11.s64 = ctx.r4.s64 - ctx.r10.s64;
	// srawi r3,r11,31
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r11.s32 >> 31;
	// xor r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 ^ ctx.r3.u64;
	// subf. r11,r3,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r3.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822cebe4
	if (ctx.cr0.eq) goto loc_822CEBE4;
	// cmplw cr6,r5,r11
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x822cebd0
	if (!ctx.cr6.gt) goto loc_822CEBD0;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x822cebc8
	if (ctx.cr6.eq) goto loc_822CEBC8;
	// cmplw cr6,r4,r10
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r10.u32, ctx.xer);
	// bgt cr6,0x822cebd0
	if (ctx.cr6.gt) goto loc_822CEBD0;
loc_822CEBC8:
	// mr r5,r11
	ctx.r5.u64 = ctx.r11.u64;
	// mr r27,r9
	ctx.r27.u64 = ctx.r9.u64;
loc_822CEBD0:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r8,r8,25
	ctx.r8.s64 = ctx.r8.s64 + 25;
	// cmplw cr6,r9,r6
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, ctx.xer);
	// blt cr6,0x822ceb54
	if (ctx.cr6.lt) goto loc_822CEB54;
	// b 0x822cebe8
	goto loc_822CEBE8;
loc_822CEBE4:
	// mr r27,r9
	ctx.r27.u64 = ctx.r9.u64;
loc_822CEBE8:
	// cmpwi cr6,r27,-1
	ctx.cr6.compare<int32_t>(ctx.r27.s32, -1, ctx.xer);
	// beq cr6,0x822ceb30
	if (ctx.cr6.eq) goto loc_822CEB30;
	// mulli r11,r27,25
	ctx.r11.s64 = ctx.r27.s64 * 25;
	// add r3,r11,r7
	ctx.r3.u64 = ctx.r11.u64 + ctx.r7.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CEC00"))) PPC_WEAK_FUNC(sub_822CEC00);
PPC_FUNC_IMPL(__imp__sub_822CEC00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e460
	ctx.lr = 0x822CEC08;
	__restfpr_26(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
	// mr r28,r7
	ctx.r28.u64 = ctx.r7.u64;
	// mr r31,r8
	ctx.r31.u64 = ctx.r8.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cec54
	if (ctx.cr6.eq) goto loc_822CEC54;
	// lwz r11,4(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cec54
	if (ctx.cr6.eq) goto loc_822CEC54;
	// lwz r11,20(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	// cmplwi cr6,r11,100
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 100, ctx.xer);
	// bne cr6,0x822cec54
	if (!ctx.cr6.eq) goto loc_822CEC54;
loc_822CEC48:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
loc_822CEC54:
	// mr r7,r30
	ctx.r7.u64 = ctx.r30.u64;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// lwz r4,4(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// bl 0x822cdf98
	ctx.lr = 0x822CEC6C;
	sub_822CDF98(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// sth r3,20(r31)
	PPC_STORE_U16(ctx.r31.u32 + 20, ctx.r3.u16);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// sth r11,22(r31)
	PPC_STORE_U16(ctx.r31.u32 + 22, ctx.r11.u16);
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// stw r4,24(r31)
	PPC_STORE_U32(ctx.r31.u32 + 24, ctx.r4.u32);
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// bl 0x822f7ad8
	ctx.lr = 0x822CEC90;
	sub_822F7AD8(ctx, base);
	// stw r3,28(r31)
	PPC_STORE_U32(ctx.r31.u32 + 28, ctx.r3.u32);
	// lbz r10,4(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + 4);
	// stw r10,44(r31)
	PPC_STORE_U32(ctx.r31.u32 + 44, ctx.r10.u32);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x822cecc8
	if (!ctx.cr6.eq) goto loc_822CECC8;
	// lwz r11,36(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 36);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822cecc8
	if (ctx.cr6.eq) goto loc_822CECC8;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,40(r31)
	PPC_STORE_U32(ctx.r31.u32 + 40, ctx.r11.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r11.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(ctx.r31.u32 + 36, ctx.r11.u32);
	// b 0x822cee04
	goto loc_822CEE04;
loc_822CECC8:
	// lhz r11,5(r28)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r28.u32 + 5);
	// stw r11,40(r31)
	PPC_STORE_U32(ctx.r31.u32 + 40, ctx.r11.u32);
	// lbz r10,7(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + 7);
	// stw r10,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r10.u32);
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x822ced04
	if (ctx.cr6.eq) goto loc_822CED04;
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// lhz r8,20(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 20);
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// lwz r6,0(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r4,4(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// bl 0x822ce1c8
	ctx.lr = 0x822CED00;
	sub_822CE1C8(ctx, base);
	// stw r3,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r3.u32);
loc_822CED04:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ced48
	if (ctx.cr6.eq) goto loc_822CED48;
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ced48
	if (!ctx.cr6.eq) goto loc_822CED48;
	// lwz r11,16(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822ced34
	if (!ctx.cr6.eq) goto loc_822CED34;
	// lwz r11,12(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ced48
	if (ctx.cr6.eq) goto loc_822CED48;
loc_822CED34:
	// lwz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// ble cr6,0x822ced44
	if (!ctx.cr6.gt) goto loc_822CED44;
	// li r11,4
	ctx.r11.s64 = 4;
loc_822CED44:
	// stw r11,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r11.u32);
loc_822CED48:
	// cmplwi cr6,r27,2
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 2, ctx.xer);
	// bgt cr6,0x822ced5c
	if (ctx.cr6.gt) goto loc_822CED5C;
	// li r11,15
	ctx.r11.s64 = 15;
	// stw r11,36(r31)
	PPC_STORE_U32(ctx.r31.u32 + 36, ctx.r11.u32);
	// b 0x822cee04
	goto loc_822CEE04;
loc_822CED5C:
	// lwz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfs f12,5256(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5256);
	ctx.f12.f64 = double(temp.f32);
	// lfd f0,80(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f13
	ctx.f11.f64 = double(ctx.f13.s64);
	// fcfid f10,f0
	ctx.f10.f64 = double(ctx.f0.s64);
	// lfs f13,5260(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5260);
	ctx.f13.f64 = double(temp.f32);
	// frsp f9,f11
	ctx.f9.f64 = double(float(ctx.f11.f64));
	// lfs f11,11112(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 11112);
	ctx.f11.f64 = double(temp.f32);
	// frsp f8,f10
	ctx.f8.f64 = double(float(ctx.f10.f64));
	// fdivs f0,f9,f8
	ctx.f0.f64 = double(float(ctx.f9.f64 / ctx.f8.f64));
	// fdivs f7,f13,f0
	ctx.f7.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
	// fctiwz f6,f7
	ctx.f6.u64 = uint64_t(int32_t(std::trunc(ctx.f7.f64)));
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r3,r4
	ctx.r3.s64 = ctx.r4.s32;
	// std r3,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r3.u64);
	// lfd f5,80(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fsubs f2,f3,f12
	ctx.f2.f64 = static_cast<float>(ctx.f3.f64 - ctx.f12.f64);
	// fcmpu cr6,f2,f11
	ctx.cr6.compare(ctx.f2.f64, ctx.f11.f64);
	// bgt cr6,0x822cedf8
	if (ctx.cr6.gt) goto loc_822CEDF8;
	// fdivs f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f13.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fsubs f11,f9,f12
	ctx.f11.f64 = static_cast<float>(ctx.f9.f64 - ctx.f12.f64);
loc_822CEDF8:
	// li r11,36
	ctx.r11.s64 = 36;
	// fctidz f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfiwx f0,r31,r11
	PPC_STORE_U32(ctx.r31.u32 + ctx.r11.u32, ctx.f0.u32);
loc_822CEE04:
	// cmplwi cr6,r27,1
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 1, ctx.xer);
	// beq cr6,0x822cee58
	if (ctx.cr6.eq) goto loc_822CEE58;
	// cmplwi cr6,r27,2
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 2, ctx.xer);
	// beq cr6,0x822cee50
	if (ctx.cr6.eq) goto loc_822CEE50;
	// cmplwi cr6,r27,3
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 3, ctx.xer);
	// bne cr6,0x822cec48
	if (!ctx.cr6.eq) goto loc_822CEC48;
	// lhz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 20);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822cee34
	if (ctx.cr6.eq) goto loc_822CEE34;
	// li r11,355
	ctx.r11.s64 = 355;
	// b 0x822cee5c
	goto loc_822CEE5C;
loc_822CEE34:
	// lwz r11,28(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 28);
	// subfic r10,r11,0
	ctx.xer.ca = ctx.r11.u32 <= 0;
	ctx.r10.s64 = 0 - ctx.r11.s64;
	// subfe r9,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// rlwinm r11,r9,0,30,30
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x2;
	// addi r8,r11,354
	ctx.r8.s64 = ctx.r11.s64 + 354;
	// sth r8,0(r31)
	PPC_STORE_U16(ctx.r31.u32 + 0, ctx.r8.u16);
	// b 0x822cee60
	goto loc_822CEE60;
loc_822CEE50:
	// li r11,353
	ctx.r11.s64 = 353;
	// b 0x822cee5c
	goto loc_822CEE5C;
loc_822CEE58:
	// li r11,352
	ctx.r11.s64 = 352;
loc_822CEE5C:
	// sth r11,0(r31)
	PPC_STORE_U16(ctx.r31.u32 + 0, ctx.r11.u16);
loc_822CEE60:
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// beq cr6,0x822cee74
	if (ctx.cr6.eq) goto loc_822CEE74;
	// lhz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// sth r11,0(r31)
	PPC_STORE_U16(ctx.r31.u32 + 0, ctx.r11.u16);
loc_822CEE74:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// lhz r8,20(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 20);
	// lwz r6,32(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// lwz r5,24(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// sth r10,2(r31)
	PPC_STORE_U16(ctx.r31.u32 + 2, ctx.r10.u16);
	// lwz r4,8(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	// stw r4,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r4.u32);
	// lbz r3,20(r28)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r28.u32 + 20);
	// sth r3,14(r31)
	PPC_STORE_U16(ctx.r31.u32 + 14, ctx.r3.u16);
	// lwz r4,4(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// bl 0x822ce140
	ctx.lr = 0x822CEEB0;
	sub_822CE140(ctx, base);
	// stw r3,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r3.u32);
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// lhz r8,20(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 20);
	// lwz r6,32(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// lwz r5,24(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// lwz r4,4(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// bl 0x822ce040
	ctx.lr = 0x822CEED0;
	sub_822CE040(ctx, base);
	// sth r3,12(r31)
	PPC_STORE_U16(ctx.r31.u32 + 12, ctx.r3.u16);
	// lwz r11,28(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 28);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822ceef8
	if (ctx.cr6.eq) goto loc_822CEEF8;
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// cmplwi cr6,r11,4096
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4096, ctx.xer);
	// rlwinm r11,r11,2,16,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFC;
	// bne cr6,0x822ceef4
	if (!ctx.cr6.eq) goto loc_822CEEF4;
	// li r11,8192
	ctx.r11.s64 = 8192;
loc_822CEEF4:
	// sth r11,12(r31)
	PPC_STORE_U16(ctx.r31.u32 + 12, ctx.r11.u16);
loc_822CEEF8:
	// lwz r11,21(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 21);
	// rlwinm r10,r11,30,31,31
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x1;
	// stw r10,48(r31)
	PPC_STORE_U32(ctx.r31.u32 + 48, ctx.r10.u32);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x822cef30
	if (ctx.cr6.eq) goto loc_822CEF30;
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822cef30
	if (!ctx.cr6.eq) goto loc_822CEF30;
	// lwz r11,20(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822cef30
	if (!ctx.cr6.eq) goto loc_822CEF30;
	// lwz r11,16(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 16);
	// stw r11,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r11.u32);
loc_822CEF30:
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// mr r7,r31
	ctx.r7.u64 = ctx.r31.u64;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// stw r26,52(r31)
	PPC_STORE_U32(ctx.r31.u32 + 52, ctx.r26.u32);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// divwu r9,r11,r10
	ctx.r9.u32 = ctx.r11.u32 / ctx.r10.u32;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mullw r8,r9,r10
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// subf r10,r8,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r8.s64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// stw r9,40(r31)
	PPC_STORE_U32(ctx.r31.u32 + 40, ctx.r9.u32);
	// bl 0x822ce250
	ctx.lr = 0x822CEF68;
	sub_822CE250(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CEF70"))) PPC_WEAK_FUNC(sub_822CEF70);
PPC_FUNC_IMPL(__imp__sub_822CEF70) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822CEF78;
	__restfpr_27(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// bl 0x822cf078
	ctx.lr = 0x822CEF90;
	sub_822CF078(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cea98
	ctx.lr = 0x822CEF9C;
	sub_822CEA98(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822cefb0
	if (!ctx.cr6.eq) goto loc_822CEFB0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822CEFB0:
	// lhz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// bl 0x822f7cc8
	ctx.lr = 0x822CEFB8;
	sub_822F7CC8(ctx, base);
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// lhz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// bl 0x822f7c70
	ctx.lr = 0x822CEFC4;
	sub_822F7C70(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// mr r7,r29
	ctx.r7.u64 = ctx.r29.u64;
	// mr r8,r28
	ctx.r8.u64 = ctx.r28.u64;
	// bl 0x822cec00
	ctx.lr = 0x822CEFDC;
	sub_822CEC00(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822CEFE4"))) PPC_WEAK_FUNC(sub_822CEFE4);
PPC_FUNC_IMPL(__imp__sub_822CEFE4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CEFE8"))) PPC_WEAK_FUNC(sub_822CEFE8);
PPC_FUNC_IMPL(__imp__sub_822CEFE8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r5,116
	ctx.r5.s64 = 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822CF008;
	sub_8233EAF0(ctx, base);
	// li r9,6
	ctx.r9.s64 = 6;
	// li r11,0
	ctx.r11.s64 = 0;
	// addi r8,r31,60
	ctx.r8.s64 = ctx.r31.s64 + 60;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// addi r10,r8,-4
	ctx.r10.s64 = ctx.r8.s64 + -4;
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r11.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(ctx.r31.u32 + 56, ctx.r11.u32);
loc_822CF034:
	// stwu r11,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r11.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822cf034
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CF034;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,84(r31)
	PPC_STORE_U32(ctx.r31.u32 + 84, ctx.r11.u32);
	// li r9,2
	ctx.r9.s64 = 2;
	// stw r11,92(r31)
	PPC_STORE_U32(ctx.r31.u32 + 92, ctx.r11.u32);
	// sth r10,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r10.u16);
	// stw r11,100(r31)
	PPC_STORE_U32(ctx.r31.u32 + 100, ctx.r11.u32);
	// stw r9,24(r31)
	PPC_STORE_U32(ctx.r31.u32 + 24, ctx.r9.u32);
	// stw r10,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r10.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(ctx.r31.u32 + 36, ctx.r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF074"))) PPC_WEAK_FUNC(sub_822CF074);
PPC_FUNC_IMPL(__imp__sub_822CF074) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CF078"))) PPC_WEAK_FUNC(sub_822CF078);
PPC_FUNC_IMPL(__imp__sub_822CF078) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// lhz r9,2(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2);
	// stw r9,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r9.u32);
	// lwz r8,16(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// stw r8,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r8.u32);
	// lhz r7,14(r3)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// stw r7,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r7.u32);
	// lhz r11,14(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// addi r6,r11,7
	ctx.r6.s64 = ctx.r11.s64 + 7;
	// srawi r5,r6,3
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 3;
	// stw r10,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, ctx.r10.u32);
	// addze r3,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r3.s64 = temp.s64;
	// stw r3,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, ctx.r3.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF0B8"))) PPC_WEAK_FUNC(sub_822CF0B8);
PPC_FUNC_IMPL(__imp__sub_822CF0B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// li r10,6
	ctx.r10.s64 = 6;
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r11,r4,-4
	ctx.r11.s64 = ctx.r4.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822CF0C8:
	// stwu r9,4(r11)
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r11.u32 = ea;
	// bdnz 0x822cf0c8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CF0C8;
	// sth r9,0(r4)
	PPC_STORE_U16(ctx.r4.u32 + 0, ctx.r9.u16);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// sth r11,2(r4)
	PPC_STORE_U16(ctx.r4.u32 + 2, ctx.r11.u16);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r8,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r8.u32);
	// stw r9,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r9.u32);
	// sth r9,12(r4)
	PPC_STORE_U16(ctx.r4.u32 + 12, ctx.r9.u16);
	// lwz r7,12(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// sth r7,14(r4)
	PPC_STORE_U16(ctx.r4.u32 + 14, ctx.r7.u16);
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r5,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, ctx.r5.u32);
	// sth r9,20(r4)
	PPC_STORE_U16(ctx.r4.u32 + 20, ctx.r9.u16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF104"))) PPC_WEAK_FUNC(sub_822CF104);
PPC_FUNC_IMPL(__imp__sub_822CF104) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CF108"))) PPC_WEAK_FUNC(sub_822CF108);
PPC_FUNC_IMPL(__imp__sub_822CF108) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lhz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// cmplwi cr6,r3,65534
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 65534, ctx.xer);
	// bne cr6,0x822cf158
	if (!ctx.cr6.eq) goto loc_822CF158;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// li r5,12
	ctx.r5.s64 = 12;
	// addi r11,r11,-1544
	ctx.r11.s64 = ctx.r11.s64 + -1544;
	// addi r3,r31,28
	ctx.r3.s64 = ctx.r31.s64 + 28;
	// addi r4,r11,4
	ctx.r4.s64 = ctx.r11.s64 + 4;
	// bl 0x82341410
	ctx.lr = 0x822CF140;
	sub_82341410(ctx, base);
	// cmpwi r3,0
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne 0x822cf154
	if (!ctx.cr0.eq) goto loc_822CF154;
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// clrlwi r3,r11,16
	ctx.r3.u64 = ctx.r11.u32 & 0xFFFF;
	// b 0x822cf158
	goto loc_822CF158;
loc_822CF154:
	// li r3,0
	ctx.r3.s64 = 0;
loc_822CF158:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF16C"))) PPC_WEAK_FUNC(sub_822CF16C);
PPC_FUNC_IMPL(__imp__sub_822CF16C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CF170"))) PPC_WEAK_FUNC(sub_822CF170);
PPC_FUNC_IMPL(__imp__sub_822CF170) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lhz r9,14(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r9,8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 8, ctx.xer);
	// beq cr6,0x822cf1a0
	if (ctx.cr6.eq) goto loc_822CF1A0;
	// cmplwi cr6,r9,16
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 16, ctx.xer);
	// beq cr6,0x822cf1a0
	if (ctx.cr6.eq) goto loc_822CF1A0;
	// cmplwi cr6,r9,24
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 24, ctx.xer);
	// beq cr6,0x822cf1a0
	if (ctx.cr6.eq) goto loc_822CF1A0;
	// addi r11,r9,-32
	ctx.r11.s64 = ctx.r9.s64 + -32;
	// addic r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 & ctx.r10.u64;
loc_822CF1A0:
	// lhz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,65534
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 65534, ctx.xer);
	// bne cr6,0x822cf1f8
	if (!ctx.cr6.eq) goto loc_822CF1F8;
	// lhz r11,18(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 18);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq 0x822cf1e8
	if (ctx.cr0.eq) goto loc_822CF1E8;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// beq cr6,0x822cf1e8
	if (ctx.cr6.eq) goto loc_822CF1E8;
	// cmplwi cr6,r11,16
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 16, ctx.xer);
	// beq cr6,0x822cf1e8
	if (ctx.cr6.eq) goto loc_822CF1E8;
	// cmplwi cr6,r11,20
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 20, ctx.xer);
	// beq cr6,0x822cf1e8
	if (ctx.cr6.eq) goto loc_822CF1E8;
	// cmplwi cr6,r11,24
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 24, ctx.xer);
	// beq cr6,0x822cf1e8
	if (ctx.cr6.eq) goto loc_822CF1E8;
	// addi r8,r11,-32
	ctx.r8.s64 = ctx.r11.s64 + -32;
	// addic r8,r8,-1
	ctx.xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// subfe r8,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r8.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 & ctx.r10.u64;
loc_822CF1E8:
	// subfc r8,r11,r9
	ctx.xer.ca = ctx.r9.u32 >= ctx.r11.u32;
	ctx.r8.s64 = ctx.r9.s64 - ctx.r11.s64;
	// addze r7,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r7.s64 = temp.s64;
	// subf r11,r7,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r7.s64;
	// and r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 & ctx.r10.u64;
loc_822CF1F8:
	// lhz r11,2(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2);
	// lhz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// mullw r11,r11,r9
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r9.s32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// subf r11,r8,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r8.s64;
	// addic r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r9,r9,-1
	ctx.xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// and r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 & ctx.r10.u64;
	// subfe r10,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 & ctx.r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF23C"))) PPC_WEAK_FUNC(sub_822CF23C);
PPC_FUNC_IMPL(__imp__sub_822CF23C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CF240"))) PPC_WEAK_FUNC(sub_822CF240);
PPC_FUNC_IMPL(__imp__sub_822CF240) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lhz r9,2(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r9,1
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 1, ctx.xer);
	// beq cr6,0x822cf264
	if (ctx.cr6.eq) goto loc_822CF264;
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
	// addic r9,r9,-1
	ctx.xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ctx.r10.u64;
loc_822CF264:
	// lhz r9,14(r11)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r11.u32 + 14);
	// lhz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r11.u32 + 16);
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// addi r8,r8,-32
	ctx.r8.s64 = ctx.r8.s64 + -32;
	// addic r9,r9,-1
	ctx.xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r8,r8,-1
	ctx.xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// and r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ctx.r10.u64;
	// subfe r9,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r3,r9,r10
	ctx.r3.u64 = ctx.r9.u64 & ctx.r10.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// lhz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 20);
	// cmplwi cr6,r10,7
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 7, ctx.xer);
	// beq cr6,0x822cf2a8
	if (ctx.cr6.eq) goto loc_822CF2A8;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x822cf304
	goto loc_822CF304;
loc_822CF2A8:
	// li r8,7
	ctx.r8.s64 = 7;
	// addi r10,r11,24
	ctx.r10.s64 = ctx.r11.s64 + 24;
	// li r9,0
	ctx.r9.s64 = 0;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// addi r8,r8,-1528
	ctx.r8.s64 = ctx.r8.s64 + -1528;
loc_822CF2C0:
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// lhzx r5,r9,r8
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r8.u32);
	// lhz r7,-2(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// lhz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// subf r7,r7,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r7.s64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// lhzx r6,r9,r6
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r6.u32);
	// addic r7,r7,-1
	ctx.xer.ca = ctx.r7.u32 > 0;
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// subf r6,r4,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r4.s64;
	// subfe r7,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r7.u64 = ~ctx.r7.u64 + ctx.r7.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r6,r6,-1
	ctx.xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// and r7,r7,r3
	ctx.r7.u64 = ctx.r7.u64 & ctx.r3.u64;
	// subfe r6,r6,r6
	temp.u8 = (~ctx.r6.u32 + ctx.r6.u32 < ~ctx.r6.u32) | (~ctx.r6.u32 + ctx.r6.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r6.u64 = ~ctx.r6.u64 + ctx.r6.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r3,r6,r7
	ctx.r3.u64 = ctx.r6.u64 & ctx.r7.u64;
	// bdnz 0x822cf2c0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CF2C0;
loc_822CF304:
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 2);
	// lhz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r11.u32 + 12);
	// mulli r8,r10,7
	ctx.r8.s64 = ctx.r10.s64 * 7;
	// lhz r7,18(r11)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + 18);
	// subf r11,r8,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r8.s64;
	// rotlwi r10,r10,2
	ctx.r10.u64 = rotl32(ctx.r10.u32, 2);
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// divw r11,r11,r10
	ctx.r11.s32 = ctx.r11.s32 / ctx.r10.s32;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// subf r11,r7,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r7.s64;
	// addic r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// subfic r10,r7,0
	ctx.xer.ca = ctx.r7.u32 <= 0;
	ctx.r10.s64 = 0 - ctx.r7.s64;
	// and r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 & ctx.r3.u64;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 & ctx.r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF348"))) PPC_WEAK_FUNC(sub_822CF348);
PPC_FUNC_IMPL(__imp__sub_822CF348) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,12(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822cf378
	if (ctx.cr6.eq) goto loc_822CF378;
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// bl 0x82082d58
	ctx.lr = 0x822CF370;
	sub_82082D58(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
loc_822CF378:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF38C"))) PPC_WEAK_FUNC(sub_822CF38C);
PPC_FUNC_IMPL(__imp__sub_822CF38C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CF390"))) PPC_WEAK_FUNC(sub_822CF390);
PPC_FUNC_IMPL(__imp__sub_822CF390) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822CF398;
	__restfpr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// li r3,1068
	ctx.r3.s64 = 1068;
	// bl 0x82082c78
	ctx.lr = 0x822CF3B0;
	sub_82082C78(ctx, base);
	// mr. r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// beq 0x822cf3dc
	if (ctx.cr0.eq) goto loc_822CF3DC;
	// li r5,1068
	ctx.r5.s64 = 1068;
	// lwz r4,4(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r28,0
	ctx.r28.s64 = 0;
	// bl 0x8233e4e0
	ctx.lr = 0x822CF3CC;
	sub_8233E4E0(ctx, base);
	// stw r31,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r31.u32);
loc_822CF3D0:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
loc_822CF3DC:
	// lis r28,-32761
	ctx.r28.s64 = -2147024896;
	// ori r28,r28,14
	ctx.r28.u64 = ctx.r28.u64 | 14;
	// b 0x822cf3d0
	goto loc_822CF3D0;
}

__attribute__((alias("__imp__sub_822CF3E8"))) PPC_WEAK_FUNC(sub_822CF3E8);
PPC_FUNC_IMPL(__imp__sub_822CF3E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,12(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// li r30,0
	ctx.r30.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r30,24(r31)
	PPC_STORE_U32(ctx.r31.u32 + 24, ctx.r30.u32);
	// beq cr6,0x822cf420
	if (ctx.cr6.eq) goto loc_822CF420;
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// bl 0x82082d58
	ctx.lr = 0x822CF41C;
	sub_82082D58(ctx, base);
	// stw r30,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r30.u32);
loc_822CF420:
	// li r11,7
	ctx.r11.s64 = 7;
	// stw r30,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r30.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r11.u32);
	// stw r10,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF44C"))) PPC_WEAK_FUNC(sub_822CF44C);
PPC_FUNC_IMPL(__imp__sub_822CF44C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CF450"))) PPC_WEAK_FUNC(sub_822CF450);
PPC_FUNC_IMPL(__imp__sub_822CF450) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplw cr6,r4,r5
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r5.u32, ctx.xer);
	// beq cr6,0x822cf4d4
	if (ctx.cr6.eq) goto loc_822CF4D4;
	// lis r31,-32256
	ctx.r31.s64 = -2113929216;
	// lwz r3,12(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// lfs f0,5256(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// beq cr6,0x822cf48c
	if (ctx.cr6.eq) goto loc_822CF48C;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_822CF48C:
	// lwz r3,16(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// mullw r6,r6,r7
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// lwz r31,20(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// stw r4,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r4.u32);
	// stw r5,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r5.u32);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// stw r31,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r31.u32);
	// stw r6,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r6.u32);
	// stw r7,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r7.u32);
	// stw r8,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r8.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822CF4D4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_822CF4D4:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF4E8"))) PPC_WEAK_FUNC(sub_822CF4E8);
PPC_FUNC_IMPL(__imp__sub_822CF4E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lhz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmplwi cr6,r10,357
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 357, ctx.xer);
	// bne cr6,0x822cf534
	if (!ctx.cr6.eq) goto loc_822CF534;
	// lhz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r11.u32 + 8);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi r8,0
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// blelr 
	if (!ctx.cr0.gt) return;
	// addi r9,r11,10
	ctx.r9.s64 = ctx.r11.s64 + 10;
loc_822CF510:
	// lhzu r11,20(r9)
	ea = 20 + ctx.r9.u32;
	ctx.r11.u64 = PPC_LOAD_U16(ea);
	ctx.r9.u32 = ea;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// clrlwi r7,r11,24
	ctx.r7.u64 = ctx.r11.u32 & 0xFF;
	// rlwinm r11,r11,24,8,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 24) & 0xFFFFFF;
	// cmpw cr6,r10,r8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, ctx.xer);
	// or r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 | ctx.r11.u64;
	// or r3,r11,r3
	ctx.r3.u64 = ctx.r11.u64 | ctx.r3.u64;
	// blt cr6,0x822cf510
	if (ctx.cr6.lt) goto loc_822CF510;
	// blr 
	return;
loc_822CF534:
	// cmplwi cr6,r10,358
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 358, ctx.xer);
	// beq cr6,0x822cf544
	if (ctx.cr6.eq) goto loc_822CF544;
	// cmplwi cr6,r10,65534
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 65534, ctx.xer);
	// bne cr6,0x822cf54c
	if (!ctx.cr6.eq) goto loc_822CF54C;
loc_822CF544:
	// lwz r3,20(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// blr 
	return;
loc_822CF54C:
	// lhz r11,2(r11)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + 2);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x822cf560
	if (!ctx.cr6.eq) goto loc_822CF560;
	// li r3,4
	ctx.r3.s64 = 4;
	// blr 
	return;
loc_822CF560:
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bne cr6,0x822cf570
	if (!ctx.cr6.eq) goto loc_822CF570;
	// li r3,3
	ctx.r3.s64 = 3;
	// blr 
	return;
loc_822CF570:
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bltlr cr6
	if (ctx.cr6.lt) return;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bgtlr cr6
	if (ctx.cr6.gt) return;
	// lis r10,-32199
	ctx.r10.s64 = -2110193664;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,12180
	ctx.r10.s64 = ctx.r10.s64 + 12180;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r3,-4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF5A4"))) PPC_WEAK_FUNC(sub_822CF5A4);
PPC_FUNC_IMPL(__imp__sub_822CF5A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CF5A8"))) PPC_WEAK_FUNC(sub_822CF5A8);
PPC_FUNC_IMPL(__imp__sub_822CF5A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lhz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 16);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r8,1
	ctx.r8.s64 = 1;
	// cmplwi cr6,r10,32
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 32, ctx.xer);
	// bge cr6,0x822cf5c4
	if (!ctx.cr6.lt) goto loc_822CF5C4;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_822CF5C4:
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 2);
	// lbz r9,11(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 11);
	// addi r6,r10,-16
	ctx.r6.s64 = ctx.r10.s64 + -16;
	// lhz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 8);
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
	// lwz r7,16(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// addic r6,r6,-1
	ctx.xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subfe r6,r6,r6
	temp.u8 = (~ctx.r6.u32 + ctx.r6.u32 < ~ctx.r6.u32) | (~ctx.r6.u32 + ctx.r6.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r6.u64 = ~ctx.r6.u64 + ctx.r6.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r9,r9,-1
	ctx.xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// and r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 & ctx.r8.u64;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// subfic r6,r10,0
	ctx.xer.ca = ctx.r10.u32 <= 0;
	ctx.r6.s64 = 0 - ctx.r10.s64;
	// and r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ctx.r8.u64;
	// subfe r8,r6,r6
	temp.u8 = (~ctx.r6.u32 + ctx.r6.u32 < ~ctx.r6.u32) | (~ctx.r6.u32 + ctx.r6.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r8.u64 = ~ctx.r6.u64 + ctx.r6.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// subfic r6,r7,0
	ctx.xer.ca = ctx.r7.u32 <= 0;
	ctx.r6.s64 = 0 - ctx.r7.s64;
	// and r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 & ctx.r9.u64;
	// subfe r8,r6,r6
	temp.u8 = (~ctx.r6.u32 + ctx.r6.u32 < ~ctx.r6.u32) | (~ctx.r6.u32 + ctx.r6.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r8.u64 = ~ctx.r6.u64 + ctx.r6.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r3,r8,r9
	ctx.r3.u64 = ctx.r8.u64 & ctx.r9.u64;
	// blelr cr6
	if (!ctx.cr6.gt) return;
	// addi r9,r11,28
	ctx.r9.s64 = ctx.r11.s64 + 28;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822CF61C:
	// lwz r11,-12(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + -12);
	// lbz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// subf r8,r11,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r11.s64;
	// lbz r11,1(r9)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// clrlwi r6,r10,28
	ctx.r6.u64 = ctx.r10.u32 & 0xF;
	// addic r8,r8,-1
	ctx.xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// subfe r8,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r8.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// subfic r5,r6,4
	ctx.xer.ca = ctx.r6.u32 <= 4;
	ctx.r5.s64 = 4 - ctx.r6.s64;
	// and r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 & ctx.r3.u64;
	// addze r4,r6
	temp.s64 = ctx.r6.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r4.s64 = temp.s64;
	// subfic r3,r10,48
	ctx.xer.ca = ctx.r10.u32 <= 48;
	ctx.r3.s64 = 48 - ctx.r10.s64;
	// subf r6,r4,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r4.s64;
	// addze r5,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r5.s64 = temp.s64;
	// and r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 & ctx.r8.u64;
	// subf r10,r5,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r5.s64;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// and r3,r10,r8
	ctx.r3.u64 = ctx.r10.u64 & ctx.r8.u64;
	// beq cr6,0x822cf678
	if (ctx.cr6.eq) goto loc_822CF678;
	// addi r11,r11,-2
	ctx.r11.s64 = ctx.r11.s64 + -2;
	// addic r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r3,r11,r3
	ctx.r3.u64 = ctx.r11.u64 & ctx.r3.u64;
loc_822CF678:
	// lhz r11,2(r9)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq 0x822cf6c8
	if (ctx.cr0.eq) goto loc_822CF6C8;
	// clrlwi. r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r10,0
	ctx.r10.s64 = 0;
	// beq 0x822cf6b0
	if (ctx.cr0.eq) goto loc_822CF6B0;
loc_822CF690:
	// addis r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 65536;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// and r11,r8,r11
	ctx.r11.u64 = ctx.r8.u64 & ctx.r11.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// clrlwi. r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822cf690
	if (!ctx.cr0.eq) goto loc_822CF690;
loc_822CF6B0:
	// lbz r11,1(r9)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// addic r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r3,r11,r3
	ctx.r3.u64 = ctx.r11.u64 & ctx.r3.u64;
loc_822CF6C8:
	// addi r9,r9,20
	ctx.r9.s64 = ctx.r9.s64 + 20;
	// bdnz 0x822cf61c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CF61C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF6D4"))) PPC_WEAK_FUNC(sub_822CF6D4);
PPC_FUNC_IMPL(__imp__sub_822CF6D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CF6D8"))) PPC_WEAK_FUNC(sub_822CF6D8);
PPC_FUNC_IMPL(__imp__sub_822CF6D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lhz r7,2(r3)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2);
	// li r11,1
	ctx.r11.s64 = 1;
	// lhz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// rotlwi r8,r7,1
	ctx.r8.u64 = rotl32(ctx.r7.u32, 1);
	// lhz r10,14(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// lhz r6,16(r3)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r3.u32 + 16);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// addic r9,r9,-1
	ctx.xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmplwi cr6,r6,34
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 34, ctx.xer);
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// and r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 & ctx.r11.u64;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 & ctx.r11.u64;
	// beq cr6,0x822cf720
	if (ctx.cr6.eq) goto loc_822CF720;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_822CF720:
	// lbz r10,49(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 49);
	// li r11,3
	ctx.r11.s64 = 3;
	// lhz r8,50(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 50);
	// subfc r6,r11,r10
	ctx.xer.ca = ctx.r10.u32 >= ctx.r11.u32;
	ctx.r6.s64 = ctx.r10.s64 - ctx.r11.s64;
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addze r5,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r5.s64 = temp.s64;
	// subfic r8,r8,0
	ctx.xer.ca = ctx.r8.u32 <= 0;
	ctx.r8.s64 = 0 - ctx.r8.s64;
	// subf r11,r5,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r5.s64;
	// subfe r8,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r8.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 & ctx.r9.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// and r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 & ctx.r11.u64;
	// beq cr6,0x822cf764
	if (ctx.cr6.eq) goto loc_822CF764;
	// lis r11,127
	ctx.r11.s64 = 8323072;
	// ori r11,r11,63488
	ctx.r11.u64 = ctx.r11.u64 | 63488;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x822cf768
	if (!ctx.cr6.gt) goto loc_822CF768;
loc_822CF764:
	// li r8,0
	ctx.r8.s64 = 0;
loc_822CF768:
	// lwz r11,20(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cf7a4
	if (ctx.cr6.eq) goto loc_822CF7A4;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822CF778:
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// addi r9,r11,-1
	ctx.r9.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// and. r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 & ctx.r11.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// bne 0x822cf778
	if (!ctx.cr0.eq) goto loc_822CF778;
	// clrlwi r11,r10,16
	ctx.r11.u64 = ctx.r10.u32 & 0xFFFF;
	// subf r11,r11,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r11.s64;
	// addic r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r8,r11,r8
	ctx.r8.u64 = ctx.r11.u64 & ctx.r8.u64;
loc_822CF7A4:
	// addi r11,r7,1
	ctx.r11.s64 = ctx.r7.s64 + 1;
	// lwz r10,36(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lhz r7,18(r3)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 18);
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// lwz r11,32(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addze r5,r9
	temp.s64 = ctx.r9.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r5.s64 = temp.s64;
	// lwz r9,44(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r10,40(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// subf r7,r7,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r7.s64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addic r9,r7,-1
	ctx.xer.ca = ctx.r7.u32 > 0;
	ctx.r9.s64 = ctx.r7.s64 + -1;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// subfc r7,r11,r6
	ctx.xer.ca = ctx.r6.u32 >= ctx.r11.u32;
	ctx.r7.s64 = ctx.r6.s64 - ctx.r11.s64;
	// and r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ctx.r8.u64;
	// addze r8,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r8.s64 = temp.s64;
	// subfc r6,r10,r6
	ctx.xer.ca = ctx.r6.u32 >= ctx.r10.u32;
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// subf r11,r8,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r8.s64;
	// addze r8,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r8.s64 = temp.s64;
	// and r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 & ctx.r9.u64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// and r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 & ctx.r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CF804"))) PPC_WEAK_FUNC(sub_822CF804);
PPC_FUNC_IMPL(__imp__sub_822CF804) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822CF808"))) PPC_WEAK_FUNC(sub_822CF808);
PPC_FUNC_IMPL(__imp__sub_822CF808) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lhz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,1
	ctx.r30.s64 = 1;
	// cmplwi cr6,r11,357
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 357, ctx.xer);
	// beq cr6,0x822cf850
	if (ctx.cr6.eq) goto loc_822CF850;
	// lhz r11,2(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subfic r11,r11,0
	ctx.xer.ca = ctx.r11.u32 <= 0;
	ctx.r11.s64 = 0 - ctx.r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// subfic r10,r10,0
	ctx.xer.ca = ctx.r10.u32 <= 0;
	ctx.r10.s64 = 0 - ctx.r10.s64;
	// and r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 & ctx.r30.u64;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r30,r10,r11
	ctx.r30.u64 = ctx.r10.u64 & ctx.r11.u64;
loc_822CF850:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cf108
	ctx.lr = 0x822CF858;
	sub_822CF108(ctx, base);
	// clrlwi r11,r3,16
	ctx.r11.u64 = ctx.r3.u32 & 0xFFFF;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x822cf99c
	if (ctx.cr6.eq) goto loc_822CF99C;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x822cf988
	if (ctx.cr6.eq) goto loc_822CF988;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// beq cr6,0x822cf8fc
	if (ctx.cr6.eq) goto loc_822CF8FC;
	// cmpwi cr6,r11,352
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 352, ctx.xer);
	// ble cr6,0x822cf9b0
	if (!ctx.cr6.gt) goto loc_822CF9B0;
	// cmpwi cr6,r11,354
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 354, ctx.xer);
	// ble cr6,0x822cf8ac
	if (!ctx.cr6.gt) goto loc_822CF8AC;
	// cmpwi cr6,r11,357
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 357, ctx.xer);
	// beq cr6,0x822cf8a0
	if (ctx.cr6.eq) goto loc_822CF8A0;
	// cmpwi cr6,r11,358
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 358, ctx.xer);
	// bne cr6,0x822cf9b0
	if (!ctx.cr6.eq) goto loc_822CF9B0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cf6d8
	ctx.lr = 0x822CF89C;
	sub_822CF6D8(ctx, base);
	// b 0x822cf990
	goto loc_822CF990;
loc_822CF8A0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cf5a8
	ctx.lr = 0x822CF8A8;
	sub_822CF5A8(ctx, base);
	// b 0x822cf990
	goto loc_822CF990;
loc_822CF8AC:
	// lhz r10,14(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 14);
	// li r11,1
	ctx.r11.s64 = 1;
	// lhz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// cmplwi cr6,r9,65534
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 65534, ctx.xer);
	// addic r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 & ctx.r11.u64;
	// bne cr6,0x822cf8e4
	if (!ctx.cr6.eq) goto loc_822CF8E4;
	// lhz r10,18(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 18);
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// addic r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 & ctx.r11.u64;
loc_822CF8E4:
	// lhz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 12);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subfic r10,r10,0
	ctx.xer.ca = ctx.r10.u32 <= 0;
	ctx.r10.s64 = 0 - ctx.r10.s64;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// subfic r9,r9,0
	ctx.xer.ca = ctx.r9.u32 <= 0;
	ctx.r9.s64 = 0 - ctx.r9.s64;
	// b 0x822cf96c
	goto loc_822CF96C;
loc_822CF8FC:
	// lhz r10,14(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 14);
	// li r11,1
	ctx.r11.s64 = 1;
	// lhz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// cmplwi cr6,r9,65534
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 65534, ctx.xer);
	// addic r9,r8,-1
	ctx.xer.ca = ctx.r8.u32 > 0;
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 & ctx.r11.u64;
	// bne cr6,0x822cf938
	if (!ctx.cr6.eq) goto loc_822CF938;
	// lhz r9,18(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 18);
	// cmplwi cr6,r9,32
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 32, ctx.xer);
	// beq cr6,0x822cf938
	if (ctx.cr6.eq) goto loc_822CF938;
	// addic r9,r9,-1
	ctx.xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 & ctx.r11.u64;
loc_822CF938:
	// lhz r9,2(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 2);
	// lhz r8,12(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 12);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r7,8(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// srawi r10,r10,3
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// addic r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addic r9,r9,-1
	ctx.xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
loc_822CF96C:
	// and r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 & ctx.r11.u64;
	// subfe r10,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 & ctx.r11.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822cf9b0
	if (!ctx.cr6.eq) goto loc_822CF9B0;
loc_822CF980:
	// li r30,0
	ctx.r30.s64 = 0;
	// b 0x822cf9b0
	goto loc_822CF9B0;
loc_822CF988:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cf240
	ctx.lr = 0x822CF990;
	sub_822CF240(ctx, base);
loc_822CF990:
	// cmpwi r3,0
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne 0x822cf9b0
	if (!ctx.cr0.eq) goto loc_822CF9B0;
	// b 0x822cf980
	goto loc_822CF980;
loc_822CF99C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cf170
	ctx.lr = 0x822CF9A4;
	sub_822CF170(ctx, base);
	// subfic r11,r3,0
	ctx.xer.ca = ctx.r3.u32 <= 0;
	ctx.r11.s64 = 0 - ctx.r3.s64;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 & ctx.r30.u64;
loc_822CF9B0:
	// lhz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// cmplwi cr6,r11,65534
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 65534, ctx.xer);
	// bne cr6,0x822cfa04
	if (!ctx.cr6.eq) goto loc_822CFA04;
	// lhz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 16);
	// cmplwi cr6,r11,22
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 22, ctx.xer);
	// blt cr6,0x822cfa00
	if (ctx.cr6.lt) goto loc_822CFA00;
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822cfa04
	if (ctx.cr6.eq) goto loc_822CFA04;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822CF9D8:
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// addi r9,r11,-1
	ctx.r9.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// and. r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 & ctx.r11.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// bne 0x822cf9d8
	if (!ctx.cr0.eq) goto loc_822CF9D8;
	// lhz r11,2(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 2);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x822cfa04
	if (ctx.cr6.eq) goto loc_822CFA04;
loc_822CFA00:
	// li r30,0
	ctx.r30.s64 = 0;
loc_822CFA04:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822CFA20"))) PPC_WEAK_FUNC(sub_822CFA20);
PPC_FUNC_IMPL(__imp__sub_822CFA20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e434
	ctx.lr = 0x822CFA28;
	__restfpr_15(ctx, base);
	// addi r12,r1,-144
	ctx.r12.s64 = ctx.r1.s64 + -144;
	// bl 0x8233fa2c
	ctx.lr = 0x822CFA30;
	sub_8233FA2C(ctx, base);
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r3
	ctx.r23.u64 = ctx.r3.u64;
	// mr r18,r4
	ctx.r18.u64 = ctx.r4.u64;
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// mr r20,r6
	ctx.r20.u64 = ctx.r6.u64;
	// mr r17,r7
	ctx.r17.u64 = ctx.r7.u64;
	// li r15,0
	ctx.r15.s64 = 0;
	// li r21,0
	ctx.r21.s64 = 0;
	// li r19,0
	ctx.r19.s64 = 0;
	// bl 0x82082c78
	ctx.lr = 0x822CFA60;
	sub_82082C78(ctx, base);
	// mr. r16,r3
	ctx.r16.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r16.s32, 0, ctx.xer);
	// beq 0x822cfa94
	if (ctx.cr0.eq) goto loc_822CFA94;
	// rlwinm r31,r18,2,0,29
	ctx.r31.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82082c78
	ctx.lr = 0x822CFA78;
	sub_82082C78(ctx, base);
	// mr. r21,r3
	ctx.r21.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r21.s32, 0, ctx.xer);
	// beq 0x822cfa94
	if (ctx.cr0.eq) goto loc_822CFA94;
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82082c78
	ctx.lr = 0x822CFA8C;
	sub_82082C78(ctx, base);
	// mr. r19,r3
	ctx.r19.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// bne 0x822cfaa0
	if (!ctx.cr0.eq) goto loc_822CFAA0;
loc_822CFA94:
	// lis r15,-32761
	ctx.r15.s64 = -2147024896;
	// ori r15,r15,14
	ctx.r15.u64 = ctx.r15.u64 | 14;
	// b 0x822cfef4
	goto loc_822CFEF4;
loc_822CFAA0:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r23,0
	ctx.cr6.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// ble cr6,0x822cfac0
	if (!ctx.cr6.gt) goto loc_822CFAC0;
	// addi r11,r16,-4
	ctx.r11.s64 = ctx.r16.s64 + -4;
	// mtctr r23
	ctx.ctr.u64 = ctx.r23.u64;
loc_822CFAB4:
	// stwu r10,4(r11)
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r11.u32 = ea;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x822cfab4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CFAB4;
loc_822CFAC0:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r18,0
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 0, ctx.xer);
	// ble cr6,0x822cfae0
	if (!ctx.cr6.gt) goto loc_822CFAE0;
	// addi r11,r21,-4
	ctx.r11.s64 = ctx.r21.s64 + -4;
	// mtctr r18
	ctx.ctr.u64 = ctx.r18.u64;
loc_822CFAD4:
	// stwu r10,4(r11)
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r11.u32 = ea;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x822cfad4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CFAD4;
loc_822CFAE0:
	// addi r6,r23,-1
	ctx.r6.s64 = ctx.r23.s64 + -1;
loc_822CFAE4:
	// li r7,0
	ctx.r7.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// ble cr6,0x822cfb40
	if (!ctx.cr6.gt) goto loc_822CFB40;
	// mr r10,r16
	ctx.r10.u64 = ctx.r16.u64;
	// addi r11,r30,4
	ctx.r11.s64 = ctx.r30.s64 + 4;
	// subf r9,r30,r16
	ctx.r9.s64 = ctx.r16.s64 - ctx.r30.s64;
loc_822CFB00:
	// lfs f0,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// ble cr6,0x822cfb2c
	if (!ctx.cr6.gt) goto loc_822CFB2C;
	// stfs f13,-4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// li r7,1
	ctx.r7.s64 = 1;
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lwz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwzx r4,r9,r11
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// stw r4,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
	// stwx r5,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, ctx.r5.u32);
loc_822CFB2C:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r8,r6
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, ctx.xer);
	// blt cr6,0x822cfb00
	if (ctx.cr6.lt) goto loc_822CFB00;
loc_822CFB40:
	// clrlwi. r11,r7,24
	ctx.r11.u64 = ctx.r7.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822cfae4
	if (!ctx.cr0.eq) goto loc_822CFAE4;
	// addi r6,r18,-1
	ctx.r6.s64 = ctx.r18.s64 + -1;
loc_822CFB4C:
	// li r7,0
	ctx.r7.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// ble cr6,0x822cfba8
	if (!ctx.cr6.gt) goto loc_822CFBA8;
	// mr r10,r21
	ctx.r10.u64 = ctx.r21.u64;
	// addi r11,r20,4
	ctx.r11.s64 = ctx.r20.s64 + 4;
	// subf r9,r20,r21
	ctx.r9.s64 = ctx.r21.s64 - ctx.r20.s64;
loc_822CFB68:
	// lfs f0,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// ble cr6,0x822cfb94
	if (!ctx.cr6.gt) goto loc_822CFB94;
	// stfs f13,-4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// li r7,1
	ctx.r7.s64 = 1;
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lwzx r5,r11,r9
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r5,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r5.u32);
	// stwx r4,r11,r9
	PPC_STORE_U32(ctx.r11.u32 + ctx.r9.u32, ctx.r4.u32);
loc_822CFB94:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r8,r6
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, ctx.xer);
	// blt cr6,0x822cfb68
	if (ctx.cr6.lt) goto loc_822CFB68;
loc_822CFBA8:
	// clrlwi. r11,r7,24
	ctx.r11.u64 = ctx.r7.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822cfb4c
	if (!ctx.cr0.eq) goto loc_822CFB4C;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// mr r7,r23
	ctx.r7.u64 = ctx.r23.u64;
	// mr r24,r18
	ctx.r24.u64 = ctx.r18.u64;
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r23,0
	ctx.cr6.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// lfs f29,-28948(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f29.f64 = double(temp.f32);
	// ble cr6,0x822cfbf4
	if (!ctx.cr6.gt) goto loc_822CFBF4;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
loc_822CFBD0:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f29
	ctx.cr6.compare(ctx.f0.f64, ctx.f29.f64);
	// bge cr6,0x822cfbf0
	if (!ctx.cr6.lt) goto loc_822CFBF0;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r23
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r23.s32, ctx.xer);
	// blt cr6,0x822cfbd0
	if (ctx.cr6.lt) goto loc_822CFBD0;
	// b 0x822cfbf4
	goto loc_822CFBF4;
loc_822CFBF0:
	// mr r7,r11
	ctx.r7.u64 = ctx.r11.u64;
loc_822CFBF4:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r18,0
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 0, ctx.xer);
	// ble cr6,0x822cfc28
	if (!ctx.cr6.gt) goto loc_822CFC28;
	// mr r10,r20
	ctx.r10.u64 = ctx.r20.u64;
loc_822CFC04:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f29
	ctx.cr6.compare(ctx.f0.f64, ctx.f29.f64);
	// bge cr6,0x822cfc24
	if (!ctx.cr6.lt) goto loc_822CFC24;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r18
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r18.s32, ctx.xer);
	// blt cr6,0x822cfc04
	if (ctx.cr6.lt) goto loc_822CFC04;
	// b 0x822cfc28
	goto loc_822CFC28;
loc_822CFC24:
	// mr r24,r11
	ctx.r24.u64 = ctx.r11.u64;
loc_822CFC28:
	// subf r26,r24,r18
	ctx.r26.s64 = ctx.r18.s64 - ctx.r24.s64;
	// cmpwi cr6,r18,0
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 0, ctx.xer);
	// ble cr6,0x822cfc78
	if (!ctx.cr6.gt) goto loc_822CFC78;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r11,r19
	ctx.r11.u64 = ctx.r19.u64;
	// mr r8,r18
	ctx.r8.u64 = ctx.r18.u64;
loc_822CFC40:
	// stfs f29,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r23,0
	ctx.cr6.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// ble cr6,0x822cfc68
	if (!ctx.cr6.gt) goto loc_822CFC68;
	// mtctr r23
	ctx.ctr.u64 = ctx.r23.u64;
loc_822CFC54:
	// add r6,r10,r9
	ctx.r6.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f29,r6,r17
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r6.u32 + ctx.r17.u32, temp.u32);
	// bdnz 0x822cfc54
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CFC54;
loc_822CFC68:
	// addic. r8,r8,-1
	ctx.xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + ctx.r23.u64;
	// bne 0x822cfc40
	if (!ctx.cr0.eq) goto loc_822CFC40;
loc_822CFC78:
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// ble cr6,0x822cfef4
	if (!ctx.cr6.gt) goto loc_822CFEF4;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// mr r22,r7
	ctx.r22.u64 = ctx.r7.u64;
	// cmpw cr6,r7,r23
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r23.s32, ctx.xer);
	// lfs f25,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f25.f64 = double(temp.f32);
	// bge cr6,0x822cfe40
	if (!ctx.cr6.lt) goto loc_822CFE40;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// add r29,r11,r16
	ctx.r29.u64 = ctx.r11.u64 + ctx.r16.u64;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// subf r25,r16,r30
	ctx.r25.s64 = ctx.r30.s64 - ctx.r16.s64;
	// lfs f26,-17964(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -17964);
	ctx.f26.f64 = double(temp.f32);
	// lfd f28,-17944(r11)
	ctx.f28.u64 = PPC_LOAD_U64(ctx.r11.u32 + -17944);
	// lfs f27,-17960(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -17960);
	ctx.f27.f64 = double(temp.f32);
loc_822CFCB8:
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
	// cmpw cr6,r24,r18
	ctx.cr6.compare<int32_t>(ctx.r24.s32, ctx.r18.s32, ctx.xer);
	// bge cr6,0x822cfcec
	if (!ctx.cr6.lt) goto loc_822CFCEC;
	// rlwinm r10,r24,2,0,29
	ctx.r10.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r25,r29
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r29.u32);
	ctx.f0.f64 = double(temp.f32);
	// add r10,r10,r20
	ctx.r10.u64 = ctx.r10.u64 + ctx.r20.u64;
loc_822CFCD0:
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// ble cr6,0x822cfcec
	if (!ctx.cr6.gt) goto loc_822CFCEC;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r18
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r18.s32, ctx.xer);
	// blt cr6,0x822cfcd0
	if (ctx.cr6.lt) goto loc_822CFCD0;
loc_822CFCEC:
	// subf r11,r24,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r24.s64;
	// add r10,r11,r26
	ctx.r10.u64 = ctx.r11.u64 + ctx.r26.u64;
	// divw r9,r11,r26
	ctx.r9.s32 = ctx.r11.s32 / ctx.r26.s32;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mullw r9,r9,r26
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r26.s32);
	// divw r8,r10,r26
	ctx.r8.s32 = ctx.r10.s32 / ctx.r26.s32;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// mullw r9,r8,r26
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r26.s32);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// add r28,r11,r24
	ctx.r28.u64 = ctx.r11.u64 + ctx.r24.u64;
	// add r27,r10,r24
	ctx.r27.u64 = ctx.r10.u64 + ctx.r24.u64;
	// rlwinm r30,r28,2,0,29
	ctx.r30.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r31,r27,2,0,29
	ctx.r31.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r30,r20
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r20.u32);
	ctx.f0.f64 = double(temp.f32);
	// lfsx f13,r31,r20
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r20.u32);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f31,f0,f13
	ctx.f31.f64 = static_cast<float>(ctx.f0.f64 - ctx.f13.f64);
	// fmuls f1,f31,f27
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// bl 0x8233ca30
	ctx.lr = 0x822CFD34;
	sub_8233CA30(ctx, base);
	// lfsx f0,r25,r29
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r29.u32);
	ctx.f0.f64 = double(temp.f32);
	// frsp f13,f1
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// lfsx f12,r31,r20
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r20.u32);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f30,f0,f12
	ctx.f30.f64 = static_cast<float>(ctx.f0.f64 - ctx.f12.f64);
	// fnmsubs f31,f13,f26,f31
	ctx.f31.f64 = -double(std::fma(float(ctx.f13.f64), float(ctx.f26.f64), -float(ctx.f31.f64)));
	// fmuls f1,f30,f27
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// bl 0x8233ca30
	ctx.lr = 0x822CFD50;
	sub_8233CA30(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// cmpw cr6,r27,r28
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r28.s32, ctx.xer);
	// fnmsubs f0,f0,f26,f30
	ctx.f0.f64 = -double(std::fma(float(ctx.f0.f64), float(ctx.f26.f64), -float(ctx.f30.f64)));
	// bne cr6,0x822cfd8c
	if (!ctx.cr6.eq) goto loc_822CFD8C;
	// lwzx r10,r31,r21
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mullw r10,r10,r23
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r23.s32);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f25,r11,r17
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r17.u32, temp.u32);
	// lwzx r11,r31,r21
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r11,r19
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r19.u32);
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f0,f25
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f25.f64));
	// b 0x822cfe2c
	goto loc_822CFE2C;
loc_822CFD8C:
	// fcmpu cr6,f31,f29
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f31.f64, ctx.f29.f64);
	// beq cr6,0x822cff2c
	if (ctx.cr6.eq) goto loc_822CFF2C;
	// fdivs f30,f0,f31
	ctx.f30.f64 = double(float(ctx.f0.f64 / ctx.f31.f64));
	// fmul f1,f30,f28
	ctx.f1.f64 = ctx.f30.f64 * ctx.f28.f64;
	// bl 0x8233c950
	ctx.lr = 0x822CFDA0;
	sub_8233C950(ctx, base);
	// frsp f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = double(float(ctx.f1.f64));
	// fcmpu cr6,f31,f29
	ctx.cr6.compare(ctx.f31.f64, ctx.f29.f64);
	// bge cr6,0x822cfdb0
	if (!ctx.cr6.lt) goto loc_822CFDB0;
	// fmr f31,f29
	ctx.f31.f64 = ctx.f29.f64;
loc_822CFDB0:
	// fmul f1,f30,f28
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f30.f64 * ctx.f28.f64;
	// bl 0x8233c870
	ctx.lr = 0x822CFDB8;
	sub_8233C870(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// fcmpu cr6,f0,f29
	ctx.cr6.compare(ctx.f0.f64, ctx.f29.f64);
	// bge cr6,0x822cfdc8
	if (!ctx.cr6.lt) goto loc_822CFDC8;
	// fmr f0,f29
	ctx.f0.f64 = ctx.f29.f64;
loc_822CFDC8:
	// cmpwi cr6,r18,1
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 1, ctx.xer);
	// bne cr6,0x822cfdd8
	if (!ctx.cr6.eq) goto loc_822CFDD8;
	// fmr f0,f25
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f25.f64;
	// fmr f31,f25
	ctx.f31.f64 = ctx.f25.f64;
loc_822CFDD8:
	// lwzx r10,r31,r21
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mullw r10,r10,r23
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r23.s32);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f31,r11,r17
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r17.u32, temp.u32);
	// lwzx r10,r30,r21
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r21.u32);
	// mullw r10,r23,r10
	ctx.r10.s64 = int64_t(ctx.r23.s32) * int64_t(ctx.r10.s32);
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f0,r11,r17
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r17.u32, temp.u32);
	// lwzx r11,r31,r21
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r11,r19
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r19.u32);
	ctx.f13.f64 = double(temp.f32);
	// fadds f13,f13,f31
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f31.f64));
	// stfsx f13,r11,r19
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r19.u32, temp.u32);
	// lwzx r11,r30,r21
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r21.u32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r11,r19
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r19.u32);
	ctx.f13.f64 = double(temp.f32);
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
loc_822CFE2C:
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// stfsx f0,r11,r19
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r19.u32, temp.u32);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// cmpw cr6,r22,r23
	ctx.cr6.compare<int32_t>(ctx.r22.s32, ctx.r23.s32, ctx.xer);
	// blt cr6,0x822cfcb8
	if (ctx.cr6.lt) goto loc_822CFCB8;
loc_822CFE40:
	// fmr f10,f29
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = ctx.f29.f64;
	// cmpwi cr6,r18,0
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 0, ctx.xer);
	// ble cr6,0x822cfef4
	if (!ctx.cr6.gt) goto loc_822CFEF4;
	// mr r11,r19
	ctx.r11.u64 = ctx.r19.u64;
	// mtctr r18
	ctx.ctr.u64 = ctx.r18.u64;
loc_822CFE54:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f10
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// ble cr6,0x822cfe64
	if (!ctx.cr6.gt) goto loc_822CFE64;
	// fmr f10,f0
	ctx.f10.f64 = ctx.f0.f64;
loc_822CFE64:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822cfe54
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CFE54;
	// fcmpu cr6,f10,f29
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f10.f64, ctx.f29.f64);
	// beq cr6,0x822cfef4
	if (ctx.cr6.eq) goto loc_822CFEF4;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r8,r18
	ctx.r8.u64 = ctx.r18.u64;
	// lfs f11,-17952(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -17952);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,5268(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5268);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,-17956(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -17956);
	ctx.f13.f64 = double(temp.f32);
loc_822CFE94:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r23,0
	ctx.cr6.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// ble cr6,0x822cfee8
	if (!ctx.cr6.gt) goto loc_822CFEE8;
	// fdivs f0,f25,f10
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f25.f64 / ctx.f10.f64));
	// mtctr r23
	ctx.ctr.u64 = ctx.r23.u64;
loc_822CFEA8:
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f9,r10,r17
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r17.u32);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f9,f0,f9
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// fmadds f9,f9,f13,f12
	ctx.f9.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f13.f64), float(ctx.f12.f64)));
	// fctiwz f9,f9
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f9.f64)));
	// stfd f9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f9.u64);
	// lwa r7,84(r1)
	ctx.r7.s64 = int32_t(PPC_LOAD_U32(ctx.r1.u32 + 84));
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// lfd f9,88(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// fmuls f9,f9,f11
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// stfsx f9,r10,r17
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r17.u32, temp.u32);
	// bdnz 0x822cfea8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822CFEA8;
loc_822CFEE8:
	// addic. r8,r8,-1
	ctx.xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + ctx.r23.u64;
	// bne 0x822cfe94
	if (!ctx.cr0.eq) goto loc_822CFE94;
loc_822CFEF4:
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// mr r3,r16
	ctx.r3.u64 = ctx.r16.u64;
	// bl 0x82082d58
	ctx.lr = 0x822CFF00;
	sub_82082D58(ctx, base);
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// mr r3,r21
	ctx.r3.u64 = ctx.r21.u64;
	// bl 0x82082d58
	ctx.lr = 0x822CFF0C;
	sub_82082D58(ctx, base);
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// bl 0x82082d58
	ctx.lr = 0x822CFF18;
	sub_82082D58(ctx, base);
	// mr r3,r15
	ctx.r3.u64 = ctx.r15.u64;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// addi r12,r1,-144
	ctx.r12.s64 = ctx.r1.s64 + -144;
	// bl 0x8233fa78
	ctx.lr = 0x822CFF28;
	__savefpr_25(ctx, base);
	// b 0x8233e484
	__restgprlr_15(ctx, base);
	return;
loc_822CFF2C:
	// lis r15,-32761
	ctx.r15.s64 = -2147024896;
	// ori r15,r15,87
	ctx.r15.u64 = ctx.r15.u64 | 87;
	// b 0x822cfef4
	goto loc_822CFEF4;
}

__attribute__((alias("__imp__sub_822CFF38"))) PPC_WEAK_FUNC(sub_822CFF38);
PPC_FUNC_IMPL(__imp__sub_822CFF38) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e450
	ctx.lr = 0x822CFF40;
	__restfpr_22(ctx, base);
	// stfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.f31.u64);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r23,r5
	ctx.r23.u64 = ctx.r5.u64;
	// mr r22,r6
	ctx.r22.u64 = ctx.r6.u64;
	// mr r24,r7
	ctx.r24.u64 = ctx.r7.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x822d03f4
	if (ctx.cr6.eq) goto loc_822D03F4;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// beq cr6,0x822d03f4
	if (ctx.cr6.eq) goto loc_822D03F4;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x822cff94
	if (ctx.cr6.eq) goto loc_822CFF94;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822CFF7C:
	// addi r9,r11,-1
	ctx.r9.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// and. r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 & ctx.r11.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822cff7c
	if (!ctx.cr0.eq) goto loc_822CFF7C;
	// cmpw cr6,r28,r10
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r10.s32, ctx.xer);
	// bne cr6,0x822d03f4
	if (!ctx.cr6.eq) goto loc_822D03F4;
loc_822CFF94:
	// cmplwi cr6,r22,0
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, 0, ctx.xer);
	// beq cr6,0x822cffbc
	if (ctx.cr6.eq) goto loc_822CFFBC;
	// mr r11,r22
	ctx.r11.u64 = ctx.r22.u64;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822CFFA4:
	// addi r9,r11,-1
	ctx.r9.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// and. r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 & ctx.r11.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822cffa4
	if (!ctx.cr0.eq) goto loc_822CFFA4;
	// cmpw cr6,r27,r10
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r10.s32, ctx.xer);
	// bne cr6,0x822d03f4
	if (!ctx.cr6.eq) goto loc_822D03F4;
loc_822CFFBC:
	// cmplwi cr6,r23,63
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 63, ctx.xer);
	// beq cr6,0x822cffcc
	if (ctx.cr6.eq) goto loc_822CFFCC;
	// cmplwi cr6,r23,1551
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 1551, ctx.xer);
	// bne cr6,0x822cffdc
	if (!ctx.cr6.eq) goto loc_822CFFDC;
loc_822CFFCC:
	// cmplwi cr6,r22,63
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, 63, ctx.xer);
	// beq cr6,0x822cffe4
	if (ctx.cr6.eq) goto loc_822CFFE4;
	// cmplwi cr6,r22,1551
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, 1551, ctx.xer);
	// beq cr6,0x822cffe4
	if (ctx.cr6.eq) goto loc_822CFFE4;
loc_822CFFDC:
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x822cffe8
	goto loc_822CFFE8;
loc_822CFFE4:
	// li r11,1
	ctx.r11.s64 = 1;
loc_822CFFE8:
	// cmplwi cr6,r23,0
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 0, ctx.xer);
	// beq cr6,0x822cfff8
	if (ctx.cr6.eq) goto loc_822CFFF8;
	// cmplwi cr6,r22,0
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, 0, ctx.xer);
	// bne cr6,0x822cfffc
	if (!ctx.cr6.eq) goto loc_822CFFFC;
loc_822CFFF8:
	// li r11,1
	ctx.r11.s64 = 1;
loc_822CFFFC:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822d0060
	if (ctx.cr6.eq) goto loc_822D0060;
	// mullw r11,r28,r27
	ctx.r11.s64 = int64_t(ctx.r28.s32) * int64_t(ctx.r27.s32);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822D0018;
	sub_8233EAF0(ctx, base);
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// cmpw cr6,r27,r28
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r28.s32, ctx.xer);
	// bge cr6,0x822d0028
	if (!ctx.cr6.lt) goto loc_822D0028;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_822D0028:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x822d0058
	if (!ctx.cr6.gt) goto loc_822D0058;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r9,r28,1
	ctx.r9.s64 = ctx.r28.s64 + 1;
	// lfs f0,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
loc_822D0044:
	// mullw r11,r9,r10
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stfsx f0,r11,r24
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r24.u32, temp.u32);
	// bdnz 0x822d0044
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D0044;
loc_822D0058:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x822d03fc
	goto loc_822D03FC;
loc_822D0060:
	// cmplwi cr6,r23,4
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 4, ctx.xer);
	// bne cr6,0x822d00ac
	if (!ctx.cr6.eq) goto loc_822D00AC;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// cmpwi cr6,r27,1
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 1, ctx.xer);
	// lfs f0,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r24)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r24.u32 + 0, temp.u32);
	// ble cr6,0x822d0058
	if (!ctx.cr6.gt) goto loc_822D0058;
	// stfs f0,4(r24)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r24.u32 + 4, temp.u32);
	// cmpwi cr6,r27,2
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 2, ctx.xer);
	// ble cr6,0x822d0058
	if (!ctx.cr6.gt) goto loc_822D0058;
	// addi r10,r24,8
	ctx.r10.s64 = ctx.r24.s64 + 8;
	// addic. r11,r27,-2
	ctx.xer.ca = ctx.r27.u32 > 1;
	ctx.r11.s64 = ctx.r27.s64 + -2;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// li r9,0
	ctx.r9.s64 = 0;
	// beq 0x822d0058
	if (ctx.cr0.eq) goto loc_822D0058;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
loc_822D00A0:
	// stwu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d00a0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D00A0;
	// b 0x822d0058
	goto loc_822D0058;
loc_822D00AC:
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// lfs f0,-17904(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -17904);
	ctx.f0.f64 = double(temp.f32);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lfs f13,-17908(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -17908);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lfs f31,-28948(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f12,-17912(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -17912);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfs f11,-17916(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -17916);
	ctx.f11.f64 = double(temp.f32);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// lfs f10,-17920(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -17920);
	ctx.f10.f64 = double(temp.f32);
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// lfs f9,-17924(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -17924);
	ctx.f9.f64 = double(temp.f32);
	// rlwinm r3,r28,2,0,29
	ctx.r3.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f8,11152(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 11152);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,-17928(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -17928);
	ctx.f7.f64 = double(temp.f32);
	// li r31,0
	ctx.r31.s64 = 0;
	// lfs f6,-17932(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -17932);
	ctx.f6.f64 = double(temp.f32);
	// li r30,0
	ctx.r30.s64 = 0;
	// lfs f5,-17936(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -17936);
	ctx.f5.f64 = double(temp.f32);
	// li r29,1
	ctx.r29.s64 = 1;
	// stfs f0,112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// li r25,0
	ctx.r25.s64 = 0;
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f31,120(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f12,124(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// stfs f11,128(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// stfs f10,132(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// stfs f9,136(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f8,140(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// stfs f7,144(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// stfs f6,148(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f5,152(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// bl 0x82082c78
	ctx.lr = 0x822D014C;
	sub_82082C78(ctx, base);
	// mr. r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// beq 0x822d0168
	if (ctx.cr0.eq) goto loc_822D0168;
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// rlwinm r3,r27,2,0,29
	ctx.r3.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82082c78
	ctx.lr = 0x822D0160;
	sub_82082C78(ctx, base);
	// mr. r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// bne 0x822d0174
	if (!ctx.cr0.eq) goto loc_822D0174;
loc_822D0168:
	// lis r31,-32761
	ctx.r31.s64 = -2147024896;
	// ori r31,r31,14
	ctx.r31.u64 = ctx.r31.u64 | 14;
	// b 0x822d03d4
	goto loc_822D03D4;
loc_822D0174:
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r9,r25,-4
	ctx.r9.s64 = ctx.r25.s64 + -4;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// addi r8,r26,-4
	ctx.r8.s64 = ctx.r26.s64 + -4;
loc_822D0184:
	// and. r7,r29,r23
	ctx.r7.u64 = ctx.r29.u64 & ctx.r23.u64;
	ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq 0x822d01a0
	if (ctx.cr0.eq) goto loc_822D01A0;
	// cmpw cr6,r31,r28
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r28.s32, ctx.xer);
	// beq cr6,0x822d0300
	if (ctx.cr6.eq) goto loc_822D0300;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// stfsu f0,4(r8)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r8.u32 = ea;
loc_822D01A0:
	// and. r7,r29,r22
	ctx.r7.u64 = ctx.r29.u64 & ctx.r22.u64;
	ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq 0x822d01bc
	if (ctx.cr0.eq) goto loc_822D01BC;
	// cmpw cr6,r30,r27
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r27.s32, ctx.xer);
	// beq cr6,0x822d0300
	if (ctx.cr6.eq) goto loc_822D0300;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// stfsu f0,4(r9)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
loc_822D01BC:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r29,r29,1,0,30
	ctx.r29.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplwi cr6,r10,11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 11, ctx.xer);
	// blt cr6,0x822d0184
	if (ctx.cr6.lt) goto loc_822D0184;
	// cmpw cr6,r31,r28
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r28.s32, ctx.xer);
	// bge cr6,0x822d01fc
	if (!ctx.cr6.lt) goto loc_822D01FC;
	// rlwinm r10,r31,2,0,29
	ctx.r10.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// subf. r11,r31,r28
	ctx.r11.s64 = ctx.r28.s64 - ctx.r31.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// add r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 + ctx.r26.u64;
	// lis r9,-15436
	ctx.r9.s64 = -1011613696;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// beq 0x822d01fc
	if (ctx.cr0.eq) goto loc_822D01FC;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
loc_822D01F4:
	// stwu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d01f4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D01F4;
loc_822D01FC:
	// cmpw cr6,r30,r27
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r27.s32, ctx.xer);
	// bge cr6,0x822d0228
	if (!ctx.cr6.lt) goto loc_822D0228;
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// subf. r11,r30,r27
	ctx.r11.s64 = ctx.r27.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + ctx.r25.u64;
	// lis r9,-15436
	ctx.r9.s64 = -1011613696;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// beq 0x822d0228
	if (ctx.cr0.eq) goto loc_822D0228;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
loc_822D0220:
	// stwu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d0220
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D0220;
loc_822D0228:
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x822cfa20
	ctx.lr = 0x822D0240;
	sub_822CFA20(ctx, base);
	// mr. r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// blt 0x822d03d4
	if (ctx.cr0.lt) goto loc_822D03D4;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f11,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f11.f64 = double(temp.f32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// cmplwi cr6,r6,11
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 11, ctx.xer);
	// bge cr6,0x822d0278
	if (!ctx.cr6.lt) goto loc_822D0278;
loc_822D026C:
	// lfs f0,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// bge cr6,0x822d039c
	if (!ctx.cr6.lt) goto loc_822D039C;
loc_822D0278:
	// and. r11,r7,r23
	ctx.r11.u64 = ctx.r7.u64 & ctx.r23.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d0294
	if (ctx.cr0.eq) goto loc_822D0294;
	// and. r10,r7,r22
	ctx.r10.u64 = ctx.r7.u64 & ctx.r22.u64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x822d0294
	if (ctx.cr0.eq) goto loc_822D0294;
	// add r10,r4,r5
	ctx.r10.u64 = ctx.r4.u64 + ctx.r5.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f11,r10,r24
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r24.u32, temp.u32);
loc_822D0294:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822d02b4
	if (!ctx.cr6.eq) goto loc_822D02B4;
	// and. r10,r7,r22
	ctx.r10.u64 = ctx.r7.u64 & ctx.r22.u64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x822d02ac
	if (ctx.cr0.eq) goto loc_822D02AC;
	// rlwinm. r10,r7,0,28,28
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x8;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x822d03cc
	if (ctx.cr0.eq) goto loc_822D03CC;
loc_822D02AC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822d039c
	if (ctx.cr6.eq) goto loc_822D039C;
loc_822D02B4:
	// and. r11,r7,r22
	ctx.r11.u64 = ctx.r7.u64 & ctx.r22.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822d039c
	if (!ctx.cr0.eq) goto loc_822D039C;
	// rlwinm. r11,r7,0,28,28
	ctx.r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x8;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d03cc
	if (ctx.cr0.eq) goto loc_822D03CC;
	// cmpwi cr6,r28,1
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 1, ctx.xer);
	// bne cr6,0x822d030c
	if (!ctx.cr6.eq) goto loc_822D030C;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// ble cr6,0x822d039c
	if (!ctx.cr6.gt) goto loc_822D039C;
	// extsw r10,r27
	ctx.r10.s64 = ctx.r27.s32;
	// mtctr r27
	ctx.ctr.u64 = ctx.r27.u64;
	// addi r11,r24,-4
	ctx.r11.s64 = ctx.r24.s64 + -4;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64));
	// fdivs f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 / ctx.f0.f64));
loc_822D02F4:
	// stfsu f0,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// bdnz 0x822d02f4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D02F4;
	// b 0x822d039c
	goto loc_822D039C;
loc_822D0300:
	// lis r31,-32761
	ctx.r31.s64 = -2147024896;
	// ori r31,r31,87
	ctx.r31.u64 = ctx.r31.u64 | 87;
	// b 0x822d03d4
	goto loc_822D03D4;
loc_822D030C:
	// addi r11,r28,-1
	ctx.r11.s64 = ctx.r28.s64 + -1;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64));
	// fdivs f12,f11,f0
	ctx.f12.f64 = double(float(ctx.f11.f64 / ctx.f0.f64));
	// ble cr6,0x822d039c
	if (!ctx.cr6.gt) goto loc_822D039C;
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r8,r27
	ctx.r8.u64 = ctx.r27.u64;
loc_822D0338:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x822d0390
	if (!ctx.cr6.gt) goto loc_822D0390;
	// extsw r10,r27
	ctx.r10.s64 = ctx.r27.s32;
	// mtctr r28
	ctx.ctr.u64 = ctx.r28.u64;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(ctx.f0.f64));
	// fadds f0,f13,f12
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
loc_822D0360:
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + ctx.r11.u64;
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bne cr6,0x822d0378
	if (!ctx.cr6.eq) goto loc_822D0378;
	// fdivs f10,f12,f0
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f12.f64 / ctx.f0.f64));
	// b 0x822d0384
	goto loc_822D0384;
loc_822D0378:
	// lfsx f10,r10,r24
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r24.u32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f10,f10,f13
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fdivs f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 / ctx.f0.f64));
loc_822D0384:
	// stfsx f10,r10,r24
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r24.u32, temp.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// bdnz 0x822d0360
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D0360;
loc_822D0390:
	// addic. r8,r8,-1
	ctx.xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + ctx.r28.u64;
	// bne 0x822d0338
	if (!ctx.cr0.eq) goto loc_822D0338;
loc_822D039C:
	// and. r11,r7,r23
	ctx.r11.u64 = ctx.r7.u64 & ctx.r23.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d03a8
	if (ctx.cr0.eq) goto loc_822D03A8;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
loc_822D03A8:
	// and. r11,r7,r22
	ctx.r11.u64 = ctx.r7.u64 & ctx.r22.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d03b4
	if (ctx.cr0.eq) goto loc_822D03B4;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + ctx.r28.u64;
loc_822D03B4:
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// cmplwi cr6,r6,11
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 11, ctx.xer);
	// blt cr6,0x822d026c
	if (ctx.cr6.lt) goto loc_822D026C;
	// b 0x822d03d4
	goto loc_822D03D4;
loc_822D03CC:
	// lis r31,-32768
	ctx.r31.s64 = -2147483648;
	// ori r31,r31,16385
	ctx.r31.u64 = ctx.r31.u64 | 16385;
loc_822D03D4:
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x82082d58
	ctx.lr = 0x822D03E0;
	sub_82082D58(ctx, base);
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x82082d58
	ctx.lr = 0x822D03EC;
	sub_82082D58(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// b 0x822d03fc
	goto loc_822D03FC;
loc_822D03F4:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
loc_822D03FC:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8233e4a0
	__restgprlr_22(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D0408"))) PPC_WEAK_FUNC(sub_822D0408);
PPC_FUNC_IMPL(__imp__sub_822D0408) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822D0410;
	__restfpr_28(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lhz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,357
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 357, ctx.xer);
	// beq cr6,0x822d043c
	if (ctx.cr6.eq) goto loc_822D043C;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// beq cr6,0x822d043c
	if (ctx.cr6.eq) goto loc_822D043C;
	// lhz r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 16);
	// addi r28,r11,18
	ctx.r28.s64 = ctx.r11.s64 + 18;
	// cmplwi cr6,r28,40
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 40, ctx.xer);
	// bge cr6,0x822d0440
	if (!ctx.cr6.lt) goto loc_822D0440;
loc_822D043C:
	// li r28,40
	ctx.r28.s64 = 40;
loc_822D0440:
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82082c78
	ctx.lr = 0x822D044C;
	sub_82082C78(ctx, base);
	// mr. r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq 0x822d0518
	if (ctx.cr0.eq) goto loc_822D0518;
	// lhz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// cmplwi cr6,r11,357
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 357, ctx.xer);
	// bne cr6,0x822d0468
	if (!ctx.cr6.eq) goto loc_822D0468;
	// li r30,32
	ctx.r30.s64 = 32;
	// b 0x822d0480
	goto loc_822D0480;
loc_822D0468:
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x822d0478
	if (!ctx.cr6.eq) goto loc_822D0478;
	// li r30,16
	ctx.r30.s64 = 16;
	// b 0x822d0480
	goto loc_822D0480;
loc_822D0478:
	// lhz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 16);
	// addi r30,r11,18
	ctx.r30.s64 = ctx.r11.s64 + 18;
loc_822D0480:
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233e4e0
	ctx.lr = 0x822D0490;
	sub_8233E4E0(ctx, base);
	// subf r5,r30,r28
	ctx.r5.s64 = ctx.r28.s64 - ctx.r30.s64;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r30,r29
	ctx.r3.u64 = ctx.r30.u64 + ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822D04A0;
	sub_8233EAF0(ctx, base);
	// lhz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// cmplwi cr6,r11,65534
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 65534, ctx.xer);
	// beq cr6,0x822d0518
	if (ctx.cr6.eq) goto loc_822D0518;
	// lhz r11,14(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 14);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// sth r11,18(r29)
	PPC_STORE_U16(ctx.r29.u32 + 18, ctx.r11.u16);
	// bl 0x822cf4e8
	ctx.lr = 0x822D04C0;
	sub_822CF4E8(ctx, base);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// li r10,22
	ctx.r10.s64 = 22;
	// stw r3,20(r29)
	PPC_STORE_U32(ctx.r29.u32 + 20, ctx.r3.u32);
	// addi r9,r11,-1544
	ctx.r9.s64 = ctx.r11.s64 + -1544;
	// li r8,-2
	ctx.r8.s64 = -2;
	// sth r10,16(r29)
	PPC_STORE_U16(ctx.r29.u32 + 16, ctx.r10.u16);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// sth r8,0(r29)
	PPC_STORE_U16(ctx.r29.u32 + 0, ctx.r8.u16);
	// addi r3,r29,24
	ctx.r3.s64 = ctx.r29.s64 + 24;
	// lwz r11,-1544(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -1544);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r6,12(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lhz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r11.u32);
	// stw r9,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r9.u32);
	// stw r6,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r6.u32);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// bl 0x8233e4e0
	ctx.lr = 0x822D0518;
	sub_8233E4E0(ctx, base);
loc_822D0518:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D0524"))) PPC_WEAK_FUNC(sub_822D0524);
PPC_FUNC_IMPL(__imp__sub_822D0524) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D0528"))) PPC_WEAK_FUNC(sub_822D0528);
PPC_FUNC_IMPL(__imp__sub_822D0528) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822D0530;
	__restfpr_28(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// bl 0x822cf808
	ctx.lr = 0x822D0548;
	sub_822CF808(ctx, base);
	// cmpwi r3,0
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne 0x822d055c
	if (!ctx.cr0.eq) goto loc_822D055C;
	// lis r30,-32761
	ctx.r30.s64 = -2147024896;
	// ori r30,r30,87
	ctx.r30.u64 = ctx.r30.u64 | 87;
	// b 0x822d06a8
	goto loc_822D06A8;
loc_822D055C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822cf108
	ctx.lr = 0x822D0564;
	sub_822CF108(ctx, base);
	// clrlwi r11,r3,16
	ctx.r11.u64 = ctx.r3.u32 & 0xFFFF;
	// lis r10,-30569
	ctx.r10.s64 = -2003369984;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// ori r28,r10,1
	ctx.r28.u64 = ctx.r10.u64 | 1;
	// beq cr6,0x822d05e0
	if (ctx.cr6.eq) goto loc_822D05E0;
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x822d05e0
	if (ctx.cr6.eq) goto loc_822D05E0;
	// lhz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// cmplwi cr6,r11,65534
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 65534, ctx.xer);
	// bne cr6,0x822d05d8
	if (!ctx.cr6.eq) goto loc_822D05D8;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r11,-1544
	ctx.r9.s64 = ctx.r11.s64 + -1544;
	// li r8,3
	ctx.r8.s64 = 3;
	// addi r3,r31,24
	ctx.r3.s64 = ctx.r31.s64 + 24;
	// lwz r11,-1544(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -1544);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r7,4(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r6,8(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r11.u32);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// stw r6,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r6.u32);
	// stw r9,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r9.u32);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// bl 0x8233e4e0
	ctx.lr = 0x822D05D4;
	sub_8233E4E0(ctx, base);
	// b 0x822d05e0
	goto loc_822D05E0;
loc_822D05D8:
	// li r11,3
	ctx.r11.s64 = 3;
	// sth r11,0(r31)
	PPC_STORE_U16(ctx.r31.u32 + 0, ctx.r11.u16);
loc_822D05E0:
	// lhz r11,2(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 2);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x822d0600
	if (!ctx.cr6.lt) goto loc_822D0600;
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x822d061c
	if (ctx.cr6.eq) goto loc_822D061C;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x822d0618
	goto loc_822D0618;
loc_822D0600:
	// cmplwi cr6,r11,64
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 64, ctx.xer);
	// ble cr6,0x822d061c
	if (!ctx.cr6.gt) goto loc_822D061C;
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x822d061c
	if (ctx.cr6.eq) goto loc_822D061C;
	// li r11,64
	ctx.r11.s64 = 64;
loc_822D0618:
	// sth r11,2(r31)
	PPC_STORE_U16(ctx.r31.u32 + 2, ctx.r11.u16);
loc_822D061C:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmplwi cr6,r11,1000
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1000, ctx.xer);
	// bge cr6,0x822d0640
	if (!ctx.cr6.lt) goto loc_822D0640;
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x822d0660
	if (ctx.cr6.eq) goto loc_822D0660;
	// li r11,1000
	ctx.r11.s64 = 1000;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// b 0x822d0660
	goto loc_822D0660;
loc_822D0640:
	// lis r10,3
	ctx.r10.s64 = 196608;
	// ori r10,r10,3392
	ctx.r10.u64 = ctx.r10.u64 | 3392;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x822d0660
	if (!ctx.cr6.gt) goto loc_822D0660;
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x822d0660
	if (ctx.cr6.eq) goto loc_822D0660;
	// stw r10,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r10.u32);
loc_822D0660:
	// lhz r10,14(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 14);
	// li r11,32
	ctx.r11.s64 = 32;
	// cmplwi cr6,r10,32
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 32, ctx.xer);
	// beq cr6,0x822d0680
	if (ctx.cr6.eq) goto loc_822D0680;
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x822d0680
	if (ctx.cr6.eq) goto loc_822D0680;
	// sth r11,14(r31)
	PPC_STORE_U16(ctx.r31.u32 + 14, ctx.r11.u16);
loc_822D0680:
	// lhz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 0);
	// cmplwi cr6,r10,65534
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 65534, ctx.xer);
	// bne cr6,0x822d06a8
	if (!ctx.cr6.eq) goto loc_822D06A8;
	// lhz r10,18(r31)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r31.u32 + 18);
	// cmplwi cr6,r10,32
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 32, ctx.xer);
	// beq cr6,0x822d06a8
	if (ctx.cr6.eq) goto loc_822D06A8;
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x822d06a8
	if (ctx.cr6.eq) goto loc_822D06A8;
	// sth r11,18(r31)
	PPC_STORE_U16(ctx.r31.u32 + 18, ctx.r11.u16);
loc_822D06A8:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D06B4"))) PPC_WEAK_FUNC(sub_822D06B4);
PPC_FUNC_IMPL(__imp__sub_822D06B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D06B8"))) PPC_WEAK_FUNC(sub_822D06B8);
PPC_FUNC_IMPL(__imp__sub_822D06B8) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lis r9,-30569
	ctx.r9.s64 = -2003369984;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// ori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 | 1;
	// lwz r11,1048(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1048);
	// clrlwi. r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d06fc
	if (ctx.cr0.eq) goto loc_822D06FC;
	// lhz r11,2(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 2);
	// lhz r8,2(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 2);
	// mr r7,r11
	ctx.r7.u64 = ctx.r11.u64;
	// cmplw cr6,r8,r11
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x822d06fc
	if (ctx.cr6.eq) goto loc_822D06FC;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x822d06fc
	if (ctx.cr6.eq) goto loc_822D06FC;
	// sth r11,2(r5)
	PPC_STORE_U16(ctx.r5.u32 + 2, ctx.r11.u16);
loc_822D06FC:
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r11,1048(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1048);
	// rlwinm. r11,r11,0,30,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d072c
	if (ctx.cr0.eq) goto loc_822D072C;
	// lwz r11,4(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmplw cr6,r8,r11
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x822d072c
	if (ctx.cr6.eq) goto loc_822D072C;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x822d072c
	if (ctx.cr6.eq) goto loc_822D072C;
	// stw r11,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r11.u32);
loc_822D072C:
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r11,1048(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1048);
	// rlwinm. r11,r11,0,29,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x4;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beqlr 
	if (ctx.cr0.eq) return;
	// lhz r11,14(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 14);
	// lhz r10,14(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 14);
	// mr r8,r11
	ctx.r8.u64 = ctx.r11.u64;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x822d0760
	if (ctx.cr6.eq) goto loc_822D0760;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x822d0760
	if (ctx.cr6.eq) goto loc_822D0760;
	// sth r11,14(r5)
	PPC_STORE_U16(ctx.r5.u32 + 14, ctx.r11.u16);
loc_822D0760:
	// lhz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// cmplwi cr6,r11,65534
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 65534, ctx.xer);
	// lhz r11,0(r5)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// bne cr6,0x822d0798
	if (!ctx.cr6.eq) goto loc_822D0798;
	// cmplwi cr6,r11,65534
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 65534, ctx.xer);
	// bne cr6,0x822d0780
	if (!ctx.cr6.eq) goto loc_822D0780;
	// lhz r11,18(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 18);
	// b 0x822d07a4
	goto loc_822D07A4;
loc_822D0780:
	// lhz r11,18(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 18);
	// lhz r10,14(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 14);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// blr 
	return;
loc_822D0798:
	// cmplwi cr6,r11,65534
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 65534, ctx.xer);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
	// lhz r11,14(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 14);
loc_822D07A4:
	// lhz r10,18(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 18);
	// mr r8,r11
	ctx.r8.u64 = ctx.r11.u64;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// sth r11,18(r5)
	PPC_STORE_U16(ctx.r5.u32 + 18, ctx.r11.u16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D07C8"))) PPC_WEAK_FUNC(sub_822D07C8);
PPC_FUNC_IMPL(__imp__sub_822D07C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e460
	ctx.lr = 0x822D07D0;
	__restfpr_26(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r26,r6
	ctx.r26.u64 = ctx.r6.u64;
	// lwz r11,52(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	// li r28,0
	ctx.r28.s64 = 0;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822D07FC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr. r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// bge 0x822d0810
	if (!ctx.cr0.lt) goto loc_822D0810;
	// lis r31,-32761
	ctx.r31.s64 = -2147024896;
	// ori r31,r31,87
	ctx.r31.u64 = ctx.r31.u64 | 87;
	// b 0x822d08b8
	goto loc_822D08B8;
loc_822D0810:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822cf808
	ctx.lr = 0x822D0818;
	sub_822CF808(ctx, base);
	// cmpwi r3,0
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne 0x822d0828
	if (!ctx.cr0.eq) goto loc_822D0828;
	// lis r31,-32761
	ctx.r31.s64 = -2147024896;
	// ori r31,r31,87
	ctx.r31.u64 = ctx.r31.u64 | 87;
loc_822D0828:
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// blt cr6,0x822d08b8
	if (ctx.cr6.lt) goto loc_822D08B8;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// beq cr6,0x822d085c
	if (ctx.cr6.eq) goto loc_822D085C;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822d0408
	ctx.lr = 0x822D0840;
	sub_822D0408(ctx, base);
	// lis r11,-32761
	ctx.r11.s64 = -2147024896;
	// addic r10,r3,-1
	ctx.xer.ca = ctx.r3.u32 > 0;
	ctx.r10.s64 = ctx.r3.s64 + -1;
	// ori r11,r11,14
	ctx.r11.u64 = ctx.r11.u64 | 14;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// li r28,1
	ctx.r28.s64 = 1;
	// and r31,r10,r11
	ctx.r31.u64 = ctx.r10.u64 & ctx.r11.u64;
loc_822D085C:
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// blt cr6,0x822d08c4
	if (ctx.cr6.lt) goto loc_822D08C4;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r11,52(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822D0880;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr. r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// blt 0x822d08c4
	if (ctx.cr0.lt) goto loc_822D08C4;
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x822d06b8
	ctx.lr = 0x822D089C;
	sub_822D06B8(ctx, base);
	// mr. r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// blt 0x822d08c4
	if (ctx.cr0.lt) goto loc_822D08C4;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x822d08b8
	if (ctx.cr6.eq) goto loc_822D08B8;
loc_822D08AC:
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82082d58
	ctx.lr = 0x822D08B8;
	sub_82082D58(ctx, base);
loc_822D08B8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
loc_822D08C4:
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x822d08b8
	if (ctx.cr6.eq) goto loc_822D08B8;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x822d08b8
	if (ctx.cr6.eq) goto loc_822D08B8;
	// lis r11,-30569
	ctx.r11.s64 = -2003369984;
	// ori r11,r11,1
	ctx.r11.u64 = ctx.r11.u64 | 1;
	// cmpw cr6,r31,r11
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x822d08ac
	if (!ctx.cr6.eq) goto loc_822D08AC;
	// stw r29,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r29.u32);
	// b 0x822d08b8
	goto loc_822D08B8;
}

__attribute__((alias("__imp__sub_822D08EC"))) PPC_WEAK_FUNC(sub_822D08EC);
PPC_FUNC_IMPL(__imp__sub_822D08EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D08F0"))) PPC_WEAK_FUNC(sub_822D08F0);
PPC_FUNC_IMPL(__imp__sub_822D08F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e44c
	ctx.lr = 0x822D08F8;
	__restfpr_21(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r25,r5
	ctx.r25.u64 = ctx.r5.u64;
	// mr r28,r7
	ctx.r28.u64 = ctx.r7.u64;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x822d0948
	if (ctx.cr6.eq) goto loc_822D0948;
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// subf r27,r7,r5
	ctx.r27.s64 = ctx.r5.s64 - ctx.r7.s64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
loc_822D091C:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,1048(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1048);
	// rlwinm. r11,r11,0,30,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d093c
	if (ctx.cr0.eq) goto loc_822D093C;
	// lwzx r3,r27,r30
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + ctx.r30.u32);
	// bl 0x822cf108
	ctx.lr = 0x822D0934;
	sub_822CF108(ctx, base);
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// bl 0x822cf108
	ctx.lr = 0x822D093C;
	sub_822CF108(ctx, base);
loc_822D093C:
	// addic. r29,r29,-1
	ctx.xer.ca = ctx.r29.u32 > 0;
	ctx.r29.s64 = ctx.r29.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// addi r30,r30,8
	ctx.r30.s64 = ctx.r30.s64 + 8;
	// bne 0x822d091c
	if (!ctx.cr0.eq) goto loc_822D091C;
loc_822D0948:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// li r26,0
	ctx.r26.s64 = 0;
	// li r21,1
	ctx.r21.s64 = 1;
	// mr r22,r26
	ctx.r22.u64 = ctx.r26.u64;
	// lwz r11,1048(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1048);
	// rlwinm r11,r11,0,30,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bne cr6,0x822d09cc
	if (!ctx.cr6.eq) goto loc_822D09CC;
	// lwz r11,0(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// lhz r29,2(r11)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r11.u32 + 2);
	// lhz r27,2(r10)
	ctx.r27.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// mullw r11,r27,r29
	ctx.r11.s64 = int64_t(ctx.r27.s32) * int64_t(ctx.r29.s32);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82082c78
	ctx.lr = 0x822D0988;
	sub_82082C78(ctx, base);
	// mr. r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// stw r7,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r7.u32);
	// bne 0x822d09dc
	if (!ctx.cr0.eq) goto loc_822D09DC;
	// lis r22,-32761
	ctx.r22.s64 = -2147024896;
	// ori r22,r22,14
	ctx.r22.u64 = ctx.r22.u64 | 14;
loc_822D099C:
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822d09b4
	if (ctx.cr6.eq) goto loc_822D09B4;
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// bl 0x82082d58
	ctx.lr = 0x822D09B0;
	sub_82082D58(ctx, base);
	// stw r26,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r26.u32);
loc_822D09B4:
	// li r11,7
	ctx.r11.s64 = 7;
	// stw r26,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r26.u32);
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// stw r26,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r26.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r11.u32);
	// blt cr6,0x822d09d0
	if (ctx.cr6.lt) goto loc_822D09D0;
loc_822D09CC:
	// stw r21,24(r31)
	PPC_STORE_U32(ctx.r31.u32 + 24, ctx.r21.u32);
loc_822D09D0:
	// mr r3,r22
	ctx.r3.u64 = ctx.r22.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8233e49c
	__restgprlr_21(ctx, base);
	return;
loc_822D09DC:
	// lwz r11,0(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	// mr r9,r26
	ctx.r9.u64 = ctx.r26.u64;
	// lhz r24,14(r11)
	ctx.r24.u64 = PPC_LOAD_U16(ctx.r11.u32 + 14);
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// mr r23,r24
	ctx.r23.u64 = ctx.r24.u64;
	// cmplwi cr6,r10,65534
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 65534, ctx.xer);
	// bne cr6,0x822d0a00
	if (!ctx.cr6.eq) goto loc_822D0A00;
	// lwz r9,20(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lhz r23,18(r11)
	ctx.r23.u64 = PPC_LOAD_U16(ctx.r11.u32 + 18);
loc_822D0A00:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// mr r10,r26
	ctx.r10.u64 = ctx.r26.u64;
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// cmplwi cr6,r8,65534
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 65534, ctx.xer);
	// bne cr6,0x822d0a18
	if (!ctx.cr6.eq) goto loc_822D0A18;
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
loc_822D0A18:
	// lis r11,-32199
	ctx.r11.s64 = -2110193664;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// addi r8,r11,12180
	ctx.r8.s64 = ctx.r11.s64 + 12180;
	// bne cr6,0x822d0a4c
	if (!ctx.cr6.eq) goto loc_822D0A4C;
	// clrlwi r11,r29,16
	ctx.r11.u64 = ctx.r29.u32 & 0xFFFF;
	// mr r9,r26
	ctx.r9.u64 = ctx.r26.u64;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// blt cr6,0x822d0a4c
	if (ctx.cr6.lt) goto loc_822D0A4C;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bgt cr6,0x822d0a4c
	if (ctx.cr6.gt) goto loc_822D0A4C;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
loc_822D0A4C:
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822d0a78
	if (!ctx.cr6.eq) goto loc_822D0A78;
	// clrlwi r11,r27,16
	ctx.r11.u64 = ctx.r27.u32 & 0xFFFF;
	// mr r10,r26
	ctx.r10.u64 = ctx.r26.u64;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// blt cr6,0x822d0a78
	if (ctx.cr6.lt) goto loc_822D0A78;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bgt cr6,0x822d0a78
	if (ctx.cr6.gt) goto loc_822D0A78;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// lwz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
loc_822D0A78:
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822cff38
	ctx.lr = 0x822D0A8C;
	sub_822CFF38(ctx, base);
	// mr. r22,r3
	ctx.r22.u64 = ctx.r3.u64;
	ctx.cr0.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// blt 0x822d099c
	if (ctx.cr0.lt) goto loc_822D099C;
	// li r30,4
	ctx.r30.s64 = 4;
	// stw r26,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r26.u32);
	// cmplw cr6,r29,r27
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r27.u32, ctx.xer);
	// bne cr6,0x822d0b24
	if (!ctx.cr6.eq) goto loc_822D0B24;
	// li r30,7
	ctx.r30.s64 = 7;
	// mr r8,r26
	ctx.r8.u64 = ctx.r26.u64;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// beq cr6,0x822d0b24
	if (ctx.cr6.eq) goto loc_822D0B24;
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// mr r9,r26
	ctx.r9.u64 = ctx.r26.u64;
	// lfs f0,-28948(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
loc_822D0AC0:
	// mr r11,r26
	ctx.r11.u64 = ctx.r26.u64;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x822d0b14
	if (ctx.cr6.eq) goto loc_822D0B14;
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// mtctr r29
	ctx.ctr.u64 = ctx.r29.u64;
loc_822D0AD4:
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + ctx.r11.u64;
	// cmplw cr6,r11,r8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r8.u32, ctx.xer);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// beq cr6,0x822d0af8
	if (ctx.cr6.eq) goto loc_822D0AF8;
	// lfsx f13,r7,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// beq cr6,0x822d0b0c
	if (ctx.cr6.eq) goto loc_822D0B0C;
	// rlwinm r30,r30,0,0,29
	ctx.r30.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 0) & 0xFFFFFFFC;
	// b 0x822d0b0c
	goto loc_822D0B0C;
loc_822D0AF8:
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfsx f12,r7,r10
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	ctx.cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// beq cr6,0x822d0b0c
	if (ctx.cr6.eq) goto loc_822D0B0C;
	// rlwinm r30,r30,0,0,30
	ctx.r30.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 0) & 0xFFFFFFFE;
loc_822D0B0C:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// bdnz 0x822d0ad4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D0AD4;
loc_822D0B14:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + ctx.r29.u64;
	// cmplw cr6,r8,r27
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r27.u32, ctx.xer);
	// blt cr6,0x822d0ac0
	if (ctx.cr6.lt) goto loc_822D0AC0;
loc_822D0B24:
	// clrlwi. r11,r30,31
	ctx.r11.u64 = ctx.r30.u32 & 0x1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d0b40
	if (ctx.cr0.eq) goto loc_822D0B40;
	// lis r4,8343
	ctx.r4.s64 = 546766848;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// bl 0x82082d58
	ctx.lr = 0x822D0B38;
	sub_82082D58(ctx, base);
	// stw r26,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r26.u32);
	// stw r21,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r21.u32);
loc_822D0B40:
	// lwz r3,0(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	// bl 0x822cf108
	ctx.lr = 0x822D0B48;
	sub_822CF108(ctx, base);
	// clrlwi r11,r3,16
	ctx.r11.u64 = ctx.r3.u32 & 0xFFFF;
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bne cr6,0x822d0c14
	if (!ctx.cr6.eq) goto loc_822D0C14;
	// clrlwi r11,r24,16
	ctx.r11.u64 = ctx.r24.u32 & 0xFFFF;
	// cmplwi cr6,r11,32
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 32, ctx.xer);
	// bne cr6,0x822d0bb0
	if (!ctx.cr6.eq) goto loc_822D0BB0;
	// clrlwi r11,r23,16
	ctx.r11.u64 = ctx.r23.u32 & 0xFFFF;
	// cmplwi cr6,r11,32
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 32, ctx.xer);
	// bne cr6,0x822d0b88
	if (!ctx.cr6.eq) goto loc_822D0B88;
	// li r3,6
	ctx.r3.s64 = 6;
	// bl 0x822d5760
	ctx.lr = 0x822D0B80;
	sub_822D5760(ctx, base);
	// li r11,6
	ctx.r11.s64 = 6;
	// b 0x822d0c20
	goto loc_822D0C20;
loc_822D0B88:
	// cmplwi cr6,r11,24
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 24, ctx.xer);
	// bne cr6,0x822d0ba0
	if (!ctx.cr6.eq) goto loc_822D0BA0;
	// li r3,5
	ctx.r3.s64 = 5;
	// bl 0x822d5760
	ctx.lr = 0x822D0B98;
	sub_822D5760(ctx, base);
	// li r11,5
	ctx.r11.s64 = 5;
	// b 0x822d0c20
	goto loc_822D0C20;
loc_822D0BA0:
	// li r3,3
	ctx.r3.s64 = 3;
	// bl 0x822d5760
	ctx.lr = 0x822D0BA8;
	sub_822D5760(ctx, base);
	// li r11,3
	ctx.r11.s64 = 3;
	// b 0x822d0c20
	goto loc_822D0C20;
loc_822D0BB0:
	// cmplwi cr6,r11,24
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 24, ctx.xer);
	// bne cr6,0x822d0be4
	if (!ctx.cr6.eq) goto loc_822D0BE4;
	// clrlwi r11,r23,16
	ctx.r11.u64 = ctx.r23.u32 & 0xFFFF;
	// cmplwi cr6,r11,24
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 24, ctx.xer);
	// bne cr6,0x822d0bd4
	if (!ctx.cr6.eq) goto loc_822D0BD4;
	// li r3,4
	ctx.r3.s64 = 4;
	// bl 0x822d5760
	ctx.lr = 0x822D0BCC;
	sub_822D5760(ctx, base);
	// li r11,4
	ctx.r11.s64 = 4;
	// b 0x822d0c20
	goto loc_822D0C20;
loc_822D0BD4:
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x822d5760
	ctx.lr = 0x822D0BDC;
	sub_822D5760(ctx, base);
	// li r11,2
	ctx.r11.s64 = 2;
	// b 0x822d0c20
	goto loc_822D0C20;
loc_822D0BE4:
	// cmplwi cr6,r11,16
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 16, ctx.xer);
	// bne cr6,0x822d0c00
	if (!ctx.cr6.eq) goto loc_822D0C00;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x822d5760
	ctx.lr = 0x822D0BF4;
	sub_822D5760(ctx, base);
	// stw r3,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r3.u32);
	// stw r21,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r21.u32);
	// b 0x822d09cc
	goto loc_822D09CC;
loc_822D0C00:
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x822d5760
	ctx.lr = 0x822D0C08;
	sub_822D5760(ctx, base);
	// stw r3,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r3.u32);
	// stw r26,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r26.u32);
	// b 0x822d09cc
	goto loc_822D09CC;
loc_822D0C14:
	// li r3,7
	ctx.r3.s64 = 7;
	// bl 0x822d5760
	ctx.lr = 0x822D0C1C;
	sub_822D5760(ctx, base);
	// li r11,7
	ctx.r11.s64 = 7;
loc_822D0C20:
	// stw r3,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r3.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r11.u32);
	// b 0x822d09cc
	goto loc_822D09CC;
}

__attribute__((alias("__imp__sub_822D0C2C"))) PPC_WEAK_FUNC(sub_822D0C2C);
PPC_FUNC_IMPL(__imp__sub_822D0C2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D0C30"))) PPC_WEAK_FUNC(sub_822D0C30);
PPC_FUNC_IMPL(__imp__sub_822D0C30) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r4,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r4.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,7
	ctx.r9.s64 = 7;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r11.u32);
	// stw r9,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r9.u32);
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// stw r11,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r11.u32);
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D0C5C"))) PPC_WEAK_FUNC(sub_822D0C5C);
PPC_FUNC_IMPL(__imp__sub_822D0C5C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D0C60"))) PPC_WEAK_FUNC(sub_822D0C60);
PPC_FUNC_IMPL(__imp__sub_822D0C60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e450
	ctx.lr = 0x822D0C68;
	__restfpr_22(ctx, base);
	// lwz r10,36(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r27,24(r3)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r29,28(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r22,32(r3)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// beq cr6,0x822d0c94
	if (ctx.cr6.eq) goto loc_822D0C94;
	// li r29,1
	ctx.r29.s64 = 1;
	// li r27,1
	ctx.r27.s64 = 1;
loc_822D0C94:
	// lwz r24,12(r3)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// twllei r27,0
	if (ctx.r27.u32 <= 0) __builtin_debugtrap();
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// bne cr6,0x822d0d44
	if (!ctx.cr6.eq) goto loc_822D0D44;
	// divwu. r10,r10,r27
	ctx.r10.u32 = ctx.r10.u32 / ctx.r27.u32;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x822d0e14
	if (ctx.cr0.eq) goto loc_822D0E14;
	// lwz r30,8(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r28,r27,2,0,29
	ctx.r28.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r26,r29,2,0,29
	ctx.r26.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
loc_822D0CB8:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x822d0d30
	if (ctx.cr6.eq) goto loc_822D0D30;
	// lfs f11,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// addi r3,r11,48
	ctx.r3.s64 = ctx.r11.s64 + 48;
	// addi r31,r8,108
	ctx.r31.s64 = ctx.r8.s64 + 108;
	// mr r5,r8
	ctx.r5.u64 = ctx.r8.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// addi r7,r30,-4
	ctx.r7.s64 = ctx.r30.s64 + -4;
loc_822D0CD8:
	// lfs f0,4(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fmuls f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// cmplwi cr6,r27,1
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 1, ctx.xer);
	// ble cr6,0x822d0d08
	if (!ctx.cr6.gt) goto loc_822D0D08;
	// addi r6,r27,-1
	ctx.r6.s64 = ctx.r27.s64 + -1;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
loc_822D0CF8:
	// lfsu f13,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// lfsu f12,4(r7)
	ea = 4 + ctx.r7.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r7.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fmadds f0,f13,f12,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f12.f64), float(ctx.f0.f64)));
	// bdnz 0x822d0cf8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D0CF8;
loc_822D0D08:
	// dcbt r0,r3
	// dcbt r0,r31
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// beq cr6,0x822d0d20
	if (ctx.cr6.eq) goto loc_822D0D20;
	// lfs f13,0(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
loc_822D0D20:
	// stfs f0,0(r5)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// addic. r4,r4,-1
	ctx.xer.ca = ctx.r4.u32 > 0;
	ctx.r4.s64 = ctx.r4.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne 0x822d0cd8
	if (!ctx.cr0.eq) goto loc_822D0CD8;
loc_822D0D30:
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// add r8,r26,r8
	ctx.r8.u64 = ctx.r26.u64 + ctx.r8.u64;
	// bne 0x822d0cb8
	if (!ctx.cr0.eq) goto loc_822D0CB8;
	// b 0x822d0e14
	goto loc_822D0E14;
loc_822D0D44:
	// divwu. r23,r10,r27
	ctx.r23.u32 = ctx.r10.u32 / ctx.r27.u32;
	ctx.cr0.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// li r7,0
	ctx.r7.s64 = 0;
	// beq 0x822d0e14
	if (ctx.cr0.eq) goto loc_822D0E14;
	// lwz r28,8(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r26,r27,2,0,29
	ctx.r26.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r29,2,0,29
	ctx.r25.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
loc_822D0D5C:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x822d0e00
	if (ctx.cr6.eq) goto loc_822D0E00;
	// clrldi r10,r7,32
	ctx.r10.u64 = ctx.r7.u64 & 0xFFFFFFFF;
	// lfs f10,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// addi r31,r11,48
	ctx.r31.s64 = ctx.r11.s64 + 48;
	// std r10,-96(r1)
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.r10.u64);
	// lfd f0,-96(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// addi r30,r8,108
	ctx.r30.s64 = ctx.r8.s64 + 108;
	// frsp f11,f0
	ctx.f11.f64 = double(float(ctx.f0.f64));
	// mr r4,r8
	ctx.r4.u64 = ctx.r8.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r10,r24,-4
	ctx.r10.s64 = ctx.r24.s64 + -4;
	// addi r9,r28,-4
	ctx.r9.s64 = ctx.r28.s64 + -4;
loc_822D0D94:
	// lfs f0,4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lfs f13,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fmadds f0,f0,f11,f13
	ctx.f0.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f11.f64), float(ctx.f13.f64)));
	// cmplwi cr6,r27,1
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 1, ctx.xer);
	// fmuls f0,f0,f10
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// ble cr6,0x822d0dd8
	if (!ctx.cr6.gt) goto loc_822D0DD8;
	// addi r5,r27,-1
	ctx.r5.s64 = ctx.r27.s64 + -1;
	// mr r6,r11
	ctx.r6.u64 = ctx.r11.u64;
	// mtctr r5
	ctx.ctr.u64 = ctx.r5.u64;
loc_822D0DC0:
	// lfsu f13,4(r10)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// lfsu f12,4(r9)
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fmadds f12,f13,f11,f12
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f12.f64)));
	// lfsu f13,4(r6)
	ea = 4 + ctx.r6.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r6.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fmadds f0,f12,f13,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f13.f64), float(ctx.f0.f64)));
	// bdnz 0x822d0dc0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D0DC0;
loc_822D0DD8:
	// dcbt r0,r31
	// dcbt r0,r30
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// beq cr6,0x822d0df0
	if (ctx.cr6.eq) goto loc_822D0DF0;
	// lfs f13,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
loc_822D0DF0:
	// stfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// addic. r3,r3,-1
	ctx.xer.ca = ctx.r3.u32 > 0;
	ctx.r3.s64 = ctx.r3.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// bne 0x822d0d94
	if (!ctx.cr0.eq) goto loc_822D0D94;
loc_822D0E00:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// add r8,r25,r8
	ctx.r8.u64 = ctx.r25.u64 + ctx.r8.u64;
	// cmplw cr6,r7,r23
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r23.u32, ctx.xer);
	// blt cr6,0x822d0d5c
	if (ctx.cr6.lt) goto loc_822D0D5C;
loc_822D0E14:
	// b 0x8233e4a0
	__restgprlr_22(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D0E18"))) PPC_WEAK_FUNC(sub_822D0E18);
PPC_FUNC_IMPL(__imp__sub_822D0E18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r7,32(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f0,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// bne cr6,0x822d0ea8
	if (!ctx.cr6.eq) goto loc_822D0EA8;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r9,r11,72
	ctx.r9.s64 = ctx.r11.s64 + 72;
	// li r8,48
	ctx.r8.s64 = 48;
loc_822D0E54:
	// lfs f12,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// dcbt r10,r8
	// dcbt r0,r9
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq cr6,0x822d0e88
	if (ctx.cr6.eq) goto loc_822D0E88;
	// lfs f11,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f10,f12,f0,f11
	ctx.f10.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// stfs f10,0(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f9,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f12,f13,f9
	ctx.f8.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f13.f64), float(ctx.f9.f64)));
	// stfs f8,0(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822d0e98
	goto loc_822D0E98;
loc_822D0E88:
	// fmuls f11,f12,f0
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f11,0(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f10,f12,f13
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfsu f10,4(r11)
	temp.f32 = float(ctx.f10.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
loc_822D0E98:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r9,r11,72
	ctx.r9.s64 = ctx.r11.s64 + 72;
	// bdnz 0x822d0e54
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D0E54;
	// blr 
	return;
loc_822D0EA8:
	// lfs f11,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lfs f10,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r9,r11,72
	ctx.r9.s64 = ctx.r11.s64 + 72;
	// li r8,48
	ctx.r8.s64 = 48;
loc_822D0EC4:
	// lfs f12,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// dcbt r10,r8
	// dcbt r0,r9
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq cr6,0x822d0ef8
	if (ctx.cr6.eq) goto loc_822D0EF8;
	// lfs f9,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f12,f0,f9
	ctx.f8.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f9.f64)));
	// stfs f8,0(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f7,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f12,f13,f7
	ctx.f6.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f13.f64), float(ctx.f7.f64)));
	// stfs f6,0(r11)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822d0f08
	goto loc_822D0F08;
loc_822D0EF8:
	// fmuls f9,f12,f0
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f9,0(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f8,f12,f13
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfsu f8,4(r11)
	temp.f32 = float(ctx.f8.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
loc_822D0F08:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fadds f0,f11,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// fadds f13,f10,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 + ctx.f13.f64));
	// addi r9,r11,72
	ctx.r9.s64 = ctx.r11.s64 + 72;
	// bdnz 0x822d0ec4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D0EC4;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D0F20"))) PPC_WEAK_FUNC(sub_822D0F20);
PPC_FUNC_IMPL(__imp__sub_822D0F20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r7,32(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f13,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,12(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,16(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,20(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// bne cr6,0x822d1010
	if (!ctx.cr6.eq) goto loc_822D1010;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// addi r9,r11,216
	ctx.r9.s64 = ctx.r11.s64 + 216;
	// li r8,48
	ctx.r8.s64 = 48;
loc_822D0F6C:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// dcbt r10,r8
	// dcbt r0,r9
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq cr6,0x822d0fd0
	if (ctx.cr6.eq) goto loc_822D0FD0;
	// lfs f7,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f0,f13,f7
	ctx.f6.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f13.f64), float(ctx.f7.f64)));
	// stfs f6,0(r11)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f5,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f5.f64 = double(temp.f32);
	// fmadds f4,f0,f12,f5
	ctx.f4.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f12.f64), float(ctx.f5.f64)));
	// stfs f4,0(r11)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f3,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f3.f64 = double(temp.f32);
	// fmadds f2,f0,f11,f3
	ctx.f2.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f11.f64), float(ctx.f3.f64)));
	// stfs f2,0(r11)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f7,f0,f10,f1
	ctx.f7.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), float(ctx.f1.f64)));
	// stfs f7,0(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f6,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f6.f64 = double(temp.f32);
	// fmadds f5,f0,f9,f6
	ctx.f5.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f9.f64), float(ctx.f6.f64)));
	// stfs f5,0(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f4,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f4.f64 = double(temp.f32);
	// fmadds f3,f0,f8,f4
	ctx.f3.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f8.f64), float(ctx.f4.f64)));
	// stfs f3,0(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822d1000
	goto loc_822D1000;
loc_822D0FD0:
	// fmuls f7,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f7,0(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f6,f0,f12
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// stfsu f6,4(r11)
	temp.f32 = float(ctx.f6.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f5,f0,f11
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// stfsu f5,4(r11)
	temp.f32 = float(ctx.f5.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f4,f0,f10
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfsu f4,4(r11)
	temp.f32 = float(ctx.f4.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f3,f0,f9
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// stfsu f3,4(r11)
	temp.f32 = float(ctx.f3.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f2,f0,f8
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// stfsu f2,4(r11)
	temp.f32 = float(ctx.f2.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
loc_822D1000:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r9,r11,216
	ctx.r9.s64 = ctx.r11.s64 + 216;
	// bdnz 0x822d0f6c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D0F6C;
	// blr 
	return;
loc_822D1010:
	// lfs f7,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f6,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f2.f64 = double(temp.f32);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// addi r9,r11,216
	ctx.r9.s64 = ctx.r11.s64 + 216;
	// li r8,48
	ctx.r8.s64 = 48;
loc_822D103C:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// dcbt r10,r8
	// dcbt r0,r9
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq cr6,0x822d10a0
	if (ctx.cr6.eq) goto loc_822D10A0;
	// lfs f1,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f0,f13,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f13.f64), float(ctx.f1.f64)));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f0,f12,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f12.f64), float(ctx.f1.f64)));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f0,f11,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f11.f64), float(ctx.f1.f64)));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f0,f10,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), float(ctx.f1.f64)));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f0,f9,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f9.f64), float(ctx.f1.f64)));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f0,f0,f8,f1
	ctx.f0.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f8.f64), float(ctx.f1.f64)));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822d10d0
	goto loc_822D10D0;
loc_822D10A0:
	// fmuls f1,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f1,f0,f12
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// stfsu f1,4(r11)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f1,f0,f11
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// stfsu f1,4(r11)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f1,f0,f10
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfsu f1,4(r11)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f1,f0,f9
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// stfsu f1,4(r11)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f0,f0,f8
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// stfsu f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
loc_822D10D0:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fadds f13,f7,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f7.f64 + ctx.f13.f64));
	// fadds f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f12.f64));
	// addi r9,r11,216
	ctx.r9.s64 = ctx.r11.s64 + 216;
	// fadds f11,f5,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 + ctx.f11.f64));
	// fadds f10,f4,f10
	ctx.f10.f64 = double(float(ctx.f4.f64 + ctx.f10.f64));
	// fadds f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 + ctx.f9.f64));
	// fadds f8,f2,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// bdnz 0x822d103c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D103C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D10F8"))) PPC_WEAK_FUNC(sub_822D10F8);
PPC_FUNC_IMPL(__imp__sub_822D10F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa00
	ctx.lr = 0x822D1108;
	sub_8233FA00(ctx, base);
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r7,32(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f12,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,12(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,16(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,20(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,24(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 24);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,28(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,32(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 32);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,36(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 36);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,40(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 40);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,44(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 44);
	ctx.f1.f64 = double(temp.f32);
	// bne cr6,0x822d126c
	if (!ctx.cr6.eq) goto loc_822D126C;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x822d13cc
	if (ctx.cr6.eq) goto loc_822D13CC;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,48
	ctx.r8.s64 = 48;
	// rlwinm r9,r9,31,1,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,216
	ctx.r9.s64 = 216;
loc_822D1178:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfsu f13,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// dcbt r10,r8
	// dcbt r11,r9
	// fmuls f31,f0,f12
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmadds f31,f13,f11,f31
	ctx.f31.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f31.f64)));
	// beq cr6,0x822d1210
	if (ctx.cr6.eq) goto loc_822D1210;
	// fmuls f29,f0,f10
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// lfs f30,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f28,f0,f8
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// fmuls f27,f0,f6
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// fmuls f26,f0,f4
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f29,f13,f9,f29
	ctx.f29.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f29.f64)));
	// fmadds f28,f13,f7,f28
	ctx.f28.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f28.f64)));
	// fmadds f27,f13,f5,f27
	ctx.f27.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f27.f64)));
	// fmadds f26,f13,f3,f26
	ctx.f26.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f3.f64), float(ctx.f26.f64)));
	// fmadds f13,f13,f1,f0
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f0.f64)));
	// fadds f0,f31,f30
	ctx.f0.f64 = double(float(ctx.f31.f64 + ctx.f30.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f29,f0
	ctx.f0.f64 = double(float(ctx.f29.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f28,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f27,f0
	ctx.f0.f64 = double(float(ctx.f27.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f26,f0
	ctx.f0.f64 = double(float(ctx.f26.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822d1250
	goto loc_822D1250;
loc_822D1210:
	// fmuls f30,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.f30.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfs f31,0(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f29,f0,f8
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// fmuls f28,f0,f6
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// fmuls f27,f0,f4
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f31,f13,f9,f30
	ctx.f31.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f30.f64)));
	// stfsu f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f31,f13,f7,f29
	ctx.f31.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f29.f64)));
	// stfsu f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f31,f13,f5,f28
	ctx.f31.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f28.f64)));
	// stfsu f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f31,f13,f3,f27
	ctx.f31.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f3.f64), float(ctx.f27.f64)));
	// stfsu f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f13,f13,f1,f0
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f0.f64)));
	// stfsu f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
loc_822D1250:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822d1178
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D1178;
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D1260;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_822D126C:
	// lfs f31,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f31.f64 = double(temp.f32);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f30,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,24(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,28(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,32(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,36(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,40(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,44(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	ctx.f20.f64 = double(temp.f32);
	// beq cr6,0x822d13cc
	if (ctx.cr6.eq) goto loc_822D13CC;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,48
	ctx.r8.s64 = 48;
	// rlwinm r9,r9,31,1,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,216
	ctx.r9.s64 = 216;
loc_822D12BC:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfsu f13,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// dcbt r10,r8
	// dcbt r11,r9
	// fmuls f19,f0,f12
	ctx.f19.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmadds f19,f13,f11,f19
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f19.f64)));
	// beq cr6,0x822d1354
	if (ctx.cr6.eq) goto loc_822D1354;
	// fmuls f17,f0,f10
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// lfs f18,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f16,f0,f8
	ctx.f16.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// fmuls f15,f0,f6
	ctx.f15.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// fmuls f14,f0,f4
	ctx.f14.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f17,f13,f9,f17
	ctx.f17.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f17.f64)));
	// fmadds f16,f13,f7,f16
	ctx.f16.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f16.f64)));
	// fmadds f15,f13,f5,f15
	ctx.f15.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f15.f64)));
	// fmadds f14,f13,f3,f14
	ctx.f14.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f3.f64), float(ctx.f14.f64)));
	// fmadds f13,f13,f1,f0
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f0.f64)));
	// fadds f0,f19,f18
	ctx.f0.f64 = double(float(ctx.f19.f64 + ctx.f18.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f17,f0
	ctx.f0.f64 = double(float(ctx.f17.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f16,f0
	ctx.f0.f64 = double(float(ctx.f16.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f15,f0
	ctx.f0.f64 = double(float(ctx.f15.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f14,f0
	ctx.f0.f64 = double(float(ctx.f14.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822d1394
	goto loc_822D1394;
loc_822D1354:
	// fmuls f18,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.f18.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfs f19,0(r11)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f17,f0,f8
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// fmuls f16,f0,f6
	ctx.f16.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// fmuls f15,f0,f4
	ctx.f15.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f19,f13,f9,f18
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f18.f64)));
	// stfsu f19,4(r11)
	temp.f32 = float(ctx.f19.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f19,f13,f7,f17
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f17.f64)));
	// stfsu f19,4(r11)
	temp.f32 = float(ctx.f19.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f19,f13,f5,f16
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f16.f64)));
	// stfsu f19,4(r11)
	temp.f32 = float(ctx.f19.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f19,f13,f3,f15
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f3.f64), float(ctx.f15.f64)));
	// stfsu f19,4(r11)
	temp.f32 = float(ctx.f19.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f13,f13,f1,f0
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f0.f64)));
	// stfsu f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
loc_822D1394:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fadds f12,f31,f12
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f31.f64 + ctx.f12.f64));
	// fadds f11,f30,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 + ctx.f11.f64));
	// fadds f10,f29,f10
	ctx.f10.f64 = double(float(ctx.f29.f64 + ctx.f10.f64));
	// fadds f9,f28,f9
	ctx.f9.f64 = double(float(ctx.f28.f64 + ctx.f9.f64));
	// fadds f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 + ctx.f8.f64));
	// fadds f7,f26,f7
	ctx.f7.f64 = double(float(ctx.f26.f64 + ctx.f7.f64));
	// fadds f6,f25,f6
	ctx.f6.f64 = double(float(ctx.f25.f64 + ctx.f6.f64));
	// fadds f5,f24,f5
	ctx.f5.f64 = double(float(ctx.f24.f64 + ctx.f5.f64));
	// fadds f4,f23,f4
	ctx.f4.f64 = double(float(ctx.f23.f64 + ctx.f4.f64));
	// fadds f3,f22,f3
	ctx.f3.f64 = double(float(ctx.f22.f64 + ctx.f3.f64));
	// fadds f2,f21,f2
	ctx.f2.f64 = double(float(ctx.f21.f64 + ctx.f2.f64));
	// fadds f1,f20,f1
	ctx.f1.f64 = double(float(ctx.f20.f64 + ctx.f1.f64));
	// bdnz 0x822d12bc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D12BC;
loc_822D13CC:
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D13D4;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D13E0"))) PPC_WEAK_FUNC(sub_822D13E0);
PPC_FUNC_IMPL(__imp__sub_822D13E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa00
	ctx.lr = 0x822D13F0;
	sub_8233FA00(ctx, base);
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r7,32(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f0,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,12(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,16(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,20(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,24(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,28(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,32(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 32);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,36(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 36);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,40(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 40);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,44(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 44);
	ctx.f3.f64 = double(temp.f32);
	// bne cr6,0x822d1500
	if (!ctx.cr6.eq) goto loc_822D1500;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x822d160c
	if (ctx.cr6.eq) goto loc_822D160C;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,6
	ctx.r8.s64 = 6;
	// divwu r9,r9,r8
	ctx.r9.u32 = ctx.r9.u32 / ctx.r8.u32;
	// li r8,96
	ctx.r8.s64 = 96;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,72
	ctx.r9.s64 = 72;
loc_822D1464:
	// lfs f2,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// lfsu f31,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f31.f64 = double(temp.f32);
	// lfsu f30,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f30.f64 = double(temp.f32);
	// lfsu f29,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f29.f64 = double(temp.f32);
	// lfsu f28,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f28.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// dcbt r11,r8
	// dcbt r10,r9
	// fmuls f27,f29,f10
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f10.f64));
	// fmuls f29,f29,f4
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f4.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmadds f27,f28,f9,f27
	ctx.f27.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f9.f64), float(ctx.f27.f64)));
	// fmadds f29,f28,f3,f29
	ctx.f29.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f3.f64), float(ctx.f29.f64)));
	// fmadds f28,f30,f11,f27
	ctx.f28.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f11.f64), float(ctx.f27.f64)));
	// fmadds f30,f30,f5,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f5.f64), float(ctx.f29.f64)));
	// fmadds f29,f31,f12,f28
	ctx.f29.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f12.f64), float(ctx.f28.f64)));
	// fmadds f31,f31,f6,f30
	ctx.f31.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f6.f64), float(ctx.f30.f64)));
	// fmadds f30,f1,f13,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f13.f64), float(ctx.f29.f64)));
	// fmadds f1,f1,f7,f31
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f7.f64), float(ctx.f31.f64)));
	// fmadds f31,f2,f0,f30
	ctx.f31.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f0.f64), float(ctx.f30.f64)));
	// fmadds f2,f2,f8,f1
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f8.f64), float(ctx.f1.f64)));
	// beq cr6,0x822d14dc
	if (ctx.cr6.eq) goto loc_822D14DC;
	// lfs f26,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f26.f64 = double(temp.f32);
	// fadds f1,f31,f26
	ctx.f1.f64 = double(float(ctx.f31.f64 + ctx.f26.f64));
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f1,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fadds f2,f2,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f1.f64));
	// stfs f2,0(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d14e4
	goto loc_822D14E4;
loc_822D14DC:
	// stfs f31,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfsu f2,4(r10)
	temp.f32 = float(ctx.f2.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
loc_822D14E4:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x822d1464
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D1464;
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D14F4;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_822D1500:
	// lfs f27,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f27.f64 = double(temp.f32);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f26,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,24(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,28(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,32(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,36(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,40(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,44(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	ctx.f16.f64 = double(temp.f32);
	// beq cr6,0x822d160c
	if (ctx.cr6.eq) goto loc_822D160C;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,6
	ctx.r8.s64 = 6;
	// divwu r9,r9,r8
	ctx.r9.u32 = ctx.r9.u32 / ctx.r8.u32;
	// li r8,96
	ctx.r8.s64 = 96;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,72
	ctx.r9.s64 = 72;
loc_822D1554:
	// lfs f2,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// lfsu f31,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f31.f64 = double(temp.f32);
	// lfsu f30,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f30.f64 = double(temp.f32);
	// lfsu f29,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f29.f64 = double(temp.f32);
	// lfsu f28,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f28.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// dcbt r11,r8
	// dcbt r10,r9
	// fmuls f15,f29,f10
	ctx.f15.f64 = double(float(ctx.f29.f64 * ctx.f10.f64));
	// fmuls f29,f29,f4
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f4.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmadds f15,f28,f9,f15
	ctx.f15.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f9.f64), float(ctx.f15.f64)));
	// fmadds f29,f28,f3,f29
	ctx.f29.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f3.f64), float(ctx.f29.f64)));
	// fmadds f28,f30,f11,f15
	ctx.f28.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f11.f64), float(ctx.f15.f64)));
	// fmadds f30,f30,f5,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f5.f64), float(ctx.f29.f64)));
	// fmadds f29,f31,f12,f28
	ctx.f29.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f12.f64), float(ctx.f28.f64)));
	// fmadds f31,f31,f6,f30
	ctx.f31.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f6.f64), float(ctx.f30.f64)));
	// fmadds f30,f1,f13,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f13.f64), float(ctx.f29.f64)));
	// fmadds f1,f1,f7,f31
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f7.f64), float(ctx.f31.f64)));
	// fmadds f31,f2,f0,f30
	ctx.f31.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f0.f64), float(ctx.f30.f64)));
	// fmadds f2,f2,f8,f1
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f8.f64), float(ctx.f1.f64)));
	// beq cr6,0x822d15cc
	if (ctx.cr6.eq) goto loc_822D15CC;
	// lfs f14,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f14.f64 = double(temp.f32);
	// fadds f1,f31,f14
	ctx.f1.f64 = double(float(ctx.f31.f64 + ctx.f14.f64));
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f1,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fadds f2,f2,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f1.f64));
	// stfs f2,0(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d15d4
	goto loc_822D15D4;
loc_822D15CC:
	// stfs f31,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfsu f2,4(r10)
	temp.f32 = float(ctx.f2.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
loc_822D15D4:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f27,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f27.f64 + ctx.f0.f64));
	// fadds f13,f26,f13
	ctx.f13.f64 = double(float(ctx.f26.f64 + ctx.f13.f64));
	// fadds f12,f25,f12
	ctx.f12.f64 = double(float(ctx.f25.f64 + ctx.f12.f64));
	// fadds f11,f24,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 + ctx.f11.f64));
	// fadds f10,f23,f10
	ctx.f10.f64 = double(float(ctx.f23.f64 + ctx.f10.f64));
	// fadds f9,f22,f9
	ctx.f9.f64 = double(float(ctx.f22.f64 + ctx.f9.f64));
	// fadds f8,f21,f8
	ctx.f8.f64 = double(float(ctx.f21.f64 + ctx.f8.f64));
	// fadds f7,f20,f7
	ctx.f7.f64 = double(float(ctx.f20.f64 + ctx.f7.f64));
	// fadds f6,f19,f6
	ctx.f6.f64 = double(float(ctx.f19.f64 + ctx.f6.f64));
	// fadds f5,f18,f5
	ctx.f5.f64 = double(float(ctx.f18.f64 + ctx.f5.f64));
	// fadds f4,f17,f4
	ctx.f4.f64 = double(float(ctx.f17.f64 + ctx.f4.f64));
	// fadds f3,f16,f3
	ctx.f3.f64 = double(float(ctx.f16.f64 + ctx.f3.f64));
	// bdnz 0x822d1554
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D1554;
loc_822D160C:
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D1614;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D1620"))) PPC_WEAK_FUNC(sub_822D1620);
PPC_FUNC_IMPL(__imp__sub_822D1620) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa00
	ctx.lr = 0x822D1630;
	sub_8233FA00(ctx, base);
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lfs f20,84(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 84);
	ctx.f20.f64 = double(temp.f32);
	// lwz r7,32(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f19,88(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 88);
	ctx.f19.f64 = double(temp.f32);
	// lfs f0,92(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,12(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,16(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,20(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,24(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 24);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,28(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,32(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 32);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,36(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 36);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,40(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 40);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,44(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 44);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,48(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 48);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,52(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 52);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,56(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 56);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,60(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 60);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,64(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 64);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,68(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 68);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,72(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 72);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,76(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 76);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,80(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// stfs f20,-172(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// stfs f19,-168(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// stfs f0,-164(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// bne cr6,0x822d1850
	if (!ctx.cr6.eq) goto loc_822D1850;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x822d1bd0
	if (ctx.cr6.eq) goto loc_822D1BD0;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,48
	ctx.r8.s64 = 48;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,216
	ctx.r9.s64 = 216;
loc_822D16DC:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfsu f13,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// lfsu f12,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// lfsu f11,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// dcbt r11,r8
	// dcbt r10,r9
	// fmuls f18,f12,f8
	ctx.f18.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fmuls f16,f12,f4
	ctx.f16.f64 = double(float(ctx.f12.f64 * ctx.f4.f64));
	// lfs f17,-164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f15,f12,f31
	ctx.f15.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmadds f18,f11,f7,f18
	ctx.f18.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f7.f64), float(ctx.f18.f64)));
	// fmadds f16,f11,f3,f16
	ctx.f16.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f3.f64), float(ctx.f16.f64)));
	// fmadds f15,f11,f30,f15
	ctx.f15.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f30.f64), float(ctx.f15.f64)));
	// beq cr6,0x822d17c8
	if (ctx.cr6.eq) goto loc_822D17C8;
	// stfd f8,-472(r1)
	PPC_STORE_U64(ctx.r1.u32 + -472, ctx.f8.u64);
	// fmuls f8,f12,f27
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// stfd f4,-480(r1)
	PPC_STORE_U64(ctx.r1.u32 + -480, ctx.f4.u64);
	// fmuls f4,f12,f23
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// fmuls f12,f12,f19
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f19.f64));
	// lfs f20,-172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	ctx.f20.f64 = double(temp.f32);
	// lfs f14,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,-168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f8,f11,f26,f8
	ctx.f8.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f26.f64), float(ctx.f8.f64)));
	// fmadds f4,f11,f22,f4
	ctx.f4.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f22.f64), float(ctx.f4.f64)));
	// fmadds f11,f11,f17,f12
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f17.f64), float(ctx.f12.f64)));
	// fmadds f12,f13,f9,f18
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f18.f64)));
	// fmadds f18,f13,f5,f16
	ctx.f18.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f16.f64)));
	// fmadds f17,f13,f1,f15
	ctx.f17.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f15.f64)));
	// fmadds f16,f13,f28,f8
	ctx.f16.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f28.f64), float(ctx.f8.f64)));
	// lfd f8,-472(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -472);
	// fmadds f15,f13,f24,f4
	ctx.f15.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f24.f64), float(ctx.f4.f64)));
	// lfd f4,-480(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -480);
	// fmadds f11,f13,f20,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f20.f64), float(ctx.f11.f64)));
	// fmadds f13,f0,f10,f12
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), float(ctx.f12.f64)));
	// fmadds f12,f0,f6,f18
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f6.f64), float(ctx.f18.f64)));
	// fmadds f18,f0,f2,f17
	ctx.f18.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f2.f64), float(ctx.f17.f64)));
	// fmadds f17,f0,f29,f16
	ctx.f17.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f29.f64), float(ctx.f16.f64)));
	// fmadds f16,f0,f25,f15
	ctx.f16.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f25.f64), float(ctx.f15.f64)));
	// fmadds f11,f0,f21,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f21.f64), float(ctx.f11.f64)));
	// fadds f0,f13,f14
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f14.f64));
	// stfs f0,0(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f13,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f0,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f18,f0
	ctx.f13.f64 = double(float(ctx.f18.f64 + ctx.f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f12,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fadds f0,f17,f12
	ctx.f0.f64 = double(float(ctx.f17.f64 + ctx.f12.f64));
	// stfs f0,0(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f13,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f16,f13
	ctx.f12.f64 = double(float(ctx.f16.f64 + ctx.f13.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f0,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f11,f0
	ctx.f13.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d1834
	goto loc_822D1834;
loc_822D17C8:
	// fmuls f14,f12,f27
	ctx.fpscr.disableFlushMode();
	ctx.f14.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// stfd f8,-480(r1)
	PPC_STORE_U64(ctx.r1.u32 + -480, ctx.f8.u64);
	// fmuls f8,f12,f23
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// fmuls f12,f12,f19
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f19.f64));
	// lfs f19,-168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f14,f11,f26,f14
	ctx.f14.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f26.f64), float(ctx.f14.f64)));
	// fmadds f8,f11,f22,f8
	ctx.f8.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f22.f64), float(ctx.f8.f64)));
	// fmadds f11,f11,f17,f12
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f17.f64), float(ctx.f12.f64)));
	// fmadds f12,f13,f9,f18
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f18.f64)));
	// fmadds f18,f13,f5,f16
	ctx.f18.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f16.f64)));
	// fmadds f17,f13,f1,f15
	ctx.f17.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f15.f64)));
	// fmadds f16,f13,f28,f14
	ctx.f16.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f28.f64), float(ctx.f14.f64)));
	// fmadds f15,f13,f24,f8
	ctx.f15.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f24.f64), float(ctx.f8.f64)));
	// lfd f8,-480(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -480);
	// fmadds f11,f13,f20,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f20.f64), float(ctx.f11.f64)));
	// fmadds f13,f0,f10,f12
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), float(ctx.f12.f64)));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmadds f12,f0,f6,f18
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f6.f64), float(ctx.f18.f64)));
	// stfsu f12,4(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f13,f0,f2,f17
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f2.f64), float(ctx.f17.f64)));
	// stfsu f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f12,f0,f29,f16
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f29.f64), float(ctx.f16.f64)));
	// stfsu f12,4(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f13,f0,f25,f15
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f25.f64), float(ctx.f15.f64)));
	// stfsu f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f12,f0,f21,f11
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f21.f64), float(ctx.f11.f64)));
	// stfsu f12,4(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
loc_822D1834:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x822d16dc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D16DC;
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D1844;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_822D1850:
	// stfd f10,-480(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -480, ctx.f10.u64);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// stfd f9,-472(r1)
	PPC_STORE_U64(ctx.r1.u32 + -472, ctx.f9.u64);
	// stfd f8,-320(r1)
	PPC_STORE_U64(ctx.r1.u32 + -320, ctx.f8.u64);
	// stfd f7,-296(r1)
	PPC_STORE_U64(ctx.r1.u32 + -296, ctx.f7.u64);
	// stfd f6,-280(r1)
	PPC_STORE_U64(ctx.r1.u32 + -280, ctx.f6.u64);
	// stfd f5,-304(r1)
	PPC_STORE_U64(ctx.r1.u32 + -304, ctx.f5.u64);
	// stfd f4,-336(r1)
	PPC_STORE_U64(ctx.r1.u32 + -336, ctx.f4.u64);
	// stfd f3,-272(r1)
	PPC_STORE_U64(ctx.r1.u32 + -272, ctx.f3.u64);
	// stfd f2,-360(r1)
	PPC_STORE_U64(ctx.r1.u32 + -360, ctx.f2.u64);
	// stfd f1,-288(r1)
	PPC_STORE_U64(ctx.r1.u32 + -288, ctx.f1.u64);
	// stfd f31,-352(r1)
	PPC_STORE_U64(ctx.r1.u32 + -352, ctx.f31.u64);
	// stfd f30,-344(r1)
	PPC_STORE_U64(ctx.r1.u32 + -344, ctx.f30.u64);
	// stfd f29,-328(r1)
	PPC_STORE_U64(ctx.r1.u32 + -328, ctx.f29.u64);
	// stfd f28,-312(r1)
	PPC_STORE_U64(ctx.r1.u32 + -312, ctx.f28.u64);
	// stfd f27,-368(r1)
	PPC_STORE_U64(ctx.r1.u32 + -368, ctx.f27.u64);
	// lfs f10,36(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,40(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,44(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,48(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,52(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 52);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,56(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 56);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,60(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 60);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,64(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 64);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,68(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 68);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,72(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 72);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,76(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 76);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,80(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 80);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,84(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,88(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 88);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,92(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 92);
	ctx.f27.f64 = double(temp.f32);
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f18,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,24(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,28(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,32(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	ctx.f14.f64 = double(temp.f32);
	// stfs f10,-428(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -428, temp.u32);
	// stfs f9,-424(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -424, temp.u32);
	// stfs f8,-420(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -420, temp.u32);
	// stfs f7,-416(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -416, temp.u32);
	// stfs f6,-412(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -412, temp.u32);
	// stfs f5,-408(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -408, temp.u32);
	// stfs f4,-404(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + -404, temp.u32);
	// stfs f3,-400(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -400, temp.u32);
	// stfs f2,-396(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -396, temp.u32);
	// stfs f1,-392(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -392, temp.u32);
	// stfs f31,-388(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + -388, temp.u32);
	// stfs f30,-384(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + -384, temp.u32);
	// stfs f29,-380(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + -380, temp.u32);
	// stfs f28,-376(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + -376, temp.u32);
	// stfs f27,-372(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + -372, temp.u32);
	// lfd f10,-480(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -480);
	// lfd f9,-472(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -472);
	// lfd f8,-320(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -320);
	// lfd f7,-296(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -296);
	// lfd f6,-280(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -280);
	// lfd f5,-304(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -304);
	// lfd f4,-336(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -336);
	// lfd f3,-272(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -272);
	// lfd f2,-360(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -360);
	// lfd f1,-288(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -288);
	// lfd f31,-352(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -352);
	// lfd f30,-344(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -344);
	// lfd f29,-328(r1)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -328);
	// lfd f28,-312(r1)
	ctx.f28.u64 = PPC_LOAD_U64(ctx.r1.u32 + -312);
	// lfd f27,-368(r1)
	ctx.f27.u64 = PPC_LOAD_U64(ctx.r1.u32 + -368);
	// stfs f0,-464(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -464, temp.u32);
	// stfs f13,-460(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -460, temp.u32);
	// stfs f12,-456(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -456, temp.u32);
	// stfs f11,-452(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -452, temp.u32);
	// stfs f18,-448(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + -448, temp.u32);
	// stfs f17,-444(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + -444, temp.u32);
	// stfs f16,-440(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + -440, temp.u32);
	// stfs f15,-436(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + -436, temp.u32);
	// stfs f14,-432(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + -432, temp.u32);
	// beq cr6,0x822d1bd0
	if (ctx.cr6.eq) goto loc_822D1BD0;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,48
	ctx.r8.s64 = 48;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,216
	ctx.r9.s64 = 216;
loc_822D19A8:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfsu f13,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// lfsu f12,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// lfsu f11,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// dcbt r11,r8
	// dcbt r10,r9
	// fmuls f18,f12,f8
	ctx.f18.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmuls f19,f12,f19
	ctx.f19.f64 = double(float(ctx.f12.f64 * ctx.f19.f64));
	// fmadds f18,f11,f7,f18
	ctx.f18.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f7.f64), float(ctx.f18.f64)));
	// beq cr6,0x822d1a88
	if (ctx.cr6.eq) goto loc_822D1A88;
	// fmuls f16,f12,f4
	ctx.f16.f64 = double(float(ctx.f12.f64 * ctx.f4.f64));
	// stfd f8,-368(r1)
	PPC_STORE_U64(ctx.r1.u32 + -368, ctx.f8.u64);
	// fmuls f15,f12,f31
	ctx.f15.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// lfs f20,-172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f14,f12,f27
	ctx.f14.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// lfs f17,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f8,f12,f23
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// lfs f12,-164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f16,f11,f3,f16
	ctx.f16.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f3.f64), float(ctx.f16.f64)));
	// fmadds f15,f11,f30,f15
	ctx.f15.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f30.f64), float(ctx.f15.f64)));
	// fmadds f14,f11,f26,f14
	ctx.f14.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f26.f64), float(ctx.f14.f64)));
	// fmadds f8,f11,f22,f8
	ctx.f8.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f22.f64), float(ctx.f8.f64)));
	// fmadds f11,f11,f12,f19
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f12.f64), float(ctx.f19.f64)));
	// fmadds f19,f13,f9,f18
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f18.f64)));
	// fmadds f18,f13,f5,f16
	ctx.f18.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f16.f64)));
	// fmadds f16,f13,f1,f15
	ctx.f16.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f15.f64)));
	// fmadds f15,f13,f28,f14
	ctx.f15.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f28.f64), float(ctx.f14.f64)));
	// fmadds f14,f13,f24,f8
	ctx.f14.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f24.f64), float(ctx.f8.f64)));
	// lfd f8,-368(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -368);
	// fmadds f13,f13,f20,f11
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f20.f64), float(ctx.f11.f64)));
	// fmadds f11,f0,f10,f19
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), float(ctx.f19.f64)));
	// fmadds f19,f0,f6,f18
	ctx.f19.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f6.f64), float(ctx.f18.f64)));
	// fmadds f18,f0,f2,f16
	ctx.f18.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f2.f64), float(ctx.f16.f64)));
	// fmadds f16,f0,f29,f15
	ctx.f16.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f29.f64), float(ctx.f15.f64)));
	// fmadds f15,f0,f25,f14
	ctx.f15.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f25.f64), float(ctx.f14.f64)));
	// fmadds f0,f0,f21,f13
	ctx.f0.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f21.f64), float(ctx.f13.f64)));
	// fadds f13,f11,f17
	ctx.f13.f64 = double(float(ctx.f11.f64 + ctx.f17.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f11,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fadds f13,f19,f11
	ctx.f13.f64 = double(float(ctx.f19.f64 + ctx.f11.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f11,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fadds f13,f18,f11
	ctx.f13.f64 = double(float(ctx.f18.f64 + ctx.f11.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f11,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fadds f13,f16,f11
	ctx.f13.f64 = double(float(ctx.f16.f64 + ctx.f11.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f11,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fadds f13,f15,f11
	ctx.f13.f64 = double(float(ctx.f15.f64 + ctx.f11.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f11,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fadds f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f11.f64));
	// stfs f0,0(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d1af8
	goto loc_822D1AF8;
loc_822D1A88:
	// fmuls f17,f12,f4
	ctx.fpscr.disableFlushMode();
	ctx.f17.f64 = double(float(ctx.f12.f64 * ctx.f4.f64));
	// fmuls f16,f12,f31
	ctx.f16.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f15,f12,f27
	ctx.f15.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// fmuls f14,f12,f23
	ctx.f14.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// lfs f12,-164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f17,f11,f3,f17
	ctx.f17.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f3.f64), float(ctx.f17.f64)));
	// fmadds f16,f11,f30,f16
	ctx.f16.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f30.f64), float(ctx.f16.f64)));
	// fmadds f15,f11,f26,f15
	ctx.f15.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f26.f64), float(ctx.f15.f64)));
	// fmadds f14,f11,f22,f14
	ctx.f14.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f22.f64), float(ctx.f14.f64)));
	// fmadds f11,f11,f12,f19
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f12.f64), float(ctx.f19.f64)));
	// fmadds f19,f13,f9,f18
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f18.f64)));
	// fmadds f18,f13,f5,f17
	ctx.f18.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f17.f64)));
	// fmadds f17,f13,f1,f16
	ctx.f17.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f16.f64)));
	// fmadds f16,f13,f28,f15
	ctx.f16.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f28.f64), float(ctx.f15.f64)));
	// fmadds f15,f13,f24,f14
	ctx.f15.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f24.f64), float(ctx.f14.f64)));
	// fmadds f13,f13,f20,f11
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f20.f64), float(ctx.f11.f64)));
	// fmadds f11,f0,f10,f19
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), float(ctx.f19.f64)));
	// stfs f11,0(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmadds f11,f0,f6,f18
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f6.f64), float(ctx.f18.f64)));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f11,f0,f2,f17
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f2.f64), float(ctx.f17.f64)));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f11,f0,f29,f16
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f29.f64), float(ctx.f16.f64)));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f11,f0,f25,f15
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f25.f64), float(ctx.f15.f64)));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f0,f0,f21,f13
	ctx.f0.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f21.f64), float(ctx.f13.f64)));
	// stfsu f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
loc_822D1AF8:
	// lfs f13,-380(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -380);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f20,f13,f20
	ctx.f20.f64 = double(float(ctx.f13.f64 + ctx.f20.f64));
	// lfs f0,-168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,-376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -376);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -372);
	ctx.f13.f64 = double(temp.f32);
	// fadds f19,f11,f0
	ctx.f19.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// fadds f0,f13,f12
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// lfs f11,-464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -464);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -460);
	ctx.f13.f64 = double(temp.f32);
	// fadds f10,f11,f10
	ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// lfs f12,-456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -456);
	ctx.f12.f64 = double(temp.f32);
	// fadds f9,f13,f9
	ctx.f9.f64 = double(float(ctx.f13.f64 + ctx.f9.f64));
	// fadds f8,f12,f8
	ctx.f8.f64 = double(float(ctx.f12.f64 + ctx.f8.f64));
	// lfs f11,-452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -452);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	ctx.f13.f64 = double(temp.f32);
	// fadds f7,f11,f7
	ctx.f7.f64 = double(float(ctx.f11.f64 + ctx.f7.f64));
	// lfs f12,-444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -444);
	ctx.f12.f64 = double(temp.f32);
	// fadds f6,f13,f6
	ctx.f6.f64 = double(float(ctx.f13.f64 + ctx.f6.f64));
	// fadds f5,f12,f5
	ctx.f5.f64 = double(float(ctx.f12.f64 + ctx.f5.f64));
	// lfs f11,-440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -440);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -436);
	ctx.f13.f64 = double(temp.f32);
	// fadds f4,f11,f4
	ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f4.f64));
	// lfs f12,-432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -432);
	ctx.f12.f64 = double(temp.f32);
	// fadds f3,f13,f3
	ctx.f3.f64 = double(float(ctx.f13.f64 + ctx.f3.f64));
	// fadds f2,f12,f2
	ctx.f2.f64 = double(float(ctx.f12.f64 + ctx.f2.f64));
	// lfs f11,-428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -428);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -424);
	ctx.f13.f64 = double(temp.f32);
	// fadds f1,f11,f1
	ctx.f1.f64 = double(float(ctx.f11.f64 + ctx.f1.f64));
	// lfs f12,-420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -420);
	ctx.f12.f64 = double(temp.f32);
	// fadds f31,f13,f31
	ctx.f31.f64 = double(float(ctx.f13.f64 + ctx.f31.f64));
	// fadds f30,f12,f30
	ctx.f30.f64 = double(float(ctx.f12.f64 + ctx.f30.f64));
	// lfs f11,-416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -416);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	ctx.f13.f64 = double(temp.f32);
	// fadds f29,f11,f29
	ctx.f29.f64 = double(float(ctx.f11.f64 + ctx.f29.f64));
	// lfs f12,-408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	ctx.f12.f64 = double(temp.f32);
	// fadds f28,f13,f28
	ctx.f28.f64 = double(float(ctx.f13.f64 + ctx.f28.f64));
	// fadds f27,f12,f27
	ctx.f27.f64 = double(float(ctx.f12.f64 + ctx.f27.f64));
	// lfs f11,-404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -404);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -400);
	ctx.f13.f64 = double(temp.f32);
	// fadds f26,f11,f26
	ctx.f26.f64 = double(float(ctx.f11.f64 + ctx.f26.f64));
	// lfs f12,-396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -396);
	ctx.f12.f64 = double(temp.f32);
	// fadds f25,f13,f25
	ctx.f25.f64 = double(float(ctx.f13.f64 + ctx.f25.f64));
	// fadds f24,f12,f24
	ctx.f24.f64 = double(float(ctx.f12.f64 + ctx.f24.f64));
	// lfs f11,-392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -392);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -388);
	ctx.f13.f64 = double(temp.f32);
	// fadds f23,f11,f23
	ctx.f23.f64 = double(float(ctx.f11.f64 + ctx.f23.f64));
	// lfs f12,-384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -384);
	ctx.f12.f64 = double(temp.f32);
	// fadds f22,f13,f22
	ctx.f22.f64 = double(float(ctx.f13.f64 + ctx.f22.f64));
	// stfs f20,-172(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// fadds f21,f12,f21
	ctx.f21.f64 = double(float(ctx.f12.f64 + ctx.f21.f64));
	// stfs f19,-168(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// stfs f0,-164(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// bdnz 0x822d19a8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D19A8;
loc_822D1BD0:
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D1BD8;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D1BE4"))) PPC_WEAK_FUNC(sub_822D1BE4);
PPC_FUNC_IMPL(__imp__sub_822D1BE4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D1BE8"))) PPC_WEAK_FUNC(sub_822D1BE8);
PPC_FUNC_IMPL(__imp__sub_822D1BE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822D1BF0;
	__restfpr_27(ctx, base);
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lwz r28,0(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r30,4(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r29,20(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r4,32(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// beq cr6,0x822d1c14
	if (ctx.cr6.eq) goto loc_822D1C14;
	// li r6,1
	ctx.r6.s64 = 1;
loc_822D1C14:
	// addi r11,r6,-1
	ctx.r11.s64 = ctx.r6.s64 + -1;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bgt cr6,0x822d1db0
	if (ctx.cr6.gt) goto loc_822D1DB0;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bdzf 4*cr6+eq,0x822d1c58
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D1C58;
	// bdzf 4*cr6+eq,0x822d1c80
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D1C80;
	// bdzf 4*cr6+eq,0x822d1cb4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D1CB4;
	// bdzf 4*cr6+eq,0x822d1cf4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D1CF4;
	// bne cr6,0x822d1d40
	if (!ctx.cr6.eq) goto loc_822D1D40;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,-112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// beq cr6,0x822d1db0
	if (ctx.cr6.eq) goto loc_822D1DB0;
	// b 0x822d1da8
	goto loc_822D1DA8;
loc_822D1C58:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// stfs f0,-112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f13,-108(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// beq cr6,0x822d1db0
	if (ctx.cr6.eq) goto loc_822D1DB0;
	// lfs f13,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// b 0x822d1da4
	goto loc_822D1DA4;
loc_822D1C80:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,32(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f0,-112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f13,-108(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f12,-104(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// beq cr6,0x822d1db0
	if (ctx.cr6.eq) goto loc_822D1DB0;
	// lfs f13,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,32(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	ctx.f12.f64 = double(temp.f32);
	// b 0x822d1da0
	goto loc_822D1DA0;
loc_822D1CB4:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,40(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,60(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	ctx.f11.f64 = double(temp.f32);
	// stfs f0,-112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f13,-108(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f12,-104(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// stfs f11,-100(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -100, temp.u32);
	// beq cr6,0x822d1db0
	if (ctx.cr6.eq) goto loc_822D1DB0;
	// lfs f13,20(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,40(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,60(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 60);
	ctx.f11.f64 = double(temp.f32);
	// b 0x822d1d9c
	goto loc_822D1D9C;
loc_822D1CF4:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,24(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,48(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,72(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 72);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,96(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 96);
	ctx.f10.f64 = double(temp.f32);
	// stfs f0,-112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f13,-108(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f12,-104(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// stfs f11,-100(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -100, temp.u32);
	// stfs f10,-96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -96, temp.u32);
	// beq cr6,0x822d1db0
	if (ctx.cr6.eq) goto loc_822D1DB0;
	// lfs f13,24(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,48(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,72(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,96(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 96);
	ctx.f10.f64 = double(temp.f32);
	// b 0x822d1d98
	goto loc_822D1D98;
loc_822D1D40:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,28(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,56(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,84(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 84);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,112(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,140(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 140);
	ctx.f9.f64 = double(temp.f32);
	// stfs f0,-112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f13,-108(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f12,-104(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// stfs f11,-100(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -100, temp.u32);
	// stfs f10,-96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -96, temp.u32);
	// stfs f9,-92(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -92, temp.u32);
	// beq cr6,0x822d1db0
	if (ctx.cr6.eq) goto loc_822D1DB0;
	// lfs f9,140(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	ctx.f9.f64 = double(temp.f32);
	// lfs f13,28(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,56(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 56);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,84(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 84);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,112(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// stfs f9,-60(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
loc_822D1D98:
	// stfs f10,-64(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
loc_822D1D9C:
	// stfs f11,-68(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -68, temp.u32);
loc_822D1DA0:
	// stfs f12,-72(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -72, temp.u32);
loc_822D1DA4:
	// stfs f13,-76(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -76, temp.u32);
loc_822D1DA8:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,-80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -80, temp.u32);
loc_822D1DB0:
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822d1f60
	if (!ctx.cr6.eq) goto loc_822D1F60;
	// cmpwi cr6,r29,4
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 4, ctx.xer);
	// blt cr6,0x822d1ef8
	if (ctx.cr6.lt) goto loc_822D1EF8;
	// addi r3,r29,-3
	ctx.r3.s64 = ctx.r29.s64 + -3;
	// addi r9,r28,12
	ctx.r9.s64 = ctx.r28.s64 + 12;
	// addi r10,r30,4
	ctx.r10.s64 = ctx.r30.s64 + 4;
	// subf r31,r30,r28
	ctx.r31.s64 = ctx.r28.s64 - ctx.r30.s64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r1,-112
	ctx.r7.s64 = ctx.r1.s64 + -112;
loc_822D1DE4:
	// lfs f0,-12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -12);
	ctx.f0.f64 = double(temp.f32);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// lfsx f13,r8,r7
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// beq cr6,0x822d1e08
	if (ctx.cr6.eq) goto loc_822D1E08;
	// lfs f13,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,-4(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// b 0x822d1e0c
	goto loc_822D1E0C;
loc_822D1E08:
	// stfs f0,-4(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
loc_822D1E0C:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lfsx f0,r31,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r10.u32);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// subfc r7,r6,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r6.u32;
	ctx.r7.s64 = ctx.r11.s64 - ctx.r6.s64;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// subfe r7,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r7.u64 = ~ctx.r7.u64 + ctx.r7.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 & ctx.r11.u64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r7,r8
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// beq cr6,0x822d1e48
	if (ctx.cr6.eq) goto loc_822D1E48;
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d1e4c
	goto loc_822D1E4C;
loc_822D1E48:
	// stfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
loc_822D1E4C:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lfs f0,-4(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// subfc r7,r6,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r6.u32;
	ctx.r7.s64 = ctx.r11.s64 - ctx.r6.s64;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// subfe r7,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r7.u64 = ~ctx.r7.u64 + ctx.r7.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 & ctx.r11.u64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r7,r8
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// beq cr6,0x822d1e88
	if (ctx.cr6.eq) goto loc_822D1E88;
	// lfs f13,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,4(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// b 0x822d1e8c
	goto loc_822D1E8C;
loc_822D1E88:
	// stfs f0,4(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
loc_822D1E8C:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lfs f0,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// subfc r7,r6,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r6.u32;
	ctx.r7.s64 = ctx.r11.s64 - ctx.r6.s64;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// subfe r7,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r7.u64 = ~ctx.r7.u64 + ctx.r7.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 & ctx.r11.u64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r7,r8
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// beq cr6,0x822d1ec8
	if (ctx.cr6.eq) goto loc_822D1EC8;
	// lfs f13,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,8(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// b 0x822d1ecc
	goto loc_822D1ECC;
loc_822D1EC8:
	// stfs f0,8(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
loc_822D1ECC:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// subfc r8,r6,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r6.u32;
	ctx.r8.s64 = ctx.r11.s64 - ctx.r6.s64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// subfe r8,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r8.u64 = ~ctx.r7.u64 + ctx.r7.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// and r11,r8,r11
	ctx.r11.u64 = ctx.r8.u64 & ctx.r11.u64;
	// cmplw cr6,r5,r3
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r3.u32, ctx.xer);
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r1,-112
	ctx.r7.s64 = ctx.r1.s64 + -112;
	// blt cr6,0x822d1de4
	if (ctx.cr6.lt) goto loc_822D1DE4;
loc_822D1EF8:
	// cmplw cr6,r5,r29
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r29.u32, ctx.xer);
	// bge cr6,0x822d2140
	if (!ctx.cr6.lt) goto loc_822D2140;
	// subf r8,r5,r29
	ctx.r8.s64 = ctx.r29.s64 - ctx.r5.s64;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r30,r28
	ctx.r9.s64 = ctx.r28.s64 - ctx.r30.s64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + ctx.r30.u64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D1F14:
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r10,r9
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	ctx.f0.f64 = double(temp.f32);
	// addi r7,r1,-112
	ctx.r7.s64 = ctx.r1.s64 + -112;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// lfsx f13,r8,r7
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// beq cr6,0x822d1f40
	if (ctx.cr6.eq) goto loc_822D1F40;
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d1f44
	goto loc_822D1F44;
loc_822D1F40:
	// stfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
loc_822D1F44:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// subfc r8,r6,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r6.u32;
	ctx.r8.s64 = ctx.r11.s64 - ctx.r6.s64;
	// subfe r5,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r5.u64 = ~ctx.r7.u64 + ctx.r7.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 & ctx.r11.u64;
	// bdnz 0x822d1f14
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D1F14;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822D1F60:
	// cmpwi cr6,r29,4
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 4, ctx.xer);
	// blt cr6,0x822d20cc
	if (ctx.cr6.lt) goto loc_822D20CC;
	// addi r31,r29,-3
	ctx.r31.s64 = ctx.r29.s64 + -3;
	// addi r8,r28,12
	ctx.r8.s64 = ctx.r28.s64 + 12;
	// addi r7,r30,4
	ctx.r7.s64 = ctx.r30.s64 + 4;
	// subf r3,r30,r28
	ctx.r3.s64 = ctx.r28.s64 - ctx.r30.s64;
loc_822D1F78:
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f0,-12(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -12);
	ctx.f0.f64 = double(temp.f32);
	// addi r9,r1,-112
	ctx.r9.s64 = ctx.r1.s64 + -112;
	// addi r27,r1,-80
	ctx.r27.s64 = ctx.r1.s64 + -80;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// lfsx f13,r10,r9
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// lfsx f12,r10,r27
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r27.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// stfsx f11,r10,r9
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, temp.u32);
	// beq cr6,0x822d1fb4
	if (ctx.cr6.eq) goto loc_822D1FB4;
	// lfs f13,-4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,-4(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + -4, temp.u32);
	// b 0x822d1fb8
	goto loc_822D1FB8;
loc_822D1FB4:
	// stfs f0,-4(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + -4, temp.u32);
loc_822D1FB8:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lfsx f0,r7,r3
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r3.u32);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r1,-112
	ctx.r10.s64 = ctx.r1.s64 + -112;
	// subfc r9,r6,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r6.u32;
	ctx.r9.s64 = ctx.r11.s64 - ctx.r6.s64;
	// addi r27,r1,-80
	ctx.r27.s64 = ctx.r1.s64 + -80;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// and r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 & ctx.r11.u64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r11,r10
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// lfsx f12,r11,r27
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r27.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// stfsx f11,r11,r10
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, temp.u32);
	// beq cr6,0x822d2004
	if (ctx.cr6.eq) goto loc_822D2004;
	// lfs f13,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,0(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// b 0x822d2008
	goto loc_822D2008;
loc_822D2004:
	// stfs f0,0(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
loc_822D2008:
	// addi r11,r9,1
	ctx.r11.s64 = ctx.r9.s64 + 1;
	// lfs f0,-4(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r1,-112
	ctx.r10.s64 = ctx.r1.s64 + -112;
	// subfc r9,r6,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r6.u32;
	ctx.r9.s64 = ctx.r11.s64 - ctx.r6.s64;
	// addi r27,r1,-80
	ctx.r27.s64 = ctx.r1.s64 + -80;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// and r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 & ctx.r11.u64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r11,r10
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// lfsx f12,r11,r27
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r27.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// stfsx f11,r11,r10
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, temp.u32);
	// beq cr6,0x822d2054
	if (ctx.cr6.eq) goto loc_822D2054;
	// lfs f13,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,4(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// b 0x822d2058
	goto loc_822D2058;
loc_822D2054:
	// stfs f0,4(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
loc_822D2058:
	// addi r11,r9,1
	ctx.r11.s64 = ctx.r9.s64 + 1;
	// lfs f0,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r1,-112
	ctx.r10.s64 = ctx.r1.s64 + -112;
	// subfc r9,r6,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r6.u32;
	ctx.r9.s64 = ctx.r11.s64 - ctx.r6.s64;
	// addi r27,r1,-80
	ctx.r27.s64 = ctx.r1.s64 + -80;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// and r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 & ctx.r11.u64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r11,r10
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// lfsx f12,r11,r27
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r27.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// stfsx f11,r11,r10
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, temp.u32);
	// beq cr6,0x822d20a4
	if (ctx.cr6.eq) goto loc_822D20A4;
	// lfs f13,8(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,8(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 8, temp.u32);
	// b 0x822d20a8
	goto loc_822D20A8;
loc_822D20A4:
	// stfs f0,8(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 8, temp.u32);
loc_822D20A8:
	// addi r11,r9,1
	ctx.r11.s64 = ctx.r9.s64 + 1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// subfc r10,r6,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r6.u32;
	ctx.r10.s64 = ctx.r11.s64 - ctx.r6.s64;
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// subfe r10,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r10.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// and r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 & ctx.r11.u64;
	// cmplw cr6,r5,r31
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r31.u32, ctx.xer);
	// blt cr6,0x822d1f78
	if (ctx.cr6.lt) goto loc_822D1F78;
loc_822D20CC:
	// cmplw cr6,r5,r29
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r29.u32, ctx.xer);
	// bge cr6,0x822d2140
	if (!ctx.cr6.lt) goto loc_822D2140;
	// subf r8,r5,r29
	ctx.r8.s64 = ctx.r29.s64 - ctx.r5.s64;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r30,r28
	ctx.r7.s64 = ctx.r28.s64 - ctx.r30.s64;
	// add r9,r10,r30
	ctx.r9.u64 = ctx.r10.u64 + ctx.r30.u64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D20E8:
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r9,r7
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r7.u32);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// addi r5,r1,-80
	ctx.r5.s64 = ctx.r1.s64 + -80;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// lfsx f13,r10,r8
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	ctx.f13.f64 = double(temp.f32);
	// lfsx f12,r10,r5
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r5.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// stfsx f11,r10,r8
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, temp.u32);
	// beq cr6,0x822d2124
	if (ctx.cr6.eq) goto loc_822D2124;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,0(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// b 0x822d2128
	goto loc_822D2128;
loc_822D2124:
	// stfs f0,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
loc_822D2128:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// subfc r10,r6,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r6.u32;
	ctx.r10.s64 = ctx.r11.s64 - ctx.r6.s64;
	// subfe r5,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r5.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r5,r11
	ctx.r11.u64 = ctx.r5.u64 & ctx.r11.u64;
	// bdnz 0x822d20e8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D20E8;
loc_822D2140:
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D2144"))) PPC_WEAK_FUNC(sub_822D2144);
PPC_FUNC_IMPL(__imp__sub_822D2144) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D2148"))) PPC_WEAK_FUNC(sub_822D2148);
PPC_FUNC_IMPL(__imp__sub_822D2148) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r4,20(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// ld r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// lwz r3,32(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// std r8,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r8.u64);
	// bne cr6,0x822d2190
	if (!ctx.cr6.eq) goto loc_822D2190;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// std r8,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r8.u64);
	// lfs f0,-28948(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,-28(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// stfs f0,-20(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -20, temp.u32);
	// b 0x822d21b8
	goto loc_822D21B8;
loc_822D2190:
	// lfs f13,4(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// fadds f10,f12,f0
	ctx.f10.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// fadds f9,f11,f13
	ctx.f9.f64 = double(float(ctx.f11.f64 + ctx.f13.f64));
	// stfs f13,-28(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// stfs f10,-40(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f9,-36(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// stfs f13,-20(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -20, temp.u32);
loc_822D21B8:
	// li r9,16
	ctx.r9.s64 = 16;
	// stfs f0,-24(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -24, temp.u32);
	// addi r7,r1,-32
	ctx.r7.s64 = ctx.r1.s64 + -32;
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r5,r1,-32
	ctx.r5.s64 = ctx.r1.s64 + -32;
	// addi r10,r1,-48
	ctx.r10.s64 = ctx.r1.s64 + -48;
	// addi r8,r1,-48
	ctx.r8.s64 = ctx.r1.s64 + -48;
	// rlwinm r30,r11,0,28,29
	ctx.r30.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xC;
	// lvrx128 v61,r9,r7
	temp.u32 = ctx.r9.u32 + ctx.r7.u32;
	simd::store_i8(ctx.v61.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// li r31,4
	ctx.r31.s64 = 4;
	// lvlx128 v60,r0,r5
	temp.u32 = ctx.r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// lvrx128 v63,r9,r10
	temp.u32 = ctx.r9.u32 + ctx.r10.u32;
	simd::store_i8(ctx.v63.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// lvlx128 v62,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v0,v62,v63
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v63.u8)));
	// vor128 v62,v60,v61
	simd::store_i8(ctx.v62.u8, simd::or_i8(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v61.u8)));
	// beq cr6,0x822d223c
	if (ctx.cr6.eq) goto loc_822D223C;
	// cmplwi cr6,r4,3
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 3, ctx.xer);
	// ble cr6,0x822d22e0
	if (!ctx.cr6.gt) goto loc_822D22E0;
	// lvlx128 v59,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// vspltw128 v58,v59,0
	simd::store_i32(ctx.v58.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v59.u32), 3));
	// vmulfp128 v63,v58,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v58.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vaddfp128 v0,v0,v62
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// beq cr6,0x822d2224
	if (ctx.cr6.eq) goto loc_822D2224;
	// lvlx128 v57,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v63,v63,v57
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v57.f32)));
loc_822D2224:
	// vpermwi128 v56,v63,17
	simd::store_i32(ctx.v56.u32, simd::permute_i32_dispatch(simd::load_i32(ctx.v63.u32), 0xEE));
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// stvewx128 v56,r0,r11
	PPC_STORE_U32((ctx.r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v56.u32), 3 - ((ctx.r11.u32) & 0xF) >> 2));
	// stvewx128 v56,r11,r31
	PPC_STORE_U32((ctx.r11.u32 + ctx.r31.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v56.u32), 3 - ((ctx.r11.u32 + ctx.r31.u32) & 0xF) >> 2));
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
loc_822D223C:
	// cmplwi cr6,r4,3
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 3, ctx.xer);
	// ble cr6,0x822d22e0
	if (!ctx.cr6.gt) goto loc_822D22E0;
	// vaddfp128 v55,v62,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v55.f32, simd::add_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// rlwinm r5,r4,2,0,27
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFF0;
	// clrlwi r4,r4,30
	ctx.r4.u64 = ctx.r4.u32 & 0x3;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// vaddfp128 v13,v0,v55
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v55.f32)));
	// vaddfp128 v63,v55,v55
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v55.f32), simd::load_f32_aligned(ctx.v55.f32)));
	// beq cr6,0x822d22dc
	if (ctx.cr6.eq) goto loc_822D22DC;
	// addi r8,r5,-1
	ctx.r8.s64 = ctx.r5.s64 + -1;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// rlwinm r8,r8,28,4,31
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0xFFFFFFF;
	// addi r7,r8,1
	ctx.r7.s64 = ctx.r8.s64 + 1;
	// addi r8,r11,288
	ctx.r8.s64 = ctx.r11.s64 + 288;
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_822D2278:
	// lvrx128 v54,r9,r10
	temp.u32 = ctx.r9.u32 + ctx.r10.u32;
	simd::store_i8(ctx.v54.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// lvlx128 v53,r0,r10
	temp.u32 = ctx.r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v52,v53,v54
	simd::store_i8(ctx.v52.u8, simd::or_i8(simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v54.u8)));
	// dcbt r0,r8
	// vmrghw128 v11,v52,v52
	simd::store_i32(ctx.v11.u32, simd::unpackhi_i32(simd::load_i32(ctx.v52.u32), simd::load_i32(ctx.v52.u32)));
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// vmrglw128 v10,v52,v52
	simd::store_i32(ctx.v10.u32, simd::unpacklo_i32(simd::load_i32(ctx.v52.u32), simd::load_i32(ctx.v52.u32)));
	// beq cr6,0x822d22b4
	if (ctx.cr6.eq) goto loc_822D22B4;
	// lvx128 v12,r0,r11
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r11.u32) & ~0xF), VectorMaskL));
	// vmaddfp v12,v11,v0,v12
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// lvx128 v11,r11,r9
	simd::store_shuffled(ctx.v11, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r9.u32) & ~0xF), VectorMaskL));
	// vmaddfp v11,v10,v13,v11
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vaddfp128 v0,v0,v63
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// vaddfp128 v13,v13,v63
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// b 0x822d22c4
	goto loc_822D22C4;
loc_822D22B4:
	// vmulfp128 v12,v11,v0
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v12.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vaddfp128 v0,v0,v63
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// vmulfp128 v11,v10,v13
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vaddfp128 v13,v0,v63
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v63.f32)));
loc_822D22C4:
	// stvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v12), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stvx128 v11,r11,r9
	ea = (ctx.r11.u32 + ctx.r9.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v11), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// addi r8,r11,288
	ctx.r8.s64 = ctx.r11.s64 + 288;
	// bdnz 0x822d2278
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D2278;
loc_822D22DC:
	// add r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 + ctx.r6.u64;
loc_822D22E0:
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x822d2324
	if (ctx.cr6.eq) goto loc_822D2324;
	// mtctr r4
	ctx.ctr.u64 = ctx.r4.u64;
loc_822D22EC:
	// lvlx128 v51,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// vspltw128 v50,v51,0
	simd::store_i32(ctx.v50.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v51.u32), 3));
	// vmulfp128 v63,v50,v0
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v50.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vaddfp128 v0,v0,v62
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// beq cr6,0x822d230c
	if (ctx.cr6.eq) goto loc_822D230C;
	// lvlx128 v49,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v63,v63,v49
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v49.f32)));
loc_822D230C:
	// vpermwi128 v48,v63,17
	simd::store_i32(ctx.v48.u32, simd::permute_i32_dispatch(simd::load_i32(ctx.v63.u32), 0xEE));
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// stvewx128 v48,r0,r11
	PPC_STORE_U32((ctx.r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v48.u32), 3 - ((ctx.r11.u32) & 0xF) >> 2));
	// stvewx128 v48,r11,r31
	PPC_STORE_U32((ctx.r11.u32 + ctx.r31.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v48.u32), 3 - ((ctx.r11.u32 + ctx.r31.u32) & 0xF) >> 2));
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bdnz 0x822d22ec
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D22EC;
loc_822D2324:
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D2330"))) PPC_WEAK_FUNC(sub_822D2330);
PPC_FUNC_IMPL(__imp__sub_822D2330) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822D2338;
	__restfpr_28(ctx, base);
	// li r9,6
	ctx.r9.s64 = 6;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r8,r1,-180
	ctx.r8.s64 = ctx.r1.s64 + -180;
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// addi r7,r10,-4
	ctx.r7.s64 = ctx.r10.s64 + -4;
	// lwz r30,20(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r28,32(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822D235C:
	// lwzu r9,4(r7)
	ea = 4 + ctx.r7.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r7.u32 = ea;
	// stwu r9,4(r8)
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r8.u32 = ea;
	// bdnz 0x822d235c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D235C;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x822d23e0
	if (!ctx.cr6.eq) goto loc_822D23E0;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// lfs f12,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// lfs f0,-28948(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
	// stfs f13,-152(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// stfs f12,-148(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// stfs f0,-128(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -128, temp.u32);
	// stfs f11,-144(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// stfs f0,-124(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -124, temp.u32);
	// stfs f10,-140(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// stfs f0,-120(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -120, temp.u32);
	// stfs f9,-136(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// stfs f0,-116(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -116, temp.u32);
	// stfs f8,-132(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
	// stfs f0,-112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f0,-108(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f0,-104(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// stfs f0,-100(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -100, temp.u32);
	// stfs f0,-96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -96, temp.u32);
	// stfs f0,-92(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -92, temp.u32);
	// stfs f0,-88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -88, temp.u32);
	// stfs f0,-84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -84, temp.u32);
	// b 0x822d2474
	goto loc_822D2474;
loc_822D23E0:
	// lfs f0,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// li r6,6
	ctx.r6.s64 = 6;
	// lfs f12,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f0,f13
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// lfs f10,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// addi r7,r1,-132
	ctx.r7.s64 = ctx.r1.s64 + -132;
	// fadds f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// lfs f9,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// addi r8,r9,-4
	ctx.r8.s64 = ctx.r9.s64 + -4;
	// lfs f6,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f6.f64 = double(temp.f32);
	// fadds f5,f9,f7
	ctx.f5.f64 = double(float(ctx.f9.f64 + ctx.f7.f64));
	// lfs f4,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
	// lfs f3,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f3.f64 = double(temp.f32);
	// fadds f2,f6,f4
	ctx.f2.f64 = double(float(ctx.f6.f64 + ctx.f4.f64));
	// lfs f1,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f3,f1
	ctx.f13.f64 = double(float(ctx.f3.f64 + ctx.f1.f64));
	// lfs f12,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// fadds f10,f0,f12
	ctx.f10.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// stfs f11,-152(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// stfs f8,-148(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// stfs f5,-144(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// stfs f2,-140(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// stfs f13,-136(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// stfs f10,-132(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
loc_822D244C:
	// lwzu r10,4(r8)
	ea = 4 + ctx.r8.u32;
	ctx.r10.u64 = PPC_LOAD_U32(ea);
	ctx.r8.u32 = ea;
	// stwu r10,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d244c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D244C;
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r8,r1,-108
	ctx.r8.s64 = ctx.r1.s64 + -108;
	// addi r10,r9,-4
	ctx.r10.s64 = ctx.r9.s64 + -4;
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_822D2468:
	// lwzu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	// stwu r9,4(r8)
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r8.u32 = ea;
	// bdnz 0x822d2468
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D2468;
loc_822D2474:
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r9,r1,-96
	ctx.r9.s64 = ctx.r1.s64 + -96;
	// addi r8,r1,-96
	ctx.r8.s64 = ctx.r1.s64 + -96;
	// addi r7,r1,-112
	ctx.r7.s64 = ctx.r1.s64 + -112;
	// addi r6,r1,-112
	ctx.r6.s64 = ctx.r1.s64 + -112;
	// addi r5,r1,-128
	ctx.r5.s64 = ctx.r1.s64 + -128;
	// addi r4,r1,-128
	ctx.r4.s64 = ctx.r1.s64 + -128;
	// lvrx128 v63,r10,r9
	temp.u32 = ctx.r10.u32 + ctx.r9.u32;
	simd::store_i8(ctx.v63.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// lvlx128 v62,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r3,r1,-144
	ctx.r3.s64 = ctx.r1.s64 + -144;
	// lvrx128 v60,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	simd::store_i8(ctx.v60.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// lvlx128 v59,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r8,r1,-160
	ctx.r8.s64 = ctx.r1.s64 + -160;
	// lvrx128 v57,r10,r5
	temp.u32 = ctx.r10.u32 + ctx.r5.u32;
	simd::store_i8(ctx.v57.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r7,r1,-160
	ctx.r7.s64 = ctx.r1.s64 + -160;
	// addi r6,r1,-176
	ctx.r6.s64 = ctx.r1.s64 + -176;
	// lvlx128 v56,r0,r4
	temp.u32 = ctx.r0.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r5,r1,-176
	ctx.r5.s64 = ctx.r1.s64 + -176;
	// lvrx128 v55,r10,r3
	temp.u32 = ctx.r10.u32 + ctx.r3.u32;
	simd::store_i8(ctx.v55.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// rlwinm r4,r11,0,28,29
	ctx.r4.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xC;
	// lvlx128 v54,r0,r9
	temp.u32 = ctx.r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v53,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	simd::store_i8(ctx.v53.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// li r29,4
	ctx.r29.s64 = 4;
	// lvlx128 v52,r0,r7
	temp.u32 = ctx.r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v61,v62,v63
	simd::store_i8(ctx.v61.u8, simd::or_i8(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v63.u8)));
	// lvrx128 v51,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	simd::store_i8(ctx.v51.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v58,v59,v60
	simd::store_i8(ctx.v58.u8, simd::or_i8(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v60.u8)));
	// lvlx128 v50,r0,r5
	temp.u32 = ctx.r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v57,v56,v57
	simd::store_i8(ctx.v57.u8, simd::or_i8(simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v57.u8)));
	// vor128 v12,v54,v55
	simd::store_i8(ctx.v12.u8, simd::or_i8(simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v55.u8)));
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// vor128 v13,v52,v53
	simd::store_i8(ctx.v13.u8, simd::or_i8(simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v53.u8)));
	// vor128 v0,v50,v51
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v51.u8)));
	// beq cr6,0x822d257c
	if (ctx.cr6.eq) goto loc_822D257C;
	// cmplwi cr6,r30,3
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 3, ctx.xer);
	// ble cr6,0x822d26cc
	if (!ctx.cr6.gt) goto loc_822D26CC;
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
loc_822D250C:
	// lvlx128 v49,r0,r31
	temp.u32 = ctx.r0.u32 + ctx.r31.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v12,v12,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// vspltw128 v63,v49,0
	simd::store_i32(ctx.v63.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v49.u32), 3));
	// vor128 v48,v63,v63
	simd::store_i8(ctx.v48.u8, simd::load_i8(ctx.v63.u8));
	// vmulfp128 v63,v63,v0
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vaddfp128 v0,v0,v57
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v57.f32)));
	// vmulfp128 v62,v48,v13
	simd::store_f32_aligned(ctx.v62.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v48.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vaddfp128 v13,v13,v58
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v58.f32)));
	// beq cr6,0x822d254c
	if (ctx.cr6.eq) goto loc_822D254C;
	// lvlx128 v47,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v46,r10,r11
	temp.u32 = ctx.r10.u32 + ctx.r11.u32;
	simd::store_i8(ctx.v46.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v45,v47,v46
	simd::store_i8(ctx.v45.u8, simd::or_i8(simd::load_i8(ctx.v47.u8), simd::load_i8(ctx.v46.u8)));
	// lvlx128 v44,r0,r9
	temp.u32 = ctx.r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v62,v62,v44
	simd::store_f32_aligned(ctx.v62.f32, simd::add_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v44.f32)));
	// vaddfp128 v63,v63,v45
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v45.f32)));
loc_822D254C:
	// vpermwi128 v43,v62,17
	simd::store_i32(ctx.v43.u32, simd::permute_i32_dispatch(simd::load_i32(ctx.v62.u32), 0xEE));
	// stvlx128 v63,r0,r11
{
	uint32_t addr = 
ctx.r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v63), 15 - i));
}
	// stvrx128 v63,r11,r10
{
	uint32_t addr = 
ctx.r11.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v63), i));
}
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// rlwinm r8,r11,0,28,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xC;
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// stvewx128 v43,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v43.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// stvewx128 v43,r9,r29
	PPC_STORE_U32((ctx.r9.u32 + ctx.r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v43.u32), 3 - ((ctx.r9.u32 + ctx.r29.u32) & 0xF) >> 2));
	// addi r9,r9,24
	ctx.r9.s64 = ctx.r9.s64 + 24;
	// bne cr6,0x822d250c
	if (!ctx.cr6.eq) goto loc_822D250C;
loc_822D257C:
	// cmplwi cr6,r30,3
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 3, ctx.xer);
	// ble cr6,0x822d26cc
	if (!ctx.cr6.gt) goto loc_822D26CC;
	// rlwinm r3,r30,2,0,27
	ctx.r3.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFF0;
	// vaddfp128 v63,v61,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v61.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// clrlwi r30,r30,30
	ctx.r30.u64 = ctx.r30.u32 & 0x3;
	// vaddfp128 v62,v58,v58
	simd::store_f32_aligned(ctx.v62.f32, simd::add_f32(simd::load_f32_aligned(ctx.v58.f32), simd::load_f32_aligned(ctx.v58.f32)));
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// vaddfp128 v61,v57,v57
	simd::store_f32_aligned(ctx.v61.f32, simd::add_f32(simd::load_f32_aligned(ctx.v57.f32), simd::load_f32_aligned(ctx.v57.f32)));
	// beq cr6,0x822d26c8
	if (ctx.cr6.eq) goto loc_822D26C8;
	// addi r9,r3,-1
	ctx.r9.s64 = ctx.r3.s64 + -1;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// rlwinm r9,r9,28,4,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 28) & 0xFFFFFFF;
	// li r5,32
	ctx.r5.s64 = 32;
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// addi r9,r11,864
	ctx.r9.s64 = ctx.r11.s64 + 864;
	// li r6,48
	ctx.r6.s64 = 48;
	// li r7,64
	ctx.r7.s64 = 64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r8,80
	ctx.r8.s64 = 80;
loc_822D25C8:
	// lvrx128 v42,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	simd::store_i8(ctx.v42.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// lvlx128 v41,r0,r4
	temp.u32 = ctx.r0.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v60,v41,v42
	simd::store_i8(ctx.v60.u8, simd::or_i8(simd::load_i8(ctx.v41.u8), simd::load_i8(ctx.v42.u8)));
	// dcbt r0,r9
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// vor128 v59,v60,v60
	simd::store_i8(ctx.v59.u8, simd::load_i8(ctx.v60.u8));
	// vmrghw128 v11,v60,v60
	simd::store_i32(ctx.v11.u32, simd::unpackhi_i32(simd::load_i32(ctx.v60.u32), simd::load_i32(ctx.v60.u32)));
	// vmrglw128 v10,v60,v60
	simd::store_i32(ctx.v10.u32, simd::unpacklo_i32(simd::load_i32(ctx.v60.u32), simd::load_i32(ctx.v60.u32)));
	// beq cr6,0x822d2660
	if (ctx.cr6.eq) goto loc_822D2660;
	// vspltw128 v9,v60,0
	simd::store_i32(ctx.v9.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v60.u32), 3));
	// lvx128 v8,r0,r11
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((ctx.r11.u32) & ~0xF), VectorMaskL));
	// vspltw v7,v11,2
	simd::store_i32(ctx.v7.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v11.u32), 1));
	// lvx128 v6,r11,r5
	simd::store_shuffled(ctx.v6, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r5.u32) & ~0xF), VectorMaskL));
	// vspltw128 v5,v59,2
	simd::store_i32(ctx.v5.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v59.u32), 1));
	// lvx128 v3,r11,r8
	simd::store_shuffled(ctx.v3, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r8.u32) & ~0xF), VectorMaskL));
	// vspltw v4,v10,2
	simd::store_i32(ctx.v4.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v10.u32), 1));
	// vmaddfp v9,v9,v0,v8
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v8.f32)));
	// lvx128 v8,r11,r10
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r10.u32) & ~0xF), VectorMaskL));
	// vmaddfp v11,v11,v13,v8
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v8.f32)));
	// vmaddfp v8,v7,v12,v6
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v6.f32)));
	// lvx128 v6,r11,r7
	simd::store_shuffled(ctx.v6, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r7.u32) & ~0xF), VectorMaskL));
	// vaddfp128 v13,v13,v62
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// lvx128 v7,r11,r6
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r6.u32) & ~0xF), VectorMaskL));
	// vaddfp128 v0,v0,v61
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vaddfp128 v12,v12,v63
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// stvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v9), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v11,r11,r10
	ea = (ctx.r11.u32 + ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v11), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v8,r11,r5
	ea = (ctx.r11.u32 + ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v8), &VectorMaskL[(ea & 0xF) * 16]);
	// vmaddfp v11,v10,v13,v6
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v6.f32)));
	// vmaddfp v9,v5,v0,v7
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v7.f32)));
	// vmaddfp v10,v4,v12,v3
	simd::store_f32_aligned(ctx.v10.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vaddfp128 v0,v0,v61
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vaddfp128 v13,v13,v62
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// vaddfp128 v12,v12,v63
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// stvx128 v11,r11,r7
	ea = (ctx.r11.u32 + ctx.r7.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v11), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v9,r11,r6
	ea = (ctx.r11.u32 + ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v9), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v10,r11,r8
	ea = (ctx.r11.u32 + ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v10), &VectorMaskL[(ea & 0xF) * 16]);
	// b 0x822d26b8
	goto loc_822D26B8;
loc_822D2660:
	// vaddfp128 v40,v13,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v40.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// vspltw128 v39,v60,0
	simd::store_i32(ctx.v39.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v60.u32), 3));
	// vaddfp128 v38,v0,v61
	simd::store_f32_aligned(ctx.v38.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vspltw128 v35,v59,2
	simd::store_i32(ctx.v35.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v59.u32), 1));
	// vaddfp128 v36,v12,v63
	simd::store_f32_aligned(ctx.v36.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// vspltw128 v37,v11,2
	simd::store_i32(ctx.v37.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v11.u32), 1));
	// vspltw128 v34,v10,2
	simd::store_i32(ctx.v34.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v10.u32), 1));
	// vmulfp128 v33,v11,v13
	simd::store_f32_aligned(ctx.v33.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v32,v39,v0
	simd::store_f32_aligned(ctx.v32.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v60,v37,v12
	simd::store_f32_aligned(ctx.v60.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v37.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vmulfp128 v59,v10,v40
	simd::store_f32_aligned(ctx.v59.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v40.f32)));
	// vmulfp128 v56,v35,v38
	simd::store_f32_aligned(ctx.v56.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v35.f32), simd::load_f32_aligned(ctx.v38.f32)));
	// vmulfp128 v55,v34,v36
	simd::store_f32_aligned(ctx.v55.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v34.f32), simd::load_f32_aligned(ctx.v36.f32)));
	// stvx128 v33,r11,r10
	ea = (ctx.r11.u32 + ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v33), &VectorMaskL[(ea & 0xF) * 16]);
	// vaddfp128 v13,v40,v62
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v40.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// stvx128 v32,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v32), &VectorMaskL[(ea & 0xF) * 16]);
	// vaddfp128 v0,v38,v61
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v38.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vaddfp128 v12,v36,v63
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v36.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// stvx128 v60,r11,r5
	ea = (ctx.r11.u32 + ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v60), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v59,r11,r7
	ea = (ctx.r11.u32 + ctx.r7.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v59), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v56,r11,r6
	ea = (ctx.r11.u32 + ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v56), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v55,r11,r8
	ea = (ctx.r11.u32 + ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v55), &VectorMaskL[(ea & 0xF) * 16]);
loc_822D26B8:
	// addi r11,r11,96
	ctx.r11.s64 = ctx.r11.s64 + 96;
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// addi r9,r11,864
	ctx.r9.s64 = ctx.r11.s64 + 864;
	// bdnz 0x822d25c8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D25C8;
loc_822D26C8:
	// add r31,r3,r31
	ctx.r31.u64 = ctx.r3.u64 + ctx.r31.u64;
loc_822D26CC:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x822d2738
	if (ctx.cr6.eq) goto loc_822D2738;
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
loc_822D26D8:
	// lvlx128 v54,r0,r31
	temp.u32 = ctx.r0.u32 + ctx.r31.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// vspltw128 v63,v54,0
	simd::store_i32(ctx.v63.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v54.u32), 3));
	// vor128 v53,v63,v63
	simd::store_i8(ctx.v53.u8, simd::load_i8(ctx.v63.u8));
	// vmulfp128 v63,v63,v0
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vaddfp128 v0,v0,v57
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v57.f32)));
	// vmulfp128 v62,v53,v13
	simd::store_f32_aligned(ctx.v62.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v53.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vaddfp128 v13,v13,v58
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v58.f32)));
	// beq cr6,0x822d2714
	if (ctx.cr6.eq) goto loc_822D2714;
	// lvlx128 v52,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v51,r10,r11
	temp.u32 = ctx.r10.u32 + ctx.r11.u32;
	simd::store_i8(ctx.v51.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v50,v52,v51
	simd::store_i8(ctx.v50.u8, simd::or_i8(simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v51.u8)));
	// lvlx128 v49,r11,r10
	temp.u32 = ctx.r11.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v62,v62,v49
	simd::store_f32_aligned(ctx.v62.f32, simd::add_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v49.f32)));
	// vaddfp128 v63,v63,v50
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v50.f32)));
loc_822D2714:
	// vpermwi128 v48,v62,17
	simd::store_i32(ctx.v48.u32, simd::permute_i32_dispatch(simd::load_i32(ctx.v62.u32), 0xEE));
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
	// stvlx128 v63,r0,r11
{
	uint32_t addr = 
ctx.r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v63), 15 - i));
}
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// stvrx128 v63,r11,r10
{
	uint32_t addr = 
ctx.r11.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v63), i));
}
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// stvewx128 v48,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v48.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// stvewx128 v48,r9,r29
	PPC_STORE_U32((ctx.r9.u32 + ctx.r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v48.u32), 3 - ((ctx.r9.u32 + ctx.r29.u32) & 0xF) >> 2));
	// bdnz 0x822d26d8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D26D8;
loc_822D2738:
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D273C"))) PPC_WEAK_FUNC(sub_822D273C);
PPC_FUNC_IMPL(__imp__sub_822D273C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D2740"))) PPC_WEAK_FUNC(sub_822D2740);
PPC_FUNC_IMPL(__imp__sub_822D2740) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822D2748;
	__restfpr_28(ctx, base);
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x8233fa20
	ctx.lr = 0x822D2750;
	sub_8233FA20(ctx, base);
	// stwu r1,-416(r1)
	ea = -416 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r30,0(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r31,4(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// lwz r29,20(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r28,32(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,24(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,20(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,28(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,32(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,40(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,36(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,44(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 44);
	ctx.f3.f64 = double(temp.f32);
	// stfs f0,136(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f13,140(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// stfs f12,152(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f11,156(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
	// stfs f10,104(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f9,108(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// stfs f13,100(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f8,120(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f12,112(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f7,124(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// stfs f11,116(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f6,128(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// stfs f5,132(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// stfs f4,144(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// stfs f3,148(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// bne cr6,0x822d2838
	if (!ctx.cr6.eq) goto loc_822D2838;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
	// li r5,96
	ctx.r5.s64 = 96;
	// fmr f13,f9
	ctx.f13.f64 = ctx.f9.f64;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// fmr f10,f8
	ctx.f10.f64 = ctx.f8.f64;
	// fmr f9,f7
	ctx.f9.f64 = ctx.f7.f64;
	// stfs f0,160(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// stfs f13,164(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// fmr f12,f6
	ctx.f12.f64 = ctx.f6.f64;
	// stfs f6,168(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// fmr f11,f5
	ctx.f11.f64 = ctx.f5.f64;
	// stfs f5,172(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// fmr f8,f4
	ctx.f8.f64 = ctx.f4.f64;
	// stfs f10,176(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// fmr f7,f3
	ctx.f7.f64 = ctx.f3.f64;
	// stfs f9,180(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f4,184(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// stfs f3,188(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822D2834;
	sub_8233EAF0(ctx, base);
	// b 0x822d2948
	goto loc_822D2948;
loc_822D2838:
	// lfs f10,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// fadds f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// lfs f7,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// fadds f6,f9,f13
	ctx.f6.f64 = double(float(ctx.f9.f64 + ctx.f13.f64));
	// lfs f5,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// fadds f4,f7,f12
	ctx.f4.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// fadds f2,f5,f11
	ctx.f2.f64 = double(float(ctx.f5.f64 + ctx.f11.f64));
	// lfs f3,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,24(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,32(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,40(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,28(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	ctx.f11.f64 = double(temp.f32);
	// lfs f31,36(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,44(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,24(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	ctx.f28.f64 = double(temp.f32);
	// fadds f29,f3,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 + ctx.f29.f64));
	// lfs f27,32(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	ctx.f27.f64 = double(temp.f32);
	// fadds f28,f1,f28
	ctx.f28.f64 = double(float(ctx.f1.f64 + ctx.f28.f64));
	// lfs f26,40(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	ctx.f26.f64 = double(temp.f32);
	// fadds f27,f0,f27
	ctx.f27.f64 = double(float(ctx.f0.f64 + ctx.f27.f64));
	// lfs f25,20(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f25.f64 = double(temp.f32);
	// fadds f26,f13,f26
	ctx.f26.f64 = double(float(ctx.f13.f64 + ctx.f26.f64));
	// lfs f24,28(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	ctx.f24.f64 = double(temp.f32);
	// fadds f25,f25,f12
	ctx.f25.f64 = double(float(ctx.f25.f64 + ctx.f12.f64));
	// lfs f23,36(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	ctx.f23.f64 = double(temp.f32);
	// fadds f24,f24,f11
	ctx.f24.f64 = double(float(ctx.f24.f64 + ctx.f11.f64));
	// lfs f22,44(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 44);
	ctx.f22.f64 = double(temp.f32);
	// fadds f23,f23,f31
	ctx.f23.f64 = double(float(ctx.f23.f64 + ctx.f31.f64));
	// fadds f22,f22,f30
	ctx.f22.f64 = double(float(ctx.f22.f64 + ctx.f30.f64));
	// stfs f8,136(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f6,140(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// stfs f4,152(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f2,156(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
	// stfs f10,192(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// stfs f9,196(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// stfs f3,200(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// stfs f1,204(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f7,208(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 208, temp.u32);
	// stfs f5,212(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 212, temp.u32);
	// stfs f12,216(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 216, temp.u32);
	// stfs f11,220(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f29,160(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// stfs f28,164(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// stfs f27,168(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// stfs f26,172(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// stfs f25,176(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// stfs f24,180(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f23,184(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// stfs f22,188(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// stfs f0,224(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
	// stfs f13,228(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 228, temp.u32);
	// stfs f10,232(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 232, temp.u32);
	// stfs f9,236(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 236, temp.u32);
	// stfs f31,240(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 240, temp.u32);
	// stfs f30,244(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 244, temp.u32);
	// stfs f7,248(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 248, temp.u32);
	// stfs f5,252(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfs f3,256(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 256, temp.u32);
	// stfs f1,260(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 260, temp.u32);
	// stfs f0,264(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 264, temp.u32);
	// stfs f13,268(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 268, temp.u32);
	// stfs f12,272(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// stfs f11,276(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 276, temp.u32);
	// stfs f31,280(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 280, temp.u32);
	// stfs f30,284(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 284, temp.u32);
loc_822D2948:
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r11,r1,240
	ctx.r11.s64 = ctx.r1.s64 + 240;
	// addi r9,r1,240
	ctx.r9.s64 = ctx.r1.s64 + 240;
	// addi r8,r1,224
	ctx.r8.s64 = ctx.r1.s64 + 224;
	// addi r7,r1,224
	ctx.r7.s64 = ctx.r1.s64 + 224;
	// addi r6,r1,208
	ctx.r6.s64 = ctx.r1.s64 + 208;
	// addi r5,r1,208
	ctx.r5.s64 = ctx.r1.s64 + 208;
	// lvrx128 v63,r10,r11
	temp.u32 = ctx.r10.u32 + ctx.r11.u32;
	simd::store_i8(ctx.v63.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// lvlx128 v62,r0,r9
	temp.u32 = ctx.r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// lvrx128 v61,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	simd::store_i8(ctx.v61.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// lvlx128 v60,r0,r7
	temp.u32 = ctx.r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v59,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	simd::store_i8(ctx.v59.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// lvlx128 v58,r0,r5
	temp.u32 = ctx.r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// lvrx128 v57,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	simd::store_i8(ctx.v57.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// lvlx128 v53,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// lvrx128 v52,r10,r11
	temp.u32 = ctx.r10.u32 + ctx.r11.u32;
	simd::store_i8(ctx.v52.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lvlx128 v51,r0,r9
	temp.u32 = ctx.r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lvrx128 v50,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	simd::store_i8(ctx.v50.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// rlwinm r11,r31,0,28,29
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 0) & 0xC;
	// lvlx128 v49,r0,r7
	temp.u32 = ctx.r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v48,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	simd::store_i8(ctx.v48.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v56,v62,v63
	simd::store_i8(ctx.v56.u8, simd::or_i8(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v63.u8)));
	// lvlx128 v47,r0,r5
	temp.u32 = ctx.r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v55,v60,v61
	simd::store_i8(ctx.v55.u8, simd::or_i8(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v61.u8)));
	// lvrx128 v46,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	simd::store_i8(ctx.v46.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// li r9,4
	ctx.r9.s64 = 4;
	// lvlx128 v45,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v54,v58,v59
	simd::store_i8(ctx.v54.u8, simd::or_i8(simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v59.u8)));
	// vor128 v53,v53,v57
	simd::store_i8(ctx.v53.u8, simd::or_i8(simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v57.u8)));
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// vor128 v60,v51,v52
	simd::store_i8(ctx.v60.u8, simd::or_i8(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v52.u8)));
	// vor128 v61,v49,v50
	simd::store_i8(ctx.v61.u8, simd::or_i8(simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v50.u8)));
	// vor128 v62,v47,v48
	simd::store_i8(ctx.v62.u8, simd::or_i8(simd::load_i8(ctx.v47.u8), simd::load_i8(ctx.v48.u8)));
	// vor128 v63,v45,v46
	simd::store_i8(ctx.v63.u8, simd::or_i8(simd::load_i8(ctx.v45.u8), simd::load_i8(ctx.v46.u8)));
	// beq cr6,0x822d2a90
	if (ctx.cr6.eq) goto loc_822D2A90;
	// cmplwi cr6,r29,7
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 7, ctx.xer);
	// ble cr6,0x822d2a90
	if (!ctx.cr6.gt) goto loc_822D2A90;
	// addi r11,r31,16
	ctx.r11.s64 = ctx.r31.s64 + 16;
loc_822D2A08:
	// lvlx128 v44,r0,r30
	temp.u32 = ctx.r0.u32 + ctx.r30.u32;
	simd::store_shuffled(ctx.v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// vspltw128 v58,v44,1
	simd::store_i32(ctx.v58.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v44.u32), 2));
	// vspltw128 v59,v44,0
	simd::store_i32(ctx.v59.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v44.u32), 3));
	// vor128 v43,v58,v58
	simd::store_i8(ctx.v43.u8, simd::load_i8(ctx.v58.u8));
	// vmulfp128 v58,v58,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v58.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v58.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// vmulfp128 v57,v59,v63
	simd::store_f32_aligned(ctx.v57.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// vmulfp128 v59,v59,v61
	simd::store_f32_aligned(ctx.v59.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vaddfp128 v63,v63,v53
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v53.f32)));
	// vmulfp128 v52,v43,v60
	simd::store_f32_aligned(ctx.v52.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v43.f32), simd::load_f32_aligned(ctx.v60.f32)));
	// vaddfp128 v62,v62,v54
	simd::store_f32_aligned(ctx.v62.f32, simd::add_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v54.f32)));
	// vaddfp128 v61,v61,v55
	simd::store_f32_aligned(ctx.v61.f32, simd::add_f32(simd::load_f32_aligned(ctx.v61.f32), simd::load_f32_aligned(ctx.v55.f32)));
	// vaddfp128 v60,v60,v56
	simd::store_f32_aligned(ctx.v60.f32, simd::add_f32(simd::load_f32_aligned(ctx.v60.f32), simd::load_f32_aligned(ctx.v56.f32)));
	// beq cr6,0x822d2a58
	if (ctx.cr6.eq) goto loc_822D2A58;
	// lvlx128 v42,r0,r31
	temp.u32 = ctx.r0.u32 + ctx.r31.u32;
	simd::store_shuffled(ctx.v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v41,r10,r31
	temp.u32 = ctx.r10.u32 + ctx.r31.u32;
	simd::store_i8(ctx.v41.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v40,v42,v41
	simd::store_i8(ctx.v40.u8, simd::or_i8(simd::load_i8(ctx.v42.u8), simd::load_i8(ctx.v41.u8)));
	// lvlx128 v39,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v59,v59,v39
	simd::store_f32_aligned(ctx.v59.f32, simd::add_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v39.f32)));
	// vaddfp128 v57,v57,v40
	simd::store_f32_aligned(ctx.v57.f32, simd::add_f32(simd::load_f32_aligned(ctx.v57.f32), simd::load_f32_aligned(ctx.v40.f32)));
loc_822D2A58:
	// vaddfp128 v38,v59,v52
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v38.f32, simd::add_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v52.f32)));
	// addi r29,r29,-2
	ctx.r29.s64 = ctx.r29.s64 + -2;
	// vaddfp128 v37,v57,v58
	simd::store_f32_aligned(ctx.v37.f32, simd::add_f32(simd::load_f32_aligned(ctx.v57.f32), simd::load_f32_aligned(ctx.v58.f32)));
	// addi r30,r30,8
	ctx.r30.s64 = ctx.r30.s64 + 8;
	// vpermwi128 v36,v38,17
	simd::store_i32(ctx.v36.u32, simd::permute_i32_dispatch(simd::load_i32(ctx.v38.u32), 0xEE));
	// stvlx128 v37,r0,r31
{
	uint32_t addr = 
ctx.r31.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v37), 15 - i));
}
	// stvrx128 v37,r31,r10
{
	uint32_t addr = 
ctx.r31.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v37), i));
}
	// addi r31,r31,24
	ctx.r31.s64 = ctx.r31.s64 + 24;
	// rlwinm r8,r31,0,28,29
	ctx.r8.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 0) & 0xC;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// stvewx128 v36,r0,r11
	PPC_STORE_U32((ctx.r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v36.u32), 3 - ((ctx.r11.u32) & 0xF) >> 2));
	// stvewx128 v36,r11,r9
	PPC_STORE_U32((ctx.r11.u32 + ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v36.u32), 3 - ((ctx.r11.u32 + ctx.r9.u32) & 0xF) >> 2));
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// bne cr6,0x822d2a08
	if (!ctx.cr6.eq) goto loc_822D2A08;
loc_822D2A90:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x822d2b24
	if (ctx.cr6.eq) goto loc_822D2B24;
	// addi r11,r29,-1
	ctx.r11.s64 = ctx.r29.s64 + -1;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
loc_822D2AA8:
	// lvlx128 v35,r0,r30
	temp.u32 = ctx.r0.u32 + ctx.r30.u32;
	simd::store_shuffled(ctx.v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// vspltw128 v58,v35,1
	simd::store_i32(ctx.v58.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v35.u32), 2));
	// vspltw128 v59,v35,0
	simd::store_i32(ctx.v59.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v35.u32), 3));
	// vor128 v34,v58,v58
	simd::store_i8(ctx.v34.u8, simd::load_i8(ctx.v58.u8));
	// vmulfp128 v58,v58,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v58.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v58.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// vmulfp128 v57,v59,v63
	simd::store_f32_aligned(ctx.v57.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// vmulfp128 v59,v59,v61
	simd::store_f32_aligned(ctx.v59.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vaddfp128 v63,v63,v53
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v53.f32)));
	// vmulfp128 v52,v34,v60
	simd::store_f32_aligned(ctx.v52.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v34.f32), simd::load_f32_aligned(ctx.v60.f32)));
	// vaddfp128 v62,v62,v54
	simd::store_f32_aligned(ctx.v62.f32, simd::add_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v54.f32)));
	// vaddfp128 v61,v61,v55
	simd::store_f32_aligned(ctx.v61.f32, simd::add_f32(simd::load_f32_aligned(ctx.v61.f32), simd::load_f32_aligned(ctx.v55.f32)));
	// vaddfp128 v60,v60,v56
	simd::store_f32_aligned(ctx.v60.f32, simd::add_f32(simd::load_f32_aligned(ctx.v60.f32), simd::load_f32_aligned(ctx.v56.f32)));
	// beq cr6,0x822d2af8
	if (ctx.cr6.eq) goto loc_822D2AF8;
	// lvlx128 v33,r0,r31
	temp.u32 = ctx.r0.u32 + ctx.r31.u32;
	simd::store_shuffled(ctx.v33,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v32,r10,r31
	temp.u32 = ctx.r10.u32 + ctx.r31.u32;
	simd::store_i8(ctx.v32.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v51,v33,v32
	simd::store_i8(ctx.v51.u8, simd::or_i8(simd::load_i8(ctx.v33.u8), simd::load_i8(ctx.v32.u8)));
	// lvlx128 v50,r31,r10
	temp.u32 = ctx.r31.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v59,v59,v50
	simd::store_f32_aligned(ctx.v59.f32, simd::add_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v50.f32)));
	// vaddfp128 v57,v57,v51
	simd::store_f32_aligned(ctx.v57.f32, simd::add_f32(simd::load_f32_aligned(ctx.v57.f32), simd::load_f32_aligned(ctx.v51.f32)));
loc_822D2AF8:
	// vaddfp128 v49,v59,v52
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v49.f32, simd::add_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v52.f32)));
	// addi r11,r31,16
	ctx.r11.s64 = ctx.r31.s64 + 16;
	// vaddfp128 v48,v57,v58
	simd::store_f32_aligned(ctx.v48.f32, simd::add_f32(simd::load_f32_aligned(ctx.v57.f32), simd::load_f32_aligned(ctx.v58.f32)));
	// addi r30,r30,8
	ctx.r30.s64 = ctx.r30.s64 + 8;
	// vpermwi128 v47,v49,17
	simd::store_i32(ctx.v47.u32, simd::permute_i32_dispatch(simd::load_i32(ctx.v49.u32), 0xEE));
	// stvlx128 v48,r0,r31
{
	uint32_t addr = 
ctx.r31.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v48), 15 - i));
}
	// stvrx128 v48,r31,r10
{
	uint32_t addr = 
ctx.r31.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v48), i));
}
	// addi r31,r31,24
	ctx.r31.s64 = ctx.r31.s64 + 24;
	// stvewx128 v47,r0,r11
	PPC_STORE_U32((ctx.r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v47.u32), 3 - ((ctx.r11.u32) & 0xF) >> 2));
	// stvewx128 v47,r11,r9
	PPC_STORE_U32((ctx.r11.u32 + ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v47.u32), 3 - ((ctx.r11.u32 + ctx.r9.u32) & 0xF) >> 2));
	// bdnz 0x822d2aa8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D2AA8;
loc_822D2B24:
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x8233fa6c
	ctx.lr = 0x822D2B30;
	__savefpr_22(ctx, base);
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D2B34"))) PPC_WEAK_FUNC(sub_822D2B34);
PPC_FUNC_IMPL(__imp__sub_822D2B34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D2B38"))) PPC_WEAK_FUNC(sub_822D2B38);
PPC_FUNC_IMPL(__imp__sub_822D2B38) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lis r8,-32249
	ctx.r8.s64 = -2113470464;
	// lwz r7,36(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// lwz r30,20(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r31,32(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f0,-28948(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
	// bne cr6,0x822d2c68
	if (!ctx.cr6.eq) goto loc_822D2C68;
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// beq cr6,0x822d2c68
	if (ctx.cr6.eq) goto loc_822D2C68;
	// cmplwi cr6,r10,2
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 2, ctx.xer);
	// beq cr6,0x822d2be8
	if (ctx.cr6.eq) goto loc_822D2BE8;
	// cmplwi cr6,r10,4
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 4, ctx.xer);
	// bne cr6,0x822d2cec
	if (!ctx.cr6.eq) goto loc_822D2CEC;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,20(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,40(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,60(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 60);
	ctx.f10.f64 = double(temp.f32);
	// stfs f13,-64(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// stfs f12,-60(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// stfs f11,-56(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// stfs f10,-52(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// bne cr6,0x822d2bc4
	if (!ctx.cr6.eq) goto loc_822D2BC4;
	// stfs f0,-48(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f0,-44(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f0,-40(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f0,-36(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// b 0x822d2cec
	goto loc_822D2CEC;
loc_822D2BC4:
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,40(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,60(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	ctx.f10.f64 = double(temp.f32);
	// stfs f13,-48(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f12,-44(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f11,-40(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f10,-36(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// b 0x822d2cec
	goto loc_822D2CEC;
loc_822D2BE8:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// stfs f13,-64(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// stfs f12,-60(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// bne cr6,0x822d2c24
	if (!ctx.cr6.eq) goto loc_822D2C24;
	// ld r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// stfs f0,-48(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f0,-44(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f0,-40(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f0,-36(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// std r10,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r10.u64);
	// b 0x822d2cec
	goto loc_822D2CEC;
loc_822D2C24:
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f12,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,12(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// fadds f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// fadds f7,f9,f11
	ctx.f7.f64 = double(float(ctx.f9.f64 + ctx.f11.f64));
	// stfs f8,-56(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// lfs f13,5260(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 5260);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f6,f12,f13
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfs f7,-52(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// fmuls f5,f11,f13
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfs f6,-48(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f5,-44(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f6,-40(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f5,-36(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// b 0x822d2cec
	goto loc_822D2CEC;
loc_822D2C68:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-64(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// bne cr6,0x822d2ca0
	if (!ctx.cr6.eq) goto loc_822D2CA0;
	// stfs f13,-60(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// ld r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// stfs f0,-48(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// std r10,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r10.u64);
	// stfs f0,-44(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f0,-40(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f0,-36(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// b 0x822d2cec
	goto loc_822D2CEC;
loc_822D2CA0:
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f11,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// lfs f10,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// fadds f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// stfs f9,-60(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// lfs f13,5272(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 5272);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f11,f13
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// lfs f12,5260(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 5260);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,5264(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 5264);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f7,f11,f12,f10
	ctx.f7.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f12.f64), float(ctx.f10.f64)));
	// fmadds f6,f11,f13,f10
	ctx.f6.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f13.f64), float(ctx.f10.f64)));
	// stfs f7,-56(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// stfs f6,-52(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// stfs f8,-48(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f8,-44(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f8,-40(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f8,-36(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
loc_822D2CEC:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stfs f0,-28(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// stfs f0,-24(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -24, temp.u32);
	// li r6,16
	ctx.r6.s64 = 16;
	// stfs f0,-20(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -20, temp.u32);
	// addi r5,r1,-64
	ctx.r5.s64 = ctx.r1.s64 + -64;
	// addi r4,r1,-64
	ctx.r4.s64 = ctx.r1.s64 + -64;
	// addi r3,r1,-48
	ctx.r3.s64 = ctx.r1.s64 + -48;
	// lfs f0,5256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r1,-48
	ctx.r10.s64 = ctx.r1.s64 + -48;
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r8,r1,-32
	ctx.r8.s64 = ctx.r1.s64 + -32;
	// lvrx128 v61,r6,r5
	temp.u32 = ctx.r6.u32 + ctx.r5.u32;
	simd::store_i8(ctx.v61.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r7,r1,-32
	ctx.r7.s64 = ctx.r1.s64 + -32;
	// lvlx128 v60,r0,r4
	temp.u32 = ctx.r0.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v59,r6,r3
	temp.u32 = ctx.r6.u32 + ctx.r3.u32;
	simd::store_i8(ctx.v59.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// lvlx128 v58,r0,r10
	temp.u32 = ctx.r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v63,r6,r8
	temp.u32 = ctx.r6.u32 + ctx.r8.u32;
	simd::store_i8(ctx.v63.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// rlwinm r8,r11,0,26,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x3C;
	// lvlx128 v62,r0,r7
	temp.u32 = ctx.r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v5,v62,v63
	simd::store_i8(ctx.v5.u8, simd::or_i8(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v63.u8)));
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// vor128 v0,v58,v59
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v59.u8)));
	// vor128 v13,v60,v61
	simd::store_i8(ctx.v13.u8, simd::or_i8(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v61.u8)));
	// beq cr6,0x822d2da8
	if (ctx.cr6.eq) goto loc_822D2DA8;
	// cmplwi cr6,r30,15
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 15, ctx.xer);
	// ble cr6,0x822d2ea0
	if (!ctx.cr6.gt) goto loc_822D2EA0;
loc_822D2D58:
	// vmaddfp v12,v0,v5,v13
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v5.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vor128 v57,v0,v0
	simd::store_i8(ctx.v57.u8, simd::load_i8(ctx.v0.u8));
	// lvlx128 v56,r0,r9
	temp.u32 = ctx.r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// vmulfp128 v63,v56,v13
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v56.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vrlimi128 v57,v0,15,1
	uint32_t sh = 15 & 31;
	for(int i=0;i<4;i++) {
		uint32_t val = ctx.v0.u32[i];
		val = (val << sh) | (val >> (32 - sh));
		if ((1 >> i) & 1) {
			ctx.v57.u32[i] = val;
		}
	}
	// vor128 v0,v57,v57
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v57.u8));
	// vor v13,v12,v12
	simd::store_i8(ctx.v13.u8, simd::load_i8(ctx.v12.u8));
	// vrlimi128 v13,v12,15,1
	uint32_t sh = 15 & 31;
	for(int i=0;i<4;i++) {
		uint32_t val = ctx.v12.u32[i];
		val = (val << sh) | (val >> (32 - sh));
		if ((1 >> i) & 1) {
			ctx.v13.u32[i] = val;
		}
	}
	// beq cr6,0x822d2d88
	if (ctx.cr6.eq) goto loc_822D2D88;
	// lvlx128 v55,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v63,v63,v55
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v55.f32)));
loc_822D2D88:
	// vspltw128 v54,v63,0
	simd::store_i32(ctx.v54.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v63.u32), 3));
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stvewx128 v54,r0,r11
	PPC_STORE_U32((ctx.r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v54.u32), 3 - ((ctx.r11.u32) & 0xF) >> 2));
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// rlwinm r10,r11,0,26,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x3C;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822d2d58
	if (!ctx.cr6.eq) goto loc_822D2D58;
loc_822D2DA8:
	// cmplwi cr6,r30,15
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 15, ctx.xer);
	// ble cr6,0x822d2ea0
	if (!ctx.cr6.gt) goto loc_822D2EA0;
	// rlwinm r10,r30,2,0,25
	ctx.r10.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFC0;
	// clrlwi r30,r30,28
	ctx.r30.u64 = ctx.r30.u32 & 0xF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822d2ea0
	if (ctx.cr6.eq) goto loc_822D2EA0;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// rlwinm r8,r10,26,6,31
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FFFFFF;
	// addi r10,r9,32
	ctx.r10.s64 = ctx.r9.s64 + 32;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// li r3,512
	ctx.r3.s64 = 512;
	// li r4,32
	ctx.r4.s64 = 32;
	// li r5,48
	ctx.r5.s64 = 48;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D2DE4:
	// lvrx128 v53,r7,r9
	temp.u32 = ctx.r7.u32 + ctx.r9.u32;
	simd::store_i8(ctx.v53.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r8,r10,-16
	ctx.r8.s64 = ctx.r10.s64 + -16;
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// lvlx128 v52,r0,r9
	temp.u32 = ctx.r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v51,r0,r10
	temp.u32 = ctx.r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v11,v52,v53
	simd::store_i8(ctx.v11.u8, simd::or_i8(simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v53.u8)));
	// lvrx128 v50,r6,r10
	temp.u32 = ctx.r6.u32 + ctx.r10.u32;
	simd::store_i8(ctx.v50.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v7,v51,v50
	simd::store_i8(ctx.v7.u8, simd::or_i8(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v50.u8)));
	// lvrx128 v49,r6,r8
	temp.u32 = ctx.r6.u32 + ctx.r8.u32;
	simd::store_i8(ctx.v49.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// lvlx128 v48,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v47,r6,r7
	temp.u32 = ctx.r6.u32 + ctx.r7.u32;
	simd::store_i8(ctx.v47.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v8,v48,v49
	simd::store_i8(ctx.v8.u8, simd::or_i8(simd::load_i8(ctx.v48.u8), simd::load_i8(ctx.v49.u8)));
	// lvlx128 v46,r0,r7
	temp.u32 = ctx.r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v6,v46,v47
	simd::store_i8(ctx.v6.u8, simd::or_i8(simd::load_i8(ctx.v46.u8), simd::load_i8(ctx.v47.u8)));
	// dcbt r11,r3
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// beq cr6,0x822d2e5c
	if (ctx.cr6.eq) goto loc_822D2E5C;
	// lvx128 v12,r0,r11
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r11.u32) & ~0xF), VectorMaskL));
	// vmaddfp v12,v11,v13,v12
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// lvx128 v11,r11,r6
	simd::store_shuffled(ctx.v11, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r6.u32) & ~0xF), VectorMaskL));
	// vaddfp v13,v13,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// lvx128 v10,r11,r4
	simd::store_shuffled(ctx.v10, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r4.u32) & ~0xF), VectorMaskL));
	// lvx128 v9,r11,r5
	simd::store_shuffled(ctx.v9, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r5.u32) & ~0xF), VectorMaskL));
	// vmaddfp v11,v8,v13,v11
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v8.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vaddfp v13,v13,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v10,v7,v13,v10
	simd::store_f32_aligned(ctx.v10.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v10.f32)));
	// vaddfp v13,v13,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v9,v6,v13,v9
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vaddfp v13,v13,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// b 0x822d2e7c
	goto loc_822D2E7C;
loc_822D2E5C:
	// vaddfp128 v45,v13,v0
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v45.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v12,v11,v13
	simd::store_f32_aligned(ctx.v12.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vaddfp128 v44,v45,v0
	simd::store_f32_aligned(ctx.v44.f32, simd::add_f32(simd::load_f32_aligned(ctx.v45.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v11,v8,v45
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v8.f32), simd::load_f32_aligned(ctx.v45.f32)));
	// vaddfp128 v43,v44,v0
	simd::store_f32_aligned(ctx.v43.f32, simd::add_f32(simd::load_f32_aligned(ctx.v44.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v10,v7,v44
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(ctx.v44.f32)));
	// vmulfp128 v9,v6,v43
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(ctx.v43.f32)));
	// vaddfp128 v13,v43,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v43.f32), simd::load_f32_aligned(ctx.v0.f32)));
loc_822D2E7C:
	// stvx128 v11,r11,r6
	ea = (ctx.r11.u32 + ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v11), &VectorMaskL[(ea & 0xF) * 16]);
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// stvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v12), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// stvx128 v10,r11,r4
	ea = (ctx.r11.u32 + ctx.r4.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v10), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// stvx128 v9,r11,r5
	ea = (ctx.r11.u32 + ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v9), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r11,r11,64
	ctx.r11.s64 = ctx.r11.s64 + 64;
	// bdnz 0x822d2de4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D2DE4;
loc_822D2EA0:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x822d2ef0
	if (ctx.cr6.eq) goto loc_822D2EF0;
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
loc_822D2EB0:
	// vmaddfp v12,v0,v5,v13
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v5.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vor128 v42,v0,v0
	simd::store_i8(ctx.v42.u8, simd::load_i8(ctx.v0.u8));
	// lvlx128 v41,r10,r11
	temp.u32 = ctx.r10.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// vmulfp128 v63,v41,v13
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v41.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vrlimi128 v42,v0,15,1
	uint32_t sh = 15 & 31;
	for(int i=0;i<4;i++) {
		uint32_t val = ctx.v0.u32[i];
		val = (val << sh) | (val >> (32 - sh));
		if ((1 >> i) & 1) {
			ctx.v42.u32[i] = val;
		}
	}
	// vor128 v0,v42,v42
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v42.u8));
	// vor v13,v12,v12
	simd::store_i8(ctx.v13.u8, simd::load_i8(ctx.v12.u8));
	// vrlimi128 v13,v12,15,1
	uint32_t sh = 15 & 31;
	for(int i=0;i<4;i++) {
		uint32_t val = ctx.v12.u32[i];
		val = (val << sh) | (val >> (32 - sh));
		if ((1 >> i) & 1) {
			ctx.v13.u32[i] = val;
		}
	}
	// beq cr6,0x822d2ee0
	if (ctx.cr6.eq) goto loc_822D2EE0;
	// lvlx128 v40,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v63,v63,v40
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v40.f32)));
loc_822D2EE0:
	// vspltw128 v39,v63,0
	simd::store_i32(ctx.v39.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v63.u32), 3));
	// stvewx128 v39,r0,r11
	PPC_STORE_U32((ctx.r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v39.u32), 3 - ((ctx.r11.u32) & 0xF) >> 2));
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822d2eb0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D2EB0;
loc_822D2EF0:
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D2EFC"))) PPC_WEAK_FUNC(sub_822D2EFC);
PPC_FUNC_IMPL(__imp__sub_822D2EFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D2F00"))) PPC_WEAK_FUNC(sub_822D2F00);
PPC_FUNC_IMPL(__imp__sub_822D2F00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e450
	ctx.lr = 0x822D2F08;
	__restfpr_22(ctx, base);
	// lwz r10,36(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r26,24(r3)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r28,28(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r22,32(r3)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// beq cr6,0x822d2f34
	if (ctx.cr6.eq) goto loc_822D2F34;
	// li r28,1
	ctx.r28.s64 = 1;
	// li r26,1
	ctx.r26.s64 = 1;
loc_822D2F34:
	// lwz r24,12(r3)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// twllei r26,0
	if (ctx.r26.u32 <= 0) __builtin_debugtrap();
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// bne cr6,0x822d3014
	if (!ctx.cr6.eq) goto loc_822D3014;
	// divwu. r10,r10,r26
	ctx.r10.u32 = ctx.r10.u32 / ctx.r26.u32;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x822d3114
	if (ctx.cr0.eq) goto loc_822D3114;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r30,8(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r29,r26,1,0,30
	ctx.r29.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r27,r28,2,0,29
	ctx.r27.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f12,-1496(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1496);
	ctx.f12.f64 = double(temp.f32);
loc_822D2F60:
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq cr6,0x822d3000
	if (ctx.cr6.eq) goto loc_822D3000;
	// lha r9,0(r11)
	ctx.r9.s64 = int16_t(PPC_LOAD_U16(ctx.r11.u32 + 0));
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// std r9,-112(r1)
	PPC_STORE_U64(ctx.r1.u32 + -112, ctx.r9.u64);
	// lfd f0,-112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// addi r31,r6,108
	ctx.r31.s64 = ctx.r6.s64 + 108;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// frsp f11,f0
	ctx.f11.f64 = double(float(ctx.f0.f64));
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// addi r8,r30,-4
	ctx.r8.s64 = ctx.r30.s64 + -4;
loc_822D2F90:
	// lfs f0,4(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// fmuls f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// cmplwi cr6,r26,1
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 1, ctx.xer);
	// fmuls f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// ble cr6,0x822d2fd8
	if (!ctx.cr6.gt) goto loc_822D2FD8;
	// addi r7,r26,-1
	ctx.r7.s64 = ctx.r26.s64 + -1;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_822D2FB4:
	// lhau r7,2(r9)
	ea = 2 + ctx.r9.u32;	ctx.r7.s64 = int16_t(PPC_LOAD_U16(ea));	ctx.r9.u32 = ea;	// lfsu f13,4(r8)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r8.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r8.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// std r7,-104(r1)
	PPC_STORE_U64(ctx.r1.u32 + -104, ctx.r7.u64);
	// lfd f10,-104(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fmuls f13,f10,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmadds f0,f13,f12,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f12.f64), float(ctx.f0.f64)));
	// bdnz 0x822d2fb4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D2FB4;
loc_822D2FD8:
	// dcbt r0,r3
	// dcbt r0,r31
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// beq cr6,0x822d2ff0
	if (ctx.cr6.eq) goto loc_822D2FF0;
	// lfs f13,0(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
loc_822D2FF0:
	// stfs f0,0(r5)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// addic. r4,r4,-1
	ctx.xer.ca = ctx.r4.u32 > 0;
	ctx.r4.s64 = ctx.r4.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne 0x822d2f90
	if (!ctx.cr0.eq) goto loc_822D2F90;
loc_822D3000:
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// add r6,r27,r6
	ctx.r6.u64 = ctx.r27.u64 + ctx.r6.u64;
	// bne 0x822d2f60
	if (!ctx.cr0.eq) goto loc_822D2F60;
	// b 0x822d3114
	goto loc_822D3114;
loc_822D3014:
	// divwu. r23,r10,r26
	ctx.r23.u32 = ctx.r10.u32 / ctx.r26.u32;
	ctx.cr0.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// li r7,0
	ctx.r7.s64 = 0;
	// beq 0x822d3114
	if (ctx.cr0.eq) goto loc_822D3114;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lwz r29,8(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r27,r26,1,0,30
	ctx.r27.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r28,2,0,29
	ctx.r25.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f10,-1496(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1496);
	ctx.f10.f64 = double(temp.f32);
loc_822D3034:
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq cr6,0x822d3100
	if (ctx.cr6.eq) goto loc_822D3100;
	// clrldi r9,r7,32
	ctx.r9.u64 = ctx.r7.u64 & 0xFFFFFFFF;
	// lha r10,0(r11)
	ctx.r10.s64 = int16_t(PPC_LOAD_U16(ctx.r11.u32 + 0));
	// std r9,-104(r1)
	PPC_STORE_U64(ctx.r1.u32 + -104, ctx.r9.u64);
	// addi r31,r11,24
	ctx.r31.s64 = ctx.r11.s64 + 24;
	// std r10,-112(r1)
	PPC_STORE_U64(ctx.r1.u32 + -112, ctx.r10.u64);
	// addi r30,r6,108
	ctx.r30.s64 = ctx.r6.s64 + 108;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r10,r24,-4
	ctx.r10.s64 = ctx.r24.s64 + -4;
	// addi r9,r29,-4
	ctx.r9.s64 = ctx.r29.s64 + -4;
	// lfd f0,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f11,f0
	ctx.f11.f64 = double(float(ctx.f0.f64));
	// lfd f0,-112(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f9,f0
	ctx.f9.f64 = double(float(ctx.f0.f64));
loc_822D307C:
	// lfs f0,4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lfs f13,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fmadds f0,f0,f11,f13
	ctx.f0.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f11.f64), float(ctx.f13.f64)));
	// cmplwi cr6,r26,1
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 1, ctx.xer);
	// fmuls f0,f0,f9
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// fmuls f0,f0,f10
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// ble cr6,0x822d30d8
	if (!ctx.cr6.gt) goto loc_822D30D8;
	// addi r5,r26,-1
	ctx.r5.s64 = ctx.r26.s64 + -1;
	// mr r8,r11
	ctx.r8.u64 = ctx.r11.u64;
	// mtctr r5
	ctx.ctr.u64 = ctx.r5.u64;
loc_822D30AC:
	// lhau r5,2(r8)
	ea = 2 + ctx.r8.u32;	ctx.r5.s64 = int16_t(PPC_LOAD_U16(ea));	ctx.r8.u32 = ea;	// lfsu f12,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// std r5,-96(r1)
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.r5.u64);
	// lfsu f13,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fmadds f13,f13,f11,f12
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f12.f64)));
	// lfd f12,-96(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmuls f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fmadds f0,f13,f10,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f10.f64), float(ctx.f0.f64)));
	// bdnz 0x822d30ac
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D30AC;
loc_822D30D8:
	// dcbt r0,r31
	// dcbt r0,r30
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// beq cr6,0x822d30f0
	if (ctx.cr6.eq) goto loc_822D30F0;
	// lfs f13,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
loc_822D30F0:
	// stfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// addic. r3,r3,-1
	ctx.xer.ca = ctx.r3.u32 > 0;
	ctx.r3.s64 = ctx.r3.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// bne 0x822d307c
	if (!ctx.cr0.eq) goto loc_822D307C;
loc_822D3100:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// add r11,r27,r11
	ctx.r11.u64 = ctx.r27.u64 + ctx.r11.u64;
	// add r6,r25,r6
	ctx.r6.u64 = ctx.r25.u64 + ctx.r6.u64;
	// cmplw cr6,r7,r23
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r23.u32, ctx.xer);
	// blt cr6,0x822d3034
	if (ctx.cr6.lt) goto loc_822D3034;
loc_822D3114:
	// b 0x8233e4a0
	__restgprlr_22(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D3118"))) PPC_WEAK_FUNC(sub_822D3118);
PPC_FUNC_IMPL(__imp__sub_822D3118) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lfs f12,-1496(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -1496);
	ctx.f12.f64 = double(temp.f32);
	// lwz r7,32(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f0,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// fmuls f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// bne cr6,0x822d31c8
	if (!ctx.cr6.eq) goto loc_822D31C8;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r8,24
	ctx.r8.s64 = 24;
	// li r9,72
	ctx.r9.s64 = 72;
loc_822D3164:
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f12,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f12,f11
	ctx.f12.f64 = double(float(ctx.f11.f64));
	// dcbt r11,r8
	// dcbt r10,r9
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq cr6,0x822d31ac
	if (ctx.cr6.eq) goto loc_822D31AC;
	// lfs f11,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f10,f12,f0,f11
	ctx.f10.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// stfs f10,0(r10)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f9,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f12,f13,f9
	ctx.f8.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f13.f64), float(ctx.f9.f64)));
	// stfs f8,0(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d31bc
	goto loc_822D31BC;
loc_822D31AC:
	// fmuls f11,f12,f0
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f11,0(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmuls f10,f12,f13
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfsu f10,4(r10)
	temp.f32 = float(ctx.f10.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
loc_822D31BC:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x822d3164
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D3164;
	// blr 
	return;
loc_822D31C8:
	// lfs f11,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f10,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f11,f11,f12
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// fmuls f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r8,24
	ctx.r8.s64 = 24;
	// li r9,72
	ctx.r9.s64 = 72;
loc_822D31EC:
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f12,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f9,f12
	ctx.f9.f64 = double(ctx.f12.s64);
	// frsp f12,f9
	ctx.f12.f64 = double(float(ctx.f9.f64));
	// dcbt r11,r8
	// dcbt r10,r9
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq cr6,0x822d3234
	if (ctx.cr6.eq) goto loc_822D3234;
	// lfs f9,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f12,f0,f9
	ctx.f8.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f9.f64)));
	// stfs f8,0(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f7,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f12,f13,f7
	ctx.f6.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f13.f64), float(ctx.f7.f64)));
	// stfs f6,0(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d3244
	goto loc_822D3244;
loc_822D3234:
	// fmuls f9,f12,f0
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f9,0(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmuls f8,f12,f13
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfsu f8,4(r10)
	temp.f32 = float(ctx.f8.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
loc_822D3244:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f11,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// fadds f13,f10,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 + ctx.f13.f64));
	// bdnz 0x822d31ec
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D31EC;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D3258"))) PPC_WEAK_FUNC(sub_822D3258);
PPC_FUNC_IMPL(__imp__sub_822D3258) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lfs f0,-1496(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// lwz r7,32(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f13,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f11,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f10,12(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f9,16(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f8,20(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// bne cr6,0x822d3378
	if (!ctx.cr6.eq) goto loc_822D3378;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r8,24
	ctx.r8.s64 = 24;
	// li r9,216
	ctx.r9.s64 = 216;
loc_822D32C4:
	// lhz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f0,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f7,f0
	ctx.f7.f64 = double(ctx.f0.s64);
	// frsp f0,f7
	ctx.f0.f64 = double(float(ctx.f7.f64));
	// dcbt r10,r8
	// dcbt r11,r9
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq cr6,0x822d333c
	if (ctx.cr6.eq) goto loc_822D333C;
	// lfs f7,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f0,f13,f7
	ctx.f6.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f13.f64), float(ctx.f7.f64)));
	// stfs f6,0(r11)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f5,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f5.f64 = double(temp.f32);
	// fmadds f4,f0,f12,f5
	ctx.f4.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f12.f64), float(ctx.f5.f64)));
	// stfs f4,0(r11)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f3,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f3.f64 = double(temp.f32);
	// fmadds f2,f0,f11,f3
	ctx.f2.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f11.f64), float(ctx.f3.f64)));
	// stfs f2,0(r11)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f7,f0,f10,f1
	ctx.f7.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), float(ctx.f1.f64)));
	// stfs f7,0(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f6,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f6.f64 = double(temp.f32);
	// fmadds f5,f0,f9,f6
	ctx.f5.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f9.f64), float(ctx.f6.f64)));
	// stfs f5,0(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f4,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f4.f64 = double(temp.f32);
	// fmadds f3,f0,f8,f4
	ctx.f3.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f8.f64), float(ctx.f4.f64)));
	// stfs f3,0(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822d336c
	goto loc_822D336C;
loc_822D333C:
	// fmuls f7,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f7,0(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f6,f0,f12
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// stfsu f6,4(r11)
	temp.f32 = float(ctx.f6.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f5,f0,f11
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// stfsu f5,4(r11)
	temp.f32 = float(ctx.f5.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f4,f0,f10
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfsu f4,4(r11)
	temp.f32 = float(ctx.f4.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f3,f0,f9
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// stfsu f3,4(r11)
	temp.f32 = float(ctx.f3.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f2,f0,f8
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// stfsu f2,4(r11)
	temp.f32 = float(ctx.f2.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
loc_822D336C:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822d32c4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D32C4;
	// blr 
	return;
loc_822D3378:
	// lfs f7,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f6,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f5,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f4,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f2,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r8,24
	ctx.r8.s64 = 24;
	// li r9,216
	ctx.r9.s64 = 216;
loc_822D33BC:
	// lhz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f0,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f1,f0
	ctx.f1.f64 = double(ctx.f0.s64);
	// frsp f0,f1
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// dcbt r10,r8
	// dcbt r11,r9
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq cr6,0x822d3434
	if (ctx.cr6.eq) goto loc_822D3434;
	// lfs f1,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f0,f13,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f13.f64), float(ctx.f1.f64)));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f0,f12,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f12.f64), float(ctx.f1.f64)));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f0,f11,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f11.f64), float(ctx.f1.f64)));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f0,f10,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), float(ctx.f1.f64)));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f0,f9,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f9.f64), float(ctx.f1.f64)));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f1,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fmadds f0,f0,f8,f1
	ctx.f0.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f8.f64), float(ctx.f1.f64)));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822d3464
	goto loc_822D3464;
loc_822D3434:
	// fmuls f1,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f1,f0,f12
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// stfsu f1,4(r11)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f1,f0,f11
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// stfsu f1,4(r11)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f1,f0,f10
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfsu f1,4(r11)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f1,f0,f9
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// stfsu f1,4(r11)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmuls f0,f0,f8
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// stfsu f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
loc_822D3464:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fadds f13,f7,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f7.f64 + ctx.f13.f64));
	// fadds f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f12.f64));
	// fadds f11,f5,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 + ctx.f11.f64));
	// fadds f10,f4,f10
	ctx.f10.f64 = double(float(ctx.f4.f64 + ctx.f10.f64));
	// fadds f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 + ctx.f9.f64));
	// fadds f8,f2,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// bdnz 0x822d33bc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D33BC;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D3488"))) PPC_WEAK_FUNC(sub_822D3488);
PPC_FUNC_IMPL(__imp__sub_822D3488) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa00
	ctx.lr = 0x822D3498;
	sub_8233FA00(ctx, base);
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lfs f0,-1496(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// lwz r7,32(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f13,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f9,12(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f8,16(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfs f7,20(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f6,24(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 24);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f5,28(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f4,32(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 32);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,36(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 36);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f2,40(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 40);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f1,44(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 44);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// bne cr6,0x822d365c
	if (!ctx.cr6.eq) goto loc_822D365C;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x822d3814
	if (ctx.cr6.eq) goto loc_822D3814;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,24
	ctx.r8.s64 = 24;
	// rlwinm r9,r9,31,1,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,216
	ctx.r9.s64 = 216;
loc_822D3540:
	// lhz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U16(ea);
	ctx.r10.u32 = ea;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// std r6,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r6.u64);
	// lfd f0,-176(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// std r5,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r5.u64);
	// lfd f13,-168(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// dcbt r10,r8
	// dcbt r11,r9
	// fmuls f31,f0,f12
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmadds f31,f13,f11,f31
	ctx.f31.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f31.f64)));
	// beq cr6,0x822d3600
	if (ctx.cr6.eq) goto loc_822D3600;
	// fmuls f29,f0,f10
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// lfs f30,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f28,f0,f8
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// fmuls f27,f0,f6
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// fmuls f26,f0,f4
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f29,f13,f9,f29
	ctx.f29.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f29.f64)));
	// fmadds f28,f13,f7,f28
	ctx.f28.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f28.f64)));
	// fmadds f27,f13,f5,f27
	ctx.f27.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f27.f64)));
	// fmadds f26,f13,f3,f26
	ctx.f26.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f3.f64), float(ctx.f26.f64)));
	// fmadds f13,f13,f1,f0
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f0.f64)));
	// fadds f0,f31,f30
	ctx.f0.f64 = double(float(ctx.f31.f64 + ctx.f30.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f29,f0
	ctx.f0.f64 = double(float(ctx.f29.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f28,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f27,f0
	ctx.f0.f64 = double(float(ctx.f27.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f26,f0
	ctx.f0.f64 = double(float(ctx.f26.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822d3640
	goto loc_822D3640;
loc_822D3600:
	// fmuls f30,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.f30.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfs f31,0(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f29,f0,f8
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// fmuls f28,f0,f6
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// fmuls f27,f0,f4
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f31,f13,f9,f30
	ctx.f31.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f30.f64)));
	// stfsu f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f31,f13,f7,f29
	ctx.f31.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f29.f64)));
	// stfsu f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f31,f13,f5,f28
	ctx.f31.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f28.f64)));
	// stfsu f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f31,f13,f3,f27
	ctx.f31.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f3.f64), float(ctx.f27.f64)));
	// stfsu f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f13,f13,f1,f0
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f0.f64)));
	// stfsu f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
loc_822D3640:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822d3540
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D3540;
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D3650;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_822D365C:
	// lfs f13,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// fmuls f31,f13,f0
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f13,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f29,f13,f0
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f13,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f27,f13,f0
	ctx.f27.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f13,24(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f25,f13,f0
	ctx.f25.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f13,32(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f23,f13,f0
	ctx.f23.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f30,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f30.f64 = double(temp.f32);
	// lfs f28,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f26,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f24,28(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f26,f26,f0
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// lfs f22,36(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f24,f0
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// lfs f13,40(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f22,f22,f0
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// lfs f20,44(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f21,f13,f0
	ctx.f21.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f20,f20,f0
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f0.f64));
	// beq cr6,0x822d3814
	if (ctx.cr6.eq) goto loc_822D3814;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,24
	ctx.r8.s64 = 24;
	// rlwinm r9,r9,31,1,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,216
	ctx.r9.s64 = 216;
loc_822D36DC:
	// lhz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhzu r5,2(r10)
	ea = 2 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U16(ea);
	ctx.r10.u32 = ea;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// std r6,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r6.u64);
	// lfd f0,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// std r5,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r5.u64);
	// lfd f0,-176(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f19,f0
	ctx.f19.f64 = double(ctx.f0.s64);
	// frsp f0,f13
	ctx.f0.f64 = double(float(ctx.f13.f64));
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// frsp f13,f19
	ctx.f13.f64 = double(float(ctx.f19.f64));
	// dcbt r10,r8
	// dcbt r11,r9
	// fmuls f19,f0,f12
	ctx.f19.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmadds f19,f13,f11,f19
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f19.f64)));
	// beq cr6,0x822d379c
	if (ctx.cr6.eq) goto loc_822D379C;
	// fmuls f17,f0,f10
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// lfs f18,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f16,f0,f8
	ctx.f16.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// fmuls f15,f0,f6
	ctx.f15.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// fmuls f14,f0,f4
	ctx.f14.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f17,f13,f9,f17
	ctx.f17.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f17.f64)));
	// fmadds f16,f13,f7,f16
	ctx.f16.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f16.f64)));
	// fmadds f15,f13,f5,f15
	ctx.f15.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f15.f64)));
	// fmadds f14,f13,f3,f14
	ctx.f14.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f3.f64), float(ctx.f14.f64)));
	// fmadds f13,f13,f1,f0
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f0.f64)));
	// fadds f0,f19,f18
	ctx.f0.f64 = double(float(ctx.f19.f64 + ctx.f18.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f17,f0
	ctx.f0.f64 = double(float(ctx.f17.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f16,f0
	ctx.f0.f64 = double(float(ctx.f16.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f15,f0
	ctx.f0.f64 = double(float(ctx.f15.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f14,f0
	ctx.f0.f64 = double(float(ctx.f14.f64 + ctx.f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfsu f0,4(r11)
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822d37dc
	goto loc_822D37DC;
loc_822D379C:
	// fmuls f18,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.f18.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfs f19,0(r11)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f17,f0,f8
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// fmuls f16,f0,f6
	ctx.f16.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// fmuls f15,f0,f4
	ctx.f15.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f19,f13,f9,f18
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f18.f64)));
	// stfsu f19,4(r11)
	temp.f32 = float(ctx.f19.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f19,f13,f7,f17
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f17.f64)));
	// stfsu f19,4(r11)
	temp.f32 = float(ctx.f19.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f19,f13,f5,f16
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f16.f64)));
	// stfsu f19,4(r11)
	temp.f32 = float(ctx.f19.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f19,f13,f3,f15
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f3.f64), float(ctx.f15.f64)));
	// stfsu f19,4(r11)
	temp.f32 = float(ctx.f19.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f13,f13,f1,f0
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f0.f64)));
	// stfsu f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
loc_822D37DC:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fadds f12,f31,f12
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f31.f64 + ctx.f12.f64));
	// fadds f11,f30,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 + ctx.f11.f64));
	// fadds f10,f29,f10
	ctx.f10.f64 = double(float(ctx.f29.f64 + ctx.f10.f64));
	// fadds f9,f28,f9
	ctx.f9.f64 = double(float(ctx.f28.f64 + ctx.f9.f64));
	// fadds f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 + ctx.f8.f64));
	// fadds f7,f26,f7
	ctx.f7.f64 = double(float(ctx.f26.f64 + ctx.f7.f64));
	// fadds f6,f25,f6
	ctx.f6.f64 = double(float(ctx.f25.f64 + ctx.f6.f64));
	// fadds f5,f24,f5
	ctx.f5.f64 = double(float(ctx.f24.f64 + ctx.f5.f64));
	// fadds f4,f23,f4
	ctx.f4.f64 = double(float(ctx.f23.f64 + ctx.f4.f64));
	// fadds f3,f22,f3
	ctx.f3.f64 = double(float(ctx.f22.f64 + ctx.f3.f64));
	// fadds f2,f21,f2
	ctx.f2.f64 = double(float(ctx.f21.f64 + ctx.f2.f64));
	// fadds f1,f20,f1
	ctx.f1.f64 = double(float(ctx.f20.f64 + ctx.f1.f64));
	// bdnz 0x822d36dc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D36DC;
loc_822D3814:
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D381C;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D3828"))) PPC_WEAK_FUNC(sub_822D3828);
PPC_FUNC_IMPL(__imp__sub_822D3828) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa00
	ctx.lr = 0x822D3838;
	sub_8233FA00(ctx, base);
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lfs f0,-1496(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// lwz r7,32(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f13,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f11,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f10,12(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f9,16(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f8,20(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfs f7,24(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 24);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f6,28(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f5,32(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 32);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f4,36(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 36);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,40(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 40);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f2,44(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 44);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// bne cr6,0x822d39f8
	if (!ctx.cr6.eq) goto loc_822D39F8;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x822d3bac
	if (ctx.cr6.eq) goto loc_822D3BAC;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,6
	ctx.r8.s64 = 6;
	// divwu r9,r9,r8
	ctx.r9.u32 = ctx.r9.u32 / ctx.r8.u32;
	// li r8,48
	ctx.r8.s64 = 48;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,72
	ctx.r9.s64 = 72;
loc_822D38E4:
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// lhzu r5,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r5.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// std r6,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r6.u64);
	// lfd f1,-200(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// std r5,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r5.u64);
	// lfd f0,-208(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// lhzu r4,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r4.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// fcfid f31,f1
	ctx.f31.f64 = double(ctx.f1.s64);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lhzu r6,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r6.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r5,r4
	ctx.r5.s64 = ctx.r4.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// std r5,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r5.u64);
	// lfd f1,-192(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// fcfid f30,f1
	ctx.f30.f64 = double(ctx.f1.s64);
	// std r6,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r6.u64);
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// lhzu r3,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r3.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// lfd f0,-184(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// fcfid f29,f0
	ctx.f29.f64 = double(ctx.f0.s64);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lhzu r4,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r4.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// frsp f0,f31
	ctx.f0.f64 = double(float(ctx.f31.f64));
	// std r3,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r3.u64);
	// lfd f31,-176(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// extsh r5,r4
	ctx.r5.s64 = ctx.r4.s16;
	// fcfid f28,f31
	ctx.f28.f64 = double(ctx.f31.s64);
	// frsp f31,f30
	ctx.f31.f64 = double(float(ctx.f30.f64));
	// std r5,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r5.u64);
	// frsp f30,f29
	ctx.f30.f64 = double(float(ctx.f29.f64));
	// lfd f29,-168(r1)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f27,f29
	ctx.f27.f64 = double(ctx.f29.s64);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// frsp f29,f28
	ctx.f29.f64 = double(float(ctx.f28.f64));
	// frsp f28,f27
	ctx.f28.f64 = double(float(ctx.f27.f64));
	// dcbt r11,r8
	// dcbt r10,r9
	// fmuls f27,f29,f9
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f9.f64));
	// fmuls f29,f29,f3
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmadds f27,f28,f8,f27
	ctx.f27.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f8.f64), float(ctx.f27.f64)));
	// fmadds f29,f28,f2,f29
	ctx.f29.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f2.f64), float(ctx.f29.f64)));
	// fmadds f28,f30,f10,f27
	ctx.f28.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f10.f64), float(ctx.f27.f64)));
	// fmadds f30,f30,f4,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f4.f64), float(ctx.f29.f64)));
	// fmadds f29,f31,f11,f28
	ctx.f29.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f11.f64), float(ctx.f28.f64)));
	// fmadds f31,f31,f5,f30
	ctx.f31.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f5.f64), float(ctx.f30.f64)));
	// fmadds f30,f1,f12,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f12.f64), float(ctx.f29.f64)));
	// fmadds f1,f1,f6,f31
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f6.f64), float(ctx.f31.f64)));
	// fmadds f31,f0,f13,f30
	ctx.f31.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f13.f64), float(ctx.f30.f64)));
	// fmadds f0,f0,f7,f1
	ctx.f0.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f7.f64), float(ctx.f1.f64)));
	// beq cr6,0x822d39d4
	if (ctx.cr6.eq) goto loc_822D39D4;
	// lfs f26,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f26.f64 = double(temp.f32);
	// fadds f1,f31,f26
	ctx.f1.f64 = double(float(ctx.f31.f64 + ctx.f26.f64));
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f1,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fadds f0,f0,f1
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f1.f64));
	// stfs f0,0(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d39dc
	goto loc_822D39DC;
loc_822D39D4:
	// stfs f31,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfsu f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
loc_822D39DC:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x822d38e4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D38E4;
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D39EC;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_822D39F8:
	// lfs f1,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f31,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f27,f1,f0
	ctx.f27.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f26,f31,f0
	ctx.f26.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f1,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f25,f1,f0
	ctx.f25.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f24,f31,f0
	ctx.f24.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f1,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f23,f1,f0
	ctx.f23.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f22,f31,f0
	ctx.f22.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f1,24(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,28(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f21,f1,f0
	ctx.f21.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f20,f31,f0
	ctx.f20.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f1,32(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,36(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f19,f1,f0
	ctx.f19.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f18,f31,f0
	ctx.f18.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f1,40(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,44(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f17,f1,f0
	ctx.f17.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f16,f31,f0
	ctx.f16.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// beq cr6,0x822d3bac
	if (ctx.cr6.eq) goto loc_822D3BAC;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,6
	ctx.r8.s64 = 6;
	// divwu r9,r9,r8
	ctx.r9.u32 = ctx.r9.u32 / ctx.r8.u32;
	// li r8,48
	ctx.r8.s64 = 48;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,72
	ctx.r9.s64 = 72;
loc_822D3A7C:
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// lhzu r5,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r5.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// std r6,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r6.u64);
	// std r5,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r5.u64);
	// lfd f31,-176(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// lhzu r4,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r4.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// fcfid f31,f31
	ctx.f31.f64 = double(ctx.f31.s64);
	// lfd f0,-168(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// extsh r5,r4
	ctx.r5.s64 = ctx.r4.s16;
	// fcfid f1,f0
	ctx.f1.f64 = double(ctx.f0.s64);
	// lhzu r6,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r6.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// std r5,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r5.u64);
	// extsh r3,r6
	ctx.r3.s64 = ctx.r6.s16;
	// std r3,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r3.u64);
	// lfd f29,-200(r1)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// lhzu r6,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r6.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// fcfid f28,f29
	ctx.f28.f64 = double(ctx.f29.s64);
	// frsp f0,f1
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// lfd f1,-184(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// fcfid f30,f1
	ctx.f30.f64 = double(ctx.f1.s64);
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// frsp f1,f31
	ctx.f1.f64 = double(float(ctx.f31.f64));
	// std r4,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r4.u64);
	// frsp f31,f30
	ctx.f31.f64 = double(float(ctx.f30.f64));
	// lfd f30,-192(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// fcfid f30,f30
	ctx.f30.f64 = double(ctx.f30.s64);
	// lhzu r3,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r3.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r5,r3
	ctx.r5.s64 = ctx.r3.s16;
	// frsp f29,f30
	ctx.f29.f64 = double(float(ctx.f30.f64));
	// frsp f30,f28
	ctx.f30.f64 = double(float(ctx.f28.f64));
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// std r5,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r5.u64);
	// lfd f28,-208(r1)
	ctx.f28.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// fcfid f28,f28
	ctx.f28.f64 = double(ctx.f28.s64);
	// frsp f28,f28
	ctx.f28.f64 = double(float(ctx.f28.f64));
	// dcbt r11,r8
	// dcbt r10,r9
	// fmuls f15,f29,f9
	ctx.f15.f64 = double(float(ctx.f29.f64 * ctx.f9.f64));
	// fmuls f29,f29,f3
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmadds f15,f28,f8,f15
	ctx.f15.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f8.f64), float(ctx.f15.f64)));
	// fmadds f29,f28,f2,f29
	ctx.f29.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f2.f64), float(ctx.f29.f64)));
	// fmadds f28,f30,f10,f15
	ctx.f28.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f10.f64), float(ctx.f15.f64)));
	// fmadds f30,f30,f4,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f4.f64), float(ctx.f29.f64)));
	// fmadds f29,f31,f11,f28
	ctx.f29.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f11.f64), float(ctx.f28.f64)));
	// fmadds f31,f31,f5,f30
	ctx.f31.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f5.f64), float(ctx.f30.f64)));
	// fmadds f30,f1,f12,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f12.f64), float(ctx.f29.f64)));
	// fmadds f1,f1,f6,f31
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f6.f64), float(ctx.f31.f64)));
	// fmadds f31,f0,f13,f30
	ctx.f31.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f13.f64), float(ctx.f30.f64)));
	// fmadds f0,f0,f7,f1
	ctx.f0.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f7.f64), float(ctx.f1.f64)));
	// beq cr6,0x822d3b6c
	if (ctx.cr6.eq) goto loc_822D3B6C;
	// lfs f14,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f14.f64 = double(temp.f32);
	// fadds f1,f31,f14
	ctx.f1.f64 = double(float(ctx.f31.f64 + ctx.f14.f64));
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f1,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f1.f64 = double(temp.f32);
	// fadds f0,f0,f1
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f1.f64));
	// stfs f0,0(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d3b74
	goto loc_822D3B74;
loc_822D3B6C:
	// stfs f31,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfsu f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
loc_822D3B74:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f13,f27,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f27.f64 + ctx.f13.f64));
	// fadds f12,f26,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 + ctx.f12.f64));
	// fadds f11,f25,f11
	ctx.f11.f64 = double(float(ctx.f25.f64 + ctx.f11.f64));
	// fadds f10,f24,f10
	ctx.f10.f64 = double(float(ctx.f24.f64 + ctx.f10.f64));
	// fadds f9,f23,f9
	ctx.f9.f64 = double(float(ctx.f23.f64 + ctx.f9.f64));
	// fadds f8,f22,f8
	ctx.f8.f64 = double(float(ctx.f22.f64 + ctx.f8.f64));
	// fadds f7,f21,f7
	ctx.f7.f64 = double(float(ctx.f21.f64 + ctx.f7.f64));
	// fadds f6,f20,f6
	ctx.f6.f64 = double(float(ctx.f20.f64 + ctx.f6.f64));
	// fadds f5,f19,f5
	ctx.f5.f64 = double(float(ctx.f19.f64 + ctx.f5.f64));
	// fadds f4,f18,f4
	ctx.f4.f64 = double(float(ctx.f18.f64 + ctx.f4.f64));
	// fadds f3,f17,f3
	ctx.f3.f64 = double(float(ctx.f17.f64 + ctx.f3.f64));
	// fadds f2,f16,f2
	ctx.f2.f64 = double(float(ctx.f16.f64 + ctx.f2.f64));
	// bdnz 0x822d3a7c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D3A7C;
loc_822D3BAC:
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D3BB4;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D3BC0"))) PPC_WEAK_FUNC(sub_822D3BC0);
PPC_FUNC_IMPL(__imp__sub_822D3BC0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa00
	ctx.lr = 0x822D3BD0;
	sub_8233FA00(ctx, base);
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lfs f0,-1496(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// lwz r7,32(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f12,88(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 88);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f19,f12,f0
	ctx.f19.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f13,84(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 84);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,92(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 92);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f20,f13,f0
	ctx.f20.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f12,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f13,f11,f0
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f11,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,40(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 40);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f31,f12,f0
	ctx.f31.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f11,44(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,48(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 48);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f30,f11,f0
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f29,f12,f0
	ctx.f29.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f11,52(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 52);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,56(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 56);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f28,f11,f0
	ctx.f28.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f27,f12,f0
	ctx.f27.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f11,60(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 60);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,64(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 64);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f26,f11,f0
	ctx.f26.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f25,f12,f0
	ctx.f25.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f11,68(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 68);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,72(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 72);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f24,f11,f0
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f23,f12,f0
	ctx.f23.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f10,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,12(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f6,16(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f5,20(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f4,24(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 24);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,28(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f2,32(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 32);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f1,36(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 36);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f11,76(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 76);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// lfs f12,80(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f22,f11,f0
	ctx.f22.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f20,-172(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// fmuls f21,f12,f0
	ctx.f21.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f19,-168(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// stfs f13,-164(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// bne cr6,0x822d3ea8
	if (!ctx.cr6.eq) goto loc_822D3EA8;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x822d42e0
	if (ctx.cr6.eq) goto loc_822D42E0;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,24
	ctx.r8.s64 = 24;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,216
	ctx.r9.s64 = 216;
loc_822D3CE4:
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// lhzu r5,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r5.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// std r6,-464(r1)
	PPC_STORE_U64(ctx.r1.u32 + -464, ctx.r6.u64);
	// lfd f13,-464(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -464);
	// std r5,-472(r1)
	PPC_STORE_U64(ctx.r1.u32 + -472, ctx.r5.u64);
	// lfd f0,-472(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -472);
	// lhzu r4,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r4.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// fcfid f11,f0
	ctx.f11.f64 = double(ctx.f0.s64);
	// lhzu r6,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r6.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r5,r4
	ctx.r5.s64 = ctx.r4.s16;
	// extsh r3,r6
	ctx.r3.s64 = ctx.r6.s16;
	// std r5,-312(r1)
	PPC_STORE_U64(ctx.r1.u32 + -312, ctx.r5.u64);
	// lfd f0,-312(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -312);
	// fcfid f18,f0
	ctx.f18.f64 = double(ctx.f0.s64);
	// std r3,-448(r1)
	PPC_STORE_U64(ctx.r1.u32 + -448, ctx.r3.u64);
	// frsp f0,f12
	ctx.f0.f64 = double(float(ctx.f12.f64));
	// lfd f12,-448(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -448);
	// frsp f13,f11
	ctx.f13.f64 = double(float(ctx.f11.f64));
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f12,f18
	ctx.f12.f64 = double(float(ctx.f18.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// dcbt r11,r8
	// dcbt r10,r9
	// fmuls f18,f12,f8
	ctx.f18.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fmuls f16,f12,f4
	ctx.f16.f64 = double(float(ctx.f12.f64 * ctx.f4.f64));
	// lfs f17,-164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f15,f12,f31
	ctx.f15.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmadds f18,f11,f7,f18
	ctx.f18.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f7.f64), float(ctx.f18.f64)));
	// fmadds f16,f11,f3,f16
	ctx.f16.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f3.f64), float(ctx.f16.f64)));
	// fmadds f15,f11,f30,f15
	ctx.f15.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f30.f64), float(ctx.f15.f64)));
	// beq cr6,0x822d3e20
	if (ctx.cr6.eq) goto loc_822D3E20;
	// stfd f8,-328(r1)
	PPC_STORE_U64(ctx.r1.u32 + -328, ctx.f8.u64);
	// fmuls f8,f12,f27
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// stfd f4,-480(r1)
	PPC_STORE_U64(ctx.r1.u32 + -480, ctx.f4.u64);
	// fmuls f4,f12,f23
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// fmuls f12,f12,f19
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f19.f64));
	// lfs f20,-172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	ctx.f20.f64 = double(temp.f32);
	// lfs f14,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,-168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f8,f11,f26,f8
	ctx.f8.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f26.f64), float(ctx.f8.f64)));
	// fmadds f4,f11,f22,f4
	ctx.f4.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f22.f64), float(ctx.f4.f64)));
	// fmadds f11,f11,f17,f12
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f17.f64), float(ctx.f12.f64)));
	// fmadds f12,f13,f9,f18
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f18.f64)));
	// fmadds f18,f13,f5,f16
	ctx.f18.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f16.f64)));
	// fmadds f17,f13,f1,f15
	ctx.f17.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f15.f64)));
	// fmadds f16,f13,f28,f8
	ctx.f16.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f28.f64), float(ctx.f8.f64)));
	// lfd f8,-328(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -328);
	// fmadds f15,f13,f24,f4
	ctx.f15.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f24.f64), float(ctx.f4.f64)));
	// lfd f4,-480(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -480);
	// fmadds f11,f13,f20,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f20.f64), float(ctx.f11.f64)));
	// fmadds f13,f0,f10,f12
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), float(ctx.f12.f64)));
	// fmadds f12,f0,f6,f18
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f6.f64), float(ctx.f18.f64)));
	// fmadds f18,f0,f2,f17
	ctx.f18.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f2.f64), float(ctx.f17.f64)));
	// fmadds f17,f0,f29,f16
	ctx.f17.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f29.f64), float(ctx.f16.f64)));
	// fmadds f16,f0,f25,f15
	ctx.f16.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f25.f64), float(ctx.f15.f64)));
	// fmadds f11,f0,f21,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f21.f64), float(ctx.f11.f64)));
	// fadds f0,f13,f14
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f14.f64));
	// stfs f0,0(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f13,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f0,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f18,f0
	ctx.f13.f64 = double(float(ctx.f18.f64 + ctx.f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f12,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fadds f0,f17,f12
	ctx.f0.f64 = double(float(ctx.f17.f64 + ctx.f12.f64));
	// stfs f0,0(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f13,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f16,f13
	ctx.f12.f64 = double(float(ctx.f16.f64 + ctx.f13.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f0,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f11,f0
	ctx.f13.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d3e8c
	goto loc_822D3E8C;
loc_822D3E20:
	// fmuls f14,f12,f27
	ctx.fpscr.disableFlushMode();
	ctx.f14.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// stfd f8,-480(r1)
	PPC_STORE_U64(ctx.r1.u32 + -480, ctx.f8.u64);
	// fmuls f8,f12,f23
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// fmuls f12,f12,f19
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f19.f64));
	// lfs f19,-168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f14,f11,f26,f14
	ctx.f14.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f26.f64), float(ctx.f14.f64)));
	// fmadds f8,f11,f22,f8
	ctx.f8.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f22.f64), float(ctx.f8.f64)));
	// fmadds f11,f11,f17,f12
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f17.f64), float(ctx.f12.f64)));
	// fmadds f12,f13,f9,f18
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f18.f64)));
	// fmadds f18,f13,f5,f16
	ctx.f18.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f16.f64)));
	// fmadds f17,f13,f1,f15
	ctx.f17.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f15.f64)));
	// fmadds f16,f13,f28,f14
	ctx.f16.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f28.f64), float(ctx.f14.f64)));
	// fmadds f15,f13,f24,f8
	ctx.f15.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f24.f64), float(ctx.f8.f64)));
	// lfd f8,-480(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -480);
	// fmadds f11,f13,f20,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f20.f64), float(ctx.f11.f64)));
	// fmadds f13,f0,f10,f12
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), float(ctx.f12.f64)));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmadds f12,f0,f6,f18
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f6.f64), float(ctx.f18.f64)));
	// stfsu f12,4(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f13,f0,f2,f17
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f2.f64), float(ctx.f17.f64)));
	// stfsu f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f12,f0,f29,f16
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f29.f64), float(ctx.f16.f64)));
	// stfsu f12,4(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f13,f0,f25,f15
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f25.f64), float(ctx.f15.f64)));
	// stfsu f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f12,f0,f21,f11
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f21.f64), float(ctx.f11.f64)));
	// stfsu f12,4(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
loc_822D3E8C:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x822d3ce4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D3CE4;
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D3E9C;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_822D3EA8:
	// lfs f13,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfd f10,-480(r1)
	PPC_STORE_U64(ctx.r1.u32 + -480, ctx.f10.u64);
	// stfd f9,-328(r1)
	PPC_STORE_U64(ctx.r1.u32 + -328, ctx.f9.u64);
	// stfd f8,-448(r1)
	PPC_STORE_U64(ctx.r1.u32 + -448, ctx.f8.u64);
	// stfd f7,-312(r1)
	PPC_STORE_U64(ctx.r1.u32 + -312, ctx.f7.u64);
	// stfd f6,-464(r1)
	PPC_STORE_U64(ctx.r1.u32 + -464, ctx.f6.u64);
	// stfd f5,-472(r1)
	PPC_STORE_U64(ctx.r1.u32 + -472, ctx.f5.u64);
	// stfd f4,-272(r1)
	PPC_STORE_U64(ctx.r1.u32 + -272, ctx.f4.u64);
	// stfd f3,-288(r1)
	PPC_STORE_U64(ctx.r1.u32 + -288, ctx.f3.u64);
	// stfd f2,-296(r1)
	PPC_STORE_U64(ctx.r1.u32 + -296, ctx.f2.u64);
	// stfd f1,-280(r1)
	PPC_STORE_U64(ctx.r1.u32 + -280, ctx.f1.u64);
	// stfd f31,-264(r1)
	PPC_STORE_U64(ctx.r1.u32 + -264, ctx.f31.u64);
	// stfd f30,-456(r1)
	PPC_STORE_U64(ctx.r1.u32 + -456, ctx.f30.u64);
	// stfd f29,-440(r1)
	PPC_STORE_U64(ctx.r1.u32 + -440, ctx.f29.u64);
	// stfd f28,-336(r1)
	PPC_STORE_U64(ctx.r1.u32 + -336, ctx.f28.u64);
	// stfd f27,-320(r1)
	PPC_STORE_U64(ctx.r1.u32 + -320, ctx.f27.u64);
	// lfs f12,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f18,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f17,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f18,f18,f0
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// lfs f16,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f17,f17,f0
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// lfs f15,24(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f16,f16,f0
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// lfs f14,28(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f15,f15,f0
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// lfs f10,32(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f14,f14,f0
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f0.f64));
	// lfs f9,36(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f8,40(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfs f7,44(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f6,48(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f5,52(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 52);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f4,56(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 56);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,60(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 60);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f2,64(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 64);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f1,68(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 68);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f31,72(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 72);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// lfs f30,76(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 76);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f31,f0
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f29,80(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 80);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f28,84(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 84);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f29,f0
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// lfs f27,88(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 88);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// stfd f26,-304(r1)
	PPC_STORE_U64(ctx.r1.u32 + -304, ctx.f26.u64);
	// fmuls f27,f27,f0
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// lfs f26,92(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 92);
	ctx.f26.f64 = double(temp.f32);
	// stfs f11,-432(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -432, temp.u32);
	// fmuls f0,f26,f0
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// stfs f12,-428(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -428, temp.u32);
	// stfs f13,-424(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -424, temp.u32);
	// stfs f18,-420(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + -420, temp.u32);
	// stfs f17,-416(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + -416, temp.u32);
	// stfs f16,-412(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + -412, temp.u32);
	// stfs f15,-408(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + -408, temp.u32);
	// stfs f14,-404(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + -404, temp.u32);
	// stfs f10,-400(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -400, temp.u32);
	// stfs f9,-396(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -396, temp.u32);
	// stfs f8,-392(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -392, temp.u32);
	// stfs f7,-388(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -388, temp.u32);
	// stfs f6,-384(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -384, temp.u32);
	// stfs f5,-380(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -380, temp.u32);
	// stfs f4,-376(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + -376, temp.u32);
	// stfs f3,-372(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -372, temp.u32);
	// stfs f2,-368(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -368, temp.u32);
	// stfs f1,-364(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -364, temp.u32);
	// stfs f31,-360(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + -360, temp.u32);
	// stfs f30,-356(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + -356, temp.u32);
	// stfs f29,-352(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + -352, temp.u32);
	// stfs f28,-348(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + -348, temp.u32);
	// stfs f27,-344(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + -344, temp.u32);
	// lfd f10,-480(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -480);
	// lfd f9,-328(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -328);
	// lfd f8,-448(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -448);
	// lfd f7,-312(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -312);
	// lfd f6,-464(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -464);
	// lfd f5,-472(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -472);
	// lfd f4,-272(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -272);
	// lfd f3,-288(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -288);
	// lfd f2,-296(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -296);
	// lfd f1,-280(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -280);
	// lfd f31,-264(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -264);
	// lfd f30,-456(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -456);
	// lfd f29,-440(r1)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -440);
	// lfd f28,-336(r1)
	ctx.f28.u64 = PPC_LOAD_U64(ctx.r1.u32 + -336);
	// lfd f27,-320(r1)
	ctx.f27.u64 = PPC_LOAD_U64(ctx.r1.u32 + -320);
	// lfd f26,-304(r1)
	ctx.f26.u64 = PPC_LOAD_U64(ctx.r1.u32 + -304);
	// stfs f0,-340(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -340, temp.u32);
	// beq cr6,0x822d42e0
	if (ctx.cr6.eq) goto loc_822D42E0;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
	// li r8,24
	ctx.r8.s64 = 24;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// li r9,216
	ctx.r9.s64 = 216;
loc_822D4068:
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// lhzu r5,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r5.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// std r6,-304(r1)
	PPC_STORE_U64(ctx.r1.u32 + -304, ctx.r6.u64);
	// lfd f0,-304(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -304);
	// std r5,-320(r1)
	PPC_STORE_U64(ctx.r1.u32 + -320, ctx.r5.u64);
	// lfd f12,-320(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -320);
	// lhzu r4,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r4.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// lhzu r6,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r6.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r5,r4
	ctx.r5.s64 = ctx.r4.s16;
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// extsh r3,r6
	ctx.r3.s64 = ctx.r6.s16;
	// std r5,-336(r1)
	PPC_STORE_U64(ctx.r1.u32 + -336, ctx.r5.u64);
	// lfd f11,-336(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -336);
	// fcfid f0,f11
	ctx.f0.f64 = double(ctx.f11.s64);
	// std r3,-440(r1)
	PPC_STORE_U64(ctx.r1.u32 + -440, ctx.r3.u64);
	// lfd f11,-440(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -440);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// dcbt r11,r8
	// dcbt r10,r9
	// fmuls f18,f0,f8
	ctx.f18.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmuls f19,f0,f19
	ctx.f19.f64 = double(float(ctx.f0.f64 * ctx.f19.f64));
	// fmadds f18,f11,f7,f18
	ctx.f18.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f7.f64), float(ctx.f18.f64)));
	// beq cr6,0x822d4198
	if (ctx.cr6.eq) goto loc_822D4198;
	// fmuls f16,f0,f4
	ctx.f16.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// stfd f8,-456(r1)
	PPC_STORE_U64(ctx.r1.u32 + -456, ctx.f8.u64);
	// fmuls f15,f0,f31
	ctx.f15.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// lfs f20,-172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f14,f0,f27
	ctx.f14.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// lfs f17,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f8,f0,f23
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f23.f64));
	// lfs f0,-164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f16,f11,f3,f16
	ctx.f16.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f3.f64), float(ctx.f16.f64)));
	// fmadds f15,f11,f30,f15
	ctx.f15.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f30.f64), float(ctx.f15.f64)));
	// fmadds f14,f11,f26,f14
	ctx.f14.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f26.f64), float(ctx.f14.f64)));
	// fmadds f8,f11,f22,f8
	ctx.f8.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f22.f64), float(ctx.f8.f64)));
	// fmadds f11,f11,f0,f19
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f0.f64), float(ctx.f19.f64)));
	// fmadds f19,f12,f9,f18
	ctx.f19.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f9.f64), float(ctx.f18.f64)));
	// fmadds f18,f12,f5,f16
	ctx.f18.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f5.f64), float(ctx.f16.f64)));
	// fmadds f16,f12,f1,f15
	ctx.f16.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f1.f64), float(ctx.f15.f64)));
	// fmadds f15,f12,f28,f14
	ctx.f15.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f28.f64), float(ctx.f14.f64)));
	// fmadds f14,f12,f24,f8
	ctx.f14.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f24.f64), float(ctx.f8.f64)));
	// lfd f8,-456(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -456);
	// fmadds f12,f12,f20,f11
	ctx.f12.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f20.f64), float(ctx.f11.f64)));
	// fmadds f11,f13,f10,f19
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f10.f64), float(ctx.f19.f64)));
	// fmadds f19,f13,f6,f18
	ctx.f19.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f6.f64), float(ctx.f18.f64)));
	// fmadds f18,f13,f2,f16
	ctx.f18.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f2.f64), float(ctx.f16.f64)));
	// fmadds f16,f13,f29,f15
	ctx.f16.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f29.f64), float(ctx.f15.f64)));
	// fmadds f15,f13,f25,f14
	ctx.f15.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f25.f64), float(ctx.f14.f64)));
	// fmadds f13,f13,f21,f12
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f21.f64), float(ctx.f12.f64)));
	// fadds f12,f11,f17
	ctx.f12.f64 = double(float(ctx.f11.f64 + ctx.f17.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f11,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fadds f12,f19,f11
	ctx.f12.f64 = double(float(ctx.f19.f64 + ctx.f11.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f11,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fadds f12,f18,f11
	ctx.f12.f64 = double(float(ctx.f18.f64 + ctx.f11.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f11,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fadds f12,f16,f11
	ctx.f12.f64 = double(float(ctx.f16.f64 + ctx.f11.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f11,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fadds f12,f15,f11
	ctx.f12.f64 = double(float(ctx.f15.f64 + ctx.f11.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsu f11,4(r10)
	ea = 4 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fadds f13,f13,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// b 0x822d4208
	goto loc_822D4208;
loc_822D4198:
	// fmuls f17,f0,f4
	ctx.fpscr.disableFlushMode();
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// fmuls f16,f0,f31
	ctx.f16.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fmuls f15,f0,f27
	ctx.f15.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// fmuls f14,f0,f23
	ctx.f14.f64 = double(float(ctx.f0.f64 * ctx.f23.f64));
	// lfs f0,-164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f17,f11,f3,f17
	ctx.f17.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f3.f64), float(ctx.f17.f64)));
	// fmadds f16,f11,f30,f16
	ctx.f16.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f30.f64), float(ctx.f16.f64)));
	// fmadds f15,f11,f26,f15
	ctx.f15.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f26.f64), float(ctx.f15.f64)));
	// fmadds f14,f11,f22,f14
	ctx.f14.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f22.f64), float(ctx.f14.f64)));
	// fmadds f11,f11,f0,f19
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f0.f64), float(ctx.f19.f64)));
	// fmadds f19,f12,f9,f18
	ctx.f19.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f9.f64), float(ctx.f18.f64)));
	// fmadds f18,f12,f5,f17
	ctx.f18.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f5.f64), float(ctx.f17.f64)));
	// fmadds f17,f12,f1,f16
	ctx.f17.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f1.f64), float(ctx.f16.f64)));
	// fmadds f16,f12,f28,f15
	ctx.f16.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f28.f64), float(ctx.f15.f64)));
	// fmadds f15,f12,f24,f14
	ctx.f15.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f24.f64), float(ctx.f14.f64)));
	// fmadds f12,f12,f20,f11
	ctx.f12.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f20.f64), float(ctx.f11.f64)));
	// fmadds f11,f13,f10,f19
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f10.f64), float(ctx.f19.f64)));
	// stfs f11,0(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmadds f11,f13,f6,f18
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f6.f64), float(ctx.f18.f64)));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f11,f13,f2,f17
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f2.f64), float(ctx.f17.f64)));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f11,f13,f29,f16
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f29.f64), float(ctx.f16.f64)));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f11,f13,f25,f15
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f25.f64), float(ctx.f15.f64)));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f13,f13,f21,f12
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f21.f64), float(ctx.f12.f64)));
	// stfsu f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
loc_822D4208:
	// lfs f12,-348(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f20,f12,f20
	ctx.f20.f64 = double(float(ctx.f12.f64 + ctx.f20.f64));
	// lfs f13,-168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,-344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	ctx.f12.f64 = double(temp.f32);
	// fadds f19,f11,f13
	ctx.f19.f64 = double(float(ctx.f11.f64 + ctx.f13.f64));
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// lfs f11,-432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -432);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -428);
	ctx.f13.f64 = double(temp.f32);
	// fadds f10,f11,f10
	ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// lfs f12,-424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -424);
	ctx.f12.f64 = double(temp.f32);
	// fadds f9,f13,f9
	ctx.f9.f64 = double(float(ctx.f13.f64 + ctx.f9.f64));
	// fadds f8,f12,f8
	ctx.f8.f64 = double(float(ctx.f12.f64 + ctx.f8.f64));
	// lfs f11,-420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -420);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -416);
	ctx.f13.f64 = double(temp.f32);
	// fadds f7,f11,f7
	ctx.f7.f64 = double(float(ctx.f11.f64 + ctx.f7.f64));
	// lfs f12,-412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -412);
	ctx.f12.f64 = double(temp.f32);
	// fadds f6,f13,f6
	ctx.f6.f64 = double(float(ctx.f13.f64 + ctx.f6.f64));
	// fadds f5,f12,f5
	ctx.f5.f64 = double(float(ctx.f12.f64 + ctx.f5.f64));
	// lfs f11,-408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -408);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -404);
	ctx.f13.f64 = double(temp.f32);
	// fadds f4,f11,f4
	ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f4.f64));
	// lfs f12,-400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -400);
	ctx.f12.f64 = double(temp.f32);
	// fadds f3,f13,f3
	ctx.f3.f64 = double(float(ctx.f13.f64 + ctx.f3.f64));
	// fadds f2,f12,f2
	ctx.f2.f64 = double(float(ctx.f12.f64 + ctx.f2.f64));
	// lfs f11,-396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -396);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -392);
	ctx.f13.f64 = double(temp.f32);
	// fadds f1,f11,f1
	ctx.f1.f64 = double(float(ctx.f11.f64 + ctx.f1.f64));
	// lfs f12,-388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -388);
	ctx.f12.f64 = double(temp.f32);
	// fadds f31,f13,f31
	ctx.f31.f64 = double(float(ctx.f13.f64 + ctx.f31.f64));
	// fadds f30,f12,f30
	ctx.f30.f64 = double(float(ctx.f12.f64 + ctx.f30.f64));
	// lfs f11,-384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -384);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -380);
	ctx.f13.f64 = double(temp.f32);
	// fadds f29,f11,f29
	ctx.f29.f64 = double(float(ctx.f11.f64 + ctx.f29.f64));
	// lfs f12,-376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -376);
	ctx.f12.f64 = double(temp.f32);
	// fadds f28,f13,f28
	ctx.f28.f64 = double(float(ctx.f13.f64 + ctx.f28.f64));
	// fadds f27,f12,f27
	ctx.f27.f64 = double(float(ctx.f12.f64 + ctx.f27.f64));
	// lfs f11,-372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -372);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	ctx.f13.f64 = double(temp.f32);
	// fadds f26,f11,f26
	ctx.f26.f64 = double(float(ctx.f11.f64 + ctx.f26.f64));
	// lfs f12,-364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	ctx.f12.f64 = double(temp.f32);
	// fadds f25,f13,f25
	ctx.f25.f64 = double(float(ctx.f13.f64 + ctx.f25.f64));
	// fadds f24,f12,f24
	ctx.f24.f64 = double(float(ctx.f12.f64 + ctx.f24.f64));
	// lfs f11,-360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -356);
	ctx.f13.f64 = double(temp.f32);
	// fadds f23,f11,f23
	ctx.f23.f64 = double(float(ctx.f11.f64 + ctx.f23.f64));
	// lfs f12,-352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	ctx.f12.f64 = double(temp.f32);
	// fadds f22,f13,f22
	ctx.f22.f64 = double(float(ctx.f13.f64 + ctx.f22.f64));
	// stfs f20,-172(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// fadds f21,f12,f21
	ctx.f21.f64 = double(float(ctx.f12.f64 + ctx.f21.f64));
	// stfs f19,-168(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// stfs f0,-164(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// bdnz 0x822d4068
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D4068;
loc_822D42E0:
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8233fa4c
	ctx.lr = 0x822D42E8;
	__savefpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D42F4"))) PPC_WEAK_FUNC(sub_822D42F4);
PPC_FUNC_IMPL(__imp__sub_822D42F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D42F8"))) PPC_WEAK_FUNC(sub_822D42F8);
PPC_FUNC_IMPL(__imp__sub_822D42F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822D4300;
	__restfpr_27(ctx, base);
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lwz r29,0(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r28,4(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r30,20(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r5,24(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r31,32(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// beq cr6,0x822d4324
	if (ctx.cr6.eq) goto loc_822D4324;
	// li r5,1
	ctx.r5.s64 = 1;
loc_822D4324:
	// addi r11,r5,-1
	ctx.r11.s64 = ctx.r5.s64 + -1;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bgt cr6,0x822d4588
	if (ctx.cr6.gt) goto loc_822D4588;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bdzf 4*cr6+eq,0x822d4380
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D4380;
	// bdzf 4*cr6+eq,0x822d43c0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D43C0;
	// bdzf 4*cr6+eq,0x822d4410
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D4410;
	// bdzf 4*cr6+eq,0x822d4470
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D4470;
	// bne cr6,0x822d44e0
	if (!ctx.cr6.eq) goto loc_822D44E0;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-1496(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f12,-112(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// beq cr6,0x822d4588
	if (ctx.cr6.eq) goto loc_822D4588;
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f12,-80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -80, temp.u32);
	// b 0x822d4588
	goto loc_822D4588;
loc_822D4380:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,-1496(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f10,f12,f0
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f11,-112(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f10,-108(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// beq cr6,0x822d4588
	if (ctx.cr6.eq) goto loc_822D4588;
	// lfs f12,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f10,f12,f0
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f10,-76(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -76, temp.u32);
	// b 0x822d457c
	goto loc_822D457C;
loc_822D43C0:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,-1496(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,32(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f11,-112(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f9,-108(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f8,-104(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// beq cr6,0x822d4588
	if (ctx.cr6.eq) goto loc_822D4588;
	// lfs f10,32(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f12,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f12.f64 = double(temp.f32);
	// stfs f8,-72(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -72, temp.u32);
	// b 0x822d4574
	goto loc_822D4574;
loc_822D4410:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,-1496(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,40(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f8,60(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f10,f0
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f6,f8,f0
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f11,-112(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f9,-108(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f7,-104(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// stfs f6,-100(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -100, temp.u32);
	// beq cr6,0x822d4588
	if (ctx.cr6.eq) goto loc_822D4588;
	// lfs f8,60(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 60);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f6,f8,f0
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f12,20(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,40(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	ctx.f10.f64 = double(temp.f32);
	// stfs f6,-68(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -68, temp.u32);
	// b 0x822d456c
	goto loc_822D456C;
loc_822D4470:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,-1496(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,24(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,48(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f8,72(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 72);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f10,f0
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f6,96(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 96);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f8,f0
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f4,f6,f0
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f11,-112(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f9,-108(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f7,-104(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// stfs f5,-100(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -100, temp.u32);
	// stfs f4,-96(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + -96, temp.u32);
	// beq cr6,0x822d4588
	if (ctx.cr6.eq) goto loc_822D4588;
	// lfs f6,96(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 96);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f4,f6,f0
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f12,24(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,48(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,72(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	ctx.f8.f64 = double(temp.f32);
	// stfs f4,-64(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// b 0x822d4564
	goto loc_822D4564;
loc_822D44E0:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f0,-1496(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,28(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,56(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f8,84(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 84);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f10,f0
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f6,112(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f8,f0
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f4,140(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 140);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f6,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f11,-112(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f9,-108(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f7,-104(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// stfs f5,-100(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -100, temp.u32);
	// stfs f3,-96(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -96, temp.u32);
	// stfs f2,-92(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -92, temp.u32);
	// beq cr6,0x822d4588
	if (ctx.cr6.eq) goto loc_822D4588;
	// lfs f6,112(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 112);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,140(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f6,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f12,28(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,56(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 56);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,84(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 84);
	ctx.f8.f64 = double(temp.f32);
	// stfs f3,-64(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// stfs f2,-60(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
loc_822D4564:
	// fmuls f5,f8,f0
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f5,-68(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -68, temp.u32);
loc_822D456C:
	// fmuls f7,f10,f0
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f7,-72(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -72, temp.u32);
loc_822D4574:
	// fmuls f9,f12,f0
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f9,-76(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -76, temp.u32);
loc_822D457C:
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f11,-80(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -80, temp.u32);
loc_822D4588:
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822d47a0
	if (!ctx.cr6.eq) goto loc_822D47A0;
	// cmpwi cr6,r30,4
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 4, ctx.xer);
	// blt cr6,0x822d471c
	if (ctx.cr6.lt) goto loc_822D471C;
	// addi r4,r30,-3
	ctx.r4.s64 = ctx.r30.s64 + -3;
	// addi r7,r28,8
	ctx.r7.s64 = ctx.r28.s64 + 8;
	// addi r10,r29,4
	ctx.r10.s64 = ctx.r29.s64 + 4;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
loc_822D45B8:
	// lhz r3,-4(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + -4);
	// lfsx f0,r9,r8
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	ctx.f0.f64 = double(temp.f32);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// extsh r8,r3
	ctx.r8.s64 = ctx.r3.s16;
	// std r8,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r8.u64);
	// lfd f13,-144(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// beq cr6,0x822d45f0
	if (ctx.cr6.eq) goto loc_822D45F0;
	// lfs f13,-8(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -8);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,-8(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + -8, temp.u32);
	// b 0x822d45f4
	goto loc_822D45F4;
loc_822D45F0:
	// stfs f0,-8(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + -8, temp.u32);
loc_822D45F4:
	// lhz r3,-2(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// subfc r9,r5,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r5.u32;
	ctx.r9.s64 = ctx.r11.s64 - ctx.r5.s64;
	// std r3,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r3.u64);
	// lfd f0,-136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 & ctx.r11.u64;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f11,r3,r8
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r8.u32);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f0,f11,f12
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// beq cr6,0x822d4644
	if (ctx.cr6.eq) goto loc_822D4644;
	// lfs f13,-4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,-4(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + -4, temp.u32);
	// b 0x822d4648
	goto loc_822D4648;
loc_822D4644:
	// stfs f0,-4(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + -4, temp.u32);
loc_822D4648:
	// lhz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// subfc r9,r5,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r5.u32;
	ctx.r9.s64 = ctx.r11.s64 - ctx.r5.s64;
	// std r3,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.r3.u64);
	// lfd f13,-128(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 & ctx.r11.u64;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r3,r8
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r8.u32);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// beq cr6,0x822d4698
	if (ctx.cr6.eq) goto loc_822D4698;
	// lfs f13,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,0(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// b 0x822d469c
	goto loc_822D469C;
loc_822D4698:
	// stfs f0,0(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
loc_822D469C:
	// lhz r3,2(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// subfc r9,r5,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r5.u32;
	ctx.r9.s64 = ctx.r11.s64 - ctx.r5.s64;
	// std r3,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r3.u64);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 & ctx.r11.u64;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f11,r3,r8
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r8.u32);
	ctx.f11.f64 = double(temp.f32);
	// lfd f0,-120(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f0,f11,f12
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// beq cr6,0x822d46ec
	if (ctx.cr6.eq) goto loc_822D46EC;
	// lfs f13,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,4(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// b 0x822d46f0
	goto loc_822D46F0;
loc_822D46EC:
	// stfs f0,4(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
loc_822D46F0:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// subfc r9,r5,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r5.u32;
	ctx.r9.s64 = ctx.r11.s64 - ctx.r5.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// subfe r3,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r3.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// and r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 & ctx.r11.u64;
	// cmplw cr6,r6,r4
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r4.u32, ctx.xer);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// blt cr6,0x822d45b8
	if (ctx.cr6.lt) goto loc_822D45B8;
loc_822D471C:
	// cmplw cr6,r6,r30
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r30.u32, ctx.xer);
	// bge cr6,0x822d49f0
	if (!ctx.cr6.lt) goto loc_822D49F0;
	// subf r7,r6,r30
	ctx.r7.s64 = ctx.r30.s64 - ctx.r6.s64;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r6,1,0,30
	ctx.r8.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r10,r28
	ctx.r9.u64 = ctx.r10.u64 + ctx.r28.u64;
	// add r10,r8,r29
	ctx.r10.u64 = ctx.r8.u64 + ctx.r29.u64;
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_822D473C:
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,-112
	ctx.r6.s64 = ctx.r1.s64 + -112;
	// extsh r3,r8
	ctx.r3.s64 = ctx.r8.s16;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// std r3,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r3.u64);
	// lfd f13,-120(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// lfsx f0,r7,r6
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r6.u32);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// beq cr6,0x822d477c
	if (ctx.cr6.eq) goto loc_822D477C;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,0(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// b 0x822d4780
	goto loc_822D4780;
loc_822D477C:
	// stfs f0,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
loc_822D4780:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// subfc r8,r5,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r5.u32;
	ctx.r8.s64 = ctx.r11.s64 - ctx.r5.s64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// subfe r6,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r6.u64 = ~ctx.r7.u64 + ctx.r7.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r6,r11
	ctx.r11.u64 = ctx.r6.u64 & ctx.r11.u64;
	// bdnz 0x822d473c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D473C;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822D47A0:
	// cmpwi cr6,r30,4
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 4, ctx.xer);
	// blt cr6,0x822d4960
	if (ctx.cr6.lt) goto loc_822D4960;
	// addi r4,r30,-3
	ctx.r4.s64 = ctx.r30.s64 + -3;
	// addi r7,r28,8
	ctx.r7.s64 = ctx.r28.s64 + 8;
	// addi r10,r29,4
	ctx.r10.s64 = ctx.r29.s64 + 4;
loc_822D47B4:
	// lhz r3,-4(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + -4);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// addi r27,r1,-80
	ctx.r27.s64 = ctx.r1.s64 + -80;
	// std r3,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r3.u64);
	// lfd f0,-120(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// lfsx f12,r9,r8
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfsx f10,r9,r27
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r27.u32);
	ctx.f10.f64 = double(temp.f32);
	// fadds f9,f10,f12
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f12.f64));
	// stfsx f9,r9,r8
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, temp.u32);
	// fmuls f0,f11,f12
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// beq cr6,0x822d4804
	if (ctx.cr6.eq) goto loc_822D4804;
	// lfs f13,-8(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -8);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,-8(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + -8, temp.u32);
	// b 0x822d4808
	goto loc_822D4808;
loc_822D4804:
	// stfs f0,-8(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + -8, temp.u32);
loc_822D4808:
	// lhz r3,-2(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r9,r1,-112
	ctx.r9.s64 = ctx.r1.s64 + -112;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// subfc r8,r5,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r5.u32;
	ctx.r8.s64 = ctx.r11.s64 - ctx.r5.s64;
	// std r3,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.r3.u64);
	// addi r3,r1,-80
	ctx.r3.s64 = ctx.r1.s64 + -80;
	// subfe r8,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r8.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// and r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 & ctx.r11.u64;
	// rlwinm r11,r8,2,0,29
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f11,r11,r9
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	ctx.f11.f64 = double(temp.f32);
	// lfsx f10,r11,r3
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
	ctx.f10.f64 = double(temp.f32);
	// fadds f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// stfsx f9,r11,r9
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r9.u32, temp.u32);
	// lfd f0,-128(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f0,f12,f11
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// beq cr6,0x822d4868
	if (ctx.cr6.eq) goto loc_822D4868;
	// lfs f13,-4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,-4(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + -4, temp.u32);
	// b 0x822d486c
	goto loc_822D486C;
loc_822D4868:
	// stfs f0,-4(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + -4, temp.u32);
loc_822D486C:
	// lhz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r11,r8,1
	ctx.r11.s64 = ctx.r8.s64 + 1;
	// addi r9,r1,-112
	ctx.r9.s64 = ctx.r1.s64 + -112;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// subfc r3,r5,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r5.u32;
	ctx.r3.s64 = ctx.r11.s64 - ctx.r5.s64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subfe r3,r3,r3
	temp.u8 = (~ctx.r3.u32 + ctx.r3.u32 < ~ctx.r3.u32) | (~ctx.r3.u32 + ctx.r3.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r3.u64 = ~ctx.r3.u64 + ctx.r3.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// std r8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r8.u64);
	// lfd f0,-136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// and r8,r3,r11
	ctx.r8.u64 = ctx.r3.u64 & ctx.r11.u64;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// rlwinm r11,r8,2,0,29
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r3,r1,-80
	ctx.r3.s64 = ctx.r1.s64 + -80;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// lfsx f11,r11,r9
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	ctx.f11.f64 = double(temp.f32);
	// lfsx f10,r11,r3
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
	ctx.f10.f64 = double(temp.f32);
	// fadds f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// stfsx f9,r11,r9
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r9.u32, temp.u32);
	// fmuls f0,f12,f11
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// beq cr6,0x822d48d0
	if (ctx.cr6.eq) goto loc_822D48D0;
	// lfs f13,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,0(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// b 0x822d48d4
	goto loc_822D48D4;
loc_822D48D0:
	// stfs f0,0(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
loc_822D48D4:
	// lhz r3,2(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// addi r11,r8,1
	ctx.r11.s64 = ctx.r8.s64 + 1;
	// addi r9,r1,-112
	ctx.r9.s64 = ctx.r1.s64 + -112;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// subfc r3,r5,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r5.u32;
	ctx.r3.s64 = ctx.r11.s64 - ctx.r5.s64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subfe r3,r3,r3
	temp.u8 = (~ctx.r3.u32 + ctx.r3.u32 < ~ctx.r3.u32) | (~ctx.r3.u32 + ctx.r3.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r3.u64 = ~ctx.r3.u64 + ctx.r3.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// std r8,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r8.u64);
	// lfd f11,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// and r8,r3,r11
	ctx.r8.u64 = ctx.r3.u64 & ctx.r11.u64;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// rlwinm r11,r8,2,0,29
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r27,r1,-80
	ctx.r27.s64 = ctx.r1.s64 + -80;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// lfsx f0,r11,r9
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	ctx.f0.f64 = double(temp.f32);
	// lfsx f13,r11,r27
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r27.u32);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfsx f12,r11,r9
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r9.u32, temp.u32);
	// fmuls f0,f9,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// beq cr6,0x822d4938
	if (ctx.cr6.eq) goto loc_822D4938;
	// lfs f13,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,4(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// b 0x822d493c
	goto loc_822D493C;
loc_822D4938:
	// stfs f0,4(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
loc_822D493C:
	// addi r11,r8,1
	ctx.r11.s64 = ctx.r8.s64 + 1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// subfc r9,r5,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r5.u32;
	ctx.r9.s64 = ctx.r11.s64 - ctx.r5.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// subfe r3,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r3.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// and r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 & ctx.r11.u64;
	// cmplw cr6,r6,r4
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r4.u32, ctx.xer);
	// blt cr6,0x822d47b4
	if (ctx.cr6.lt) goto loc_822D47B4;
loc_822D4960:
	// cmplw cr6,r6,r30
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r30.u32, ctx.xer);
	// bge cr6,0x822d49f0
	if (!ctx.cr6.lt) goto loc_822D49F0;
	// subf r8,r6,r30
	ctx.r8.s64 = ctx.r30.s64 - ctx.r6.s64;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r6,1,0,30
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r10,r28
	ctx.r7.u64 = ctx.r10.u64 + ctx.r28.u64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + ctx.r29.u64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D4980:
	// lhz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// addi r3,r1,-80
	ctx.r3.s64 = ctx.r1.s64 + -80;
	// std r6,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r6.u64);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// lfsx f0,r10,r8
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	ctx.f0.f64 = double(temp.f32);
	// lfsx f13,r10,r3
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfsx f12,r10,r8
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, temp.u32);
	// lfd f11,-120(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f0,f9,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// beq cr6,0x822d49d0
	if (ctx.cr6.eq) goto loc_822D49D0;
	// lfs f13,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,0(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// b 0x822d49d4
	goto loc_822D49D4;
loc_822D49D0:
	// stfs f0,0(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
loc_822D49D4:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// subfc r10,r5,r11
	ctx.xer.ca = ctx.r11.u32 >= ctx.r5.u32;
	ctx.r10.s64 = ctx.r11.s64 - ctx.r5.s64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// subfe r6,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r6.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// and r11,r6,r11
	ctx.r11.u64 = ctx.r6.u64 & ctx.r11.u64;
	// bdnz 0x822d4980
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D4980;
loc_822D49F0:
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D49F4"))) PPC_WEAK_FUNC(sub_822D49F4);
PPC_FUNC_IMPL(__imp__sub_822D49F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D49F8"))) PPC_WEAK_FUNC(sub_822D49F8);
PPC_FUNC_IMPL(__imp__sub_822D49F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e460
	ctx.lr = 0x822D4A00;
	__restfpr_26(ctx, base);
	// li r9,6
	ctx.r9.s64 = 6;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r8,r1,-196
	ctx.r8.s64 = ctx.r1.s64 + -196;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// addi r7,r10,-4
	ctx.r7.s64 = ctx.r10.s64 + -4;
	// lwz r27,20(r3)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r26,32(r3)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822D4A24:
	// lwzu r9,4(r7)
	ea = 4 + ctx.r7.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r7.u32 = ea;
	// stwu r9,4(r8)
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r8.u32 = ea;
	// bdnz 0x822d4a24
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D4A24;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x822d4aa8
	if (!ctx.cr6.eq) goto loc_822D4AA8;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// lfs f12,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// lfs f0,-28948(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
	// stfs f13,-168(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// stfs f12,-164(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// stfs f0,-144(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// stfs f11,-160(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// stfs f0,-140(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// stfs f10,-156(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// stfs f0,-136(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// stfs f9,-152(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// stfs f0,-132(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
	// stfs f8,-148(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// stfs f0,-128(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -128, temp.u32);
	// stfs f0,-124(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -124, temp.u32);
	// stfs f0,-120(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -120, temp.u32);
	// stfs f0,-116(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -116, temp.u32);
	// stfs f0,-112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f0,-108(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f0,-104(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// stfs f0,-100(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -100, temp.u32);
	// b 0x822d4b3c
	goto loc_822D4B3C;
loc_822D4AA8:
	// lfs f0,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// li r5,6
	ctx.r5.s64 = 6;
	// lfs f12,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f0,f13
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// lfs f10,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// addi r7,r1,-148
	ctx.r7.s64 = ctx.r1.s64 + -148;
	// fadds f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// lfs f9,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// addi r8,r9,-4
	ctx.r8.s64 = ctx.r9.s64 + -4;
	// lfs f6,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f6.f64 = double(temp.f32);
	// fadds f5,f9,f7
	ctx.f5.f64 = double(float(ctx.f9.f64 + ctx.f7.f64));
	// lfs f4,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// mtctr r5
	ctx.ctr.u64 = ctx.r5.u64;
	// lfs f3,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f3.f64 = double(temp.f32);
	// fadds f2,f6,f4
	ctx.f2.f64 = double(float(ctx.f6.f64 + ctx.f4.f64));
	// lfs f1,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,20(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f3,f1
	ctx.f13.f64 = double(float(ctx.f3.f64 + ctx.f1.f64));
	// lfs f12,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// fadds f10,f0,f12
	ctx.f10.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// stfs f11,-168(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// stfs f8,-164(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// stfs f5,-160(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// stfs f2,-156(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// stfs f13,-152(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// stfs f10,-148(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
loc_822D4B14:
	// lwzu r10,4(r8)
	ea = 4 + ctx.r8.u32;
	ctx.r10.u64 = PPC_LOAD_U32(ea);
	ctx.r8.u32 = ea;
	// stwu r10,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d4b14
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D4B14;
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r8,r1,-124
	ctx.r8.s64 = ctx.r1.s64 + -124;
	// addi r10,r9,-4
	ctx.r10.s64 = ctx.r9.s64 + -4;
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_822D4B30:
	// lwzu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	// stwu r9,4(r8)
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r8.u32 = ea;
	// bdnz 0x822d4b30
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D4B30;
loc_822D4B3C:
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r1,-112
	ctx.r10.s64 = ctx.r1.s64 + -112;
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// addi r7,r1,-128
	ctx.r7.s64 = ctx.r1.s64 + -128;
	// addi r5,r1,-128
	ctx.r5.s64 = ctx.r1.s64 + -128;
	// addi r4,r1,-144
	ctx.r4.s64 = ctx.r1.s64 + -144;
	// addi r3,r1,-144
	ctx.r3.s64 = ctx.r1.s64 + -144;
	// lvrx128 v63,r9,r10
	temp.u32 = ctx.r9.u32 + ctx.r10.u32;
	simd::store_i8(ctx.v63.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r10,r1,-160
	ctx.r10.s64 = ctx.r1.s64 + -160;
	// lvlx128 v62,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v60,r9,r7
	temp.u32 = ctx.r9.u32 + ctx.r7.u32;
	simd::store_i8(ctx.v60.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r8,r1,-160
	ctx.r8.s64 = ctx.r1.s64 + -160;
	// lvlx128 v59,r0,r5
	temp.u32 = ctx.r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r7,r1,-176
	ctx.r7.s64 = ctx.r1.s64 + -176;
	// lvrx128 v57,r9,r4
	temp.u32 = ctx.r9.u32 + ctx.r4.u32;
	simd::store_i8(ctx.v57.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r5,r1,-176
	ctx.r5.s64 = ctx.r1.s64 + -176;
	// lvlx128 v56,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r4,r1,-192
	ctx.r4.s64 = ctx.r1.s64 + -192;
	// addi r3,r1,-192
	ctx.r3.s64 = ctx.r1.s64 + -192;
	// lvrx128 v55,r9,r10
	temp.u32 = ctx.r9.u32 + ctx.r10.u32;
	simd::store_i8(ctx.v55.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// rlwinm r10,r11,0,28,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xC;
	// lvlx128 v54,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v53,r9,r7
	temp.u32 = ctx.r9.u32 + ctx.r7.u32;
	simd::store_i8(ctx.v53.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// li r28,4
	ctx.r28.s64 = 4;
	// lvlx128 v52,r0,r5
	temp.u32 = ctx.r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v61,v62,v63
	simd::store_i8(ctx.v61.u8, simd::or_i8(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v63.u8)));
	// lvrx128 v51,r9,r4
	temp.u32 = ctx.r9.u32 + ctx.r4.u32;
	simd::store_i8(ctx.v51.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v58,v59,v60
	simd::store_i8(ctx.v58.u8, simd::or_i8(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v60.u8)));
	// lvlx128 v50,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v57,v56,v57
	simd::store_i8(ctx.v57.u8, simd::or_i8(simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v57.u8)));
	// vor128 v12,v54,v55
	simd::store_i8(ctx.v12.u8, simd::or_i8(simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v55.u8)));
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// vor128 v13,v52,v53
	simd::store_i8(ctx.v13.u8, simd::or_i8(simd::load_i8(ctx.v52.u8), simd::load_i8(ctx.v53.u8)));
	// vor128 v0,v50,v51
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v51.u8)));
	// beq cr6,0x822d4c54
	if (ctx.cr6.eq) goto loc_822D4C54;
	// cmplwi cr6,r27,3
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 3, ctx.xer);
	// ble cr6,0x822d4dc0
	if (!ctx.cr6.gt) goto loc_822D4DC0;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
loc_822D4BD4:
	// lvehx v11,r0,r6
	simd::store_shuffled(ctx.v11, simd::load_and_shuffle(base + ((ctx.r6.u32) & ~0xF), VectorMaskL));
	// vaddfp128 v12,v12,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// lvsl v7,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shift_table_entry(ctx.v7.u8, VectorShiftTableL, temp.u32);
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// vperm v6,v11,v11,v7
	simd::store_i8(ctx.v6.u8, simd::permute_bytes(simd::load_i8(ctx.v11.u8), simd::load_i8(ctx.v11.u8), simd::load_i8(ctx.v7.u8)));
	// vsplth v5,v6,0
	simd::store_i16(reinterpret_cast<uint16_t*>(ctx.v5.u16), simd::splat_halfword(*reinterpret_cast<const simd::vec128i*>(ctx.v6.u16), 7));
	// vupkhsb128 v49,v5,v96
	simd::store_i32(ctx.v49.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v5.s16)));
	// vcsxwfp128 v63,v49,15
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v49.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vor128 v48,v63,v63
	simd::store_i8(ctx.v48.u8, simd::load_i8(ctx.v63.u8));
	// vmulfp128 v63,v63,v0
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vaddfp128 v0,v0,v57
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v57.f32)));
	// vmulfp128 v62,v48,v13
	simd::store_f32_aligned(ctx.v62.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v48.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vaddfp128 v13,v13,v58
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v58.f32)));
	// beq cr6,0x822d4c24
	if (ctx.cr6.eq) goto loc_822D4C24;
	// lvlx128 v47,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v46,r9,r11
	temp.u32 = ctx.r9.u32 + ctx.r11.u32;
	simd::store_i8(ctx.v46.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v45,v47,v46
	simd::store_i8(ctx.v45.u8, simd::or_i8(simd::load_i8(ctx.v47.u8), simd::load_i8(ctx.v46.u8)));
	// lvlx128 v44,r0,r10
	temp.u32 = ctx.r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v62,v62,v44
	simd::store_f32_aligned(ctx.v62.f32, simd::add_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v44.f32)));
	// vaddfp128 v63,v63,v45
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v45.f32)));
loc_822D4C24:
	// vpermwi128 v43,v62,17
	simd::store_i32(ctx.v43.u32, simd::permute_i32_dispatch(simd::load_i32(ctx.v62.u32), 0xEE));
	// stvlx128 v63,r0,r11
{
	uint32_t addr = 
ctx.r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v63), 15 - i));
}
	// stvrx128 v63,r11,r9
{
	uint32_t addr = 
ctx.r11.u32 + ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v63), i));
}
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// addi r27,r27,-1
	ctx.r27.s64 = ctx.r27.s64 + -1;
	// rlwinm r8,r11,0,28,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xC;
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// stvewx128 v43,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v43.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// stvewx128 v43,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + ctx.r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v43.u32), 3 - ((ctx.r10.u32 + ctx.r28.u32) & 0xF) >> 2));
	// addi r10,r10,24
	ctx.r10.s64 = ctx.r10.s64 + 24;
	// bne cr6,0x822d4bd4
	if (!ctx.cr6.eq) goto loc_822D4BD4;
loc_822D4C54:
	// cmplwi cr6,r27,3
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 3, ctx.xer);
	// ble cr6,0x822d4dc0
	if (!ctx.cr6.gt) goto loc_822D4DC0;
	// rlwinm r29,r27,1,0,28
	ctx.r29.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 1) & 0xFFFFFFF8;
	// vaddfp128 v63,v61,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v61.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// clrlwi r27,r27,30
	ctx.r27.u64 = ctx.r27.u32 & 0x3;
	// vaddfp128 v62,v58,v58
	simd::store_f32_aligned(ctx.v62.f32, simd::add_f32(simd::load_f32_aligned(ctx.v58.f32), simd::load_f32_aligned(ctx.v58.f32)));
	// li r7,0
	ctx.r7.s64 = 0;
	// vaddfp128 v61,v57,v57
	simd::store_f32_aligned(ctx.v61.f32, simd::add_f32(simd::load_f32_aligned(ctx.v57.f32), simd::load_f32_aligned(ctx.v57.f32)));
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x822d4dbc
	if (ctx.cr6.eq) goto loc_822D4DBC;
	// addi r10,r29,-1
	ctx.r10.s64 = ctx.r29.s64 + -1;
	// addi r30,r6,96
	ctx.r30.s64 = ctx.r6.s64 + 96;
	// rlwinm r10,r10,29,3,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
	// addi r8,r11,864
	ctx.r8.s64 = ctx.r11.s64 + 864;
	// addi r5,r10,1
	ctx.r5.s64 = ctx.r10.s64 + 1;
	// mr r10,r28
	ctx.r10.u64 = ctx.r28.u64;
	// li r31,32
	ctx.r31.s64 = 32;
	// li r3,48
	ctx.r3.s64 = 48;
	// li r4,64
	ctx.r4.s64 = 64;
	// mtctr r5
	ctx.ctr.u64 = ctx.r5.u64;
	// li r5,80
	ctx.r5.s64 = 80;
loc_822D4CA8:
	// dcbt r30,r7
	// dcbt r0,r8
	// add r8,r7,r6
	ctx.r8.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lvlx128 v42,r7,r6
	temp.u32 = ctx.r7.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// lvlx128 v41,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v42,v41,4,3
	ctx.fpscr.enableFlushMode();
	uint32_t sh = 4 & 31;
	for(int i=0;i<4;i++) {
		uint32_t val = ctx.v41.u32[i];
		val = (val << sh) | (val >> (32 - sh));
		if ((3 >> i) & 1) {
			ctx.v42.u32[i] = val;
		}
	}
	// vupkhsb128 v40,v42,v96
	simd::store_i32(ctx.v40.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v42.s16)));
	// vcsxwfp128 v60,v40,15
	simd::store_f32_aligned(ctx.v60.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v40.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vor128 v59,v60,v60
	simd::store_i8(ctx.v59.u8, simd::load_i8(ctx.v60.u8));
	// vmrghw128 v11,v60,v60
	simd::store_i32(ctx.v11.u32, simd::unpackhi_i32(simd::load_i32(ctx.v60.u32), simd::load_i32(ctx.v60.u32)));
	// vmrglw128 v10,v60,v60
	simd::store_i32(ctx.v10.u32, simd::unpacklo_i32(simd::load_i32(ctx.v60.u32), simd::load_i32(ctx.v60.u32)));
	// beq cr6,0x822d4d50
	if (ctx.cr6.eq) goto loc_822D4D50;
	// vspltw128 v9,v60,0
	simd::store_i32(ctx.v9.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v60.u32), 3));
	// lvx128 v8,r0,r11
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((ctx.r11.u32) & ~0xF), VectorMaskL));
	// vspltw v7,v11,2
	simd::store_i32(ctx.v7.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v11.u32), 1));
	// lvx128 v6,r11,r31
	simd::store_shuffled(ctx.v6, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r31.u32) & ~0xF), VectorMaskL));
	// vspltw128 v5,v59,2
	simd::store_i32(ctx.v5.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v59.u32), 1));
	// lvx128 v3,r11,r5
	simd::store_shuffled(ctx.v3, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r5.u32) & ~0xF), VectorMaskL));
	// vspltw v4,v10,2
	simd::store_i32(ctx.v4.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v10.u32), 1));
	// vmaddfp v9,v9,v0,v8
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v8.f32)));
	// lvx128 v8,r11,r9
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r9.u32) & ~0xF), VectorMaskL));
	// vmaddfp v11,v11,v13,v8
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v8.f32)));
	// vmaddfp v8,v7,v12,v6
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v6.f32)));
	// lvx128 v6,r11,r4
	simd::store_shuffled(ctx.v6, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r4.u32) & ~0xF), VectorMaskL));
	// vaddfp128 v13,v13,v62
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// lvx128 v7,r11,r3
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r3.u32) & ~0xF), VectorMaskL));
	// vaddfp128 v0,v0,v61
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vaddfp128 v12,v12,v63
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// stvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v9), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v11,r11,r9
	ea = (ctx.r11.u32 + ctx.r9.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v11), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v8,r11,r31
	ea = (ctx.r11.u32 + ctx.r31.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v8), &VectorMaskL[(ea & 0xF) * 16]);
	// vmaddfp v11,v10,v13,v6
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v6.f32)));
	// vmaddfp v9,v5,v0,v7
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v7.f32)));
	// vmaddfp v10,v4,v12,v3
	simd::store_f32_aligned(ctx.v10.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vaddfp128 v0,v0,v61
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vaddfp128 v13,v13,v62
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// vaddfp128 v12,v12,v63
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// stvx128 v11,r11,r4
	ea = (ctx.r11.u32 + ctx.r4.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v11), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v9,r11,r3
	ea = (ctx.r11.u32 + ctx.r3.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v9), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v10,r11,r5
	ea = (ctx.r11.u32 + ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v10), &VectorMaskL[(ea & 0xF) * 16]);
	// b 0x822d4da8
	goto loc_822D4DA8;
loc_822D4D50:
	// vaddfp128 v39,v13,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v39.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// vspltw128 v34,v59,2
	simd::store_i32(ctx.v34.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v59.u32), 1));
	// vaddfp128 v37,v0,v61
	simd::store_f32_aligned(ctx.v37.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// vspltw128 v38,v60,0
	simd::store_i32(ctx.v38.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v60.u32), 3));
	// vaddfp128 v35,v12,v63
	simd::store_f32_aligned(ctx.v35.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// vspltw128 v36,v11,2
	simd::store_i32(ctx.v36.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v11.u32), 1));
	// vspltw128 v33,v10,2
	simd::store_i32(ctx.v33.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v10.u32), 1));
	// vmulfp128 v32,v11,v13
	simd::store_f32_aligned(ctx.v32.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v60,v38,v0
	simd::store_f32_aligned(ctx.v60.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v38.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v59,v36,v12
	simd::store_f32_aligned(ctx.v59.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v36.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vmulfp128 v56,v10,v39
	simd::store_f32_aligned(ctx.v56.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v39.f32)));
	// vmulfp128 v55,v34,v37
	simd::store_f32_aligned(ctx.v55.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v34.f32), simd::load_f32_aligned(ctx.v37.f32)));
	// vmulfp128 v54,v33,v35
	simd::store_f32_aligned(ctx.v54.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v33.f32), simd::load_f32_aligned(ctx.v35.f32)));
	// stvx128 v32,r11,r9
	ea = (ctx.r11.u32 + ctx.r9.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v32), &VectorMaskL[(ea & 0xF) * 16]);
	// vaddfp128 v13,v39,v62
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v62.f32)));
	// vaddfp128 v0,v37,v61
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v37.f32), simd::load_f32_aligned(ctx.v61.f32)));
	// stvx128 v60,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v60), &VectorMaskL[(ea & 0xF) * 16]);
	// vaddfp128 v12,v35,v63
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v35.f32), simd::load_f32_aligned(ctx.v63.f32)));
	// stvx128 v59,r11,r31
	ea = (ctx.r11.u32 + ctx.r31.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v59), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v56,r11,r4
	ea = (ctx.r11.u32 + ctx.r4.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v56), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v55,r11,r3
	ea = (ctx.r11.u32 + ctx.r3.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v55), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v54,r11,r5
	ea = (ctx.r11.u32 + ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v54), &VectorMaskL[(ea & 0xF) * 16]);
loc_822D4DA8:
	// addi r11,r11,96
	ctx.r11.s64 = ctx.r11.s64 + 96;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// mr r10,r28
	ctx.r10.u64 = ctx.r28.u64;
	// addi r8,r11,864
	ctx.r8.s64 = ctx.r11.s64 + 864;
	// bdnz 0x822d4ca8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D4CA8;
loc_822D4DBC:
	// add r6,r29,r6
	ctx.r6.u64 = ctx.r29.u64 + ctx.r6.u64;
loc_822D4DC0:
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// beq cr6,0x822d4e3c
	if (ctx.cr6.eq) goto loc_822D4E3C;
	// mtctr r27
	ctx.ctr.u64 = ctx.r27.u64;
loc_822D4DCC:
	// lvehx v12,r0,r6
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r6.u32) & ~0xF), VectorMaskL));
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// lvsl v7,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shift_table_entry(ctx.v7.u8, VectorShiftTableL, temp.u32);
	// vperm v6,v12,v12,v7
	simd::store_i8(ctx.v6.u8, simd::permute_bytes(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v7.u8)));
	// vsplth v5,v6,0
	simd::store_i16(reinterpret_cast<uint16_t*>(ctx.v5.u16), simd::splat_halfword(*reinterpret_cast<const simd::vec128i*>(ctx.v6.u16), 7));
	// vupkhsb128 v53,v5,v96
	simd::store_i32(ctx.v53.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v5.s16)));
	// vcsxwfp128 v63,v53,15
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v53.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vor128 v52,v63,v63
	simd::store_i8(ctx.v52.u8, simd::load_i8(ctx.v63.u8));
	// vmulfp128 v63,v63,v0
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vaddfp128 v0,v0,v57
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v57.f32)));
	// vmulfp128 v62,v52,v13
	simd::store_f32_aligned(ctx.v62.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v52.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vaddfp128 v13,v13,v58
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v58.f32)));
	// beq cr6,0x822d4e18
	if (ctx.cr6.eq) goto loc_822D4E18;
	// lvlx128 v51,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v50,r9,r11
	temp.u32 = ctx.r9.u32 + ctx.r11.u32;
	simd::store_i8(ctx.v50.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v49,v51,v50
	simd::store_i8(ctx.v49.u8, simd::or_i8(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v50.u8)));
	// lvlx128 v48,r11,r9
	temp.u32 = ctx.r11.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v62,v62,v48
	simd::store_f32_aligned(ctx.v62.f32, simd::add_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v48.f32)));
	// vaddfp128 v63,v63,v49
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v49.f32)));
loc_822D4E18:
	// vpermwi128 v47,v62,17
	simd::store_i32(ctx.v47.u32, simd::permute_i32_dispatch(simd::load_i32(ctx.v62.u32), 0xEE));
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// stvlx128 v63,r0,r11
{
	uint32_t addr = 
ctx.r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v63), 15 - i));
}
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// stvrx128 v63,r11,r9
{
	uint32_t addr = 
ctx.r11.u32 + ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v63), i));
}
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// stvewx128 v47,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v47.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// stvewx128 v47,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + ctx.r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v47.u32), 3 - ((ctx.r10.u32 + ctx.r28.u32) & 0xF) >> 2));
	// bdnz 0x822d4dcc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D4DCC;
loc_822D4E3C:
	// b 0x8233e4b0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D4E40"))) PPC_WEAK_FUNC(sub_822D4E40);
PPC_FUNC_IMPL(__imp__sub_822D4E40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lis r8,-32249
	ctx.r8.s64 = -2113470464;
	// lwz r7,36(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// lwz r30,20(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r31,32(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lfs f0,-28948(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
	// bne cr6,0x822d4f70
	if (!ctx.cr6.eq) goto loc_822D4F70;
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// beq cr6,0x822d4f70
	if (ctx.cr6.eq) goto loc_822D4F70;
	// cmplwi cr6,r10,2
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 2, ctx.xer);
	// beq cr6,0x822d4ef0
	if (ctx.cr6.eq) goto loc_822D4EF0;
	// cmplwi cr6,r10,4
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 4, ctx.xer);
	// bne cr6,0x822d4ff4
	if (!ctx.cr6.eq) goto loc_822D4FF4;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,20(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,40(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,60(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 60);
	ctx.f10.f64 = double(temp.f32);
	// stfs f13,-64(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// stfs f12,-60(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// stfs f11,-56(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// stfs f10,-52(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// bne cr6,0x822d4ecc
	if (!ctx.cr6.eq) goto loc_822D4ECC;
	// stfs f0,-48(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f0,-44(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f0,-40(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f0,-36(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// b 0x822d4ff4
	goto loc_822D4FF4;
loc_822D4ECC:
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,40(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,60(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	ctx.f10.f64 = double(temp.f32);
	// stfs f13,-48(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f12,-44(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f11,-40(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f10,-36(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// b 0x822d4ff4
	goto loc_822D4FF4;
loc_822D4EF0:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// stfs f13,-64(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// stfs f12,-60(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// bne cr6,0x822d4f2c
	if (!ctx.cr6.eq) goto loc_822D4F2C;
	// ld r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// stfs f0,-48(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f0,-44(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f0,-40(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f0,-36(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// std r10,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r10.u64);
	// b 0x822d4ff4
	goto loc_822D4FF4;
loc_822D4F2C:
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f12,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,12(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// fadds f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// fadds f7,f11,f9
	ctx.f7.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// stfs f8,-56(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// lfs f13,5260(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 5260);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f6,f12,f13
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfs f7,-52(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// fmuls f5,f11,f13
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfs f6,-48(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f5,-44(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f6,-40(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f5,-36(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// b 0x822d4ff4
	goto loc_822D4FF4;
loc_822D4F70:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-64(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// bne cr6,0x822d4fa8
	if (!ctx.cr6.eq) goto loc_822D4FA8;
	// stfs f13,-60(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// ld r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// stfs f0,-48(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// std r10,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r10.u64);
	// stfs f0,-44(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f0,-40(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f0,-36(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// b 0x822d4ff4
	goto loc_822D4FF4;
loc_822D4FA8:
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f11,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// lfs f10,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// fadds f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// stfs f9,-60(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// lfs f13,5272(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 5272);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f11,f13
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// lfs f12,5260(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 5260);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,5264(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 5264);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f7,f11,f12,f10
	ctx.f7.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f12.f64), float(ctx.f10.f64)));
	// fmadds f6,f11,f13,f10
	ctx.f6.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f13.f64), float(ctx.f10.f64)));
	// stfs f7,-56(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// stfs f6,-52(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// stfs f8,-48(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f8,-44(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f8,-40(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f8,-36(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
loc_822D4FF4:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stfs f0,-28(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// stfs f0,-24(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -24, temp.u32);
	// li r8,16
	ctx.r8.s64 = 16;
	// stfs f0,-20(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -20, temp.u32);
	// addi r5,r1,-64
	ctx.r5.s64 = ctx.r1.s64 + -64;
	// addi r4,r1,-64
	ctx.r4.s64 = ctx.r1.s64 + -64;
	// addi r3,r1,-48
	ctx.r3.s64 = ctx.r1.s64 + -48;
	// lfs f0,5256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r1,-48
	ctx.r10.s64 = ctx.r1.s64 + -48;
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r7,r1,-32
	ctx.r7.s64 = ctx.r1.s64 + -32;
	// lvrx128 v61,r8,r5
	temp.u32 = ctx.r8.u32 + ctx.r5.u32;
	simd::store_i8(ctx.v61.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r6,r1,-32
	ctx.r6.s64 = ctx.r1.s64 + -32;
	// lvlx128 v60,r0,r4
	temp.u32 = ctx.r0.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v59,r8,r3
	temp.u32 = ctx.r8.u32 + ctx.r3.u32;
	simd::store_i8(ctx.v59.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// lvlx128 v58,r0,r10
	temp.u32 = ctx.r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v63,r8,r7
	temp.u32 = ctx.r8.u32 + ctx.r7.u32;
	simd::store_i8(ctx.v63.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// rlwinm r7,r11,0,26,29
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x3C;
	// lvlx128 v62,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v5,v62,v63
	simd::store_i8(ctx.v5.u8, simd::or_i8(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v63.u8)));
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// vor128 v0,v58,v59
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v59.u8)));
	// vor128 v13,v60,v61
	simd::store_i8(ctx.v13.u8, simd::or_i8(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v61.u8)));
	// beq cr6,0x822d50c4
	if (ctx.cr6.eq) goto loc_822D50C4;
	// cmplwi cr6,r30,15
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 15, ctx.xer);
	// ble cr6,0x822d51c4
	if (!ctx.cr6.gt) goto loc_822D51C4;
loc_822D5060:
	// lvehx v12,r0,r9
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// vmaddfp v11,v0,v5,v13
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v5.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r0.u32 + ctx.r9.u32;
	simd::store_shift_table_entry(ctx.v7.u8, VectorShiftTableL, temp.u32);
	// vor128 v57,v0,v0
	simd::store_i8(ctx.v57.u8, simd::load_i8(ctx.v0.u8));
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// vperm128 v56,v12,v12,v7
	simd::store_i8(ctx.v56.u8, simd::permute_bytes(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v7.u8)));
	// vrlimi128 v57,v0,15,1
	uint32_t sh = 15 & 31;
	for(int i=0;i<4;i++) {
		uint32_t val = ctx.v0.u32[i];
		val = (val << sh) | (val >> (32 - sh));
		if ((1 >> i) & 1) {
			ctx.v57.u32[i] = val;
		}
	}
	// vupkhsb128 v55,v56,v96
	simd::store_i32(ctx.v55.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v56.s16)));
	// vor128 v0,v57,v57
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v57.u8));
	// vcsxwfp128 v54,v55,15
	simd::store_f32_aligned(ctx.v54.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v55.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vor128 v53,v11,v11
	simd::store_i8(ctx.v53.u8, simd::load_i8(ctx.v11.u8));
	// vrlimi128 v53,v11,15,1
	uint32_t sh = 15 & 31;
	for(int i=0;i<4;i++) {
		uint32_t val = ctx.v11.u32[i];
		val = (val << sh) | (val >> (32 - sh));
		if ((1 >> i) & 1) {
			ctx.v53.u32[i] = val;
		}
	}
	// vmulfp128 v63,v54,v13
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v54.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vor128 v13,v53,v53
	simd::store_i8(ctx.v13.u8, simd::load_i8(ctx.v53.u8));
	// beq cr6,0x822d50a4
	if (ctx.cr6.eq) goto loc_822D50A4;
	// lvlx128 v52,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v63,v63,v52
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v52.f32)));
loc_822D50A4:
	// vspltw128 v51,v63,0
	simd::store_i32(ctx.v51.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v63.u32), 3));
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// stvewx128 v51,r0,r11
	PPC_STORE_U32((ctx.r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v51.u32), 3 - ((ctx.r11.u32) & 0xF) >> 2));
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// rlwinm r10,r11,0,26,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x3C;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822d5060
	if (!ctx.cr6.eq) goto loc_822D5060;
loc_822D50C4:
	// cmplwi cr6,r30,15
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 15, ctx.xer);
	// ble cr6,0x822d51c4
	if (!ctx.cr6.gt) goto loc_822D51C4;
	// rlwinm r10,r30,1,0,26
	ctx.r10.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFE0;
	// clrlwi r30,r30,28
	ctx.r30.u64 = ctx.r30.u32 & 0xF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822d51c4
	if (ctx.cr6.eq) goto loc_822D51C4;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// rlwinm r6,r10,27,5,31
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// addi r10,r9,16
	ctx.r10.s64 = ctx.r9.s64 + 16;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// li r3,240
	ctx.r3.s64 = 240;
	// li r4,512
	ctx.r4.s64 = 512;
	// li r5,32
	ctx.r5.s64 = 32;
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
	// li r6,48
	ctx.r6.s64 = 48;
loc_822D5104:
	// lvrx128 v50,r8,r9
	temp.u32 = ctx.r8.u32 + ctx.r9.u32;
	simd::store_i8(ctx.v50.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// lvlx128 v49,r0,r9
	temp.u32 = ctx.r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx128 v48,r7,r10
	temp.u32 = ctx.r7.u32 + ctx.r10.u32;
	simd::store_i8(ctx.v48.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor128 v47,v49,v50
	simd::store_i8(ctx.v47.u8, simd::or_i8(simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v50.u8)));
	// lvlx128 v46,r0,r10
	temp.u32 = ctx.r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v45,v46,v48
	simd::store_i8(ctx.v45.u8, simd::or_i8(simd::load_i8(ctx.v46.u8), simd::load_i8(ctx.v48.u8)));
	// vupklsb128 v44,v47,v96
	simd::store_i32(ctx.v44.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v47.s16)));
	// vupklsb128 v43,v45,v96
	simd::store_i32(ctx.v43.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v45.s16)));
	// dcbt r10,r3
	// dcbt r11,r4
	// vupkhsb128 v42,v47,v96
	simd::store_i32(ctx.v42.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v47.s16)));
	// vupkhsb128 v41,v45,v96
	simd::store_i32(ctx.v41.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v45.s16)));
	// vcsxwfp128 v8,v44,15
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v8.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v44.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v7,v43,15
	simd::store_f32_aligned(ctx.v7.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v43.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// vcsxwfp128 v11,v42,15
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v42.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v6,v41,15
	simd::store_f32_aligned(ctx.v6.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v41.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// beq cr6,0x822d5180
	if (ctx.cr6.eq) goto loc_822D5180;
	// lvx128 v12,r0,r11
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r11.u32) & ~0xF), VectorMaskL));
	// vmaddfp v12,v11,v13,v12
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// lvx128 v11,r11,r8
	simd::store_shuffled(ctx.v11, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r8.u32) & ~0xF), VectorMaskL));
	// vaddfp v13,v13,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// lvx128 v10,r11,r5
	simd::store_shuffled(ctx.v10, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r5.u32) & ~0xF), VectorMaskL));
	// lvx128 v9,r11,r6
	simd::store_shuffled(ctx.v9, simd::load_and_shuffle(base + ((ctx.r11.u32 + ctx.r6.u32) & ~0xF), VectorMaskL));
	// vmaddfp v11,v8,v13,v11
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v8.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vaddfp v13,v13,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v10,v6,v13,v10
	simd::store_f32_aligned(ctx.v10.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v10.f32)));
	// vaddfp v13,v13,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v9,v7,v13,v9
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vaddfp v13,v13,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// b 0x822d51a0
	goto loc_822D51A0;
loc_822D5180:
	// vaddfp128 v40,v13,v0
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v40.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v12,v11,v13
	simd::store_f32_aligned(ctx.v12.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vaddfp128 v39,v40,v0
	simd::store_f32_aligned(ctx.v39.f32, simd::add_f32(simd::load_f32_aligned(ctx.v40.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v11,v8,v40
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v8.f32), simd::load_f32_aligned(ctx.v40.f32)));
	// vaddfp128 v38,v39,v0
	simd::store_f32_aligned(ctx.v38.f32, simd::add_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v10,v6,v39
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(ctx.v39.f32)));
	// vmulfp128 v9,v7,v38
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(ctx.v38.f32)));
	// vaddfp128 v13,v38,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v38.f32), simd::load_f32_aligned(ctx.v0.f32)));
loc_822D51A0:
	// stvx128 v11,r11,r8
	ea = (ctx.r11.u32 + ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v11), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// stvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v12), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// stvx128 v10,r11,r5
	ea = (ctx.r11.u32 + ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v10), &VectorMaskL[(ea & 0xF) * 16]);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// stvx128 v9,r11,r6
	ea = (ctx.r11.u32 + ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v9), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r11,r11,64
	ctx.r11.s64 = ctx.r11.s64 + 64;
	// bdnz 0x822d5104
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D5104;
loc_822D51C4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x822d5228
	if (ctx.cr6.eq) goto loc_822D5228;
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
loc_822D51D0:
	// lvehx v12,r0,r9
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// vmaddfp v11,v0,v5,v13
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v5.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// lvsl v7,r0,r9
	temp.u32 = ctx.r0.u32 + ctx.r9.u32;
	simd::store_shift_table_entry(ctx.v7.u8, VectorShiftTableL, temp.u32);
	// vor128 v37,v0,v0
	simd::store_i8(ctx.v37.u8, simd::load_i8(ctx.v0.u8));
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// vperm128 v36,v12,v12,v7
	simd::store_i8(ctx.v36.u8, simd::permute_bytes(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v7.u8)));
	// vrlimi128 v37,v0,15,1
	uint32_t sh = 15 & 31;
	for(int i=0;i<4;i++) {
		uint32_t val = ctx.v0.u32[i];
		val = (val << sh) | (val >> (32 - sh));
		if ((1 >> i) & 1) {
			ctx.v37.u32[i] = val;
		}
	}
	// vupkhsb128 v35,v36,v96
	simd::store_i32(ctx.v35.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v36.s16)));
	// vor128 v0,v37,v37
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v37.u8));
	// vcsxwfp128 v34,v35,15
	simd::store_f32_aligned(ctx.v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v35.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vor128 v33,v11,v11
	simd::store_i8(ctx.v33.u8, simd::load_i8(ctx.v11.u8));
	// vrlimi128 v33,v11,15,1
	uint32_t sh = 15 & 31;
	for(int i=0;i<4;i++) {
		uint32_t val = ctx.v11.u32[i];
		val = (val << sh) | (val >> (32 - sh));
		if ((1 >> i) & 1) {
			ctx.v33.u32[i] = val;
		}
	}
	// vmulfp128 v63,v34,v13
	simd::store_f32_aligned(ctx.v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v34.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vor128 v13,v33,v33
	simd::store_i8(ctx.v13.u8, simd::load_i8(ctx.v33.u8));
	// beq cr6,0x822d5214
	if (ctx.cr6.eq) goto loc_822D5214;
	// lvlx128 v32,r0,r11
	temp.u32 = ctx.r0.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddfp128 v63,v63,v32
	simd::store_f32_aligned(ctx.v63.f32, simd::add_f32(simd::load_f32_aligned(ctx.v63.f32), simd::load_f32_aligned(ctx.v32.f32)));
loc_822D5214:
	// vspltw128 v63,v63,0
	simd::store_i32(ctx.v63.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v63.u32), 3));
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// stvewx128 v63,r0,r11
	PPC_STORE_U32((ctx.r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v63.u32), 3 - ((ctx.r11.u32) & 0xF) >> 2));
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822d51d0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D51D0;
loc_822D5228:
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D5234"))) PPC_WEAK_FUNC(sub_822D5234);
PPC_FUNC_IMPL(__imp__sub_822D5234) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D5238"))) PPC_WEAK_FUNC(sub_822D5238);
PPC_FUNC_IMPL(__imp__sub_822D5238) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lhz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// extsh r8,r11
	ctx.r8.s64 = ctx.r11.s16;
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfd f13,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// lfs f0,-1496(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1496);
	ctx.f0.f64 = double(temp.f32);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f1,f11,f0
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D5260"))) PPC_WEAK_FUNC(sub_822D5260);
PPC_FUNC_IMPL(__imp__sub_822D5260) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lbz r11,2(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lbz r9,1(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// rotlwi r8,r11,8
	ctx.r8.u64 = rotl32(ctx.r11.u32, 8);
	// lbz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// or r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 | ctx.r9.u64;
	// lfs f0,-1488(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1488);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r5,r6,8,0,23
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// or r4,r5,r7
	ctx.r4.u64 = ctx.r5.u64 | ctx.r7.u64;
	// rlwinm r3,r4,8,0,23
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r11,r3,12
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFF) != 0);
	ctx.r11.s64 = ctx.r3.s32 >> 12;
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
	// lfd f13,-16(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f1,f11,f0
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D52A8"))) PPC_WEAK_FUNC(sub_822D52A8);
PPC_FUNC_IMPL(__imp__sub_822D52A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// srawi r9,r11,12
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xFFF) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 12;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// lfs f0,-1488(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1488);
	ctx.f0.f64 = double(temp.f32);
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfd f13,-16(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f1,f11,f0
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D52D4"))) PPC_WEAK_FUNC(sub_822D52D4);
PPC_FUNC_IMPL(__imp__sub_822D52D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D52D8"))) PPC_WEAK_FUNC(sub_822D52D8);
PPC_FUNC_IMPL(__imp__sub_822D52D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// srawi r9,r11,8
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 8;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// lfs f0,-1484(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1484);
	ctx.f0.f64 = double(temp.f32);
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfd f13,-16(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f1,f11,f0
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D5304"))) PPC_WEAK_FUNC(sub_822D5304);
PPC_FUNC_IMPL(__imp__sub_822D5304) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D5308"))) PPC_WEAK_FUNC(sub_822D5308);
PPC_FUNC_IMPL(__imp__sub_822D5308) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x822D5310;
	__restfpr_14(ctx, base);
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa38
	ctx.lr = 0x822D5318;
	sub_8233FA38(ctx, base);
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// mr r22,r3
	ctx.r22.u64 = ctx.r3.u64;
	// lwz r27,0(r3)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r16,4(r3)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r18,24(r3)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r15,28(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r14,32(r3)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// beq cr6,0x822d534c
	if (ctx.cr6.eq) goto loc_822D534C;
	// li r15,1
	ctx.r15.s64 = 1;
	// li r18,1
	ctx.r18.s64 = 1;
loc_822D534C:
	// lwz r30,16(r22)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r22.u32 + 16);
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bdzf 4*cr6+eq,0x822d5380
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D5380;
	// bdzf 4*cr6+eq,0x822d5390
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D5390;
	// bdzf 4*cr6+eq,0x822d53b0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D53B0;
	// bdzf 4*cr6+eq,0x822d53a0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D53A0;
	// bdzf 4*cr6+eq,0x822d53bc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D53BC;
	// bne cr6,0x822d53c8
	if (!ctx.cr6.eq) goto loc_822D53C8;
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// li r17,1
	ctx.r17.s64 = 1;
	// addi r26,r11,-26792
	ctx.r26.s64 = ctx.r11.s64 + -26792;
	// b 0x822d53d4
	goto loc_822D53D4;
loc_822D5380:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// li r17,2
	ctx.r17.s64 = 2;
	// addi r26,r11,21048
	ctx.r26.s64 = ctx.r11.s64 + 21048;
	// b 0x822d53d4
	goto loc_822D53D4;
loc_822D5390:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// li r17,3
	ctx.r17.s64 = 3;
	// addi r26,r11,21088
	ctx.r26.s64 = ctx.r11.s64 + 21088;
	// b 0x822d53d4
	goto loc_822D53D4;
loc_822D53A0:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// li r17,3
	ctx.r17.s64 = 3;
	// addi r26,r11,-26744
	ctx.r26.s64 = ctx.r11.s64 + -26744;
	// b 0x822d53d4
	goto loc_822D53D4;
loc_822D53B0:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r26,r11,21160
	ctx.r26.s64 = ctx.r11.s64 + 21160;
	// b 0x822d53d0
	goto loc_822D53D0;
loc_822D53BC:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r26,r11,21208
	ctx.r26.s64 = ctx.r11.s64 + 21208;
	// b 0x822d53d0
	goto loc_822D53D0;
loc_822D53C8:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r26,r11,-26672
	ctx.r26.s64 = ctx.r11.s64 + -26672;
loc_822D53D0:
	// li r17,4
	ctx.r17.s64 = 4;
loc_822D53D4:
	// lwz r11,12(r22)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r22.u32 + 12);
	// twllei r18,0
	if (ctx.r18.u32 <= 0) __builtin_debugtrap();
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822d5670
	if (!ctx.cr6.eq) goto loc_822D5670;
	// divwu. r3,r10,r18
	ctx.r3.u32 = ctx.r10.u32 / ctx.r18.u32;
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// li r4,0
	ctx.r4.s64 = 0;
	// beq 0x822d5660
	if (ctx.cr0.eq) goto loc_822D5660;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lwz r31,8(r22)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// lfs f8,-1480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -1480);
	ctx.f8.f64 = double(temp.f32);
	// lis r5,-32249
	ctx.r5.s64 = -2113470464;
	// lfs f11,-1484(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1484);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-1488(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1488);
	ctx.f12.f64 = double(temp.f32);
	// lfs f7,-1496(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -1496);
	ctx.f7.f64 = double(temp.f32);
	// lfs f9,-1492(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -1492);
	ctx.f9.f64 = double(temp.f32);
	// lfs f10,11260(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 11260);
	ctx.f10.f64 = double(temp.f32);
	// lfs f6,-28948(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -28948);
	ctx.f6.f64 = double(temp.f32);
loc_822D542C:
	// li r5,0
	ctx.r5.s64 = 0;
	// cmplwi cr6,r15,0
	ctx.cr6.compare<uint32_t>(ctx.r15.u32, 0, ctx.xer);
	// beq cr6,0x822d5644
	if (ctx.cr6.eq) goto loc_822D5644;
	// mr r6,r16
	ctx.r6.u64 = ctx.r16.u64;
	// addi r11,r31,-4
	ctx.r11.s64 = ctx.r31.s64 + -4;
loc_822D5440:
	// fmr f0,f6
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f6.f64;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmplwi cr6,r18,0
	ctx.cr6.compare<uint32_t>(ctx.r18.u32, 0, ctx.xer);
	// beq cr6,0x822d5618
	if (ctx.cr6.eq) goto loc_822D5618;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r9,r27
	ctx.r9.u64 = ctx.r27.u64;
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
loc_822D545C:
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bdzf 4*cr6+eq,0x822d54a8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D54A8;
	// bdzf 4*cr6+eq,0x822d54d4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D54D4;
	// bdzf 4*cr6+eq,0x822d557c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D557C;
	// bdzf 4*cr6+eq,0x822d5528
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D5528;
	// bdzf 4*cr6+eq,0x822d55ac
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D55AC;
	// bne cr6,0x822d55dc
	if (!ctx.cr6.eq) goto loc_822D55DC;
	// lbzx r29,r8,r27
	ctx.r29.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r27.u32);
	// lfs f13,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// std r29,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r29.u64);
	// lfd f5,80(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fsubs f2,f3,f10
	ctx.f2.f64 = static_cast<float>(ctx.f3.f64 - ctx.f10.f64);
	// fmuls f1,f2,f13
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmadds f0,f1,f9,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f9.f64), float(ctx.f0.f64)));
	// b 0x822d5600
	goto loc_822D5600;
loc_822D54A8:
	// lhz r29,0(r7)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// lfs f13,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsh r29,r29
	ctx.r29.s64 = ctx.r29.s16;
	// std r29,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r29.u64);
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmadds f0,f2,f7,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f7.f64), float(ctx.f0.f64)));
	// b 0x822d5600
	goto loc_822D5600;
loc_822D54D4:
	// add r29,r10,r27
	ctx.r29.u64 = ctx.r10.u64 + ctx.r27.u64;
	// lbzx r28,r10,r27
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r27.u32);
	// add r26,r10,r27
	ctx.r26.u64 = ctx.r10.u64 + ctx.r27.u64;
	// lfs f13,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lbz r29,2(r29)
	ctx.r29.u64 = PPC_LOAD_U8(ctx.r29.u32 + 2);
	// lbz r26,1(r26)
	ctx.r26.u64 = PPC_LOAD_U8(ctx.r26.u32 + 1);
	// rotlwi r29,r29,8
	ctx.r29.u64 = rotl32(ctx.r29.u32, 8);
	// or r29,r29,r26
	ctx.r29.u64 = ctx.r29.u64 | ctx.r26.u64;
	// rlwinm r29,r29,8,0,23
	ctx.r29.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r28
	ctx.r29.u64 = ctx.r29.u64 | ctx.r28.u64;
	// rlwinm r29,r29,8,0,23
	ctx.r29.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r29,r29,12
	ctx.xer.ca = (ctx.r29.s32 < 0) & ((ctx.r29.u32 & 0xFFF) != 0);
	ctx.r29.s64 = ctx.r29.s32 >> 12;
	// extsw r29,r29
	ctx.r29.s64 = ctx.r29.s32;
	// std r29,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r29.u64);
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmadds f0,f2,f12,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f12.f64), float(ctx.f0.f64)));
	// b 0x822d5600
	goto loc_822D5600;
loc_822D5528:
	// add r29,r10,r27
	ctx.r29.u64 = ctx.r10.u64 + ctx.r27.u64;
	// lbzx r28,r10,r27
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r27.u32);
	// add r26,r10,r27
	ctx.r26.u64 = ctx.r10.u64 + ctx.r27.u64;
	// lfs f13,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lbz r29,2(r29)
	ctx.r29.u64 = PPC_LOAD_U8(ctx.r29.u32 + 2);
	// lbz r26,1(r26)
	ctx.r26.u64 = PPC_LOAD_U8(ctx.r26.u32 + 1);
	// rotlwi r29,r29,8
	ctx.r29.u64 = rotl32(ctx.r29.u32, 8);
	// or r29,r29,r26
	ctx.r29.u64 = ctx.r29.u64 | ctx.r26.u64;
	// rlwinm r29,r29,8,0,23
	ctx.r29.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r28
	ctx.r29.u64 = ctx.r29.u64 | ctx.r28.u64;
	// rlwinm r29,r29,8,0,23
	ctx.r29.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r29,r29,8
	ctx.xer.ca = (ctx.r29.s32 < 0) & ((ctx.r29.u32 & 0xFF) != 0);
	ctx.r29.s64 = ctx.r29.s32 >> 8;
	// extsw r29,r29
	ctx.r29.s64 = ctx.r29.s32;
	// std r29,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r29.u64);
	// lfd f5,104(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmadds f0,f2,f11,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f11.f64), float(ctx.f0.f64)));
	// b 0x822d5600
	goto loc_822D5600;
loc_822D557C:
	// lwz r29,0(r9)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lfs f13,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// srawi r29,r29,12
	ctx.xer.ca = (ctx.r29.s32 < 0) & ((ctx.r29.u32 & 0xFFF) != 0);
	ctx.r29.s64 = ctx.r29.s32 >> 12;
	// extsw r29,r29
	ctx.r29.s64 = ctx.r29.s32;
	// std r29,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r29.u64);
	// lfd f5,112(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmadds f0,f2,f12,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f12.f64), float(ctx.f0.f64)));
	// b 0x822d5600
	goto loc_822D5600;
loc_822D55AC:
	// lwz r29,0(r9)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lfs f13,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// srawi r29,r29,8
	ctx.xer.ca = (ctx.r29.s32 < 0) & ((ctx.r29.u32 & 0xFF) != 0);
	ctx.r29.s64 = ctx.r29.s32 >> 8;
	// extsw r29,r29
	ctx.r29.s64 = ctx.r29.s32;
	// std r29,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r29.u64);
	// lfd f5,120(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmadds f0,f2,f11,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f11.f64), float(ctx.f0.f64)));
	// b 0x822d5600
	goto loc_822D5600;
loc_822D55DC:
	// lwz r29,0(r9)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lfsu f13,4(r11)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// extsw r29,r29
	ctx.r29.s64 = ctx.r29.s32;
	// std r29,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r29.u64);
	// lfd f5,128(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmadds f0,f2,f8,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f8.f64), float(ctx.f0.f64)));
loc_822D5600:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplw cr6,r8,r18
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r18.u32, ctx.xer);
	// blt cr6,0x822d545c
	if (ctx.cr6.lt) goto loc_822D545C;
loc_822D5618:
	// cmpwi cr6,r14,0
	ctx.cr6.compare<int32_t>(ctx.r14.s32, 0, ctx.xer);
	// beq cr6,0x822d5630
	if (ctx.cr6.eq) goto loc_822D5630;
	// lfs f13,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f5,f13,f0
	ctx.f5.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f5,0(r6)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// b 0x822d5634
	goto loc_822D5634;
loc_822D5630:
	// stfs f0,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
loc_822D5634:
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// cmplw cr6,r5,r15
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r15.u32, ctx.xer);
	// blt cr6,0x822d5440
	if (ctx.cr6.lt) goto loc_822D5440;
loc_822D5644:
	// mullw r11,r17,r18
	ctx.r11.s64 = int64_t(ctx.r17.s32) * int64_t(ctx.r18.s32);
	// rlwinm r10,r15,2,0,29
	ctx.r10.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// add r27,r11,r27
	ctx.r27.u64 = ctx.r11.u64 + ctx.r27.u64;
	// add r16,r10,r16
	ctx.r16.u64 = ctx.r10.u64 + ctx.r16.u64;
	// cmplw cr6,r4,r3
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r3.u32, ctx.xer);
	// blt cr6,0x822d542c
	if (ctx.cr6.lt) goto loc_822D542C;
loc_822D5660:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa84
	ctx.lr = 0x822D566C;
	__savefpr_28(ctx, base);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
loc_822D5670:
	// divwu. r19,r10,r18
	ctx.r19.u32 = ctx.r10.u32 / ctx.r18.u32;
	ctx.cr0.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// li r23,0
	ctx.r23.s64 = 0;
	// beq 0x822d5660
	if (ctx.cr0.eq) goto loc_822D5660;
	// mullw r21,r17,r18
	ctx.r21.s64 = int64_t(ctx.r17.s32) * int64_t(ctx.r18.s32);
	// rlwinm r20,r15,2,0,29
	ctx.r20.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0xFFFFFFFC;
loc_822D5684:
	// lwz r11,8(r22)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8);
	// cmplwi cr6,r15,0
	ctx.cr6.compare<uint32_t>(ctx.r15.u32, 0, ctx.xer);
	// lwz r10,12(r22)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r22.u32 + 12);
	// beq cr6,0x822d573c
	if (ctx.cr6.eq) goto loc_822D573C;
	// clrldi r9,r23,32
	ctx.r9.u64 = ctx.r23.u64 & 0xFFFFFFFF;
	// mr r25,r16
	ctx.r25.u64 = ctx.r16.u64;
	// std r9,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r9.u64);
	// mr r24,r15
	ctx.r24.u64 = ctx.r15.u64;
	// addi r29,r10,-4
	ctx.r29.s64 = ctx.r10.s64 + -4;
	// addi r28,r11,-4
	ctx.r28.s64 = ctx.r11.s64 + -4;
	// lfd f0,128(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f28,f13
	ctx.f28.f64 = double(float(ctx.f13.f64));
loc_822D56B8:
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// lfs f31,4(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,4(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	ctx.f30.f64 = double(temp.f32);
	// mtctr r26
	ctx.ctr.u64 = ctx.r26.u64;
	// bctrl 
	ctx.lr = 0x822D56CC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// fmadds f0,f31,f28,f30
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f28.f64), float(ctx.f30.f64)));
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// cmplwi cr6,r18,1
	ctx.cr6.compare<uint32_t>(ctx.r18.u32, 1, ctx.xer);
	// fmuls f31,f1,f0
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// ble cr6,0x822d5714
	if (!ctx.cr6.gt) goto loc_822D5714;
	// add r30,r17,r27
	ctx.r30.u64 = ctx.r17.u64 + ctx.r27.u64;
	// addi r31,r18,-1
	ctx.r31.s64 = ctx.r18.s64 + -1;
loc_822D56EC:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lfsu f30,4(r29)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r29.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r29.u32 = ea;
	ctx.f30.f64 = double(temp.f32);
	// lfsu f29,4(r28)
	ea = 4 + ctx.r28.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r28.u32 = ea;
	ctx.f29.f64 = double(temp.f32);
	// mtctr r26
	ctx.ctr.u64 = ctx.r26.u64;
	// bctrl 
	ctx.lr = 0x822D5700;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// fmadds f0,f30,f28,f29
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f28.f64), float(ctx.f29.f64)));
	// addic. r31,r31,-1
	ctx.xer.ca = ctx.r31.u32 > 0;
	ctx.r31.s64 = ctx.r31.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// add r30,r30,r17
	ctx.r30.u64 = ctx.r30.u64 + ctx.r17.u64;
	// fmadds f31,f1,f0,f31
	ctx.f31.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// bne 0x822d56ec
	if (!ctx.cr0.eq) goto loc_822D56EC;
loc_822D5714:
	// cmpwi cr6,r14,0
	ctx.cr6.compare<int32_t>(ctx.r14.s32, 0, ctx.xer);
	// beq cr6,0x822d572c
	if (ctx.cr6.eq) goto loc_822D572C;
	// lfs f0,0(r25)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f0,f31
	ctx.f13.f64 = double(float(ctx.f0.f64 + ctx.f31.f64));
	// stfs f13,0(r25)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r25.u32 + 0, temp.u32);
	// b 0x822d5730
	goto loc_822D5730;
loc_822D572C:
	// stfs f31,0(r25)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 0, temp.u32);
loc_822D5730:
	// addic. r24,r24,-1
	ctx.xer.ca = ctx.r24.u32 > 0;
	ctx.r24.s64 = ctx.r24.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// addi r25,r25,4
	ctx.r25.s64 = ctx.r25.s64 + 4;
	// bne 0x822d56b8
	if (!ctx.cr0.eq) goto loc_822D56B8;
loc_822D573C:
	// addi r23,r23,1
	ctx.r23.s64 = ctx.r23.s64 + 1;
	// add r27,r21,r27
	ctx.r27.u64 = ctx.r21.u64 + ctx.r27.u64;
	// add r16,r20,r16
	ctx.r16.u64 = ctx.r20.u64 + ctx.r16.u64;
	// cmplw cr6,r23,r19
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, ctx.r19.u32, ctx.xer);
	// blt cr6,0x822d5684
	if (ctx.cr6.lt) goto loc_822D5684;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa84
	ctx.lr = 0x822D575C;
	__savefpr_28(ctx, base);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D5760"))) PPC_WEAK_FUNC(sub_822D5760);
PPC_FUNC_IMPL(__imp__sub_822D5760) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// blt cr6,0x822d597c
	if (ctx.cr6.lt) goto loc_822D597C;
	// beq cr6,0x822d5890
	if (ctx.cr6.eq) goto loc_822D5890;
	// cmplwi cr6,r3,7
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 7, ctx.xer);
	// blt cr6,0x822d597c
	if (ctx.cr6.lt) goto loc_822D597C;
	// bne cr6,0x822d5984
	if (!ctx.cr6.eq) goto loc_822D5984;
	// cmplwi cr6,r4,1
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 1, ctx.xer);
	// bne cr6,0x822d57d4
	if (!ctx.cr6.eq) goto loc_822D57D4;
	// cmplwi cr6,r5,2
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 2, ctx.xer);
	// bne cr6,0x822d57ac
	if (!ctx.cr6.eq) goto loc_822D57AC;
	// rlwinm. r11,r6,0,29,29
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x4;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d57a0
	if (ctx.cr0.eq) goto loc_822D57A0;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,8520
	ctx.r11.s64 = ctx.r11.s64 + 8520;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D57A0:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,3608
	ctx.r11.s64 = ctx.r11.s64 + 3608;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D57AC:
	// cmplwi cr6,r5,6
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 6, ctx.xer);
	// bne cr6,0x822d583c
	if (!ctx.cr6.eq) goto loc_822D583C;
	// rlwinm. r11,r6,0,29,29
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x4;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d57c8
	if (ctx.cr0.eq) goto loc_822D57C8;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,9008
	ctx.r11.s64 = ctx.r11.s64 + 9008;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D57C8:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,3872
	ctx.r11.s64 = ctx.r11.s64 + 3872;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D57D4:
	// cmplwi cr6,r4,2
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 2, ctx.xer);
	// bne cr6,0x822d5804
	if (!ctx.cr6.eq) goto loc_822D5804;
	// cmplwi cr6,r5,6
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 6, ctx.xer);
	// bne cr6,0x822d583c
	if (!ctx.cr6.eq) goto loc_822D583C;
	// rlwinm. r11,r6,0,29,29
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x4;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d57f8
	if (ctx.cr0.eq) goto loc_822D57F8;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,10048
	ctx.r11.s64 = ctx.r11.s64 + 10048;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D57F8:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,4344
	ctx.r11.s64 = ctx.r11.s64 + 4344;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D5804:
	// cmplwi cr6,r4,6
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 6, ctx.xer);
	// bne cr6,0x822d5820
	if (!ctx.cr6.eq) goto loc_822D5820;
	// cmplwi cr6,r5,2
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 2, ctx.xer);
	// bne cr6,0x822d583c
	if (!ctx.cr6.eq) goto loc_822D583C;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,5088
	ctx.r11.s64 = ctx.r11.s64 + 5088;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D5820:
	// cmplwi cr6,r4,4
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 4, ctx.xer);
	// bne cr6,0x822d583c
	if (!ctx.cr6.eq) goto loc_822D583C;
	// cmplwi cr6,r5,6
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 6, ctx.xer);
	// bne cr6,0x822d583c
	if (!ctx.cr6.eq) goto loc_822D583C;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,5664
	ctx.r11.s64 = ctx.r11.s64 + 5664;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D583C:
	// rlwinm. r11,r6,0,30,30
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x2;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d5884
	if (ctx.cr0.eq) goto loc_822D5884;
	// clrlwi. r11,r6,31
	ctx.r11.u64 = ctx.r6.u32 & 0x1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822d5878
	if (!ctx.cr0.eq) goto loc_822D5878;
	// cmplwi cr6,r4,4
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 4, ctx.xer);
	// bgt cr6,0x822d5864
	if (ctx.cr6.gt) goto loc_822D5864;
	// cmplwi cr6,r4,3
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 3, ctx.xer);
	// beq cr6,0x822d5864
	if (ctx.cr6.eq) goto loc_822D5864;
	// rlwinm. r11,r6,0,29,29
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x4;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822d5878
	if (!ctx.cr0.eq) goto loc_822D5878;
loc_822D5864:
	// cmplwi cr6,r4,6
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 6, ctx.xer);
	// bgt cr6,0x822d5884
	if (ctx.cr6.gt) goto loc_822D5884;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,7144
	ctx.r11.s64 = ctx.r11.s64 + 7144;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D5878:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,11064
	ctx.r11.s64 = ctx.r11.s64 + 11064;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D5884:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,3168
	ctx.r11.s64 = ctx.r11.s64 + 3168;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D5890:
	// cmplwi cr6,r4,1
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 1, ctx.xer);
	// bne cr6,0x822d58d4
	if (!ctx.cr6.eq) goto loc_822D58D4;
	// cmplwi cr6,r5,2
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 2, ctx.xer);
	// bne cr6,0x822d58ac
	if (!ctx.cr6.eq) goto loc_822D58AC;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,12568
	ctx.r11.s64 = ctx.r11.s64 + 12568;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D58AC:
	// cmplwi cr6,r5,6
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 6, ctx.xer);
	// bne cr6,0x822d5928
	if (!ctx.cr6.eq) goto loc_822D5928;
	// rlwinm. r11,r6,0,29,29
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x4;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d58c8
	if (ctx.cr0.eq) goto loc_822D58C8;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,18936
	ctx.r11.s64 = ctx.r11.s64 + 18936;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D58C8:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,12888
	ctx.r11.s64 = ctx.r11.s64 + 12888;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D58D4:
	// cmplwi cr6,r4,2
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 2, ctx.xer);
	// bne cr6,0x822d58f0
	if (!ctx.cr6.eq) goto loc_822D58F0;
	// cmplwi cr6,r5,6
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 6, ctx.xer);
	// bne cr6,0x822d5928
	if (!ctx.cr6.eq) goto loc_822D5928;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,13448
	ctx.r11.s64 = ctx.r11.s64 + 13448;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D58F0:
	// cmplwi cr6,r4,6
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 6, ctx.xer);
	// bne cr6,0x822d590c
	if (!ctx.cr6.eq) goto loc_822D590C;
	// cmplwi cr6,r5,2
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 2, ctx.xer);
	// bne cr6,0x822d5928
	if (!ctx.cr6.eq) goto loc_822D5928;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,14376
	ctx.r11.s64 = ctx.r11.s64 + 14376;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D590C:
	// cmplwi cr6,r4,4
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 4, ctx.xer);
	// bne cr6,0x822d5928
	if (!ctx.cr6.eq) goto loc_822D5928;
	// cmplwi cr6,r5,6
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 6, ctx.xer);
	// bne cr6,0x822d5928
	if (!ctx.cr6.eq) goto loc_822D5928;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,15296
	ctx.r11.s64 = ctx.r11.s64 + 15296;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D5928:
	// rlwinm. r11,r6,0,30,30
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x2;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822d5970
	if (ctx.cr0.eq) goto loc_822D5970;
	// clrlwi. r11,r6,31
	ctx.r11.u64 = ctx.r6.u32 & 0x1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822d5964
	if (!ctx.cr0.eq) goto loc_822D5964;
	// cmplwi cr6,r4,4
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 4, ctx.xer);
	// bgt cr6,0x822d5950
	if (ctx.cr6.gt) goto loc_822D5950;
	// cmplwi cr6,r4,3
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 3, ctx.xer);
	// beq cr6,0x822d5950
	if (ctx.cr6.eq) goto loc_822D5950;
	// rlwinm. r11,r6,0,29,29
	ctx.r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x4;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822d5964
	if (!ctx.cr0.eq) goto loc_822D5964;
loc_822D5950:
	// cmplwi cr6,r4,6
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 6, ctx.xer);
	// bgt cr6,0x822d5970
	if (ctx.cr6.gt) goto loc_822D5970;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,17144
	ctx.r11.s64 = ctx.r11.s64 + 17144;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D5964:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,20032
	ctx.r11.s64 = ctx.r11.s64 + 20032;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D5970:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,12032
	ctx.r11.s64 = ctx.r11.s64 + 12032;
	// b 0x822d5984
	goto loc_822D5984;
loc_822D597C:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,21256
	ctx.r11.s64 = ctx.r11.s64 + 21256;
loc_822D5984:
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D598C"))) PPC_WEAK_FUNC(sub_822D598C);
PPC_FUNC_IMPL(__imp__sub_822D598C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D5990"))) PPC_WEAK_FUNC(sub_822D5990);
PPC_FUNC_IMPL(__imp__sub_822D5990) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e444
	ctx.lr = 0x822D5998;
	__restfpr_19(ctx, base);
	// stfd f30,-128(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.f30.u64);
	// stfd f31,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.f31.u64);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r9,16(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// mr r22,r3
	ctx.r22.u64 = ctx.r3.u64;
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// li r31,0
	ctx.r31.s64 = 0;
	// lwz r30,32(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r21,24(r3)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// rlwinm r10,r30,3,0,28
	ctx.r10.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f13.u64);
	// lwz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// lwz r23,0(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r25,4(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lfs f31,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f31.f64 = double(temp.f32);
	// lwz r20,28(r3)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// add r26,r10,r11
	ctx.r26.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r29,36(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// cmplwi cr6,r21,0
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 0, ctx.xer);
	// mullw r24,r7,r30
	ctx.r24.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r30.s32);
	// fsubs f30,f0,f10
	ctx.f30.f64 = static_cast<float>(ctx.f0.f64 - ctx.f10.f64);
	// beq cr6,0x822d5a30
	if (ctx.cr6.eq) goto loc_822D5A30;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x822d5a30
	if (ctx.cr6.eq) goto loc_822D5A30;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// rlwinm r5,r30,2,0,29
	ctx.r5.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233e968
	ctx.lr = 0x822D5A30;
	sub_8233E968(ctx, base);
loc_822D5A30:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// cmpw cr6,r29,r30
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r30.s32, ctx.xer);
	// lfs f12,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f12.f64 = double(temp.f32);
	// bge cr6,0x822d5b98
	if (!ctx.cr6.lt) goto loc_822D5B98;
	// subf r11,r30,r29
	ctx.r11.s64 = ctx.r29.s64 - ctx.r30.s64;
	// addi r10,r29,2
	ctx.r10.s64 = ctx.r29.s64 + 2;
	// addi r9,r11,2
	ctx.r9.s64 = ctx.r11.s64 + 2;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r29,r30
	ctx.r11.u64 = ctx.r29.u64 + ctx.r30.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r29,r30
	ctx.r6.u64 = ctx.r29.u64 + ctx.r30.u64;
	// rlwinm r28,r30,2,0,29
	ctx.r28.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r24,2,0,29
	ctx.r27.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r10,r26
	ctx.r4.u64 = ctx.r10.u64 + ctx.r26.u64;
	// addi r5,r11,-3
	ctx.r5.s64 = ctx.r11.s64 + -3;
	// add r3,r9,r26
	ctx.r3.u64 = ctx.r9.u64 + ctx.r26.u64;
loc_822D5A70:
	// cmplw cr6,r31,r20
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r20.u32, ctx.xer);
	// bge cr6,0x822d5cf0
	if (!ctx.cr6.lt) goto loc_822D5CF0;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
	// cmpw cr6,r29,r6
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r6.s32, ctx.xer);
	// bge cr6,0x822d5b58
	if (!ctx.cr6.lt) goto loc_822D5B58;
	// subf r10,r29,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r29.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d5b04
	if (ctx.cr6.lt) goto loc_822D5B04;
	// rlwinm r9,r31,2,0,29
	ctx.r9.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r4,-12
	ctx.r10.s64 = ctx.r4.s64 + -12;
	// add r8,r9,r25
	ctx.r8.u64 = ctx.r9.u64 + ctx.r25.u64;
	// addi r9,r3,-12
	ctx.r9.s64 = ctx.r3.s64 + -12;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
loc_822D5AA4:
	// lfs f11,4(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lfs f8,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// lfs f10,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f6,f8,f11
	ctx.f6.f64 = static_cast<float>(ctx.f8.f64 - ctx.f11.f64);
	// lfs f9,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// lfs f7,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// fsubs f4,f7,f10
	ctx.f4.f64 = static_cast<float>(ctx.f7.f64 - ctx.f10.f64);
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fsubs f3,f5,f9
	ctx.f3.f64 = static_cast<float>(ctx.f5.f64 - ctx.f9.f64);
	// lfsu f13,16(r10)
	ea = 16 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fsubs f2,f13,f0
	ctx.f2.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// fmadds f1,f6,f31,f11
	ctx.f1.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f31.f64), float(ctx.f11.f64)));
	// stfs f1,4(r8)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// fmadds f13,f4,f31,f10
	ctx.f13.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f31.f64), float(ctx.f10.f64)));
	// stfs f13,8(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// fmadds f11,f3,f31,f9
	ctx.f11.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f31.f64), float(ctx.f9.f64)));
	// stfs f11,12(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 12, temp.u32);
	// fmadds f10,f2,f31,f0
	ctx.f10.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f31.f64), float(ctx.f0.f64)));
	// stfsu f10,16(r8)
	temp.f32 = float(ctx.f10.f64);
	ea = 16 + ctx.r8.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r8.u32 = ea;
	// blt cr6,0x822d5aa4
	if (ctx.cr6.lt) goto loc_822D5AA4;
loc_822D5B04:
	// cmpw cr6,r11,r6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r6.s32, ctx.xer);
	// bge cr6,0x822d5b58
	if (!ctx.cr6.lt) goto loc_822D5B58;
	// subf r9,r30,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r30.s64;
	// rlwinm r8,r31,2,0,29
	ctx.r8.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r11,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r11.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r19,r9,-1
	ctx.r19.s64 = ctx.r9.s64 + -1;
	// add r11,r8,r25
	ctx.r11.u64 = ctx.r8.u64 + ctx.r25.u64;
	// add r9,r7,r26
	ctx.r9.u64 = ctx.r7.u64 + ctx.r26.u64;
	// rlwinm r8,r19,2,0,29
	ctx.r8.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r7,r11,-4
	ctx.r7.s64 = ctx.r11.s64 + -4;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// add r11,r8,r26
	ctx.r11.u64 = ctx.r8.u64 + ctx.r26.u64;
	// add r31,r10,r31
	ctx.r31.u64 = ctx.r10.u64 + ctx.r31.u64;
loc_822D5B40:
	// lfsu f0,4(r11)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// lfsu f13,4(r9)
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fsubs f13,f13,f0
	ctx.f13.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// fmadds f11,f13,f31,f0
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f31.f64), float(ctx.f0.f64)));
	// stfsu f11,4(r7)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d5b40
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D5B40;
loc_822D5B58:
	// fadds f31,f30,f31
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = double(float(ctx.f30.f64 + ctx.f31.f64));
	// add r29,r24,r29
	ctx.r29.u64 = ctx.r24.u64 + ctx.r29.u64;
	// add r6,r6,r24
	ctx.r6.u64 = ctx.r6.u64 + ctx.r24.u64;
	// add r5,r5,r24
	ctx.r5.u64 = ctx.r5.u64 + ctx.r24.u64;
	// add r4,r27,r4
	ctx.r4.u64 = ctx.r27.u64 + ctx.r4.u64;
	// add r3,r27,r3
	ctx.r3.u64 = ctx.r27.u64 + ctx.r3.u64;
	// fcmpu cr6,f31,f12
	ctx.cr6.compare(ctx.f31.f64, ctx.f12.f64);
	// blt cr6,0x822d5b90
	if (ctx.cr6.lt) goto loc_822D5B90;
	// fsubs f31,f31,f12
	ctx.f31.f64 = static_cast<float>(ctx.f31.f64 - ctx.f12.f64);
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + ctx.r30.u64;
	// add r29,r29,r30
	ctx.r29.u64 = ctx.r29.u64 + ctx.r30.u64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + ctx.r30.u64;
	// add r4,r28,r4
	ctx.r4.u64 = ctx.r28.u64 + ctx.r4.u64;
	// add r3,r28,r3
	ctx.r3.u64 = ctx.r28.u64 + ctx.r3.u64;
loc_822D5B90:
	// cmpw cr6,r29,r30
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r30.s32, ctx.xer);
	// blt cr6,0x822d5a70
	if (ctx.cr6.lt) goto loc_822D5A70;
loc_822D5B98:
	// cmplw cr6,r31,r20
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r20.u32, ctx.xer);
	// bge cr6,0x822d5cf0
	if (!ctx.cr6.lt) goto loc_822D5CF0;
	// subf r11,r30,r29
	ctx.r11.s64 = ctx.r29.s64 - ctx.r30.s64;
	// addi r10,r29,2
	ctx.r10.s64 = ctx.r29.s64 + 2;
	// addi r9,r11,2
	ctx.r9.s64 = ctx.r11.s64 + 2;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r29,r30
	ctx.r11.u64 = ctx.r29.u64 + ctx.r30.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r29,r30
	ctx.r6.u64 = ctx.r29.u64 + ctx.r30.u64;
	// rlwinm r28,r30,2,0,29
	ctx.r28.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r24,2,0,29
	ctx.r27.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r10,r23
	ctx.r4.u64 = ctx.r10.u64 + ctx.r23.u64;
	// addi r5,r11,-3
	ctx.r5.s64 = ctx.r11.s64 + -3;
	// add r3,r9,r23
	ctx.r3.u64 = ctx.r9.u64 + ctx.r23.u64;
loc_822D5BD0:
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
	// cmpw cr6,r29,r6
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r6.s32, ctx.xer);
	// bge cr6,0x822d5cb0
	if (!ctx.cr6.lt) goto loc_822D5CB0;
	// subf r10,r29,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r29.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d5c5c
	if (ctx.cr6.lt) goto loc_822D5C5C;
	// rlwinm r9,r31,2,0,29
	ctx.r9.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r4,-12
	ctx.r10.s64 = ctx.r4.s64 + -12;
	// add r8,r9,r25
	ctx.r8.u64 = ctx.r9.u64 + ctx.r25.u64;
	// addi r9,r3,-12
	ctx.r9.s64 = ctx.r3.s64 + -12;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
loc_822D5BFC:
	// lfs f11,4(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lfs f8,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// lfs f10,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f6,f8,f11
	ctx.f6.f64 = static_cast<float>(ctx.f8.f64 - ctx.f11.f64);
	// lfs f9,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// lfs f7,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// fsubs f4,f7,f10
	ctx.f4.f64 = static_cast<float>(ctx.f7.f64 - ctx.f10.f64);
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fsubs f3,f5,f9
	ctx.f3.f64 = static_cast<float>(ctx.f5.f64 - ctx.f9.f64);
	// lfsu f13,16(r10)
	ea = 16 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fsubs f2,f13,f0
	ctx.f2.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// fmadds f1,f6,f31,f11
	ctx.f1.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f31.f64), float(ctx.f11.f64)));
	// stfs f1,4(r8)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// fmadds f13,f4,f31,f10
	ctx.f13.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f31.f64), float(ctx.f10.f64)));
	// stfs f13,8(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// fmadds f11,f3,f31,f9
	ctx.f11.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f31.f64), float(ctx.f9.f64)));
	// stfs f11,12(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 12, temp.u32);
	// fmadds f10,f2,f31,f0
	ctx.f10.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f31.f64), float(ctx.f0.f64)));
	// stfsu f10,16(r8)
	temp.f32 = float(ctx.f10.f64);
	ea = 16 + ctx.r8.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r8.u32 = ea;
	// blt cr6,0x822d5bfc
	if (ctx.cr6.lt) goto loc_822D5BFC;
loc_822D5C5C:
	// cmpw cr6,r11,r6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r6.s32, ctx.xer);
	// bge cr6,0x822d5cb0
	if (!ctx.cr6.lt) goto loc_822D5CB0;
	// subf r9,r30,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r30.s64;
	// rlwinm r8,r31,2,0,29
	ctx.r8.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r11,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r11.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r19,r9,-1
	ctx.r19.s64 = ctx.r9.s64 + -1;
	// add r11,r8,r25
	ctx.r11.u64 = ctx.r8.u64 + ctx.r25.u64;
	// add r9,r7,r23
	ctx.r9.u64 = ctx.r7.u64 + ctx.r23.u64;
	// rlwinm r8,r19,2,0,29
	ctx.r8.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r7,r11,-4
	ctx.r7.s64 = ctx.r11.s64 + -4;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// add r11,r8,r23
	ctx.r11.u64 = ctx.r8.u64 + ctx.r23.u64;
	// add r31,r10,r31
	ctx.r31.u64 = ctx.r10.u64 + ctx.r31.u64;
loc_822D5C98:
	// lfsu f0,4(r11)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// lfsu f13,4(r9)
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fsubs f13,f13,f0
	ctx.f13.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// fmadds f11,f13,f31,f0
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f31.f64), float(ctx.f0.f64)));
	// stfsu f11,4(r7)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d5c98
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D5C98;
loc_822D5CB0:
	// fadds f31,f30,f31
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = double(float(ctx.f30.f64 + ctx.f31.f64));
	// add r29,r24,r29
	ctx.r29.u64 = ctx.r24.u64 + ctx.r29.u64;
	// add r6,r6,r24
	ctx.r6.u64 = ctx.r6.u64 + ctx.r24.u64;
	// add r5,r5,r24
	ctx.r5.u64 = ctx.r5.u64 + ctx.r24.u64;
	// add r4,r27,r4
	ctx.r4.u64 = ctx.r27.u64 + ctx.r4.u64;
	// add r3,r27,r3
	ctx.r3.u64 = ctx.r27.u64 + ctx.r3.u64;
	// fcmpu cr6,f31,f12
	ctx.cr6.compare(ctx.f31.f64, ctx.f12.f64);
	// blt cr6,0x822d5ce8
	if (ctx.cr6.lt) goto loc_822D5CE8;
	// fsubs f31,f31,f12
	ctx.f31.f64 = static_cast<float>(ctx.f31.f64 - ctx.f12.f64);
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + ctx.r30.u64;
	// add r29,r29,r30
	ctx.r29.u64 = ctx.r29.u64 + ctx.r30.u64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + ctx.r30.u64;
	// add r4,r28,r4
	ctx.r4.u64 = ctx.r28.u64 + ctx.r4.u64;
	// add r3,r28,r3
	ctx.r3.u64 = ctx.r28.u64 + ctx.r3.u64;
loc_822D5CE8:
	// cmplw cr6,r31,r20
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r20.u32, ctx.xer);
	// blt cr6,0x822d5bd0
	if (ctx.cr6.lt) goto loc_822D5BD0;
loc_822D5CF0:
	// subf r11,r30,r21
	ctx.r11.s64 = ctx.r21.s64 - ctx.r30.s64;
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x822d5d00
	if (!ctx.cr6.lt) goto loc_822D5D00;
	// mr r29,r11
	ctx.r29.u64 = ctx.r11.u64;
loc_822D5D00:
	// subf r10,r21,r29
	ctx.r10.s64 = ctx.r29.s64 - ctx.r21.s64;
	// stfs f31,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// neg r8,r21
	ctx.r8.s64 = -ctx.r21.s64;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r11,r30,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r30.s64;
	// stw r9,12(r22)
	PPC_STORE_U32(ctx.r22.u32 + 12, ctx.r9.u32);
	// stw r10,36(r22)
	PPC_STORE_U32(ctx.r22.u32 + 36, ctx.r10.u32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d5dd0
	if (!ctx.cr6.lt) goto loc_822D5DD0;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d5d94
	if (ctx.cr6.lt) goto loc_822D5D94;
	// add r10,r11,r21
	ctx.r10.u64 = ctx.r11.u64 + ctx.r21.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r9,r26
	ctx.r10.u64 = ctx.r9.u64 + ctx.r26.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r21,2
	ctx.r7.s64 = ctx.r21.s64 + 2;
	// addi r6,r21,3
	ctx.r6.s64 = ctx.r21.s64 + 3;
	// addi r5,r8,-3
	ctx.r5.s64 = ctx.r8.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + ctx.r26.u64;
loc_822D5D58:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r4,r7,r11
	ctx.r4.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r3,r6,r11
	ctx.r3.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r4,r26
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r26.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f12,r3,r26
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r26.u32);
	ctx.f12.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d5d58
	if (ctx.cr6.lt) goto loc_822D5D58;
loc_822D5D94:
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d5dd0
	if (!ctx.cr6.lt) goto loc_822D5DD0;
	// add r7,r11,r21
	ctx.r7.u64 = ctx.r11.u64 + ctx.r21.u64;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + ctx.r26.u64;
	// add r8,r8,r26
	ctx.r8.u64 = ctx.r8.u64 + ctx.r26.u64;
	// addi r7,r9,-4
	ctx.r7.s64 = ctx.r9.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_822D5DC4:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d5dc4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D5DC4;
loc_822D5DD0:
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d5e48
	if (ctx.cr6.lt) goto loc_822D5E48;
	// subfic r9,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r9.s64 = -4 - ctx.r11.s64;
	// add r10,r11,r21
	ctx.r10.u64 = ctx.r11.u64 + ctx.r21.u64;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r8,r26
	ctx.r10.u64 = ctx.r8.u64 + ctx.r26.u64;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r21,2
	ctx.r7.s64 = ctx.r21.s64 + 2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r6,r21,3
	ctx.r6.s64 = ctx.r21.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r8,r23
	ctx.r9.u64 = ctx.r8.u64 + ctx.r23.u64;
loc_822D5E10:
	// add r8,r7,r11
	ctx.r8.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lfs f13,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// add r5,r6,r11
	ctx.r5.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r4,r8,2,0,29
	ctx.r4.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// rlwinm r3,r5,2,0,29
	ctx.r3.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lfsx f12,r4,r23
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r23.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfsx f11,r3,r23
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r23.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfs f12,12(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfsu f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d5e10
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D5E10;
loc_822D5E48:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822d5e70
	if (!ctx.cr6.lt) goto loc_822D5E70;
	// add r9,r11,r21
	ctx.r9.u64 = ctx.r11.u64 + ctx.r21.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r8,r11
	ctx.r8.s64 = -ctx.r11.s64;
	// add r3,r10,r26
	ctx.r3.u64 = ctx.r10.u64 + ctx.r26.u64;
	// add r4,r9,r23
	ctx.r4.u64 = ctx.r9.u64 + ctx.r23.u64;
	// rlwinm r5,r8,2,0,29
	ctx.r5.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233e968
	ctx.lr = 0x822D5E70;
	sub_8233E968(ctx, base);
loc_822D5E70:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// lfd f30,-128(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// lfd f31,-120(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D5E80"))) PPC_WEAK_FUNC(sub_822D5E80);
PPC_FUNC_IMPL(__imp__sub_822D5E80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822D5E88;
	__restfpr_27(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r30,24(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// lwz r28,0(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lfs f13,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// fctiwz f12,f13
	ctx.f12.u64 = uint64_t(int32_t(std::trunc(ctx.f13.f64)));
	// stfd f12,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f12.u64);
	// lwz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lwz r29,4(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r27,28(r3)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// rlwinm r4,r7,1,0,30
	ctx.r4.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// fsubs f11,f13,f9
	ctx.f11.f64 = static_cast<float>(ctx.f13.f64 - ctx.f9.f64);
	// beq cr6,0x822d5f04
	if (ctx.cr6.eq) goto loc_822D5F04;
	// lfs f13,0(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfs f13,0(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// stfs f12,4(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
loc_822D5F04:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// lfs f13,5256(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f13.f64 = double(temp.f32);
	// bge cr6,0x822d5f98
	if (!ctx.cr6.lt) goto loc_822D5F98;
	// addi r10,r11,-2
	ctx.r10.s64 = ctx.r11.s64 + -2;
	// rlwinm r6,r4,2,0,29
	ctx.r6.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r6,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r6.s64;
	// mr r9,r29
	ctx.r9.u64 = ctx.r29.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
loc_822D5F34:
	// cmplw cr6,r8,r27
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r27.u32, ctx.xer);
	// bge cr6,0x822d6020
	if (!ctx.cr6.lt) goto loc_822D6020;
	// lfs f10,-4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f10.f64 = double(temp.f32);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// lfsux f12,r7,r6
	ea = ctx.r7.u32 + ctx.r6.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r7.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// lfs f9,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fsubs f7,f9,f12
	ctx.f7.f64 = static_cast<float>(ctx.f9.f64 - ctx.f12.f64);
	// fsubs f6,f8,f10
	ctx.f6.f64 = static_cast<float>(ctx.f8.f64 - ctx.f10.f64);
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// fmadds f5,f7,f0,f12
	ctx.f5.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfs f5,0(r9)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f4,f6,f0,f10
	ctx.f4.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f0.f64), float(ctx.f10.f64)));
	// stfsu f4,4(r9)
	temp.f32 = float(ctx.f4.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fadds f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// blt cr6,0x822d5f90
	if (ctx.cr6.lt) goto loc_822D5F90;
	// fsubs f0,f0,f13
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f13.f64);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
loc_822D5F90:
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// blt cr6,0x822d5f34
	if (ctx.cr6.lt) goto loc_822D5F34;
loc_822D5F98:
	// cmplw cr6,r8,r27
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r27.u32, ctx.xer);
	// bge cr6,0x822d6020
	if (!ctx.cr6.lt) goto loc_822D6020;
	// addi r10,r11,-2
	ctx.r10.s64 = ctx.r11.s64 + -2;
	// rlwinm r6,r4,2,0,29
	ctx.r6.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r31,r6,r9
	ctx.r31.s64 = ctx.r9.s64 - ctx.r6.s64;
	// add r9,r10,r29
	ctx.r9.u64 = ctx.r10.u64 + ctx.r29.u64;
	// add r10,r7,r28
	ctx.r10.u64 = ctx.r7.u64 + ctx.r28.u64;
	// add r7,r31,r28
	ctx.r7.u64 = ctx.r31.u64 + ctx.r28.u64;
loc_822D5FC4:
	// lfs f10,-4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f10.f64 = double(temp.f32);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// lfsux f12,r7,r6
	ea = ctx.r7.u32 + ctx.r6.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r7.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// lfs f9,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fsubs f7,f9,f12
	ctx.f7.f64 = static_cast<float>(ctx.f9.f64 - ctx.f12.f64);
	// fsubs f6,f8,f10
	ctx.f6.f64 = static_cast<float>(ctx.f8.f64 - ctx.f10.f64);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// fmadds f5,f7,f0,f12
	ctx.f5.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfs f5,0(r9)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f4,f6,f0,f10
	ctx.f4.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f0.f64), float(ctx.f10.f64)));
	// stfsu f4,4(r9)
	temp.f32 = float(ctx.f4.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fadds f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// blt cr6,0x822d6018
	if (ctx.cr6.lt) goto loc_822D6018;
	// fsubs f0,f0,f13
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f13.f64);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
loc_822D6018:
	// cmplw cr6,r8,r27
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r27.u32, ctx.xer);
	// blt cr6,0x822d5fc4
	if (ctx.cr6.lt) goto loc_822D5FC4;
loc_822D6020:
	// addi r10,r30,-2
	ctx.r10.s64 = ctx.r30.s64 + -2;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x822d6030
	if (!ctx.cr6.lt) goto loc_822D6030;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822D6030:
	// subf r10,r30,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r30.s64;
	// stfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// neg r8,r30
	ctx.r8.s64 = -ctx.r30.s64;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// stw r9,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r9.u32);
	// stw r10,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r10.u32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d6100
	if (!ctx.cr6.lt) goto loc_822D6100;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d60c4
	if (ctx.cr6.lt) goto loc_822D60C4;
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r9,r5
	ctx.r10.u64 = ctx.r9.u64 + ctx.r5.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r30,2
	ctx.r7.s64 = ctx.r30.s64 + 2;
	// addi r6,r30,3
	ctx.r6.s64 = ctx.r30.s64 + 3;
	// addi r4,r8,-3
	ctx.r4.s64 = ctx.r8.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
loc_822D6088:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r3,r7,r11
	ctx.r3.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r31,r6,r11
	ctx.r31.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// rlwinm r31,r31,2,0,29
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r3,r5
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r5.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f12,r31,r5
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r5.u32);
	ctx.f12.f64 = double(temp.f32);
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d6088
	if (ctx.cr6.lt) goto loc_822D6088;
loc_822D60C4:
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d6100
	if (!ctx.cr6.lt) goto loc_822D6100;
	// add r7,r11,r30
	ctx.r7.u64 = ctx.r11.u64 + ctx.r30.u64;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r7,r9,-4
	ctx.r7.s64 = ctx.r9.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_822D60F4:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d60f4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D60F4;
loc_822D6100:
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d6178
	if (ctx.cr6.lt) goto loc_822D6178;
	// subfic r10,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r10.s64 = -4 - ctx.r11.s64;
	// add r9,r11,r30
	ctx.r9.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r8,r10,30,2,31
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r7,r9,-3
	ctx.r7.s64 = ctx.r9.s64 + -3;
	// addi r9,r8,1
	ctx.r9.s64 = ctx.r8.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// addi r7,r30,2
	ctx.r7.s64 = ctx.r30.s64 + 2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r6,r30,3
	ctx.r6.s64 = ctx.r30.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r8,r28
	ctx.r9.u64 = ctx.r8.u64 + ctx.r28.u64;
loc_822D6140:
	// add r8,r7,r11
	ctx.r8.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lfs f13,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// add r4,r6,r11
	ctx.r4.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r3,r8,2,0,29
	ctx.r3.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// rlwinm r8,r4,2,0,29
	ctx.r8.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lfsx f12,r3,r28
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r28.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfsx f11,r8,r28
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r28.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfs f12,12(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfsu f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d6140
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D6140;
loc_822D6178:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822d61a0
	if (!ctx.cr6.lt) goto loc_822D61A0;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + ctx.r30.u64;
	// add r3,r9,r5
	ctx.r3.u64 = ctx.r9.u64 + ctx.r5.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r9,r11
	ctx.r9.s64 = -ctx.r11.s64;
	// add r4,r10,r28
	ctx.r4.u64 = ctx.r10.u64 + ctx.r28.u64;
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233e968
	ctx.lr = 0x822D61A0;
	sub_8233E968(ctx, base);
loc_822D61A0:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D61A8"))) PPC_WEAK_FUNC(sub_822D61A8);
PPC_FUNC_IMPL(__imp__sub_822D61A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e44c
	ctx.lr = 0x822D61B0;
	__restfpr_21(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r22,24(r3)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r27,r11,32
	ctx.r27.s64 = ctx.r11.s64 + 32;
	// lwz r28,0(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lfs f13,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// fctiwz f12,f13
	ctx.f12.u64 = uint64_t(int32_t(std::trunc(ctx.f13.f64)));
	// stfd f12,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f12.u64);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r7,r8
	ctx.r7.s64 = ctx.r8.s32;
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lwz r23,4(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r21,28(r3)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// cmplwi cr6,r22,0
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, 0, ctx.xer);
	// rlwinm r26,r8,2,0,29
	ctx.r26.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fsubs f8,f13,f9
	ctx.f8.f64 = static_cast<float>(ctx.f13.f64 - ctx.f9.f64);
	// beq cr6,0x822d623c
	if (ctx.cr6.eq) goto loc_822D623C;
	// lfs f13,0(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,12(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// stfs f13,0(r27)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// stfs f12,4(r27)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r27.u32 + 4, temp.u32);
	// stfs f11,8(r27)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r27.u32 + 8, temp.u32);
	// stfs f10,12(r27)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r27.u32 + 12, temp.u32);
loc_822D623C:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// lfs f10,5256(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f10.f64 = double(temp.f32);
	// bge cr6,0x822d6330
	if (!ctx.cr6.lt) goto loc_822D6330;
	// addi r10,r11,-2
	ctx.r10.s64 = ctx.r11.s64 + -2;
	// addi r8,r11,2
	ctx.r8.s64 = ctx.r11.s64 + 2;
	// addi r7,r11,-3
	ctx.r7.s64 = ctx.r11.s64 + -3;
	// addi r6,r11,-4
	ctx.r6.s64 = ctx.r11.s64 + -4;
	// rlwinm r9,r26,2,0,29
	ctx.r9.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r8,2,0,29
	ctx.r4.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r30,r6,2,0,29
	ctx.r30.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r9,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r9.s64;
	// subf r6,r9,r4
	ctx.r6.s64 = ctx.r4.s64 - ctx.r9.s64;
	// subf r31,r9,r10
	ctx.r31.s64 = ctx.r10.s64 - ctx.r9.s64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r30,r9,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r9.s64;
	// add r4,r7,r27
	ctx.r4.u64 = ctx.r7.u64 + ctx.r27.u64;
	// add r5,r6,r27
	ctx.r5.u64 = ctx.r6.u64 + ctx.r27.u64;
	// mr r10,r23
	ctx.r10.u64 = ctx.r23.u64;
	// add r8,r8,r27
	ctx.r8.u64 = ctx.r8.u64 + ctx.r27.u64;
	// add r6,r31,r27
	ctx.r6.u64 = ctx.r31.u64 + ctx.r27.u64;
	// add r7,r30,r27
	ctx.r7.u64 = ctx.r30.u64 + ctx.r27.u64;
loc_822D629C:
	// cmplw cr6,r29,r21
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r21.u32, ctx.xer);
	// bge cr6,0x822d6448
	if (!ctx.cr6.lt) goto loc_822D6448;
	// lfs f9,4(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// lfs f7,-4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	ctx.f7.f64 = double(temp.f32);
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// lfsux f12,r6,r9
	ea = ctx.r6.u32 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r6.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fsubs f6,f9,f7
	ctx.f6.f64 = static_cast<float>(ctx.f9.f64 - ctx.f7.f64);
	// fsubs f3,f9,f12
	ctx.f3.f64 = static_cast<float>(ctx.f9.f64 - ctx.f12.f64);
	// lfsux f13,r7,r9
	ea = ctx.r7.u32 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r7.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// lfs f5,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lfsux f11,r4,r9
	ea = ctx.r4.u32 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fsubs f4,f5,f13
	ctx.f4.f64 = static_cast<float>(ctx.f5.f64 - ctx.f13.f64);
	// lfsux f9,r5,r9
	ea = ctx.r5.u32 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r5.u32 = ea;
	ctx.f9.f64 = double(temp.f32);
	// fsubs f2,f9,f11
	ctx.f2.f64 = static_cast<float>(ctx.f9.f64 - ctx.f11.f64);
	// fmadds f1,f6,f0,f7
	ctx.f1.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f0.f64), float(ctx.f7.f64)));
	// fmadds f12,f3,f0,f12
	ctx.f12.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// fmadds f13,f4,f0,f13
	ctx.f13.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfsu f12,4(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fmadds f11,f2,f0,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fadds f0,f8,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
	// stfsu f1,4(r10)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fcmpu cr6,f0,f10
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// blt cr6,0x822d6328
	if (ctx.cr6.lt) goto loc_822D6328;
	// fsubs f0,f0,f10
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f10.f64);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// addi r6,r6,16
	ctx.r6.s64 = ctx.r6.s64 + 16;
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
loc_822D6328:
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// blt cr6,0x822d629c
	if (ctx.cr6.lt) goto loc_822D629C;
loc_822D6330:
	// cmplw cr6,r29,r21
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r21.u32, ctx.xer);
	// bge cr6,0x822d6448
	if (!ctx.cr6.lt) goto loc_822D6448;
	// addi r9,r11,3
	ctx.r9.s64 = ctx.r11.s64 + 3;
	// addi r8,r11,-2
	ctx.r8.s64 = ctx.r11.s64 + -2;
	// addi r7,r11,2
	ctx.r7.s64 = ctx.r11.s64 + 2;
	// addi r6,r11,-3
	ctx.r6.s64 = ctx.r11.s64 + -3;
	// addi r5,r11,-4
	ctx.r5.s64 = ctx.r11.s64 + -4;
	// rlwinm r4,r9,2,0,29
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r30,r7,2,0,29
	ctx.r30.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r6,2,0,29
	ctx.r25.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r24,r5,2,0,29
	ctx.r24.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r31,r11,64
	ctx.r31.s64 = ctx.r11.s64 + 64;
	// subf r6,r10,r4
	ctx.r6.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r5,r10,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r10.s64;
	// rlwinm r7,r31,2,0,29
	ctx.r7.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r10,r30
	ctx.r4.s64 = ctx.r30.s64 - ctx.r10.s64;
	// rlwinm r9,r29,2,0,29
	ctx.r9.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r25,r10,r25
	ctx.r25.s64 = ctx.r25.s64 - ctx.r10.s64;
	// subf r24,r10,r24
	ctx.r24.s64 = ctx.r24.s64 - ctx.r10.s64;
	// add r30,r6,r28
	ctx.r30.u64 = ctx.r6.u64 + ctx.r28.u64;
	// add r31,r5,r28
	ctx.r31.u64 = ctx.r5.u64 + ctx.r28.u64;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + ctx.r23.u64;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + ctx.r28.u64;
	// add r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 + ctx.r28.u64;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + ctx.r28.u64;
	// add r5,r25,r28
	ctx.r5.u64 = ctx.r25.u64 + ctx.r28.u64;
	// add r6,r24,r28
	ctx.r6.u64 = ctx.r24.u64 + ctx.r28.u64;
loc_822D63A8:
	// lfsux f13,r6,r10
	ctx.fpscr.disableFlushMode();
	ea = ctx.r6.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r6.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// lfsux f11,r31,r10
	ea = ctx.r31.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r31.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// lfsux f9,r4,r10
	ea = ctx.r4.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f6,f9,f11
	ctx.f6.f64 = static_cast<float>(ctx.f9.f64 - ctx.f11.f64);
	// lfsux f12,r5,r10
	ea = ctx.r5.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r5.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fsubs f5,f7,f13
	ctx.f5.f64 = static_cast<float>(ctx.f7.f64 - ctx.f13.f64);
	// lfs f4,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,-4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	ctx.f3.f64 = double(temp.f32);
	// fsubs f2,f4,f12
	ctx.f2.f64 = static_cast<float>(ctx.f4.f64 - ctx.f12.f64);
	// lfsux f9,r30,r10
	ea = ctx.r30.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r30.u32 = ea;
	ctx.f9.f64 = double(temp.f32);
	// fsubs f1,f9,f3
	ctx.f1.f64 = static_cast<float>(ctx.f9.f64 - ctx.f3.f64);
	// fmadds f11,f6,f0,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// fmadds f9,f5,f0,f13
	ctx.f9.f64 = double(std::fma(float(ctx.f5.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfs f9,0(r9)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f7,f2,f0,f12
	ctx.f7.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfsu f7,4(r9)
	temp.f32 = float(ctx.f7.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// stfsu f11,4(r9)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmadds f6,f1,f0,f3
	ctx.f6.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f0.f64), float(ctx.f3.f64)));
	// stfsu f6,4(r9)
	temp.f32 = float(ctx.f6.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// dcbt r0,r7
	// fadds f0,f8,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// fcmpu cr6,f0,f10
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// blt cr6,0x822d6440
	if (ctx.cr6.lt) goto loc_822D6440;
	// fsubs f0,f0,f10
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f10.f64);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r6,r6,16
	ctx.r6.s64 = ctx.r6.s64 + 16;
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// addi r31,r31,16
	ctx.r31.s64 = ctx.r31.s64 + 16;
	// addi r30,r30,16
	ctx.r30.s64 = ctx.r30.s64 + 16;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
loc_822D6440:
	// cmplw cr6,r29,r21
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r21.u32, ctx.xer);
	// blt cr6,0x822d63a8
	if (ctx.cr6.lt) goto loc_822D63A8;
loc_822D6448:
	// addi r10,r22,-4
	ctx.r10.s64 = ctx.r22.s64 + -4;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x822d6458
	if (!ctx.cr6.lt) goto loc_822D6458;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822D6458:
	// subf r10,r22,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r22.s64;
	// stfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// neg r8,r22
	ctx.r8.s64 = -ctx.r22.s64;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r10,-4
	ctx.r11.s64 = ctx.r10.s64 + -4;
	// stw r9,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r9.u32);
	// stw r10,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r10.u32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d6528
	if (!ctx.cr6.lt) goto loc_822D6528;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d64ec
	if (ctx.cr6.lt) goto loc_822D64EC;
	// add r10,r11,r22
	ctx.r10.u64 = ctx.r11.u64 + ctx.r22.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r9,r27
	ctx.r10.u64 = ctx.r9.u64 + ctx.r27.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r22,2
	ctx.r7.s64 = ctx.r22.s64 + 2;
	// addi r6,r22,3
	ctx.r6.s64 = ctx.r22.s64 + 3;
	// addi r5,r8,-3
	ctx.r5.s64 = ctx.r8.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + ctx.r27.u64;
loc_822D64B0:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r4,r7,r11
	ctx.r4.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r3,r6,r11
	ctx.r3.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r4,r27
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r27.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f12,r3,r27
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r27.u32);
	ctx.f12.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d64b0
	if (ctx.cr6.lt) goto loc_822D64B0;
loc_822D64EC:
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d6528
	if (!ctx.cr6.lt) goto loc_822D6528;
	// add r7,r11,r22
	ctx.r7.u64 = ctx.r11.u64 + ctx.r22.u64;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + ctx.r27.u64;
	// add r8,r8,r27
	ctx.r8.u64 = ctx.r8.u64 + ctx.r27.u64;
	// addi r7,r9,-4
	ctx.r7.s64 = ctx.r9.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_822D651C:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d651c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D651C;
loc_822D6528:
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d65a0
	if (ctx.cr6.lt) goto loc_822D65A0;
	// subfic r10,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r10.s64 = -4 - ctx.r11.s64;
	// add r8,r11,r22
	ctx.r8.u64 = ctx.r11.u64 + ctx.r22.u64;
	// rlwinm r9,r10,30,2,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r8,r8,-3
	ctx.r8.s64 = ctx.r8.s64 + -3;
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + ctx.r27.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r22,2
	ctx.r7.s64 = ctx.r22.s64 + 2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r6,r22,3
	ctx.r6.s64 = ctx.r22.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r8,r28
	ctx.r9.u64 = ctx.r8.u64 + ctx.r28.u64;
loc_822D6568:
	// add r8,r7,r11
	ctx.r8.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lfs f13,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// add r5,r6,r11
	ctx.r5.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r4,r8,2,0,29
	ctx.r4.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// rlwinm r3,r5,2,0,29
	ctx.r3.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lfsx f12,r4,r28
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r28.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfsx f11,r3,r28
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r28.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfs f12,12(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfsu f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d6568
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D6568;
loc_822D65A0:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822d65c8
	if (!ctx.cr6.lt) goto loc_822D65C8;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r22
	ctx.r10.u64 = ctx.r11.u64 + ctx.r22.u64;
	// add r3,r9,r27
	ctx.r3.u64 = ctx.r9.u64 + ctx.r27.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r9,r11
	ctx.r9.s64 = -ctx.r11.s64;
	// add r4,r10,r28
	ctx.r4.u64 = ctx.r10.u64 + ctx.r28.u64;
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233e968
	ctx.lr = 0x822D65C8;
	sub_8233E968(ctx, base);
loc_822D65C8:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8233e49c
	__restgprlr_21(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D65D0"))) PPC_WEAK_FUNC(sub_822D65D0);
PPC_FUNC_IMPL(__imp__sub_822D65D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x822D65D8;
	__restfpr_14(ctx, base);
	// stfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.f31.u64);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// li r23,0
	ctx.r23.s64 = 0;
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r17,24(r3)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r7,r11,48
	ctx.r7.s64 = ctx.r11.s64 + 48;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lfs f13,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// fctiwz f12,f13
	ctx.f12.u64 = uint64_t(int32_t(std::trunc(ctx.f13.f64)));
	// stfd f12,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f12.u64);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r5,r10
	ctx.r5.s64 = ctx.r10.s32;
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// add r4,r10,r9
	ctx.r4.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lwz r18,4(r3)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r17,0
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 0, ctx.xer);
	// lwz r16,28(r3)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// rlwinm r22,r4,1,0,30
	ctx.r22.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// fsubs f5,f13,f9
	ctx.f5.f64 = static_cast<float>(ctx.f13.f64 - ctx.f9.f64);
	// beq cr6,0x822d6680
	if (ctx.cr6.eq) goto loc_822D6680;
	// lfs f13,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,12(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,16(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,20(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// stfs f13,0(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// stfs f12,4(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// stfs f11,8(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 8, temp.u32);
	// stfs f10,12(r7)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r7.u32 + 12, temp.u32);
	// stfs f9,16(r7)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r7.u32 + 16, temp.u32);
	// stfs f8,20(r7)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r7.u32 + 20, temp.u32);
loc_822D6680:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// lfs f8,5256(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f8.f64 = double(temp.f32);
	// bge cr6,0x822d6804
	if (!ctx.cr6.lt) goto loc_822D6804;
	// addi r31,r11,3
	ctx.r31.s64 = ctx.r11.s64 + 3;
	// addi r30,r11,-4
	ctx.r30.s64 = ctx.r11.s64 + -4;
	// addi r5,r11,4
	ctx.r5.s64 = ctx.r11.s64 + 4;
	// addi r4,r11,-3
	ctx.r4.s64 = ctx.r11.s64 + -3;
	// addi r29,r11,2
	ctx.r29.s64 = ctx.r11.s64 + 2;
	// addi r9,r11,5
	ctx.r9.s64 = ctx.r11.s64 + 5;
	// addi r8,r11,-2
	ctx.r8.s64 = ctx.r11.s64 + -2;
	// rlwinm r10,r22,2,0,29
	ctx.r10.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r24,r31,2,0,29
	ctx.r24.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r28,r11,-5
	ctx.r28.s64 = ctx.r11.s64 + -5;
	// addi r27,r11,-6
	ctx.r27.s64 = ctx.r11.s64 + -6;
	// rlwinm r21,r30,2,0,29
	ctx.r21.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r26,r5,2,0,29
	ctx.r26.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r4,2,0,29
	ctx.r25.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r20,r29,2,0,29
	ctx.r20.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r10,r24
	ctx.r29.s64 = ctx.r24.s64 - ctx.r10.s64;
	// subf r24,r10,r21
	ctx.r24.s64 = ctx.r21.s64 - ctx.r10.s64;
	// subf r4,r10,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r5,r10,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r10.s64;
	// subf r31,r10,r26
	ctx.r31.s64 = ctx.r26.s64 - ctx.r10.s64;
	// subf r30,r10,r25
	ctx.r30.s64 = ctx.r25.s64 - ctx.r10.s64;
	// subf r21,r10,r20
	ctx.r21.s64 = ctx.r20.s64 - ctx.r10.s64;
	// subf r20,r10,r28
	ctx.r20.s64 = ctx.r28.s64 - ctx.r10.s64;
	// subf r19,r10,r27
	ctx.r19.s64 = ctx.r27.s64 - ctx.r10.s64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r25,r5,r7
	ctx.r25.u64 = ctx.r5.u64 + ctx.r7.u64;
	// add r26,r4,r7
	ctx.r26.u64 = ctx.r4.u64 + ctx.r7.u64;
	// add r27,r31,r7
	ctx.r27.u64 = ctx.r31.u64 + ctx.r7.u64;
	// add r28,r30,r7
	ctx.r28.u64 = ctx.r30.u64 + ctx.r7.u64;
	// mr r9,r18
	ctx.r9.u64 = ctx.r18.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r29,r29,r7
	ctx.r29.u64 = ctx.r29.u64 + ctx.r7.u64;
	// add r30,r24,r7
	ctx.r30.u64 = ctx.r24.u64 + ctx.r7.u64;
	// add r31,r21,r7
	ctx.r31.u64 = ctx.r21.u64 + ctx.r7.u64;
	// add r4,r20,r7
	ctx.r4.u64 = ctx.r20.u64 + ctx.r7.u64;
	// add r5,r19,r7
	ctx.r5.u64 = ctx.r19.u64 + ctx.r7.u64;
loc_822D6730:
	// cmplw cr6,r23,r16
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, ctx.r16.u32, ctx.xer);
	// bge cr6,0x822d6994
	if (!ctx.cr6.lt) goto loc_822D6994;
	// lfsux f11,r30,r10
	ctx.fpscr.disableFlushMode();
	ea = ctx.r30.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r30.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// addi r23,r23,6
	ctx.r23.s64 = ctx.r23.s64 + 6;
	// lfsux f7,r31,r10
	ea = ctx.r31.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r31.u32 = ea;
	ctx.f7.f64 = double(temp.f32);
	// add r11,r22,r11
	ctx.r11.u64 = ctx.r22.u64 + ctx.r11.u64;
	// fsubs f4,f7,f11
	ctx.f4.f64 = static_cast<float>(ctx.f7.f64 - ctx.f11.f64);
	// lfsux f12,r4,r10
	ea = ctx.r4.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// lfs f7,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfsux f10,r28,r10
	ea = ctx.r28.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r28.u32 = ea;
	ctx.f10.f64 = double(temp.f32);
	// fsubs f31,f7,f12
	ctx.f31.f64 = static_cast<float>(ctx.f7.f64 - ctx.f12.f64);
	// lfsux f6,r29,r10
	ea = ctx.r29.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r29.u32 = ea;
	ctx.f6.f64 = double(temp.f32);
	// lfsux f13,r5,r10
	ea = ctx.r5.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r5.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fsubs f3,f6,f10
	ctx.f3.f64 = static_cast<float>(ctx.f6.f64 - ctx.f10.f64);
	// lfs f1,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,-4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	ctx.f2.f64 = double(temp.f32);
	// fsubs f1,f1,f13
	ctx.f1.f64 = static_cast<float>(ctx.f1.f64 - ctx.f13.f64);
	// lfsux f9,r26,r10
	ea = ctx.r26.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r26.u32 = ea;
	ctx.f9.f64 = double(temp.f32);
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lfsux f7,r27,r10
	ea = ctx.r27.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r27.u32 = ea;
	ctx.f7.f64 = double(temp.f32);
	// lfsux f6,r25,r10
	ea = ctx.r25.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r25.u32 = ea;
	ctx.f6.f64 = double(temp.f32);
	// fsubs f7,f7,f9
	ctx.f7.f64 = static_cast<float>(ctx.f7.f64 - ctx.f9.f64);
	// fsubs f6,f6,f2
	ctx.f6.f64 = static_cast<float>(ctx.f6.f64 - ctx.f2.f64);
	// fmadds f4,f4,f0,f11
	ctx.f4.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// fmadds f3,f3,f0,f10
	ctx.f3.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f10.f64)));
	// fmadds f1,f1,f0,f13
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfs f1,0(r9)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f13,f31,f0,f12
	ctx.f13.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfsu f13,4(r9)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// stfsu f4,4(r9)
	temp.f32 = float(ctx.f4.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmadds f12,f7,f0,f9
	ctx.f12.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f0.f64), float(ctx.f9.f64)));
	// stfsu f3,4(r9)
	temp.f32 = float(ctx.f3.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmadds f11,f6,f0,f2
	ctx.f11.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f0.f64), float(ctx.f2.f64)));
	// stfsu f12,4(r9)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fadds f0,f5,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 + ctx.f0.f64));
	// stfsu f11,4(r9)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fcmpu cr6,f0,f8
	ctx.cr6.compare(ctx.f0.f64, ctx.f8.f64);
	// blt cr6,0x822d67fc
	if (ctx.cr6.lt) goto loc_822D67FC;
	// fsubs f0,f0,f8
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f8.f64);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// addi r5,r5,24
	ctx.r5.s64 = ctx.r5.s64 + 24;
	// addi r4,r4,24
	ctx.r4.s64 = ctx.r4.s64 + 24;
	// addi r31,r31,24
	ctx.r31.s64 = ctx.r31.s64 + 24;
	// addi r30,r30,24
	ctx.r30.s64 = ctx.r30.s64 + 24;
	// addi r29,r29,24
	ctx.r29.s64 = ctx.r29.s64 + 24;
	// addi r28,r28,24
	ctx.r28.s64 = ctx.r28.s64 + 24;
	// addi r27,r27,24
	ctx.r27.s64 = ctx.r27.s64 + 24;
	// addi r26,r26,24
	ctx.r26.s64 = ctx.r26.s64 + 24;
	// addi r25,r25,24
	ctx.r25.s64 = ctx.r25.s64 + 24;
	// addi r8,r8,24
	ctx.r8.s64 = ctx.r8.s64 + 24;
loc_822D67FC:
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// blt cr6,0x822d6730
	if (ctx.cr6.lt) goto loc_822D6730;
loc_822D6804:
	// cmplw cr6,r23,r16
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, ctx.r16.u32, ctx.xer);
	// bge cr6,0x822d6994
	if (!ctx.cr6.lt) goto loc_822D6994;
	// addi r9,r11,5
	ctx.r9.s64 = ctx.r11.s64 + 5;
	// addi r8,r11,-2
	ctx.r8.s64 = ctx.r11.s64 + -2;
	// addi r5,r11,4
	ctx.r5.s64 = ctx.r11.s64 + 4;
	// addi r4,r11,-3
	ctx.r4.s64 = ctx.r11.s64 + -3;
	// addi r31,r11,3
	ctx.r31.s64 = ctx.r11.s64 + 3;
	// addi r30,r11,-4
	ctx.r30.s64 = ctx.r11.s64 + -4;
	// addi r29,r11,2
	ctx.r29.s64 = ctx.r11.s64 + 2;
	// addi r28,r11,-5
	ctx.r28.s64 = ctx.r11.s64 + -5;
	// addi r27,r11,-6
	ctx.r27.s64 = ctx.r11.s64 + -6;
	// rlwinm r10,r22,2,0,29
	ctx.r10.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r31,r31,2,0,29
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r30,2,0,29
	ctx.r25.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r24,r29,2,0,29
	ctx.r24.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r15,r28,2,0,29
	ctx.r15.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r14,r27,2,0,29
	ctx.r14.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r26,r11,64
	ctx.r26.s64 = ctx.r11.s64 + 64;
	// subf r30,r10,r9
	ctx.r30.s64 = ctx.r9.s64 - ctx.r10.s64;
	// subf r29,r10,r8
	ctx.r29.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r28,r10,r5
	ctx.r28.s64 = ctx.r5.s64 - ctx.r10.s64;
	// subf r27,r10,r4
	ctx.r27.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r21,r10,r31
	ctx.r21.s64 = ctx.r31.s64 - ctx.r10.s64;
	// rlwinm r5,r26,2,0,29
	ctx.r5.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r20,r10,r25
	ctx.r20.s64 = ctx.r25.s64 - ctx.r10.s64;
	// subf r19,r10,r24
	ctx.r19.s64 = ctx.r24.s64 - ctx.r10.s64;
	// rlwinm r9,r23,2,0,29
	ctx.r9.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r31,r10,r15
	ctx.r31.s64 = ctx.r15.s64 - ctx.r10.s64;
	// subf r4,r10,r14
	ctx.r4.s64 = ctx.r14.s64 - ctx.r10.s64;
	// add r24,r30,r6
	ctx.r24.u64 = ctx.r30.u64 + ctx.r6.u64;
	// add r25,r29,r6
	ctx.r25.u64 = ctx.r29.u64 + ctx.r6.u64;
	// add r26,r28,r6
	ctx.r26.u64 = ctx.r28.u64 + ctx.r6.u64;
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + ctx.r18.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// add r27,r27,r6
	ctx.r27.u64 = ctx.r27.u64 + ctx.r6.u64;
	// add r28,r21,r6
	ctx.r28.u64 = ctx.r21.u64 + ctx.r6.u64;
	// add r29,r20,r6
	ctx.r29.u64 = ctx.r20.u64 + ctx.r6.u64;
	// add r30,r19,r6
	ctx.r30.u64 = ctx.r19.u64 + ctx.r6.u64;
	// add r31,r31,r6
	ctx.r31.u64 = ctx.r31.u64 + ctx.r6.u64;
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
loc_822D68BC:
	// lfsux f11,r29,r10
	ctx.fpscr.disableFlushMode();
	ea = ctx.r29.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r29.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// addi r23,r23,6
	ctx.r23.s64 = ctx.r23.s64 + 6;
	// lfsux f9,r30,r10
	ea = ctx.r30.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r30.u32 = ea;
	ctx.f9.f64 = double(temp.f32);
	// lfsux f10,r27,r10
	ea = ctx.r27.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r27.u32 = ea;
	ctx.f10.f64 = double(temp.f32);
	// fsubs f6,f9,f11
	ctx.f6.f64 = static_cast<float>(ctx.f9.f64 - ctx.f11.f64);
	// lfsux f7,r28,r10
	ea = ctx.r28.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r28.u32 = ea;
	ctx.f7.f64 = double(temp.f32);
	// lfsux f13,r4,r10
	ea = ctx.r4.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fsubs f4,f7,f10
	ctx.f4.f64 = static_cast<float>(ctx.f7.f64 - ctx.f10.f64);
	// lfs f3,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// lfsux f12,r31,r10
	ea = ctx.r31.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r31.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fsubs f2,f3,f13
	ctx.f2.f64 = static_cast<float>(ctx.f3.f64 - ctx.f13.f64);
	// lfs f1,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f1.f64 = double(temp.f32);
	// lfsux f9,r25,r10
	ea = ctx.r25.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r25.u32 = ea;
	ctx.f9.f64 = double(temp.f32);
	// fsubs f3,f1,f12
	ctx.f3.f64 = static_cast<float>(ctx.f1.f64 - ctx.f12.f64);
	// lfsux f7,r26,r10
	ea = ctx.r26.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r26.u32 = ea;
	ctx.f7.f64 = double(temp.f32);
	// fsubs f1,f7,f9
	ctx.f1.f64 = static_cast<float>(ctx.f7.f64 - ctx.f9.f64);
	// lfs f31,-4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	ctx.f31.f64 = double(temp.f32);
	// lfsux f7,r24,r10
	ea = ctx.r24.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r24.u32 = ea;
	ctx.f7.f64 = double(temp.f32);
	// fsubs f7,f7,f31
	ctx.f7.f64 = static_cast<float>(ctx.f7.f64 - ctx.f31.f64);
	// fmadds f6,f6,f0,f11
	ctx.f6.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// fmadds f4,f4,f0,f10
	ctx.f4.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f0.f64), float(ctx.f10.f64)));
	// fmadds f2,f2,f0,f13
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfs f2,0(r9)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f13,f3,f0,f12
	ctx.f13.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfsu f13,4(r9)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// stfsu f6,4(r9)
	temp.f32 = float(ctx.f6.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// stfsu f4,4(r9)
	temp.f32 = float(ctx.f4.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmadds f12,f1,f0,f9
	ctx.f12.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f0.f64), float(ctx.f9.f64)));
	// stfsu f12,4(r9)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmadds f11,f7,f0,f31
	ctx.f11.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// stfsu f11,4(r9)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// dcbt r0,r5
	// fadds f0,f5,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 + ctx.f0.f64));
	// add r11,r22,r11
	ctx.r11.u64 = ctx.r22.u64 + ctx.r11.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// fcmpu cr6,f0,f8
	ctx.cr6.compare(ctx.f0.f64, ctx.f8.f64);
	// blt cr6,0x822d698c
	if (ctx.cr6.lt) goto loc_822D698C;
	// fsubs f0,f0,f8
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f8.f64);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// addi r4,r4,24
	ctx.r4.s64 = ctx.r4.s64 + 24;
	// addi r31,r31,24
	ctx.r31.s64 = ctx.r31.s64 + 24;
	// addi r30,r30,24
	ctx.r30.s64 = ctx.r30.s64 + 24;
	// addi r29,r29,24
	ctx.r29.s64 = ctx.r29.s64 + 24;
	// addi r28,r28,24
	ctx.r28.s64 = ctx.r28.s64 + 24;
	// addi r27,r27,24
	ctx.r27.s64 = ctx.r27.s64 + 24;
	// addi r26,r26,24
	ctx.r26.s64 = ctx.r26.s64 + 24;
	// addi r25,r25,24
	ctx.r25.s64 = ctx.r25.s64 + 24;
	// addi r24,r24,24
	ctx.r24.s64 = ctx.r24.s64 + 24;
	// addi r8,r8,24
	ctx.r8.s64 = ctx.r8.s64 + 24;
	// addi r5,r5,24
	ctx.r5.s64 = ctx.r5.s64 + 24;
loc_822D698C:
	// cmplw cr6,r23,r16
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, ctx.r16.u32, ctx.xer);
	// blt cr6,0x822d68bc
	if (ctx.cr6.lt) goto loc_822D68BC;
loc_822D6994:
	// addi r10,r17,-6
	ctx.r10.s64 = ctx.r17.s64 + -6;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x822d69a4
	if (!ctx.cr6.lt) goto loc_822D69A4;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822D69A4:
	// subf r10,r17,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r17.s64;
	// stfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// neg r8,r17
	ctx.r8.s64 = -ctx.r17.s64;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r10,-6
	ctx.r11.s64 = ctx.r10.s64 + -6;
	// stw r9,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r9.u32);
	// stw r10,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r10.u32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d6a74
	if (!ctx.cr6.lt) goto loc_822D6A74;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d6a38
	if (ctx.cr6.lt) goto loc_822D6A38;
	// add r9,r11,r17
	ctx.r9.u64 = ctx.r11.u64 + ctx.r17.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,-3
	ctx.r9.s64 = ctx.r9.s64 + -3;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r17,2
	ctx.r5.s64 = ctx.r17.s64 + 2;
	// addi r4,r17,3
	ctx.r4.s64 = ctx.r17.s64 + 3;
	// addi r3,r8,-3
	ctx.r3.s64 = ctx.r8.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
loc_822D69FC:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r31,r5,r11
	ctx.r31.u64 = ctx.r5.u64 + ctx.r11.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r30,r4,r11
	ctx.r30.u64 = ctx.r4.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r31,r31,2,0,29
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// rlwinm r30,r30,2,0,29
	ctx.r30.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r31,r7
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r7.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f12,r30,r7
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r7.u32);
	ctx.f12.f64 = double(temp.f32);
	// cmpw cr6,r11,r3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r3.s32, ctx.xer);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d69fc
	if (ctx.cr6.lt) goto loc_822D69FC;
loc_822D6A38:
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d6a74
	if (!ctx.cr6.lt) goto loc_822D6A74;
	// add r5,r11,r17
	ctx.r5.u64 = ctx.r11.u64 + ctx.r17.u64;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r5,r9,-4
	ctx.r5.s64 = ctx.r9.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_822D6A68:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r5)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r5.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r5.u32 = ea;
	// bdnz 0x822d6a68
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D6A68;
loc_822D6A74:
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d6aec
	if (ctx.cr6.lt) goto loc_822D6AEC;
	// subfic r10,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r10.s64 = -4 - ctx.r11.s64;
	// add r8,r11,r17
	ctx.r8.u64 = ctx.r11.u64 + ctx.r17.u64;
	// rlwinm r9,r10,30,2,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r8,r8,-3
	ctx.r8.s64 = ctx.r8.s64 + -3;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r17,2
	ctx.r5.s64 = ctx.r17.s64 + 2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r4,r17,3
	ctx.r4.s64 = ctx.r17.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r8,r6
	ctx.r9.u64 = ctx.r8.u64 + ctx.r6.u64;
loc_822D6AB4:
	// add r8,r5,r11
	ctx.r8.u64 = ctx.r5.u64 + ctx.r11.u64;
	// lfs f13,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// add r3,r4,r11
	ctx.r3.u64 = ctx.r4.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lfsx f12,r8,r6
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r6.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfsx f11,r3,r6
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r6.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfs f12,12(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfsu f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d6ab4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D6AB4;
loc_822D6AEC:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822d6b14
	if (!ctx.cr6.lt) goto loc_822D6B14;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r17
	ctx.r10.u64 = ctx.r11.u64 + ctx.r17.u64;
	// add r3,r9,r7
	ctx.r3.u64 = ctx.r9.u64 + ctx.r7.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r9,r11
	ctx.r9.s64 = -ctx.r11.s64;
	// add r4,r10,r6
	ctx.r4.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233e968
	ctx.lr = 0x822D6B14;
	sub_8233E968(ctx, base);
loc_822D6B14:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D6B20"))) PPC_WEAK_FUNC(sub_822D6B20);
PPC_FUNC_IMPL(__imp__sub_822D6B20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e448
	ctx.lr = 0x822D6B28;
	__restfpr_20(ctx, base);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r23,r6
	ctx.r23.u64 = ctx.r6.u64;
	// mr r22,r3
	ctx.r22.u64 = ctx.r3.u64;
	// dcbt r0,r3
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f13,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// li r11,4
	ctx.r11.s64 = 4;
	// vspltisw128 v63,0
	simd::store_i32(ctx.v63.u32, simd::set1_i32(int32_t(0x0)));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// rlwinm r9,r5,0,0,27
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFF0;
	// lfd f0,-1464(r10)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + -1464);
	// li r7,16
	ctx.r7.s64 = 16;
	// fmul f12,f1,f0
	ctx.f12.f64 = ctx.f1.f64 * ctx.f0.f64;
	// rlwinm r25,r5,0,28,29
	ctx.r25.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xC;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// clrlwi r24,r5,30
	ctx.r24.u64 = ctx.r5.u32 & 0x3;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f10.u64);
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fctidz f9,f11
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// stfd f9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f9.u64);
	// lvlx128 v57,r11,r8
	temp.u32 = ctx.r11.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r3,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r3.u64);
	// rldicr r27,r10,3,60
	ctx.r27.u64 = rotl64(ctx.r10.u64, 3) & 0xFFFFFFFFFFFFFFF8;
	// vspltw128 v31,v57,0
	simd::store_i32(ctx.v31.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v57.u32), 3));
	// lvlx128 v56,r11,r6
	temp.u32 = ctx.r11.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v8,v56,0
	simd::store_i32(ctx.v8.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v56.u32), 3));
	// vadduwm v0,v31,v31
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v31.u32), simd::load_u32(ctx.v31.u32)));
	// vsldoi128 v55,v63,v31,4
	simd::store_i8(ctx.v55.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v31.u8), 12));
	// vadduwm v9,v31,v0
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v31.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v54,v55,v0,4
	simd::store_i8(ctx.v54.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v0.u8), 12));
	// vadduwm v30,v0,v0
	simd::store_u32(ctx.v30.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v0,v54,v9,4
	simd::store_i8(ctx.v0.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v9.u8), 12));
	// vadduwm v6,v30,v30
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v30.u32), simd::load_u32(ctx.v30.u32)));
	// vadduwm v7,v0,v30
	simd::store_u32(ctx.v7.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v30.u32)));
	// vadduwm v0,v0,v8
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v8.u32)));
	// vadduwm v9,v6,v6
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v6.u32)));
	// vadduwm v8,v7,v8
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v7.u32), simd::load_u32(ctx.v8.u32)));
	// vadduwm v7,v0,v6
	simd::store_u32(ctx.v7.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v6.u32)));
	// vadduwm v6,v8,v6
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v6.u32)));
	// beq cr6,0x822d6eac
	if (ctx.cr6.eq) goto loc_822D6EAC;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// vspltisw128 v63,1
	simd::store_i32(ctx.v63.u32, simd::set1_i32(int32_t(0x1)));
	// addi r8,r4,32
	ctx.r8.s64 = ctx.r4.s64 + 32;
	// rlwinm r9,r9,28,4,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 28) & 0xFFFFFFF;
	// li r26,256
	ctx.r26.s64 = 256;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822D6BFC:
	// add r9,r27,r3
	ctx.r9.u64 = ctx.r27.u64 + ctx.r3.u64;
	// rldicl r30,r3,32,32
	ctx.r30.u64 = rotl64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// clrldi r5,r9,32
	ctx.r5.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// rldicl r9,r9,32,32
	ctx.r9.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// add r6,r10,r3
	ctx.r6.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rlwinm r3,r30,2,0,29
	ctx.r3.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rldicl r29,r5,32,32
	ctx.r29.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + ctx.r31.u64;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + ctx.r31.u64;
	// rldicl r28,r5,32,32
	ctx.r28.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// rlwinm r30,r29,2,0,29
	ctx.r30.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r29,r28,2,0,29
	ctx.r29.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r30,r9
	ctx.r30.u64 = ctx.r30.u64 + ctx.r9.u64;
	// lvlx128 v53,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v52,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r3,r29,r9
	ctx.r3.u64 = ctx.r29.u64 + ctx.r9.u64;
	// rldicl r28,r6,32,32
	ctx.r28.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v51,v12,v53,4
	simd::store_i8(ctx.v51.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v53.u8), 12));
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// vsldoi128 v50,v61,v52,4
	simd::store_i8(ctx.v50.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v52.u8), 12));
	// lvlx128 v49,r29,r9
	temp.u32 = ctx.r29.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// lvlx128 v48,r0,r30
	temp.u32 = ctx.r0.u32 + ctx.r30.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rldicl r29,r5,32,32
	ctx.r29.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v47,r30,r11
	temp.u32 = ctx.r30.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r30,r28,2,0,29
	ctx.r30.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v46,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// add r3,r30,r31
	ctx.r3.u64 = ctx.r30.u64 + ctx.r31.u64;
	// vsldoi128 v45,v13,v48,4
	simd::store_i8(ctx.v45.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v13.u8), simd::load_i8(ctx.v48.u8), 12));
	// rldicl r30,r5,32,32
	ctx.r30.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v44,v62,v47,4
	simd::store_i8(ctx.v44.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v47.u8), 12));
	// rldicl r28,r6,32,32
	ctx.r28.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFF;
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r29,r29,2,0,29
	ctx.r29.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v43,v45,v49,4
	simd::store_i8(ctx.v43.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v45.u8), simd::load_i8(ctx.v49.u8), 12));
	// lvlx128 v42,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rldicl r21,r6,32,32
	ctx.r21.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v41,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r3,r30,2,0,29
	ctx.r3.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r29,r9
	ctx.r30.u64 = ctx.r29.u64 + ctx.r9.u64;
	// vsldoi128 v40,v44,v46,4
	simd::store_i8(ctx.v40.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v44.u8), simd::load_i8(ctx.v46.u8), 12));
	// add r3,r3,r9
	ctx.r3.u64 = ctx.r3.u64 + ctx.r9.u64;
	// vsldoi128 v39,v51,v42,4
	simd::store_i8(ctx.v39.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v42.u8), 12));
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// vsldoi128 v38,v50,v41,4
	simd::store_i8(ctx.v38.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v41.u8), 12));
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lvlx128 v37,r29,r9
	temp.u32 = ctx.r29.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v37,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rldicl r20,r6,32,32
	ctx.r20.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v36,r30,r11
	temp.u32 = ctx.r30.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rldicl r30,r5,32,32
	ctx.r30.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v35,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// lvlx128 v34,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v34,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r3,r28,2,0,29
	ctx.r3.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v33,v43,v37,4
	simd::store_i8(ctx.v33.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v43.u8), simd::load_i8(ctx.v37.u8), 12));
	// rldicl r29,r6,32,32
	ctx.r29.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFF;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + ctx.r31.u64;
	// vsldoi128 v32,v40,v36,4
	simd::store_i8(ctx.v32.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v40.u8), simd::load_i8(ctx.v36.u8), 12));
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// rlwinm r30,r30,2,0,29
	ctx.r30.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v13,v33,v35,4
	simd::store_i8(ctx.v13.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v33.u8), simd::load_i8(ctx.v35.u8), 12));
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// vsldoi128 v62,v32,v34,4
	simd::store_i8(ctx.v62.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v32.u8), simd::load_i8(ctx.v34.u8), 12));
	// add r30,r30,r9
	ctx.r30.u64 = ctx.r30.u64 + ctx.r9.u64;
	// lvlx128 v61,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rldicl r28,r5,32,32
	ctx.r28.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v57,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r3,r21,2,0,29
	ctx.r3.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v56,v39,v61,4
	simd::store_i8(ctx.v56.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v39.u8), simd::load_i8(ctx.v61.u8), 12));
	// rlwinm r29,r29,2,0,29
	ctx.r29.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + ctx.r31.u64;
	// vsldoi128 v55,v38,v57,4
	simd::store_i8(ctx.v55.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v38.u8), simd::load_i8(ctx.v57.u8), 12));
	// lvlx128 v54,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r3,r20,2,0,29
	ctx.r3.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v12,v56,v54,4
	simd::store_i8(ctx.v12.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v54.u8), 12));
	// vsldoi128 v61,v55,v53,4
	simd::store_i8(ctx.v61.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v53.u8), 12));
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + ctx.r31.u64;
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v52,r0,r30
	temp.u32 = ctx.r0.u32 + ctx.r30.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v51,r30,r11
	temp.u32 = ctx.r30.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// add r30,r28,r9
	ctx.r30.u64 = ctx.r28.u64 + ctx.r9.u64;
	// lvlx128 v50,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v49,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rldicl r21,r6,32,32
	ctx.r21.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v48,r29,r31
	temp.u32 = ctx.r29.u32 + ctx.r31.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r3,r29,r31
	ctx.r3.u64 = ctx.r29.u64 + ctx.r31.u64;
	// rldicl r29,r5,32,32
	ctx.r29.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v41,r28,r9
	temp.u32 = ctx.r28.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// vsldoi128 v44,v11,v52,4
	simd::store_i8(ctx.v44.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v11.u8), simd::load_i8(ctx.v52.u8), 12));
	// lvlx128 v45,r30,r11
	temp.u32 = ctx.r30.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r30,r10,r6
	ctx.r30.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r6,r21,2,0,29
	ctx.r6.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v46,v59,v49,4
	simd::store_i8(ctx.v46.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v49.u8), 12));
	// rlwinm r29,r29,2,0,29
	ctx.r29.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v43,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + ctx.r31.u64;
	// vsldoi128 v42,v60,v51,4
	simd::store_i8(ctx.v42.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v51.u8), 12));
	// add r3,r29,r9
	ctx.r3.u64 = ctx.r29.u64 + ctx.r9.u64;
	// vsldoi128 v47,v10,v50,4
	simd::store_i8(ctx.v47.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v10.u8), simd::load_i8(ctx.v50.u8), 12));
	// rldicl r28,r5,32,32
	ctx.r28.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v36,v44,v41,4
	simd::store_i8(ctx.v36.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v44.u8), simd::load_i8(ctx.v41.u8), 12));
	// rldicl r30,r30,32,32
	ctx.r30.u64 = rotl64(ctx.r30.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v39,v46,v43,4
	simd::store_i8(ctx.v39.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v46.u8), simd::load_i8(ctx.v43.u8), 12));
	// lvlx128 v38,r29,r9
	temp.u32 = ctx.r29.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v38,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r29,r28,2,0,29
	ctx.r29.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v37,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v37,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v40,v47,v48,4
	simd::store_i8(ctx.v40.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v47.u8), simd::load_i8(ctx.v48.u8), 12));
	// lvlx128 v35,r6,r11
	temp.u32 = ctx.r6.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r6,r30,2,0,29
	ctx.r6.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v34,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v34,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r3,r29,r9
	ctx.r3.u64 = ctx.r29.u64 + ctx.r9.u64;
	// vsldoi128 v33,v42,v45,4
	simd::store_i8(ctx.v33.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v42.u8), simd::load_i8(ctx.v45.u8), 12));
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + ctx.r31.u64;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// vsldoi128 v60,v36,v38,4
	simd::store_i8(ctx.v60.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v36.u8), simd::load_i8(ctx.v38.u8), 12));
	// vsldoi128 v32,v40,v37,4
	simd::store_i8(ctx.v32.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v40.u8), simd::load_i8(ctx.v37.u8), 12));
	// rldicl r31,r5,32,32
	ctx.r31.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v59,v39,v35,4
	simd::store_i8(ctx.v59.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v39.u8), simd::load_i8(ctx.v35.u8), 12));
	// vsldoi128 v57,v33,v34,4
	simd::store_i8(ctx.v57.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v33.u8), simd::load_i8(ctx.v34.u8), 12));
	// lvlx128 v56,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v55,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r29,r31,2,0,29
	ctx.r29.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v54,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v11,v60,v56,4
	simd::store_i8(ctx.v11.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v56.u8), 12));
	// lvlx128 v53,r6,r11
	temp.u32 = ctx.r6.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// clrldi r3,r5,32
	ctx.r3.u64 = ctx.r5.u64 & 0xFFFFFFFF;
	// vsldoi128 v60,v57,v55,4
	simd::store_i8(ctx.v60.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v55.u8), 12));
	// add r31,r29,r9
	ctx.r31.u64 = ctx.r29.u64 + ctx.r9.u64;
	// vsldoi128 v10,v32,v54,4
	simd::store_i8(ctx.v10.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v32.u8), simd::load_i8(ctx.v54.u8), 12));
	// vsldoi128 v59,v59,v53,4
	simd::store_i8(ctx.v59.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v53.u8), 12));
	// dcbt r31,r26
	// vsrw128 v52,v0,v63
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(ctx.v52, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// vsubfp128 v5,v61,v12
	simd::store_f32_aligned(ctx.v5.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v61.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsrw128 v51,v8,v63
simd::store_shuffled(ctx.v51, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v8), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// vsubfp128 v3,v59,v10
	simd::store_f32_aligned(ctx.v3.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v59.f32), simd::load_f32_aligned(ctx.v10.f32)));
	// vsrw128 v50,v7,v63
simd::store_shuffled(ctx.v50, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v7), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// vsubfp128 v1,v60,v11
	simd::store_f32_aligned(ctx.v1.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v60.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// vsrw128 v49,v6,v63
simd::store_shuffled(ctx.v49, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v6), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// addi r9,r8,-16
	ctx.r9.s64 = ctx.r8.s64 + -16;
	// vcuxwfp128 v4,v52,31
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v52.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vadduwm v0,v0,v9
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v9.u32)));
	// vcuxwfp128 v2,v51,31
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v51.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vadduwm v8,v8,v9
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v9.u32)));
	// vadduwm v7,v7,v9
	simd::store_u32(ctx.v7.u32, simd::add_u32(simd::load_u32(ctx.v7.u32), simd::load_u32(ctx.v9.u32)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// vadduwm v6,v6,v9
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v9.u32)));
	// vmaddfp v5,v5,v4,v12
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v4.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v4,v62,v13
	simd::store_f32_aligned(ctx.v4.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v62.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vor128 v48,v5,v5
	simd::store_i8(ctx.v48.u8, simd::load_i8(ctx.v5.u8));
	// vmaddfp v5,v3,v2,v10
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v2.f32)), simd::load_f32_aligned(ctx.v10.f32)));
	// vcuxwfp128 v3,v50,31
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v50.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcuxwfp128 v2,v49,31
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v49.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// stvlx128 v48,r0,r4
{
	uint32_t addr = 
ctx.r4.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v48), 15 - i));
}
	// vor128 v47,v5,v5
	simd::store_i8(ctx.v47.u8, simd::load_i8(ctx.v5.u8));
	// vmaddfp v5,v4,v3,v13
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v3.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v4,v1,v2,v11
	simd::store_f32_aligned(ctx.v4.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v2.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// stvrx128 v48,r4,r7
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v48), i));
}
	// addi r4,r4,64
	ctx.r4.s64 = ctx.r4.s64 + 64;
	// stvlx128 v47,r0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v47), 15 - i));
}
	// stvrx128 v47,r9,r7
{
	uint32_t addr = 
ctx.r9.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v47), i));
}
	// stvlx v5,0,r8
{
	uint32_t addr = 
ctx.r8.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v5), 15 - i));
}
	// stvrx v5,r8,r7
{
	uint32_t addr = 
ctx.r8.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v5), i));
}
	// addi r8,r8,64
	ctx.r8.s64 = ctx.r8.s64 + 64;
	// stvlx v4,0,r6
{
	uint32_t addr = 
ctx.r6.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v4), 15 - i));
}
	// stvrx v4,r6,r7
{
	uint32_t addr = 
ctx.r6.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v4), i));
}
	// bdnz 0x822d6bfc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D6BFC;
loc_822D6EAC:
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// beq cr6,0x822d6f84
	if (ctx.cr6.eq) goto loc_822D6F84;
	// addi r9,r25,-1
	ctx.r9.s64 = ctx.r25.s64 + -1;
	// vspltisw128 v63,1
	simd::store_i32(ctx.v63.u32, simd::set1_i32(int32_t(0x1)));
	// li r6,64
	ctx.r6.s64 = 64;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822D6ECC:
	// rldicl r8,r3,32,32
	ctx.r8.u64 = rotl64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rldicl r5,r9,32,32
	ctx.r5.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rldicl r30,r9,32,32
	ctx.r30.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lvlx128 v46,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v45,r8,r11
	temp.u32 = ctx.r8.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rldicl r5,r9,32,32
	ctx.r5.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v44,v29,v46,4
	simd::store_i8(ctx.v44.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v29.u8), simd::load_i8(ctx.v46.u8), 12));
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// vsldoi128 v43,v58,v45,4
	simd::store_i8(ctx.v43.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v45.u8), 12));
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rldicl r29,r9,32,32
	ctx.r29.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// clrldi r3,r9,32
	ctx.r3.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lvlx128 v42,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r9,r29,2,0,29
	ctx.r9.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v41,r8,r11
	temp.u32 = ctx.r8.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r8,r30,2,0,29
	ctx.r8.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v40,v44,v42,4
	simd::store_i8(ctx.v40.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v44.u8), simd::load_i8(ctx.v42.u8), 12));
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// vsldoi128 v39,v43,v41,4
	simd::store_i8(ctx.v39.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v43.u8), simd::load_i8(ctx.v41.u8), 12));
	// lvlx128 v38,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v38,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v37,r8,r11
	temp.u32 = ctx.r8.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v37,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v36,v40,v38,4
	simd::store_i8(ctx.v36.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v40.u8), simd::load_i8(ctx.v38.u8), 12));
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// vsldoi128 v35,v39,v37,4
	simd::store_i8(ctx.v35.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v39.u8), simd::load_i8(ctx.v37.u8), 12));
	// add r31,r9,r31
	ctx.r31.u64 = ctx.r9.u64 + ctx.r31.u64;
	// lvlx128 v34,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v34,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v33,r8,r11
	temp.u32 = ctx.r8.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v33,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v29,v36,v34,4
	simd::store_i8(ctx.v29.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v36.u8), simd::load_i8(ctx.v34.u8), 12));
	// vsldoi128 v58,v35,v33,4
	simd::store_i8(ctx.v58.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v35.u8), simd::load_i8(ctx.v33.u8), 12));
	// dcbt r31,r6
	// vsrw128 v32,v0,v63
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(ctx.v32, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// vsubfp128 v13,v58,v29
	simd::store_f32_aligned(ctx.v13.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v58.f32), simd::load_f32_aligned(ctx.v29.f32)));
	// vadduwm v0,v0,v30
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v30.u32)));
	// vcuxwfp128 v12,v32,31
	simd::store_f32_aligned(ctx.v12.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v32.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vmaddfp v13,v13,v12,v29
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v29.f32)));
	// stvlx v13,0,r4
{
	uint32_t addr = 
ctx.r4.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v13), 15 - i));
}
	// stvrx v13,r4,r7
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v13), i));
}
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// bdnz 0x822d6ecc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D6ECC;
loc_822D6F84:
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x822d6fe0
	if (ctx.cr6.eq) goto loc_822D6FE0;
	// vspltisw128 v63,1
	simd::store_i32(ctx.v63.u32, simd::set1_i32(int32_t(0x1)));
	// mtctr r24
	ctx.ctr.u64 = ctx.r24.u64;
loc_822D6F94:
	// rldicl r8,r3,32,32
	ctx.r8.u64 = rotl64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vsrw128 v62,v0,v63
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(ctx.v62, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vadduwm v0,v0,v31
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v31.u32)));
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rldicl r7,r9,32,32
	ctx.r7.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// vcuxwfp128 v11,v62,31
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v62.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// clrldi r3,r9,32
	ctx.r3.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r9,r31
	ctx.r31.u64 = ctx.r9.u64 + ctx.r31.u64;
	// lvlx v13,0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v61,r8,r11
	temp.u32 = ctx.r8.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsubfp128 v12,v61,v13
	simd::store_f32_aligned(ctx.v12.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v61.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v13,v12,v11,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vspltw128 v60,v13,0
	simd::store_i32(ctx.v60.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 3));
	// stvewx128 v60,r0,r4
	PPC_STORE_U32((ctx.r4.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v60.u32), 3 - ((ctx.r4.u32) & 0xF) >> 2));
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// bdnz 0x822d6f94
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D6F94;
loc_822D6FE0:
	// bl 0x823413b0
	ctx.lr = 0x822D6FE4;
	sub_823413B0(ctx, base);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// subf r10,r22,r31
	ctx.r10.s64 = ctx.r31.s64 - ctx.r22.s64;
	// srawi r3,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r10.s32 >> 2;
	// lfd f0,-1472(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -1472);
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(ctx.f0.f64));
	// stfs f13,0(r23)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r23.u32 + 0, temp.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8233e498
	__restgprlr_20(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D7008"))) PPC_WEAK_FUNC(sub_822D7008);
PPC_FUNC_IMPL(__imp__sub_822D7008) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822D7010;
	__restfpr_27(ctx, base);
	// stfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lwz r8,16(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r30,24(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r27,0(r3)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r28,r11,8
	ctx.r28.s64 = ctx.r11.s64 + 8;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// fctiwz f13,f1
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f1.f64)));
	// stfd f13,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f13.u64);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r7,r9
	ctx.r7.s64 = ctx.r9.s32;
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// lwz r5,28(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r31,36(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// fsubs f13,f1,f10
	ctx.f13.f64 = static_cast<float>(ctx.f1.f64 - ctx.f10.f64);
	// beq cr6,0x822d708c
	if (ctx.cr6.eq) goto loc_822D708C;
	// lfs f12,0(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,0(r28)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r28.u32 + 0, temp.u32);
loc_822D708C:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// cmpwi cr6,r31,1
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 1, ctx.xer);
	// lfs f31,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f31.f64 = double(temp.f32);
	// bge cr6,0x822d70f8
	if (!ctx.cr6.lt) goto loc_822D70F8;
	// rlwinm r11,r31,2,0,29
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r4,-4
	ctx.r8.s64 = ctx.r4.s64 + -4;
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + ctx.r28.u64;
loc_822D70AC:
	// cmplw cr6,r10,r5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, ctx.xer);
	// bge cr6,0x822d70f4
	if (!ctx.cr6.lt) goto loc_822D70F4;
	// lfs f12,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lfs f11,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// add r31,r9,r31
	ctx.r31.u64 = ctx.r9.u64 + ctx.r31.u64;
	// fsubs f10,f11,f12
	ctx.f10.f64 = static_cast<float>(ctx.f11.f64 - ctx.f12.f64);
	// add r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 + ctx.r11.u64;
	// fmadds f9,f10,f0,f12
	ctx.f9.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfsu f9,4(r8)
	temp.f32 = float(ctx.f9.f64);
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r8.u32 = ea;
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// blt cr6,0x822d70ec
	if (ctx.cr6.lt) goto loc_822D70EC;
	// fsubs f0,f0,f31
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f31.f64);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
loc_822D70EC:
	// cmpwi cr6,r31,1
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 1, ctx.xer);
	// blt cr6,0x822d70ac
	if (ctx.cr6.lt) goto loc_822D70AC;
loc_822D70F4:
	// stfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
loc_822D70F8:
	// rlwinm r11,r31,2,0,29
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// subf r5,r10,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r10.s64;
	// add r4,r9,r4
	ctx.r4.u64 = ctx.r9.u64 + ctx.r4.u64;
	// addi r3,r11,-4
	ctx.r3.s64 = ctx.r11.s64 + -4;
	// bl 0x822d6b20
	ctx.lr = 0x822D7118;
	sub_822D6B20(ctx, base);
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// add r11,r3,r31
	ctx.r11.u64 = ctx.r3.u64 + ctx.r31.u64;
	// blt cr6,0x822d7134
	if (ctx.cr6.lt) goto loc_822D7134;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// lfs f0,-28948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
loc_822D7134:
	// addi r10,r30,-1
	ctx.r10.s64 = ctx.r30.s64 + -1;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x822d7144
	if (!ctx.cr6.lt) goto loc_822D7144;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822D7144:
	// subf r10,r30,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r30.s64;
	// stfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// neg r8,r30
	ctx.r8.s64 = -ctx.r30.s64;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r10,-1
	ctx.r11.s64 = ctx.r10.s64 + -1;
	// stw r9,12(r29)
	PPC_STORE_U32(ctx.r29.u32 + 12, ctx.r9.u32);
	// stw r10,36(r29)
	PPC_STORE_U32(ctx.r29.u32 + 36, ctx.r10.u32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d7214
	if (!ctx.cr6.lt) goto loc_822D7214;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d71d8
	if (ctx.cr6.lt) goto loc_822D71D8;
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r9,r28
	ctx.r10.u64 = ctx.r9.u64 + ctx.r28.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r30,2
	ctx.r7.s64 = ctx.r30.s64 + 2;
	// addi r6,r30,3
	ctx.r6.s64 = ctx.r30.s64 + 3;
	// addi r5,r8,-3
	ctx.r5.s64 = ctx.r8.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + ctx.r28.u64;
loc_822D719C:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r4,r7,r11
	ctx.r4.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r3,r6,r11
	ctx.r3.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r4,r28
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r28.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f12,r3,r28
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r28.u32);
	ctx.f12.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d719c
	if (ctx.cr6.lt) goto loc_822D719C;
loc_822D71D8:
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d7214
	if (!ctx.cr6.lt) goto loc_822D7214;
	// add r7,r11,r30
	ctx.r7.u64 = ctx.r11.u64 + ctx.r30.u64;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + ctx.r28.u64;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + ctx.r28.u64;
	// addi r7,r9,-4
	ctx.r7.s64 = ctx.r9.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_822D7208:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d7208
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D7208;
loc_822D7214:
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d728c
	if (ctx.cr6.lt) goto loc_822D728C;
	// subfic r10,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r10.s64 = -4 - ctx.r11.s64;
	// add r8,r11,r30
	ctx.r8.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r9,r10,30,2,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r8,r8,-3
	ctx.r8.s64 = ctx.r8.s64 + -3;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + ctx.r28.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r30,2
	ctx.r7.s64 = ctx.r30.s64 + 2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r6,r30,3
	ctx.r6.s64 = ctx.r30.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r8,r27
	ctx.r9.u64 = ctx.r8.u64 + ctx.r27.u64;
loc_822D7254:
	// add r8,r7,r11
	ctx.r8.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lfs f13,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// add r5,r6,r11
	ctx.r5.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r4,r8,2,0,29
	ctx.r4.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// rlwinm r3,r5,2,0,29
	ctx.r3.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lfsx f12,r4,r27
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r27.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfsx f11,r3,r27
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r27.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfs f12,12(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfsu f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d7254
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D7254;
loc_822D728C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822d72b4
	if (!ctx.cr6.lt) goto loc_822D72B4;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + ctx.r30.u64;
	// add r3,r9,r28
	ctx.r3.u64 = ctx.r9.u64 + ctx.r28.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r9,r11
	ctx.r9.s64 = -ctx.r11.s64;
	// add r4,r10,r27
	ctx.r4.u64 = ctx.r10.u64 + ctx.r27.u64;
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8233e968
	ctx.lr = 0x822D72B4;
	sub_8233E968(ctx, base);
loc_822D72B4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D72C0"))) PPC_WEAK_FUNC(sub_822D72C0);
PPC_FUNC_IMPL(__imp__sub_822D72C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e438
	ctx.lr = 0x822D72C8;
	__restfpr_16(ctx, base);
	// lwz r9,16(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lwz r5,12(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r6,32(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r21,24(r3)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// rlwinm r10,r6,3,0,28
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, ctx.r9.u32);
	// lfs f12,-224(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -224);
	ctx.f12.f64 = double(temp.f32);
	// fctiwz f11,f12
	ctx.f11.u64 = uint64_t(int32_t(std::trunc(ctx.f12.f64)));
	// stfd f11,-224(r1)
	PPC_STORE_U64(ctx.r1.u32 + -224, ctx.f11.u64);
	// lwz r4,-220(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -220);
	// extsw r9,r4
	ctx.r9.s64 = ctx.r4.s32;
	// std r9,-224(r1)
	PPC_STORE_U64(ctx.r1.u32 + -224, ctx.r9.u64);
	// lfd f10,-224(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -224);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// stw r5,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, ctx.r5.u32);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// lwz r22,0(r3)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r24,4(r3)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lfs f0,-224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -224);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-1496(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -1496);
	ctx.f13.f64 = double(temp.f32);
	// lwz r31,36(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// add r25,r10,r11
	ctx.r25.u64 = ctx.r10.u64 + ctx.r11.u64;
	// cmplwi cr6,r21,0
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 0, ctx.xer);
	// mullw r23,r4,r6
	ctx.r23.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r6.s32);
	// fsubs f9,f12,f8
	ctx.f9.f64 = static_cast<float>(ctx.f12.f64 - ctx.f8.f64);
	// beq cr6,0x822d7430
	if (ctx.cr6.eq) goto loc_822D7430;
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r6,4
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 4, ctx.xer);
	// blt cr6,0x822d73e0
	if (ctx.cr6.lt) goto loc_822D73E0;
	// addi r5,r6,-3
	ctx.r5.s64 = ctx.r6.s64 + -3;
	// addi r9,r25,-4
	ctx.r9.s64 = ctx.r25.s64 + -4;
	// addi r10,r22,-2
	ctx.r10.s64 = ctx.r22.s64 + -2;
loc_822D7354:
	// lhz r29,2(r10)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lhz r4,4(r10)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lhz r30,6(r10)
	ctx.r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// extsh r29,r29
	ctx.r29.s64 = ctx.r29.s16;
	// lhzu r8,8(r10)
	ea = 8 + ctx.r10.u32;
	ctx.r8.u64 = PPC_LOAD_U16(ea);
	ctx.r10.u32 = ea;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r29,-216(r1)
	PPC_STORE_U64(ctx.r1.u32 + -216, ctx.r29.u64);
	// lfd f11,-216(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -216);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// std r4,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r4.u64);
	// lfd f8,-200(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// extsh r30,r30
	ctx.r30.s64 = ctx.r30.s16;
	// std r8,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r8.u64);
	// lfd f10,-208(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// std r30,-224(r1)
	PPC_STORE_U64(ctx.r1.u32 + -224, ctx.r30.u64);
	// lfd f12,-224(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -224);
	// fcfid f5,f12
	ctx.f5.f64 = double(ctx.f12.s64);
	// cmplw cr6,r11,r5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r5.u32, ctx.xer);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// fcfid f6,f11
	ctx.f6.f64 = double(ctx.f11.s64);
	// fcfid f4,f10
	ctx.f4.f64 = double(ctx.f10.s64);
	// frsp f1,f5
	ctx.f1.f64 = double(float(ctx.f5.f64));
	// frsp f3,f7
	ctx.f3.f64 = double(float(ctx.f7.f64));
	// frsp f2,f6
	ctx.f2.f64 = double(float(ctx.f6.f64));
	// frsp f12,f4
	ctx.f12.f64 = double(float(ctx.f4.f64));
	// fmuls f8,f1,f13
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// stfs f8,12(r9)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
	// fmuls f11,f3,f13
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// stfs f11,8(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// fmuls f10,f2,f13
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// stfs f10,4(r9)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// fmuls f7,f12,f13
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfsu f7,16(r9)
	temp.f32 = float(ctx.f7.f64);
	ea = 16 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// blt cr6,0x822d7354
	if (ctx.cr6.lt) goto loc_822D7354;
loc_822D73E0:
	// cmplw cr6,r11,r6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r6.u32, ctx.xer);
	// bge cr6,0x822d7430
	if (!ctx.cr6.lt) goto loc_822D7430;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r8,r11,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r11.s64;
	// add r11,r10,r25
	ctx.r11.u64 = ctx.r10.u64 + ctx.r25.u64;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + ctx.r22.u64;
	// addi r10,r11,-4
	ctx.r10.s64 = ctx.r11.s64 + -4;
	// addi r11,r9,-2
	ctx.r11.s64 = ctx.r9.s64 + -2;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D7408:
	// lhzu r9,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// std r8,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r8.u64);
	// lfd f12,-200(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f8,f10,f13
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// stfsu f8,4(r10)
	temp.f32 = float(ctx.f8.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d7408
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D7408;
loc_822D7430:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// cmpw cr6,r31,r6
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r6.s32, ctx.xer);
	// lfs f10,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f10.f64 = double(temp.f32);
	// bge cr6,0x822d759c
	if (!ctx.cr6.lt) goto loc_822D759C;
	// subf r11,r6,r31
	ctx.r11.s64 = ctx.r31.s64 - ctx.r6.s64;
	// addi r10,r31,2
	ctx.r10.s64 = ctx.r31.s64 + 2;
	// addi r9,r11,2
	ctx.r9.s64 = ctx.r11.s64 + 2;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r31,r6
	ctx.r11.u64 = ctx.r31.u64 + ctx.r6.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r31,r6
	ctx.r4.u64 = ctx.r31.u64 + ctx.r6.u64;
	// rlwinm r27,r6,2,0,29
	ctx.r27.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r26,r23,2,0,29
	ctx.r26.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r10,r25
	ctx.r29.u64 = ctx.r10.u64 + ctx.r25.u64;
	// addi r30,r11,-3
	ctx.r30.s64 = ctx.r11.s64 + -3;
	// add r28,r9,r25
	ctx.r28.u64 = ctx.r9.u64 + ctx.r25.u64;
loc_822D7470:
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// cmplw cr6,r7,r11
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x822d77e0
	if (!ctx.cr6.lt) goto loc_822D77E0;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// cmpw cr6,r31,r4
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r4.s32, ctx.xer);
	// bge cr6,0x822d755c
	if (!ctx.cr6.lt) goto loc_822D755C;
	// subf r10,r31,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r31.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d7508
	if (ctx.cr6.lt) goto loc_822D7508;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r29,-12
	ctx.r10.s64 = ctx.r29.s64 + -12;
	// add r8,r9,r24
	ctx.r8.u64 = ctx.r9.u64 + ctx.r24.u64;
	// addi r9,r28,-12
	ctx.r9.s64 = ctx.r28.s64 + -12;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
loc_822D74A8:
	// lfs f8,4(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lfs f7,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lfs f5,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f5.f64 = double(temp.f32);
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// lfs f4,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f4.f64 = double(temp.f32);
	// fsubs f3,f5,f8
	ctx.f3.f64 = static_cast<float>(ctx.f5.f64 - ctx.f8.f64);
	// lfs f6,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f6.f64 = double(temp.f32);
	// fsubs f1,f4,f7
	ctx.f1.f64 = static_cast<float>(ctx.f4.f64 - ctx.f7.f64);
	// lfs f2,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// lfsu f12,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fsubs f5,f2,f6
	ctx.f5.f64 = static_cast<float>(ctx.f2.f64 - ctx.f6.f64);
	// lfsu f11,16(r10)
	ea = 16 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fsubs f4,f11,f12
	ctx.f4.f64 = static_cast<float>(ctx.f11.f64 - ctx.f12.f64);
	// fmadds f3,f3,f0,f8
	ctx.f3.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f8.f64)));
	// stfs f3,4(r8)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// fmadds f2,f1,f0,f7
	ctx.f2.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f0.f64), float(ctx.f7.f64)));
	// stfs f2,8(r8)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// fmadds f1,f5,f0,f6
	ctx.f1.f64 = double(std::fma(float(ctx.f5.f64), float(ctx.f0.f64), float(ctx.f6.f64)));
	// stfs f1,12(r8)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r8.u32 + 12, temp.u32);
	// fmadds f12,f4,f0,f12
	ctx.f12.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfsu f12,16(r8)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r8.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r8.u32 = ea;
	// blt cr6,0x822d74a8
	if (ctx.cr6.lt) goto loc_822D74A8;
loc_822D7508:
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// bge cr6,0x822d755c
	if (!ctx.cr6.lt) goto loc_822D755C;
	// subf r9,r6,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r6.s64;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r11,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r11.s64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r20,r9,-1
	ctx.r20.s64 = ctx.r9.s64 + -1;
	// add r11,r8,r24
	ctx.r11.u64 = ctx.r8.u64 + ctx.r24.u64;
	// add r9,r5,r25
	ctx.r9.u64 = ctx.r5.u64 + ctx.r25.u64;
	// rlwinm r8,r20,2,0,29
	ctx.r8.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r5,r11,-4
	ctx.r5.s64 = ctx.r11.s64 + -4;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// add r11,r8,r25
	ctx.r11.u64 = ctx.r8.u64 + ctx.r25.u64;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
loc_822D7544:
	// lfsu f12,4(r11)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// lfsu f11,4(r9)
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fsubs f11,f11,f12
	ctx.f11.f64 = static_cast<float>(ctx.f11.f64 - ctx.f12.f64);
	// fmadds f8,f11,f0,f12
	ctx.f8.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfsu f8,4(r5)
	temp.f32 = float(ctx.f8.f64);
	ea = 4 + ctx.r5.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r5.u32 = ea;
	// bdnz 0x822d7544
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D7544;
loc_822D755C:
	// fadds f0,f9,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// add r31,r23,r31
	ctx.r31.u64 = ctx.r23.u64 + ctx.r31.u64;
	// add r4,r4,r23
	ctx.r4.u64 = ctx.r4.u64 + ctx.r23.u64;
	// add r30,r30,r23
	ctx.r30.u64 = ctx.r30.u64 + ctx.r23.u64;
	// add r29,r26,r29
	ctx.r29.u64 = ctx.r26.u64 + ctx.r29.u64;
	// add r28,r26,r28
	ctx.r28.u64 = ctx.r26.u64 + ctx.r28.u64;
	// fcmpu cr6,f0,f10
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// blt cr6,0x822d7594
	if (ctx.cr6.lt) goto loc_822D7594;
	// fsubs f0,f0,f10
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f10.f64);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// add r31,r31,r6
	ctx.r31.u64 = ctx.r31.u64 + ctx.r6.u64;
	// add r30,r30,r6
	ctx.r30.u64 = ctx.r30.u64 + ctx.r6.u64;
	// add r29,r27,r29
	ctx.r29.u64 = ctx.r27.u64 + ctx.r29.u64;
	// add r28,r27,r28
	ctx.r28.u64 = ctx.r27.u64 + ctx.r28.u64;
loc_822D7594:
	// cmpw cr6,r31,r6
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r6.s32, ctx.xer);
	// blt cr6,0x822d7470
	if (ctx.cr6.lt) goto loc_822D7470;
loc_822D759C:
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// cmplw cr6,r7,r11
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x822d77e0
	if (!ctx.cr6.lt) goto loc_822D77E0;
	// subf r11,r6,r31
	ctx.r11.s64 = ctx.r31.s64 - ctx.r6.s64;
	// addi r10,r31,2
	ctx.r10.s64 = ctx.r31.s64 + 2;
	// addi r9,r11,2
	ctx.r9.s64 = ctx.r11.s64 + 2;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r31,r6
	ctx.r11.u64 = ctx.r31.u64 + ctx.r6.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r31,r6
	ctx.r4.u64 = ctx.r31.u64 + ctx.r6.u64;
	// rlwinm r27,r6,1,0,30
	ctx.r27.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r26,r23,1,0,30
	ctx.r26.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 1) & 0xFFFFFFFE;
	// add r29,r10,r22
	ctx.r29.u64 = ctx.r10.u64 + ctx.r22.u64;
	// addi r30,r11,-3
	ctx.r30.s64 = ctx.r11.s64 + -3;
	// add r28,r9,r22
	ctx.r28.u64 = ctx.r9.u64 + ctx.r22.u64;
loc_822D75D8:
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// cmpw cr6,r31,r4
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r4.s32, ctx.xer);
	// bge cr6,0x822d779c
	if (!ctx.cr6.lt) goto loc_822D779C;
	// subf r10,r31,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r31.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d771c
	if (ctx.cr6.lt) goto loc_822D771C;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r29,-6
	ctx.r9.s64 = ctx.r29.s64 + -6;
	// add r8,r10,r24
	ctx.r8.u64 = ctx.r10.u64 + ctx.r24.u64;
	// addi r10,r28,-6
	ctx.r10.s64 = ctx.r28.s64 + -6;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
loc_822D7604:
	// lhz r19,4(r10)
	ctx.r19.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lhz r20,2(r10)
	ctx.r20.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lhz r18,6(r10)
	ctx.r18.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// extsh r19,r19
	ctx.r19.s64 = ctx.r19.s16;
	// extsh r20,r20
	ctx.r20.s64 = ctx.r20.s16;
	// lhzu r5,8(r10)
	ea = 8 + ctx.r10.u32;
	ctx.r5.u64 = PPC_LOAD_U16(ea);
	ctx.r10.u32 = ea;
	// std r19,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r19.u64);
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// std r20,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r20.u64);
	// extsh r19,r5
	ctx.r19.s64 = ctx.r5.s16;
	// lhz r17,2(r9)
	ctx.r17.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lhz r16,4(r9)
	ctx.r16.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lhz r20,6(r9)
	ctx.r20.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// extsh r17,r17
	ctx.r17.s64 = ctx.r17.s16;
	// lhzu r5,8(r9)
	ea = 8 + ctx.r9.u32;
	ctx.r5.u64 = PPC_LOAD_U16(ea);
	ctx.r9.u32 = ea;
	// extsh r16,r16
	ctx.r16.s64 = ctx.r16.s16;
	// extsh r20,r20
	ctx.r20.s64 = ctx.r20.s16;
	// std r17,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r17.u64);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// std r16,-216(r1)
	PPC_STORE_U64(ctx.r1.u32 + -216, ctx.r16.u64);
	// std r20,-224(r1)
	PPC_STORE_U64(ctx.r1.u32 + -224, ctx.r20.u64);
	// extsh r20,r18
	ctx.r20.s64 = ctx.r18.s16;
	// std r5,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r5.u64);
	// extsw r5,r19
	ctx.r5.s64 = ctx.r19.s32;
	// lfd f8,-216(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -216);
	// std r5,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r5.u64);
	// extsw r5,r20
	ctx.r5.s64 = ctx.r20.s32;
	// lfd f2,-168(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// std r5,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r5.u64);
	// lfd f4,-176(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// lfd f7,-224(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -224);
	// lfd f5,-184(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// lfd f6,-192(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// fcfid f6,f6
	ctx.f6.f64 = double(ctx.f6.s64);
	// lfd f12,-200(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// lfd f11,-208(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// fcfid f1,f12
	ctx.f1.f64 = double(ctx.f12.s64);
	// fcfid f12,f11
	ctx.f12.f64 = double(ctx.f11.s64);
	// fcfid f11,f2
	ctx.f11.f64 = double(ctx.f2.s64);
	// fcfid f4,f8
	ctx.f4.f64 = double(ctx.f8.s64);
	// fcfid f2,f7
	ctx.f2.f64 = double(ctx.f7.s64);
	// fcfid f7,f5
	ctx.f7.f64 = double(ctx.f5.s64);
	// frsp f8,f3
	ctx.f8.f64 = double(float(ctx.f3.f64));
	// frsp f5,f1
	ctx.f5.f64 = double(float(ctx.f1.f64));
	// frsp f3,f12
	ctx.f3.f64 = double(float(ctx.f12.f64));
	// frsp f1,f11
	ctx.f1.f64 = double(float(ctx.f11.f64));
	// frsp f12,f6
	ctx.f12.f64 = double(float(ctx.f6.f64));
	// frsp f11,f4
	ctx.f11.f64 = double(float(ctx.f4.f64));
	// frsp f6,f2
	ctx.f6.f64 = double(float(ctx.f2.f64));
	// frsp f2,f7
	ctx.f2.f64 = double(float(ctx.f7.f64));
	// fmuls f4,f8,f13
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmuls f8,f5,f13
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// fmuls f7,f3,f13
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmuls f5,f1,f13
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// fmsubs f3,f6,f13,f4
	ctx.f3.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f13.f64), -float(ctx.f4.f64)));
	// fmsubs f1,f11,f13,f8
	ctx.f1.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f13.f64), -float(ctx.f8.f64)));
	// fmsubs f12,f12,f13,f7
	ctx.f12.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f13.f64), -float(ctx.f7.f64)));
	// fmsubs f11,f2,f13,f5
	ctx.f11.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f13.f64), -float(ctx.f5.f64)));
	// fmadds f6,f3,f0,f4
	ctx.f6.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f4.f64)));
	// stfs f6,12(r8)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r8.u32 + 12, temp.u32);
	// fmadds f4,f1,f0,f8
	ctx.f4.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f0.f64), float(ctx.f8.f64)));
	// stfs f4,8(r8)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// fmadds f3,f12,f0,f7
	ctx.f3.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f7.f64)));
	// stfs f3,4(r8)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// fmadds f2,f11,f0,f5
	ctx.f2.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f0.f64), float(ctx.f5.f64)));
	// stfsu f2,16(r8)
	temp.f32 = float(ctx.f2.f64);
	ea = 16 + ctx.r8.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r8.u32 = ea;
	// blt cr6,0x822d7604
	if (ctx.cr6.lt) goto loc_822D7604;
loc_822D771C:
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// bge cr6,0x822d779c
	if (!ctx.cr6.lt) goto loc_822D779C;
	// subf r9,r6,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r6.s64;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r11,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r11.s64;
	// addi r20,r9,-1
	ctx.r20.s64 = ctx.r9.s64 + -1;
	// add r11,r8,r24
	ctx.r11.u64 = ctx.r8.u64 + ctx.r24.u64;
	// add r9,r5,r22
	ctx.r9.u64 = ctx.r5.u64 + ctx.r22.u64;
	// rlwinm r5,r20,1,0,30
	ctx.r5.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r11,-4
	ctx.r8.s64 = ctx.r11.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
	// add r11,r5,r22
	ctx.r11.u64 = ctx.r5.u64 + ctx.r22.u64;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
loc_822D7758:
	// lhzu r5,2(r9)
	ea = 2 + ctx.r9.u32;
	ctx.r5.u64 = PPC_LOAD_U16(ea);
	ctx.r9.u32 = ea;
	// lhzu r10,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r10.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// std r5,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r5.u64);
	// lfd f12,-160(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// std r10,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r10.u64);
	// lfd f8,-152(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f6,f8
	ctx.f6.f64 = double(ctx.f8.s64);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// frsp f7,f11
	ctx.f7.f64 = double(float(ctx.f11.f64));
	// fmuls f4,f5,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// fmsubs f3,f7,f13,f4
	ctx.f3.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f13.f64), -float(ctx.f4.f64)));
	// fmadds f2,f3,f0,f4
	ctx.f2.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f4.f64)));
	// stfsu f2,4(r8)
	temp.f32 = float(ctx.f2.f64);
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r8.u32 = ea;
	// bdnz 0x822d7758
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D7758;
loc_822D779C:
	// fadds f0,f9,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// add r31,r23,r31
	ctx.r31.u64 = ctx.r23.u64 + ctx.r31.u64;
	// add r4,r4,r23
	ctx.r4.u64 = ctx.r4.u64 + ctx.r23.u64;
	// add r30,r30,r23
	ctx.r30.u64 = ctx.r30.u64 + ctx.r23.u64;
	// add r29,r26,r29
	ctx.r29.u64 = ctx.r26.u64 + ctx.r29.u64;
	// add r28,r26,r28
	ctx.r28.u64 = ctx.r26.u64 + ctx.r28.u64;
	// fcmpu cr6,f0,f10
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// blt cr6,0x822d77d4
	if (ctx.cr6.lt) goto loc_822D77D4;
	// fsubs f0,f0,f10
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f10.f64);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// add r31,r31,r6
	ctx.r31.u64 = ctx.r31.u64 + ctx.r6.u64;
	// add r30,r30,r6
	ctx.r30.u64 = ctx.r30.u64 + ctx.r6.u64;
	// add r29,r27,r29
	ctx.r29.u64 = ctx.r27.u64 + ctx.r29.u64;
	// add r28,r27,r28
	ctx.r28.u64 = ctx.r27.u64 + ctx.r28.u64;
loc_822D77D4:
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// cmplw cr6,r7,r11
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822d75d8
	if (ctx.cr6.lt) goto loc_822D75D8;
loc_822D77E0:
	// subf r11,r6,r21
	ctx.r11.s64 = ctx.r21.s64 - ctx.r6.s64;
	// cmpw cr6,r31,r11
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x822d77f0
	if (!ctx.cr6.lt) goto loc_822D77F0;
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
loc_822D77F0:
	// stfs f0,-224(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -224, temp.u32);
	// subf r10,r21,r31
	ctx.r10.s64 = ctx.r31.s64 - ctx.r21.s64;
	// neg r8,r21
	ctx.r8.s64 = -ctx.r21.s64;
	// subf r11,r6,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r6.s64;
	// stw r10,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r10.u32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// lwz r9,-224(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -224);
	// stw r9,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r9.u32);
	// bge cr6,0x822d78c0
	if (!ctx.cr6.lt) goto loc_822D78C0;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d7884
	if (ctx.cr6.lt) goto loc_822D7884;
	// add r10,r11,r21
	ctx.r10.u64 = ctx.r11.u64 + ctx.r21.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r9,r25
	ctx.r10.u64 = ctx.r9.u64 + ctx.r25.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r21,2
	ctx.r7.s64 = ctx.r21.s64 + 2;
	// addi r6,r21,3
	ctx.r6.s64 = ctx.r21.s64 + 3;
	// addi r5,r8,-3
	ctx.r5.s64 = ctx.r8.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r25
	ctx.r9.u64 = ctx.r9.u64 + ctx.r25.u64;
loc_822D7848:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r4,r7,r11
	ctx.r4.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r3,r6,r11
	ctx.r3.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f12,r4,r25
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r25.u32);
	ctx.f12.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f12,12(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f11,r3,r25
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r25.u32);
	ctx.f11.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// stfsu f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d7848
	if (ctx.cr6.lt) goto loc_822D7848;
loc_822D7884:
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d78c0
	if (!ctx.cr6.lt) goto loc_822D78C0;
	// add r7,r11,r21
	ctx.r7.u64 = ctx.r11.u64 + ctx.r21.u64;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r25
	ctx.r9.u64 = ctx.r9.u64 + ctx.r25.u64;
	// add r8,r8,r25
	ctx.r8.u64 = ctx.r8.u64 + ctx.r25.u64;
	// addi r7,r9,-4
	ctx.r7.s64 = ctx.r9.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_822D78B4:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d78b4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D78B4;
loc_822D78C0:
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d79a0
	if (ctx.cr6.lt) goto loc_822D79A0;
	// subfic r9,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r9.s64 = -4 - ctx.r11.s64;
	// add r10,r11,r21
	ctx.r10.u64 = ctx.r11.u64 + ctx.r21.u64;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r8,r25
	ctx.r10.u64 = ctx.r8.u64 + ctx.r25.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r21,2
	ctx.r7.s64 = ctx.r21.s64 + 2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r6,r21,3
	ctx.r6.s64 = ctx.r21.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r8,r22
	ctx.r9.u64 = ctx.r8.u64 + ctx.r22.u64;
loc_822D7900:
	// add r5,r6,r11
	ctx.r5.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lhz r4,6(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// add r3,r7,r11
	ctx.r3.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lhzu r8,8(r9)
	ea = 8 + ctx.r9.u32;
	ctx.r8.u64 = PPC_LOAD_U16(ea);
	ctx.r9.u32 = ea;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// std r4,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r4.u64);
	// extsh r4,r8
	ctx.r4.s64 = ctx.r8.s16;
	// lfd f10,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// lhzx r5,r5,r22
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r22.u32);
	// frsp f4,f9
	ctx.f4.f64 = double(float(ctx.f9.f64));
	// std r4,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
	// lfd f0,-152(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// lhzx r5,r3,r22
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r3.u32 + ctx.r22.u32);
	// fcfid f12,f0
	ctx.f12.f64 = double(ctx.f0.s64);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsh r3,r5
	ctx.r3.s64 = ctx.r5.s16;
	// frsp f8,f12
	ctx.f8.f64 = double(float(ctx.f12.f64));
	// extsh r4,r8
	ctx.r4.s64 = ctx.r8.s16;
	// fmuls f0,f4,f13
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// extsw r8,r3
	ctx.r8.s64 = ctx.r3.s32;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// std r4,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r4.u64);
	// lfd f11,-160(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// std r8,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r8.u64);
	// lfd f7,-176(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f5,f11
	ctx.f5.f64 = double(ctx.f11.s64);
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// frsp f1,f5
	ctx.f1.f64 = double(float(ctx.f5.f64));
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f2,8(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// frsp f3,f6
	ctx.f3.f64 = double(float(ctx.f6.f64));
	// fmuls f11,f1,f13
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// fmuls f12,f3,f13
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// stfs f12,12(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfsu f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d7900
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D7900;
loc_822D79A0:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822d79f4
	if (!ctx.cr6.lt) goto loc_822D79F4;
	// add r9,r11,r21
	ctx.r9.u64 = ctx.r11.u64 + ctx.r21.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r8,r11
	ctx.r8.s64 = -ctx.r11.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r25
	ctx.r11.u64 = ctx.r10.u64 + ctx.r25.u64;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + ctx.r22.u64;
	// addi r10,r11,-4
	ctx.r10.s64 = ctx.r11.s64 + -4;
	// addi r11,r9,-2
	ctx.r11.s64 = ctx.r9.s64 + -2;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D79CC:
	// lhzu r9,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// std r8,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r8.u64);
	// lfd f0,-152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f12,f0
	ctx.f12.f64 = double(ctx.f0.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfsu f10,4(r10)
	temp.f32 = float(ctx.f10.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d79cc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D79CC;
loc_822D79F4:
	// b 0x8233e488
	__restgprlr_16(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D79F8"))) PPC_WEAK_FUNC(sub_822D79F8);
PPC_FUNC_IMPL(__imp__sub_822D79F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e434
	ctx.lr = 0x822D7A00;
	__restfpr_15(ctx, base);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r22,24(r3)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r28,r11,32
	ctx.r28.s64 = ctx.r11.s64 + 32;
	// lwz r24,0(r3)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r10,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, ctx.r10.u32);
	// lfs f13,-208(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -208);
	ctx.f13.f64 = double(temp.f32);
	// fctiwz f12,f13
	ctx.f12.u64 = uint64_t(int32_t(std::trunc(ctx.f13.f64)));
	// stfd f12,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.f12.u64);
	// lwz r7,-204(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -204);
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// std r6,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r6.u64);
	// lfd f11,-208(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// stw r8,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, ctx.r8.u32);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lwz r23,4(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r21,28(r3)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,-208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -208);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lfs f7,-1496(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1496);
	ctx.f7.f64 = double(temp.f32);
	// cmplwi cr6,r22,0
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, 0, ctx.xer);
	// rlwinm r25,r7,2,0,29
	ctx.r25.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// fsubs f6,f13,f9
	ctx.f6.f64 = static_cast<float>(ctx.f13.f64 - ctx.f9.f64);
	// beq cr6,0x822d7af0
	if (ctx.cr6.eq) goto loc_822D7AF0;
	// lhz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r24.u32 + 0);
	// lhz r9,2(r24)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r24.u32 + 2);
	// extsh r4,r10
	ctx.r4.s64 = ctx.r10.s16;
	// lhz r5,4(r24)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r24.u32 + 4);
	// extsh r10,r9
	ctx.r10.s64 = ctx.r9.s16;
	// lhz r7,6(r24)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r24.u32 + 6);
	// std r4,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r4.u64);
	// extsh r6,r5
	ctx.r6.s64 = ctx.r5.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// std r7,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r7.u64);
	// lfd f9,-200(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f4,f9
	ctx.f4.f64 = double(ctx.f9.s64);
	// lfd f13,-208(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// std r10,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r10.u64);
	// lfd f12,-208(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// std r6,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r6.u64);
	// lfd f11,-208(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// fcfid f8,f12
	ctx.f8.f64 = double(ctx.f12.s64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// fcfid f5,f13
	ctx.f5.f64 = double(ctx.f13.s64);
	// frsp f2,f8
	ctx.f2.f64 = double(float(ctx.f8.f64));
	// frsp f13,f4
	ctx.f13.f64 = double(float(ctx.f4.f64));
	// frsp f3,f10
	ctx.f3.f64 = double(float(ctx.f10.f64));
	// frsp f1,f5
	ctx.f1.f64 = double(float(ctx.f5.f64));
	// fmuls f11,f2,f7
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f7.f64));
	// stfs f11,4(r28)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r28.u32 + 4, temp.u32);
	// fmuls f9,f13,f7
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f7.f64));
	// stfs f9,12(r28)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r28.u32 + 12, temp.u32);
	// fmuls f12,f3,f7
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// stfs f12,8(r28)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8, temp.u32);
	// fmuls f10,f1,f7
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// stfs f10,0(r28)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r28.u32 + 0, temp.u32);
loc_822D7AF0:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// lfs f10,5256(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f10.f64 = double(temp.f32);
	// bge cr6,0x822d7bfc
	if (!ctx.cr6.lt) goto loc_822D7BFC;
	// addi r9,r11,3
	ctx.r9.s64 = ctx.r11.s64 + 3;
	// addi r8,r11,-2
	ctx.r8.s64 = ctx.r11.s64 + -2;
	// addi r7,r11,2
	ctx.r7.s64 = ctx.r11.s64 + 2;
	// addi r6,r11,-3
	ctx.r6.s64 = ctx.r11.s64 + -3;
	// addi r5,r11,-4
	ctx.r5.s64 = ctx.r11.s64 + -4;
	// rlwinm r4,r9,2,0,29
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r25,2,0,29
	ctx.r10.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r31,r7,2,0,29
	ctx.r31.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r30,r6,2,0,29
	ctx.r30.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r5,2,0,29
	ctx.r27.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r10,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r6,r10,r9
	ctx.r6.s64 = ctx.r9.s64 - ctx.r10.s64;
	// subf r5,r10,r31
	ctx.r5.s64 = ctx.r31.s64 - ctx.r10.s64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r30,r10,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r10.s64;
	// subf r27,r10,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r10.s64;
	// add r31,r7,r28
	ctx.r31.u64 = ctx.r7.u64 + ctx.r28.u64;
	// add r4,r6,r28
	ctx.r4.u64 = ctx.r6.u64 + ctx.r28.u64;
	// mr r9,r23
	ctx.r9.u64 = ctx.r23.u64;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + ctx.r28.u64;
	// add r5,r5,r28
	ctx.r5.u64 = ctx.r5.u64 + ctx.r28.u64;
	// add r6,r30,r28
	ctx.r6.u64 = ctx.r30.u64 + ctx.r28.u64;
	// add r7,r27,r28
	ctx.r7.u64 = ctx.r27.u64 + ctx.r28.u64;
loc_822D7B60:
	// cmplw cr6,r29,r21
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r21.u32, ctx.xer);
	// bge cr6,0x822d7de4
	if (!ctx.cr6.lt) goto loc_822D7DE4;
	// lfs f4,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// lfsux f13,r7,r10
	ea = ctx.r7.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r7.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// add r11,r25,r11
	ctx.r11.u64 = ctx.r25.u64 + ctx.r11.u64;
	// lfs f2,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// fsubs f3,f4,f13
	ctx.f3.f64 = static_cast<float>(ctx.f4.f64 - ctx.f13.f64);
	// lfsux f12,r6,r10
	ea = ctx.r6.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r6.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// lfs f5,-4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	ctx.f5.f64 = double(temp.f32);
	// fsubs f1,f2,f12
	ctx.f1.f64 = static_cast<float>(ctx.f2.f64 - ctx.f12.f64);
	// lfsux f11,r4,r10
	ea = ctx.r4.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lfsux f9,r5,r10
	ea = ctx.r5.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r5.u32 = ea;
	ctx.f9.f64 = double(temp.f32);
	// lfsux f8,r31,r10
	ea = ctx.r31.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r31.u32 = ea;
	ctx.f8.f64 = double(temp.f32);
	// fsubs f9,f9,f11
	ctx.f9.f64 = static_cast<float>(ctx.f9.f64 - ctx.f11.f64);
	// fsubs f8,f8,f5
	ctx.f8.f64 = static_cast<float>(ctx.f8.f64 - ctx.f5.f64);
	// fmadds f4,f3,f0,f13
	ctx.f4.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfs f4,0(r9)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f3,f1,f0,f12
	ctx.f3.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfsu f3,4(r9)
	temp.f32 = float(ctx.f3.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmadds f2,f9,f0,f11
	ctx.f2.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// stfsu f2,4(r9)
	temp.f32 = float(ctx.f2.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmadds f1,f8,f0,f5
	ctx.f1.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f0.f64), float(ctx.f5.f64)));
	// stfsu f1,4(r9)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fadds f0,f6,f0
	ctx.f0.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fcmpu cr6,f0,f10
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// blt cr6,0x822d7bf4
	if (ctx.cr6.lt) goto loc_822D7BF4;
	// fsubs f0,f0,f10
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f10.f64);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// addi r6,r6,16
	ctx.r6.s64 = ctx.r6.s64 + 16;
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// addi r31,r31,16
	ctx.r31.s64 = ctx.r31.s64 + 16;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
loc_822D7BF4:
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// blt cr6,0x822d7b60
	if (ctx.cr6.lt) goto loc_822D7B60;
loc_822D7BFC:
	// cmplw cr6,r29,r21
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r21.u32, ctx.xer);
	// bge cr6,0x822d7de4
	if (!ctx.cr6.lt) goto loc_822D7DE4;
	// addi r9,r11,3
	ctx.r9.s64 = ctx.r11.s64 + 3;
	// addi r8,r11,-2
	ctx.r8.s64 = ctx.r11.s64 + -2;
	// addi r7,r11,2
	ctx.r7.s64 = ctx.r11.s64 + 2;
	// addi r6,r11,-3
	ctx.r6.s64 = ctx.r11.s64 + -3;
	// addi r5,r11,-4
	ctx.r5.s64 = ctx.r11.s64 + -4;
	// rlwinm r4,r9,1,0,30
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r25,1,0,30
	ctx.r10.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r30,r7,1,0,30
	ctx.r30.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r27,r6,1,0,30
	ctx.r27.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r26,r5,1,0,30
	ctx.r26.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r31,r11,64
	ctx.r31.s64 = ctx.r11.s64 + 64;
	// subf r6,r10,r4
	ctx.r6.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r5,r10,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r10.s64;
	// rlwinm r7,r31,1,0,30
	ctx.r7.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r4,r10,r30
	ctx.r4.s64 = ctx.r30.s64 - ctx.r10.s64;
	// rlwinm r9,r29,2,0,29
	ctx.r9.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r27,r10,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r10.s64;
	// subf r26,r10,r26
	ctx.r26.s64 = ctx.r26.s64 - ctx.r10.s64;
	// add r30,r6,r24
	ctx.r30.u64 = ctx.r6.u64 + ctx.r24.u64;
	// add r31,r5,r24
	ctx.r31.u64 = ctx.r5.u64 + ctx.r24.u64;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + ctx.r23.u64;
	// add r8,r8,r24
	ctx.r8.u64 = ctx.r8.u64 + ctx.r24.u64;
	// add r7,r7,r24
	ctx.r7.u64 = ctx.r7.u64 + ctx.r24.u64;
	// add r4,r4,r24
	ctx.r4.u64 = ctx.r4.u64 + ctx.r24.u64;
	// add r5,r27,r24
	ctx.r5.u64 = ctx.r27.u64 + ctx.r24.u64;
	// add r6,r26,r24
	ctx.r6.u64 = ctx.r26.u64 + ctx.r24.u64;
loc_822D7C74:
	// lhzux r26,r6,r10
	ea = ctx.r6.u32 + ctx.r10.u32;
	ctx.r26.u64 = PPC_LOAD_U16(ea);
	ctx.r6.u32 = ea;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// lhzux r27,r5,r10
	ea = ctx.r5.u32 + ctx.r10.u32;
	ctx.r27.u64 = PPC_LOAD_U16(ea);
	ctx.r5.u32 = ea;
	// extsh r23,r26
	ctx.r23.s64 = ctx.r26.s16;
	// lhzux r26,r31,r10
	ea = ctx.r31.u32 + ctx.r10.u32;
	ctx.r26.u64 = PPC_LOAD_U16(ea);
	ctx.r31.u32 = ea;
	// extsh r19,r27
	ctx.r19.s64 = ctx.r27.s16;
	// lhzux r27,r4,r10
	ea = ctx.r4.u32 + ctx.r10.u32;
	ctx.r27.u64 = PPC_LOAD_U16(ea);
	ctx.r4.u32 = ea;
	// extsh r18,r26
	ctx.r18.s64 = ctx.r26.s16;
	// lhz r20,2(r8)
	ctx.r20.u64 = PPC_LOAD_U16(ctx.r8.u32 + 2);
	// extsh r27,r27
	ctx.r27.s64 = ctx.r27.s16;
	// lhz r16,0(r8)
	ctx.r16.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// extsh r20,r20
	ctx.r20.s64 = ctx.r20.s16;
	// lhz r17,-2(r8)
	ctx.r17.u64 = PPC_LOAD_U16(ctx.r8.u32 + -2);
	// subf r27,r18,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r18.s64;
	// lhzux r26,r30,r10
	ea = ctx.r30.u32 + ctx.r10.u32;
	ctx.r26.u64 = PPC_LOAD_U16(ea);
	ctx.r30.u32 = ea;
	// subf r20,r19,r20
	ctx.r20.s64 = ctx.r20.s64 - ctx.r19.s64;
	// extsw r27,r27
	ctx.r27.s64 = ctx.r27.s32;
	// extsh r17,r17
	ctx.r17.s64 = ctx.r17.s16;
	// std r27,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r27.u64);
	// extsh r27,r16
	ctx.r27.s64 = ctx.r16.s16;
	// extsh r26,r26
	ctx.r26.s64 = ctx.r26.s16;
	// lfd f9,-184(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// subf r27,r23,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r23.s64;
	// extsw r20,r20
	ctx.r20.s64 = ctx.r20.s32;
	// subf r26,r17,r26
	ctx.r26.s64 = ctx.r26.s64 - ctx.r17.s64;
	// extsw r19,r19
	ctx.r19.s64 = ctx.r19.s32;
	// std r20,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r20.u64);
	// extsw r27,r27
	ctx.r27.s64 = ctx.r27.s32;
	// lfd f12,-208(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// extsw r18,r18
	ctx.r18.s64 = ctx.r18.s32;
	// std r19,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r19.u64);
	// extsw r26,r26
	ctx.r26.s64 = ctx.r26.s32;
	// lfd f11,-192(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// extsw r20,r17
	ctx.r20.s64 = ctx.r17.s32;
	// std r27,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r27.u64);
	// lfd f3,-152(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// std r18,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r18.u64);
	// lfd f8,-176(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// std r26,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r26.u64);
	// lfd f5,-168(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// std r20,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r20.u64);
	// lfd f4,-160(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// extsw r15,r23
	ctx.r15.s64 = ctx.r23.s32;
	// std r15,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r15.u64);
	// lfd f13,-200(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f1,f13
	ctx.f1.f64 = double(ctx.f13.s64);
	// fcfid f13,f12
	ctx.f13.f64 = double(ctx.f12.s64);
	// fcfid f12,f11
	ctx.f12.f64 = double(ctx.f11.s64);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// fcfid f11,f9
	ctx.f11.f64 = double(ctx.f9.s64);
	// fcfid f9,f8
	ctx.f9.f64 = double(ctx.f8.s64);
	// fcfid f8,f5
	ctx.f8.f64 = double(ctx.f5.s64);
	// fcfid f5,f4
	ctx.f5.f64 = double(ctx.f4.s64);
	// frsp f3,f1
	ctx.f3.f64 = double(float(ctx.f1.f64));
	// frsp f1,f12
	ctx.f1.f64 = double(float(ctx.f12.f64));
	// frsp f4,f2
	ctx.f4.f64 = double(float(ctx.f2.f64));
	// frsp f2,f13
	ctx.f2.f64 = double(float(ctx.f13.f64));
	// frsp f13,f11
	ctx.f13.f64 = double(float(ctx.f11.f64));
	// frsp f12,f9
	ctx.f12.f64 = double(float(ctx.f9.f64));
	// frsp f11,f8
	ctx.f11.f64 = double(float(ctx.f8.f64));
	// frsp f9,f5
	ctx.f9.f64 = double(float(ctx.f5.f64));
	// fmadds f8,f4,f0,f3
	ctx.f8.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f0.f64), float(ctx.f3.f64)));
	// fmadds f5,f2,f0,f1
	ctx.f5.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f0.f64), float(ctx.f1.f64)));
	// fmadds f4,f13,f0,f12
	ctx.f4.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// fmadds f3,f11,f0,f9
	ctx.f3.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f0.f64), float(ctx.f9.f64)));
	// fmuls f2,f8,f7
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// stfs f2,0(r9)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmuls f1,f5,f7
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// stfsu f1,4(r9)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmuls f13,f4,f7
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// stfsu f13,4(r9)
	temp.f32 = float(ctx.f13.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmuls f12,f3,f7
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// stfsu f12,4(r9)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// dcbt r0,r7
	// fadds f0,f6,f0
	ctx.f0.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// add r11,r25,r11
	ctx.r11.u64 = ctx.r25.u64 + ctx.r11.u64;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// fcmpu cr6,f0,f10
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// blt cr6,0x822d7ddc
	if (ctx.cr6.lt) goto loc_822D7DDC;
	// fsubs f0,f0,f10
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f10.f64);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// addi r31,r31,8
	ctx.r31.s64 = ctx.r31.s64 + 8;
	// addi r30,r30,8
	ctx.r30.s64 = ctx.r30.s64 + 8;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
loc_822D7DDC:
	// cmplw cr6,r29,r21
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r21.u32, ctx.xer);
	// blt cr6,0x822d7c74
	if (ctx.cr6.lt) goto loc_822D7C74;
loc_822D7DE4:
	// addi r10,r22,-4
	ctx.r10.s64 = ctx.r22.s64 + -4;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x822d7df4
	if (!ctx.cr6.lt) goto loc_822D7DF4;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822D7DF4:
	// subf r10,r22,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r22.s64;
	// stfs f0,-208(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -208, temp.u32);
	// neg r8,r22
	ctx.r8.s64 = -ctx.r22.s64;
	// lwz r9,-208(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -208);
	// addi r11,r10,-4
	ctx.r11.s64 = ctx.r10.s64 + -4;
	// stw r9,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r9.u32);
	// stw r10,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r10.u32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d7ec4
	if (!ctx.cr6.lt) goto loc_822D7EC4;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d7e88
	if (ctx.cr6.lt) goto loc_822D7E88;
	// add r10,r11,r22
	ctx.r10.u64 = ctx.r11.u64 + ctx.r22.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r9,r28
	ctx.r10.u64 = ctx.r9.u64 + ctx.r28.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r22,2
	ctx.r7.s64 = ctx.r22.s64 + 2;
	// addi r6,r22,3
	ctx.r6.s64 = ctx.r22.s64 + 3;
	// addi r5,r8,-3
	ctx.r5.s64 = ctx.r8.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + ctx.r28.u64;
loc_822D7E4C:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r4,r7,r11
	ctx.r4.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r3,r6,r11
	ctx.r3.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r4,r28
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r28.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f12,r3,r28
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r28.u32);
	ctx.f12.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d7e4c
	if (ctx.cr6.lt) goto loc_822D7E4C;
loc_822D7E88:
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d7ec4
	if (!ctx.cr6.lt) goto loc_822D7EC4;
	// add r7,r11,r22
	ctx.r7.u64 = ctx.r11.u64 + ctx.r22.u64;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + ctx.r28.u64;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + ctx.r28.u64;
	// addi r7,r9,-4
	ctx.r7.s64 = ctx.r9.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_822D7EB8:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d7eb8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D7EB8;
loc_822D7EC4:
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d7fa0
	if (ctx.cr6.lt) goto loc_822D7FA0;
	// subfic r9,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r9.s64 = -4 - ctx.r11.s64;
	// add r10,r11,r22
	ctx.r10.u64 = ctx.r11.u64 + ctx.r22.u64;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r8,r28
	ctx.r10.u64 = ctx.r8.u64 + ctx.r28.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r22,2
	ctx.r7.s64 = ctx.r22.s64 + 2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r6,r22,3
	ctx.r6.s64 = ctx.r22.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r8,r24
	ctx.r9.u64 = ctx.r8.u64 + ctx.r24.u64;
loc_822D7F04:
	// add r3,r6,r11
	ctx.r3.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lhz r4,6(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// add r5,r7,r11
	ctx.r5.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lhzu r8,8(r9)
	ea = 8 + ctx.r9.u32;
	ctx.r8.u64 = PPC_LOAD_U16(ea);
	ctx.r9.u32 = ea;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r8,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r8.u64);
	// lfd f0,-152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// lhzx r3,r3,r24
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r3.u32 + ctx.r24.u32);
	// fcfid f9,f0
	ctx.f9.f64 = double(ctx.f0.s64);
	// lhzx r5,r5,r24
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r24.u32);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// std r4,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r4.u64);
	// lfd f13,-160(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// extsh r8,r5
	ctx.r8.s64 = ctx.r5.s16;
	// fcfid f10,f13
	ctx.f10.f64 = double(ctx.f13.s64);
	// extsw r5,r3
	ctx.r5.s64 = ctx.r3.s32;
	// std r8,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r8.u64);
	// lfd f12,-168(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f8,f12
	ctx.f8.f64 = double(ctx.f12.s64);
	// std r5,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r5.u64);
	// lfd f11,-176(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f6,f11
	ctx.f6.f64 = double(ctx.f11.s64);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// frsp f5,f10
	ctx.f5.f64 = double(float(ctx.f10.f64));
	// frsp f4,f9
	ctx.f4.f64 = double(float(ctx.f9.f64));
	// frsp f3,f8
	ctx.f3.f64 = double(float(ctx.f8.f64));
	// frsp f2,f6
	ctx.f2.f64 = double(float(ctx.f6.f64));
	// fmuls f1,f5,f7
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// stfs f1,4(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// fmuls f0,f4,f7
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// fmuls f13,f3,f7
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// fmuls f12,f2,f7
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f7.f64));
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d7f04
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D7F04;
loc_822D7FA0:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822d7ff4
	if (!ctx.cr6.lt) goto loc_822D7FF4;
	// add r9,r11,r22
	ctx.r9.u64 = ctx.r11.u64 + ctx.r22.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r8,r11
	ctx.r8.s64 = -ctx.r11.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r28
	ctx.r11.u64 = ctx.r10.u64 + ctx.r28.u64;
	// add r9,r9,r24
	ctx.r9.u64 = ctx.r9.u64 + ctx.r24.u64;
	// addi r10,r11,-4
	ctx.r10.s64 = ctx.r11.s64 + -4;
	// addi r11,r9,-2
	ctx.r11.s64 = ctx.r9.s64 + -2;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D7FCC:
	// lhzu r9,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// std r8,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r8.u64);
	// lfd f0,-152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f7
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d7fcc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D7FCC;
loc_822D7FF4:
	// b 0x8233e484
	__restgprlr_15(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D7FF8"))) PPC_WEAK_FUNC(sub_822D7FF8);
PPC_FUNC_IMPL(__imp__sub_822D7FF8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x822D8000;
	__restfpr_14(ctx, base);
	// stfd f30,-168(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.f30.u64);
	// stfd f31,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.f31.u64);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lwz r7,12(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// li r23,0
	ctx.r23.s64 = 0;
	// lwz r18,24(r3)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r18,0
	ctx.cr6.compare<uint32_t>(ctx.r18.u32, 0, ctx.xer);
	// stw r10,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r10.u32);
	// lfs f13,-288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	ctx.f13.f64 = double(temp.f32);
	// fctiwz f12,f13
	ctx.f12.u64 = uint64_t(int32_t(std::trunc(ctx.f13.f64)));
	// stfd f12,-272(r1)
	PPC_STORE_U64(ctx.r1.u32 + -272, ctx.f12.u64);
	// lwz r10,-268(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// extsw r5,r10
	ctx.r5.s64 = ctx.r10.s32;
	// std r5,-272(r1)
	PPC_STORE_U64(ctx.r1.u32 + -272, ctx.r5.u64);
	// lfd f11,-272(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -272);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// add r4,r10,r9
	ctx.r4.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r5,r11,48
	ctx.r5.s64 = ctx.r11.s64 + 48;
	// stw r7,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r7.u32);
	// rlwinm r24,r4,1,0,30
	ctx.r24.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r17,4(r3)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lfs f5,-1496(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -1496);
	ctx.f5.f64 = double(temp.f32);
	// lwz r16,28(r3)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lfs f0,-288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	ctx.f0.f64 = double(temp.f32);
	// stw r3,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r3.u32);
	// stw r18,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r18.u32);
	// stw r24,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r24.u32);
	// fsubs f4,f13,f9
	ctx.f4.f64 = static_cast<float>(ctx.f13.f64 - ctx.f9.f64);
	// beq cr6,0x822d814c
	if (ctx.cr6.eq) goto loc_822D814C;
	// lhz r7,8(r6)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r6.u32 + 8);
	// lhz r4,10(r6)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r6.u32 + 10);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r9,4(r6)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r6.u32 + 4);
	// lhz r31,6(r6)
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r6.u32 + 6);
	// std r7,-288(r1)
	PPC_STORE_U64(ctx.r1.u32 + -288, ctx.r7.u64);
	// extsh r7,r4
	ctx.r7.s64 = ctx.r4.s16;
	// lhz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// extsh r4,r9
	ctx.r4.s64 = ctx.r9.s16;
	// lhz r8,2(r6)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// extsh r9,r31
	ctx.r9.s64 = ctx.r31.s16;
	// std r7,-248(r1)
	PPC_STORE_U64(ctx.r1.u32 + -248, ctx.r7.u64);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lfd f9,-248(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -248);
	// lfd f13,-288(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -288);
	// std r9,-248(r1)
	PPC_STORE_U64(ctx.r1.u32 + -248, ctx.r9.u64);
	// lfd f8,-248(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -248);
	// std r10,-264(r1)
	PPC_STORE_U64(ctx.r1.u32 + -264, ctx.r10.u64);
	// std r8,-256(r1)
	PPC_STORE_U64(ctx.r1.u32 + -256, ctx.r8.u64);
	// lfd f12,-264(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -264);
	// std r4,-288(r1)
	PPC_STORE_U64(ctx.r1.u32 + -288, ctx.r4.u64);
	// lfd f11,-288(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -288);
	// lfd f10,-256(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -256);
	// fcfid f1,f13
	ctx.f1.f64 = double(ctx.f13.s64);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// fcfid f3,f12
	ctx.f3.f64 = double(ctx.f12.s64);
	// fcfid f13,f9
	ctx.f13.f64 = double(ctx.f9.s64);
	// fcfid f2,f10
	ctx.f2.f64 = double(ctx.f10.s64);
	// fcfid f6,f11
	ctx.f6.f64 = double(ctx.f11.s64);
	// frsp f8,f1
	ctx.f8.f64 = double(float(ctx.f1.f64));
	// frsp f12,f7
	ctx.f12.f64 = double(float(ctx.f7.f64));
	// frsp f10,f3
	ctx.f10.f64 = double(float(ctx.f3.f64));
	// frsp f7,f13
	ctx.f7.f64 = double(float(ctx.f13.f64));
	// frsp f9,f2
	ctx.f9.f64 = double(float(ctx.f2.f64));
	// frsp f11,f6
	ctx.f11.f64 = double(float(ctx.f6.f64));
	// fmuls f13,f8,f5
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// stfs f13,16(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 16, temp.u32);
	// fmuls f6,f12,f5
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f5.f64));
	// stfs f6,12(r5)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r5.u32 + 12, temp.u32);
	// fmuls f2,f10,f5
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// stfs f2,0(r5)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// fmuls f12,f7,f5
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// stfs f12,20(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
	// fmuls f1,f9,f5
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// stfs f1,4(r5)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// fmuls f3,f11,f5
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f5.f64));
	// stfs f3,8(r5)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
loc_822D814C:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// lfs f6,5256(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f6.f64 = double(temp.f32);
	// bge cr6,0x822d82d0
	if (!ctx.cr6.lt) goto loc_822D82D0;
	// addi r31,r11,3
	ctx.r31.s64 = ctx.r11.s64 + 3;
	// addi r30,r11,-4
	ctx.r30.s64 = ctx.r11.s64 + -4;
	// addi r7,r11,4
	ctx.r7.s64 = ctx.r11.s64 + 4;
	// addi r4,r11,-3
	ctx.r4.s64 = ctx.r11.s64 + -3;
	// addi r29,r11,2
	ctx.r29.s64 = ctx.r11.s64 + 2;
	// addi r9,r11,5
	ctx.r9.s64 = ctx.r11.s64 + 5;
	// addi r8,r11,-2
	ctx.r8.s64 = ctx.r11.s64 + -2;
	// rlwinm r10,r24,2,0,29
	ctx.r10.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r22,r31,2,0,29
	ctx.r22.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r28,r11,-5
	ctx.r28.s64 = ctx.r11.s64 + -5;
	// addi r27,r11,-6
	ctx.r27.s64 = ctx.r11.s64 + -6;
	// rlwinm r21,r30,2,0,29
	ctx.r21.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r26,r7,2,0,29
	ctx.r26.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r4,2,0,29
	ctx.r25.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r20,r29,2,0,29
	ctx.r20.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r10,r22
	ctx.r29.s64 = ctx.r22.s64 - ctx.r10.s64;
	// subf r22,r10,r21
	ctx.r22.s64 = ctx.r21.s64 - ctx.r10.s64;
	// subf r4,r10,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r7,r10,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r10.s64;
	// subf r31,r10,r26
	ctx.r31.s64 = ctx.r26.s64 - ctx.r10.s64;
	// subf r30,r10,r25
	ctx.r30.s64 = ctx.r25.s64 - ctx.r10.s64;
	// subf r21,r10,r20
	ctx.r21.s64 = ctx.r20.s64 - ctx.r10.s64;
	// subf r20,r10,r28
	ctx.r20.s64 = ctx.r28.s64 - ctx.r10.s64;
	// subf r19,r10,r27
	ctx.r19.s64 = ctx.r27.s64 - ctx.r10.s64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r25,r7,r5
	ctx.r25.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r26,r4,r5
	ctx.r26.u64 = ctx.r4.u64 + ctx.r5.u64;
	// add r27,r31,r5
	ctx.r27.u64 = ctx.r31.u64 + ctx.r5.u64;
	// add r28,r30,r5
	ctx.r28.u64 = ctx.r30.u64 + ctx.r5.u64;
	// mr r9,r17
	ctx.r9.u64 = ctx.r17.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r29,r29,r5
	ctx.r29.u64 = ctx.r29.u64 + ctx.r5.u64;
	// add r30,r22,r5
	ctx.r30.u64 = ctx.r22.u64 + ctx.r5.u64;
	// add r31,r21,r5
	ctx.r31.u64 = ctx.r21.u64 + ctx.r5.u64;
	// add r4,r20,r5
	ctx.r4.u64 = ctx.r20.u64 + ctx.r5.u64;
	// add r7,r19,r5
	ctx.r7.u64 = ctx.r19.u64 + ctx.r5.u64;
loc_822D81FC:
	// cmplw cr6,r23,r16
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, ctx.r16.u32, ctx.xer);
	// bge cr6,0x822d85b0
	if (!ctx.cr6.lt) goto loc_822D85B0;
	// lfsux f11,r30,r10
	ctx.fpscr.disableFlushMode();
	ea = ctx.r30.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r30.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// addi r23,r23,6
	ctx.r23.s64 = ctx.r23.s64 + 6;
	// lfsux f8,r31,r10
	ea = ctx.r31.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r31.u32 = ea;
	ctx.f8.f64 = double(temp.f32);
	// add r11,r24,r11
	ctx.r11.u64 = ctx.r24.u64 + ctx.r11.u64;
	// fsubs f3,f8,f11
	ctx.f3.f64 = static_cast<float>(ctx.f8.f64 - ctx.f11.f64);
	// lfsux f10,r28,r10
	ea = ctx.r28.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r28.u32 = ea;
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// lfsux f7,r29,r10
	ea = ctx.r29.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r29.u32 = ea;
	ctx.f7.f64 = double(temp.f32);
	// lfsux f13,r7,r10
	ea = ctx.r7.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r7.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fsubs f2,f7,f10
	ctx.f2.f64 = static_cast<float>(ctx.f7.f64 - ctx.f10.f64);
	// lfsux f12,r4,r10
	ea = ctx.r4.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fsubs f31,f9,f13
	ctx.f31.f64 = static_cast<float>(ctx.f9.f64 - ctx.f13.f64);
	// fsubs f30,f8,f12
	ctx.f30.f64 = static_cast<float>(ctx.f8.f64 - ctx.f12.f64);
	// lfs f1,-4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	ctx.f1.f64 = double(temp.f32);
	// lfsux f9,r26,r10
	ea = ctx.r26.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r26.u32 = ea;
	ctx.f9.f64 = double(temp.f32);
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lfsux f8,r27,r10
	ea = ctx.r27.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r27.u32 = ea;
	ctx.f8.f64 = double(temp.f32);
	// lfsux f7,r25,r10
	ea = ctx.r25.u32 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r25.u32 = ea;
	ctx.f7.f64 = double(temp.f32);
	// fsubs f8,f8,f9
	ctx.f8.f64 = static_cast<float>(ctx.f8.f64 - ctx.f9.f64);
	// fsubs f7,f7,f1
	ctx.f7.f64 = static_cast<float>(ctx.f7.f64 - ctx.f1.f64);
	// fmadds f3,f3,f0,f11
	ctx.f3.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// fmadds f2,f2,f0,f10
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f0.f64), float(ctx.f10.f64)));
	// fmadds f13,f31,f0,f13
	ctx.f13.f64 = double(std::fma(float(ctx.f31.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f12,f30,f0,f12
	ctx.f12.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfsu f12,4(r9)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// stfsu f3,4(r9)
	temp.f32 = float(ctx.f3.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// stfsu f2,4(r9)
	temp.f32 = float(ctx.f2.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmadds f11,f8,f0,f9
	ctx.f11.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f0.f64), float(ctx.f9.f64)));
	// stfsu f11,4(r9)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmadds f10,f7,f0,f1
	ctx.f10.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f0.f64), float(ctx.f1.f64)));
	// stfsu f10,4(r9)
	temp.f32 = float(ctx.f10.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fadds f0,f4,f0
	ctx.f0.f64 = double(float(ctx.f4.f64 + ctx.f0.f64));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fcmpu cr6,f0,f6
	ctx.cr6.compare(ctx.f0.f64, ctx.f6.f64);
	// blt cr6,0x822d82c8
	if (ctx.cr6.lt) goto loc_822D82C8;
	// fsubs f0,f0,f6
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f6.f64);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// addi r7,r7,24
	ctx.r7.s64 = ctx.r7.s64 + 24;
	// addi r4,r4,24
	ctx.r4.s64 = ctx.r4.s64 + 24;
	// addi r31,r31,24
	ctx.r31.s64 = ctx.r31.s64 + 24;
	// addi r30,r30,24
	ctx.r30.s64 = ctx.r30.s64 + 24;
	// addi r29,r29,24
	ctx.r29.s64 = ctx.r29.s64 + 24;
	// addi r28,r28,24
	ctx.r28.s64 = ctx.r28.s64 + 24;
	// addi r27,r27,24
	ctx.r27.s64 = ctx.r27.s64 + 24;
	// addi r26,r26,24
	ctx.r26.s64 = ctx.r26.s64 + 24;
	// addi r25,r25,24
	ctx.r25.s64 = ctx.r25.s64 + 24;
	// addi r8,r8,24
	ctx.r8.s64 = ctx.r8.s64 + 24;
loc_822D82C8:
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// blt cr6,0x822d81fc
	if (ctx.cr6.lt) goto loc_822D81FC;
loc_822D82D0:
	// cmplw cr6,r23,r16
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, ctx.r16.u32, ctx.xer);
	// bge cr6,0x822d85b0
	if (!ctx.cr6.lt) goto loc_822D85B0;
	// addi r4,r11,3
	ctx.r4.s64 = ctx.r11.s64 + 3;
	// addi r9,r11,5
	ctx.r9.s64 = ctx.r11.s64 + 5;
	// addi r8,r11,4
	ctx.r8.s64 = ctx.r11.s64 + 4;
	// addi r7,r11,-3
	ctx.r7.s64 = ctx.r11.s64 + -3;
	// addi r31,r11,-4
	ctx.r31.s64 = ctx.r11.s64 + -4;
	// addi r30,r11,2
	ctx.r30.s64 = ctx.r11.s64 + 2;
	// addi r29,r11,-5
	ctx.r29.s64 = ctx.r11.s64 + -5;
	// addi r3,r11,-2
	ctx.r3.s64 = ctx.r11.s64 + -2;
	// addi r28,r11,-6
	ctx.r28.s64 = ctx.r11.s64 + -6;
	// rlwinm r10,r24,1,0,30
	ctx.r10.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r27,r4,1,0,30
	ctx.r27.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r31,1,0,30
	ctx.r25.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r24,r30,1,0,30
	ctx.r24.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r20,r29,1,0,30
	ctx.r20.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r26,r11,64
	ctx.r26.s64 = ctx.r11.s64 + 64;
	// rlwinm r28,r28,1,0,30
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r4,r10,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r10.s64;
	// subf r31,r10,r8
	ctx.r31.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r30,r10,r7
	ctx.r30.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r29,r10,r27
	ctx.r29.s64 = ctx.r27.s64 - ctx.r10.s64;
	// rlwinm r7,r26,1,0,30
	ctx.r7.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r22,r10,r25
	ctx.r22.s64 = ctx.r25.s64 - ctx.r10.s64;
	// subf r21,r10,r24
	ctx.r21.s64 = ctx.r24.s64 - ctx.r10.s64;
	// subf r19,r10,r28
	ctx.r19.s64 = ctx.r28.s64 - ctx.r10.s64;
	// rlwinm r9,r23,2,0,29
	ctx.r9.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r3,r10,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r10.s64;
	// subf r20,r10,r20
	ctx.r20.s64 = ctx.r20.s64 - ctx.r10.s64;
	// add r24,r4,r6
	ctx.r24.u64 = ctx.r4.u64 + ctx.r6.u64;
	// add r26,r31,r6
	ctx.r26.u64 = ctx.r31.u64 + ctx.r6.u64;
	// add r27,r30,r6
	ctx.r27.u64 = ctx.r30.u64 + ctx.r6.u64;
	// add r28,r29,r6
	ctx.r28.u64 = ctx.r29.u64 + ctx.r6.u64;
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + ctx.r17.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// add r25,r3,r6
	ctx.r25.u64 = ctx.r3.u64 + ctx.r6.u64;
	// add r29,r22,r6
	ctx.r29.u64 = ctx.r22.u64 + ctx.r6.u64;
	// add r30,r21,r6
	ctx.r30.u64 = ctx.r21.u64 + ctx.r6.u64;
	// add r31,r20,r6
	ctx.r31.u64 = ctx.r20.u64 + ctx.r6.u64;
	// add r4,r19,r6
	ctx.r4.u64 = ctx.r19.u64 + ctx.r6.u64;
loc_822D8388:
	// lhzux r22,r29,r10
	ea = ctx.r29.u32 + ctx.r10.u32;
	ctx.r22.u64 = PPC_LOAD_U16(ea);
	ctx.r29.u32 = ea;
	// addi r23,r23,6
	ctx.r23.s64 = ctx.r23.s64 + 6;
	// lhzux r21,r4,r10
	ea = ctx.r4.u32 + ctx.r10.u32;
	ctx.r21.u64 = PPC_LOAD_U16(ea);
	ctx.r4.u32 = ea;
	// lhz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// extsh r18,r22
	ctx.r18.s64 = ctx.r22.s16;
	// lhz r19,2(r8)
	ctx.r19.u64 = PPC_LOAD_U16(ctx.r8.u32 + 2);
	// extsh r17,r21
	ctx.r17.s64 = ctx.r21.s16;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lhzux r22,r31,r10
	ea = ctx.r31.u32 + ctx.r10.u32;
	ctx.r22.u64 = PPC_LOAD_U16(ea);
	ctx.r31.u32 = ea;
	// lhzux r21,r30,r10
	ea = ctx.r30.u32 + ctx.r10.u32;
	ctx.r21.u64 = PPC_LOAD_U16(ea);
	ctx.r30.u32 = ea;
	// extsh r16,r19
	ctx.r16.s64 = ctx.r19.s16;
	// subf r3,r17,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r17.s64;
	// lhzux r20,r27,r10
	ea = ctx.r27.u32 + ctx.r10.u32;
	ctx.r20.u64 = PPC_LOAD_U16(ea);
	ctx.r27.u32 = ea;
	// extsh r22,r22
	ctx.r22.s64 = ctx.r22.s16;
	// lhzux r19,r28,r10
	ea = ctx.r28.u32 + ctx.r10.u32;
	ctx.r19.u64 = PPC_LOAD_U16(ea);
	ctx.r28.u32 = ea;
	// extsh r15,r21
	ctx.r15.s64 = ctx.r21.s16;
	// lhzux r14,r26,r10
	ea = ctx.r26.u32 + ctx.r10.u32;
	ctx.r14.u64 = PPC_LOAD_U16(ea);
	ctx.r26.u32 = ea;
	// extsw r3,r3
	ctx.r3.s64 = ctx.r3.s32;
	// lhzux r21,r25,r10
	ea = ctx.r25.u32 + ctx.r10.u32;
	ctx.r21.u64 = PPC_LOAD_U16(ea);
	ctx.r25.u32 = ea;
	// subf r16,r22,r16
	ctx.r16.s64 = ctx.r16.s64 - ctx.r22.s64;
	// extsh r20,r20
	ctx.r20.s64 = ctx.r20.s16;
	// std r3,-248(r1)
	PPC_STORE_U64(ctx.r1.u32 + -248, ctx.r3.u64);
	// extsh r19,r19
	ctx.r19.s64 = ctx.r19.s16;
	// lfd f13,-248(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -248);
	// extsw r17,r17
	ctx.r17.s64 = ctx.r17.s32;
	// fcfid f2,f13
	ctx.f2.f64 = double(ctx.f13.s64);
	// subf r15,r18,r15
	ctx.r15.s64 = ctx.r15.s64 - ctx.r18.s64;
	// std r17,-256(r1)
	PPC_STORE_U64(ctx.r1.u32 + -256, ctx.r17.u64);
	// extsw r22,r22
	ctx.r22.s64 = ctx.r22.s32;
	// lfd f12,-256(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -256);
	// extsw r3,r16
	ctx.r3.s64 = ctx.r16.s32;
	// fcfid f1,f12
	ctx.f1.f64 = double(ctx.f12.s64);
	// subf r19,r20,r19
	ctx.r19.s64 = ctx.r19.s64 - ctx.r20.s64;
	// std r22,-264(r1)
	PPC_STORE_U64(ctx.r1.u32 + -264, ctx.r22.u64);
	// extsw r17,r15
	ctx.r17.s64 = ctx.r15.s32;
	// std r3,-240(r1)
	PPC_STORE_U64(ctx.r1.u32 + -240, ctx.r3.u64);
	// extsw r18,r18
	ctx.r18.s64 = ctx.r18.s32;
	// lfd f10,-240(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -240);
	// extsw r19,r19
	ctx.r19.s64 = ctx.r19.s32;
	// std r17,-232(r1)
	PPC_STORE_U64(ctx.r1.u32 + -232, ctx.r17.u64);
	// extsw r22,r20
	ctx.r22.s64 = ctx.r20.s32;
	// lfd f9,-232(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -232);
	// std r18,-224(r1)
	PPC_STORE_U64(ctx.r1.u32 + -224, ctx.r18.u64);
	// lfd f8,-224(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -224);
	// std r19,-216(r1)
	PPC_STORE_U64(ctx.r1.u32 + -216, ctx.r19.u64);
	// lfd f7,-216(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -216);
	// std r22,-208(r1)
	PPC_STORE_U64(ctx.r1.u32 + -208, ctx.r22.u64);
	// lfd f3,-208(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
	// lfd f11,-264(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -264);
	// fcfid f12,f11
	ctx.f12.f64 = double(ctx.f11.s64);
	// fcfid f11,f10
	ctx.f11.f64 = double(ctx.f10.s64);
	// sth r14,-288(r1)
	PPC_STORE_U16(ctx.r1.u32 + -288, ctx.r14.u16);
	// fcfid f10,f9
	ctx.f10.f64 = double(ctx.f9.s64);
	// clrlwi r14,r14,16
	ctx.r14.u64 = ctx.r14.u32 & 0xFFFF;
	// fcfid f9,f8
	ctx.f9.f64 = double(ctx.f8.s64);
	// extsh r3,r21
	ctx.r3.s64 = ctx.r21.s16;
	// fcfid f8,f7
	ctx.f8.f64 = double(ctx.f7.s64);
	// extsh r22,r14
	ctx.r22.s64 = ctx.r14.s16;
	// fcfid f7,f3
	ctx.f7.f64 = double(ctx.f3.s64);
	// frsp f3,f2
	ctx.f3.f64 = double(float(ctx.f2.f64));
	// subf r22,r3,r22
	ctx.r22.s64 = ctx.r22.s64 - ctx.r3.s64;
	// frsp f2,f1
	ctx.f2.f64 = double(float(ctx.f1.f64));
	// frsp f1,f12
	ctx.f1.f64 = double(float(ctx.f12.f64));
	// extsw r22,r22
	ctx.r22.s64 = ctx.r22.s32;
	// std r22,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r22.u64);
	// lfd f13,-200(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// frsp f12,f11
	ctx.f12.f64 = double(float(ctx.f11.f64));
	// frsp f11,f10
	ctx.f11.f64 = double(float(ctx.f10.f64));
	// frsp f10,f9
	ctx.f10.f64 = double(float(ctx.f9.f64));
	// frsp f9,f8
	ctx.f9.f64 = double(float(ctx.f8.f64));
	// frsp f8,f7
	ctx.f8.f64 = double(float(ctx.f7.f64));
	// fmadds f7,f3,f0,f2
	ctx.f7.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f2.f64)));
	// fmadds f3,f12,f0,f1
	ctx.f3.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f1.f64)));
	// fmadds f2,f11,f0,f10
	ctx.f2.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f0.f64), float(ctx.f10.f64)));
	// fmadds f1,f9,f0,f8
	ctx.f1.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f0.f64), float(ctx.f8.f64)));
	// fmuls f12,f7,f5
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// stfs f12,0(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmuls f11,f3,f5
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// stfsu f11,4(r9)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmuls f10,f2,f5
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// fmuls f9,f1,f5
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// lhz r22,-2(r8)
	ctx.r22.u64 = PPC_LOAD_U16(ctx.r8.u32 + -2);
	// extsw r21,r3
	ctx.r21.s64 = ctx.r3.s32;
	// lhzux r3,r24,r10
	ea = ctx.r24.u32 + ctx.r10.u32;
	ctx.r3.u64 = PPC_LOAD_U16(ea);
	ctx.r24.u32 = ea;
	// fcfid f8,f13
	ctx.f8.f64 = double(ctx.f13.s64);
	// extsh r22,r22
	ctx.r22.s64 = ctx.r22.s16;
	// std r21,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r21.u64);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lfd f7,-192(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// extsw r21,r22
	ctx.r21.s64 = ctx.r22.s32;
	// fcfid f3,f7
	ctx.f3.f64 = double(ctx.f7.s64);
	// std r21,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r21.u64);
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r22.s64;
	// frsp f13,f8
	ctx.f13.f64 = double(float(ctx.f8.f64));
	// stfsu f10,4(r9)
	temp.f32 = float(ctx.f10.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// extsw r3,r3
	ctx.r3.s64 = ctx.r3.s32;
	// frsp f12,f3
	ctx.f12.f64 = double(float(ctx.f3.f64));
	// stfsu f9,4(r9)
	temp.f32 = float(ctx.f9.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// std r3,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r3.u64);
	// lfd f8,-176(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// lfd f2,-184(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// frsp f3,f7
	ctx.f3.f64 = double(float(ctx.f7.f64));
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// fmadds f10,f13,f0,f12
	ctx.f10.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// frsp f11,f1
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// fmuls f9,f10,f5
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// stfsu f9,4(r9)
	temp.f32 = float(ctx.f9.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// fmadds f2,f3,f0,f11
	ctx.f2.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// fmuls f1,f2,f5
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// stfsu f1,4(r9)
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r9.u32 = ea;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// dcbt r0,r7
	// fadds f0,f4,f0
	ctx.f0.f64 = double(float(ctx.f4.f64 + ctx.f0.f64));
	// lwz r3,-280(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 + ctx.r11.u64;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// fcmpu cr6,f0,f6
	ctx.cr6.compare(ctx.f0.f64, ctx.f6.f64);
	// blt cr6,0x822d859c
	if (ctx.cr6.lt) goto loc_822D859C;
	// fsubs f0,f0,f6
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f6.f64);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// addi r4,r4,12
	ctx.r4.s64 = ctx.r4.s64 + 12;
	// addi r31,r31,12
	ctx.r31.s64 = ctx.r31.s64 + 12;
	// addi r30,r30,12
	ctx.r30.s64 = ctx.r30.s64 + 12;
	// addi r29,r29,12
	ctx.r29.s64 = ctx.r29.s64 + 12;
	// addi r28,r28,12
	ctx.r28.s64 = ctx.r28.s64 + 12;
	// addi r27,r27,12
	ctx.r27.s64 = ctx.r27.s64 + 12;
	// addi r26,r26,12
	ctx.r26.s64 = ctx.r26.s64 + 12;
	// addi r25,r25,12
	ctx.r25.s64 = ctx.r25.s64 + 12;
	// addi r24,r24,12
	ctx.r24.s64 = ctx.r24.s64 + 12;
	// addi r8,r8,12
	ctx.r8.s64 = ctx.r8.s64 + 12;
	// addi r7,r7,12
	ctx.r7.s64 = ctx.r7.s64 + 12;
loc_822D859C:
	// lwz r3,-276(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// lwz r22,28(r3)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// cmplw cr6,r23,r22
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, ctx.r22.u32, ctx.xer);
	// blt cr6,0x822d8388
	if (ctx.cr6.lt) goto loc_822D8388;
	// lwz r18,-272(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
loc_822D85B0:
	// addi r10,r18,-6
	ctx.r10.s64 = ctx.r18.s64 + -6;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x822d85c0
	if (!ctx.cr6.lt) goto loc_822D85C0;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822D85C0:
	// subf r10,r18,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r18.s64;
	// stfs f0,-272(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -272, temp.u32);
	// neg r8,r18
	ctx.r8.s64 = -ctx.r18.s64;
	// lwz r9,-272(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// addi r11,r10,-6
	ctx.r11.s64 = ctx.r10.s64 + -6;
	// stw r9,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r9.u32);
	// stw r10,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r10.u32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d8690
	if (!ctx.cr6.lt) goto loc_822D8690;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d8654
	if (ctx.cr6.lt) goto loc_822D8654;
	// add r10,r11,r18
	ctx.r10.u64 = ctx.r11.u64 + ctx.r18.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r9,r5
	ctx.r10.u64 = ctx.r9.u64 + ctx.r5.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r18,2
	ctx.r7.s64 = ctx.r18.s64 + 2;
	// addi r4,r18,3
	ctx.r4.s64 = ctx.r18.s64 + 3;
	// addi r3,r8,-3
	ctx.r3.s64 = ctx.r8.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
loc_822D8618:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r31,r7,r11
	ctx.r31.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r30,r4,r11
	ctx.r30.u64 = ctx.r4.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r31,r31,2,0,29
	ctx.r31.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// rlwinm r30,r30,2,0,29
	ctx.r30.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r31,r5
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r5.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f12,r30,r5
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r5.u32);
	ctx.f12.f64 = double(temp.f32);
	// cmpw cr6,r11,r3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r3.s32, ctx.xer);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d8618
	if (ctx.cr6.lt) goto loc_822D8618;
loc_822D8654:
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d8690
	if (!ctx.cr6.lt) goto loc_822D8690;
	// add r7,r11,r18
	ctx.r7.u64 = ctx.r11.u64 + ctx.r18.u64;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r7,r9,-4
	ctx.r7.s64 = ctx.r9.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_822D8684:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d8684
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D8684;
loc_822D8690:
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d876c
	if (ctx.cr6.lt) goto loc_822D876C;
	// subfic r9,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r9.s64 = -4 - ctx.r11.s64;
	// add r10,r11,r18
	ctx.r10.u64 = ctx.r11.u64 + ctx.r18.u64;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r8,r5
	ctx.r10.u64 = ctx.r8.u64 + ctx.r5.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r18,2
	ctx.r7.s64 = ctx.r18.s64 + 2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r4,r18,3
	ctx.r4.s64 = ctx.r18.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r8,r6
	ctx.r9.u64 = ctx.r8.u64 + ctx.r6.u64;
loc_822D86D0:
	// lhz r31,6(r9)
	ctx.r31.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// add r3,r4,r11
	ctx.r3.u64 = ctx.r4.u64 + ctx.r11.u64;
	// add r30,r7,r11
	ctx.r30.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lhzu r8,8(r9)
	ea = 8 + ctx.r9.u32;
	ctx.r8.u64 = PPC_LOAD_U16(ea);
	ctx.r9.u32 = ea;
	// extsh r31,r31
	ctx.r31.s64 = ctx.r31.s16;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// std r31,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r31.u64);
	// lfd f10,-192(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// rlwinm r30,r30,1,0,30
	ctx.r30.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lhzx r3,r3,r6
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r3.u32 + ctx.r6.u32);
	// std r8,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r8.u64);
	// lfd f12,-184(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lhzx r8,r30,r6
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r30.u32 + ctx.r6.u32);
	// fcfid f9,f12
	ctx.f9.f64 = double(ctx.f12.s64);
	// std r3,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r3.u64);
	// extsh r3,r8
	ctx.r3.s64 = ctx.r8.s16;
	// lfd f8,-200(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f7,f10
	ctx.f7.f64 = double(ctx.f10.s64);
	// extsw r8,r3
	ctx.r8.s64 = ctx.r3.s32;
	// fcfid f6,f8
	ctx.f6.f64 = double(ctx.f8.s64);
	// std r8,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r8.u64);
	// frsp f3,f9
	ctx.f3.f64 = double(float(ctx.f9.f64));
	// frsp f2,f7
	ctx.f2.f64 = double(float(ctx.f7.f64));
	// frsp f1,f6
	ctx.f1.f64 = double(float(ctx.f6.f64));
	// fmuls f12,f1,f5
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// lfd f0,-176(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// fmuls f0,f3,f5
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f2,f5
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// fmuls f4,f11,f5
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f5.f64));
	// stfs f4,12(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d86d0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D86D0;
loc_822D876C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822d87c0
	if (!ctx.cr6.lt) goto loc_822D87C0;
	// add r10,r11,r18
	ctx.r10.u64 = ctx.r11.u64 + ctx.r18.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r8,r11
	ctx.r8.s64 = -ctx.r11.s64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r9,r5
	ctx.r11.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r9,r10,r6
	ctx.r9.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r10,r11,-4
	ctx.r10.s64 = ctx.r11.s64 + -4;
	// addi r11,r9,-2
	ctx.r11.s64 = ctx.r9.s64 + -2;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D8798:
	// lhzu r9,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// std r8,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r8.u64);
	// lfd f0,-176(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f5
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f5.f64));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d8798
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D8798;
loc_822D87C0:
	// lfd f30,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lfd f31,-160(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D87CC"))) PPC_WEAK_FUNC(sub_822D87CC);
PPC_FUNC_IMPL(__imp__sub_822D87CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822D87D0"))) PPC_WEAK_FUNC(sub_822D87D0);
PPC_FUNC_IMPL(__imp__sub_822D87D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e448
	ctx.lr = 0x822D87D8;
	__restfpr_20(ctx, base);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r23,r6
	ctx.r23.u64 = ctx.r6.u64;
	// mr r22,r3
	ctx.r22.u64 = ctx.r3.u64;
	// dcbt r0,r3
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lfs f13,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// li r8,4
	ctx.r8.s64 = 4;
	// vspltisw128 v63,0
	simd::store_i32(ctx.v63.u32, simd::set1_i32(int32_t(0x0)));
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// rlwinm r9,r5,0,0,27
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFF0;
	// lfd f0,-1464(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -1464);
	// li r10,2
	ctx.r10.s64 = 2;
	// fmul f12,f1,f0
	ctx.f12.f64 = ctx.f1.f64 * ctx.f0.f64;
	// li r29,16
	ctx.r29.s64 = 16;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// rlwinm r25,r5,0,28,29
	ctx.r25.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xC;
	// clrlwi r24,r5,30
	ctx.r24.u64 = ctx.r5.u32 & 0x3;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f10.u64);
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fctidz f9,f11
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// stfd f9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f9.u64);
	// lvlx128 v56,r8,r7
	temp.u32 = ctx.r8.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r3,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r3.u64);
	// rldicr r26,r11,3,60
	ctx.r26.u64 = rotl64(ctx.r11.u64, 3) & 0xFFFFFFFFFFFFFFF8;
	// vspltw128 v27,v56,0
	simd::store_i32(ctx.v27.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v56.u32), 3));
	// lvlx128 v55,r8,r6
	temp.u32 = ctx.r8.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v12,v55,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v55.u32), 3));
	// vadduwm v0,v27,v27
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v27.u32), simd::load_u32(ctx.v27.u32)));
	// vsldoi128 v54,v63,v27,4
	simd::store_i8(ctx.v54.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v27.u8), 12));
	// vadduwm v13,v27,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v27.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v53,v54,v0,4
	simd::store_i8(ctx.v53.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v0.u8), 12));
	// vadduwm v28,v0,v0
	simd::store_u32(ctx.v28.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v0,v53,v13,4
	simd::store_i8(ctx.v0.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v53.u8), simd::load_i8(ctx.v13.u8), 12));
	// vadduwm v10,v28,v28
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v28.u32), simd::load_u32(ctx.v28.u32)));
	// vadduwm v11,v0,v28
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v28.u32)));
	// vadduwm v0,v0,v12
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v12.u32)));
	// vadduwm v13,v10,v10
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v10.u32)));
	// vadduwm v12,v11,v12
	simd::store_u32(ctx.v12.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v12.u32)));
	// vadduwm v11,v0,v10
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v10.u32)));
	// vadduwm v10,v12,v10
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v10.u32)));
	// beq cr6,0x822d8aa4
	if (ctx.cr6.eq) goto loc_822D8AA4;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// vspltisw128 v63,1
	simd::store_i32(ctx.v63.u32, simd::set1_i32(int32_t(0x1)));
	// addi r30,r4,32
	ctx.r30.s64 = ctx.r4.s64 + 32;
	// rlwinm r9,r9,28,4,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 28) & 0xFFFFFFF;
	// li r27,128
	ctx.r27.s64 = 128;
	// addi r28,r9,1
	ctx.r28.s64 = ctx.r9.s64 + 1;
loc_822D88AC:
	// add r9,r26,r3
	ctx.r9.u64 = ctx.r26.u64 + ctx.r3.u64;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// rldicl r7,r9,32,32
	ctx.r7.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// clrldi r8,r9,32
	ctx.r8.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// rlwinm r9,r7,1,0,30
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + ctx.r31.u64;
loc_822D88C4:
	// rldicl r7,r3,32,32
	ctx.r7.u64 = rotl64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// rldicl r5,r8,32,32
	ctx.r5.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF;
	// rlwinm r6,r7,1,0,30
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + ctx.r31.u64;
	// rldicl r21,r8,32,32
	ctx.r21.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r11,r3
	ctx.r7.u64 = ctx.r11.u64 + ctx.r3.u64;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// rlwinm r3,r21,1,0,30
	ctx.r3.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v52,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lvlx128 v51,r6,r10
	temp.u32 = ctx.r6.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r6,r3,r9
	ctx.r6.u64 = ctx.r3.u64 + ctx.r9.u64;
	// vsldoi128 v50,v60,v52,2
	simd::store_i8(ctx.v50.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v52.u8), 14));
	// rldicl r21,r8,32,32
	ctx.r21.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v49,v59,v51,2
	simd::store_i8(ctx.v49.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v51.u8), 14));
	// lvlx128 v48,r0,r5
	temp.u32 = ctx.r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lvlx128 v47,r5,r10
	temp.u32 = ctx.r5.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r5,r21,1,0,30
	ctx.r5.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 1) & 0xFFFFFFFE;
	// rldicl r21,r8,32,32
	ctx.r21.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v46,r3,r9
	temp.u32 = ctx.r3.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v45,r6,r10
	temp.u32 = ctx.r6.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r6,r5,r9
	ctx.r6.u64 = ctx.r5.u64 + ctx.r9.u64;
	// rlwinm r3,r21,1,0,30
	ctx.r3.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 1) & 0xFFFFFFFE;
	// vsldoi128 v43,v61,v47,2
	simd::store_i8(ctx.v43.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v47.u8), 14));
	// rldicl r5,r7,32,32
	ctx.r5.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v44,v62,v48,2
	simd::store_i8(ctx.v44.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v48.u8), 14));
	// add r7,r11,r7
	ctx.r7.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v42,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rldicl r21,r7,32,32
	ctx.r21.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v41,r6,r10
	temp.u32 = ctx.r6.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r6,r3,r9
	ctx.r6.u64 = ctx.r3.u64 + ctx.r9.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + ctx.r31.u64;
	// lvlx128 v40,r3,r9
	temp.u32 = ctx.r3.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r7,r11,r7
	ctx.r7.u64 = ctx.r11.u64 + ctx.r7.u64;
	// vsldoi128 v39,v44,v46,2
	simd::store_i8(ctx.v39.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v44.u8), simd::load_i8(ctx.v46.u8), 14));
	// vsldoi128 v38,v43,v45,2
	simd::store_i8(ctx.v38.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v43.u8), simd::load_i8(ctx.v45.u8), 14));
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// rldicl r20,r7,32,32
	ctx.r20.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v37,r6,r10
	temp.u32 = ctx.r6.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v37,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r6,r21,1,0,30
	ctx.r6.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v36,r0,r5
	temp.u32 = ctx.r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r3,r11,r7
	ctx.r3.u64 = ctx.r11.u64 + ctx.r7.u64;
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + ctx.r31.u64;
	// lvlx128 v35,r5,r10
	temp.u32 = ctx.r5.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r7,r20,1,0,30
	ctx.r7.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 1) & 0xFFFFFFFE;
	// vsldoi128 v34,v50,v36,2
	simd::store_i8(ctx.v34.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v36.u8), 14));
	// vsldoi128 v33,v49,v35,2
	simd::store_i8(ctx.v33.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v49.u8), simd::load_i8(ctx.v35.u8), 14));
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + ctx.r31.u64;
	// vsldoi128 v32,v39,v42,2
	simd::store_i8(ctx.v32.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v39.u8), simd::load_i8(ctx.v42.u8), 14));
	// vsldoi128 v61,v38,v41,2
	simd::store_i8(ctx.v61.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v38.u8), simd::load_i8(ctx.v41.u8), 14));
	// lvlx128 v60,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v59,r6,r10
	temp.u32 = ctx.r6.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v56,v34,v60,2
	simd::store_i8(ctx.v56.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v34.u8), simd::load_i8(ctx.v60.u8), 14));
	// vsldoi128 v55,v33,v59,2
	simd::store_i8(ctx.v55.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v33.u8), simd::load_i8(ctx.v59.u8), 14));
	// lvlx128 v54,r0,r7
	temp.u32 = ctx.r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r7,r10
	temp.u32 = ctx.r7.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v62,v32,v40,2
	simd::store_i8(ctx.v62.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v32.u8), simd::load_i8(ctx.v40.u8), 14));
	// vsldoi128 v61,v61,v37,2
	simd::store_i8(ctx.v61.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v37.u8), 14));
	// vsldoi128 v60,v56,v54,2
	simd::store_i8(ctx.v60.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v54.u8), 14));
	// vsldoi128 v59,v55,v53,2
	simd::store_i8(ctx.v59.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v53.u8), 14));
	// bdnz 0x822d88c4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D88C4;
	// rldicl r7,r8,32,32
	ctx.r7.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF;
	// clrldi r3,r8,32
	ctx.r3.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r8,r9
	ctx.r31.u64 = ctx.r8.u64 + ctx.r9.u64;
	// dcbt r31,r27
	// vupkhsb128 v52,v60,v96
	simd::store_i32(ctx.v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v60.s16)));
	// vsrw128 v51,v0,v63
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(ctx.v51, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// vupkhsb128 v50,v59,v96
	simd::store_i32(ctx.v50.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v59.s16)));
	// vsrw128 v49,v12,v63
simd::store_shuffled(ctx.v49, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v12), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// vupklsb128 v48,v60,v96
	simd::store_i32(ctx.v48.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v60.s16)));
	// vsrw128 v47,v11,v63
simd::store_shuffled(ctx.v47, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v11), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// vupklsb128 v46,v59,v96
	simd::store_i32(ctx.v46.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v59.s16)));
	// vsrw128 v45,v10,v63
simd::store_shuffled(ctx.v45, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v10), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// vupkhsb128 v44,v62,v96
	simd::store_i32(ctx.v44.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v62.s16)));
	// vcsxwfp128 v9,v52,15
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v52.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v43,v61,v96
	simd::store_i32(ctx.v43.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v61.s16)));
	// vcsxwfp128 v42,v50,15
	simd::store_f32_aligned(ctx.v42.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v50.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v41,v62,v96
	simd::store_i32(ctx.v41.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v62.s16)));
	// vcsxwfp128 v8,v48,15
	simd::store_f32_aligned(ctx.v8.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v40,v61,v96
	simd::store_i32(ctx.v40.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v61.s16)));
	// vcsxwfp128 v39,v46,15
	simd::store_f32_aligned(ctx.v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v46.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v7,v44,15
	simd::store_f32_aligned(ctx.v7.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v44.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// addi r9,r30,-16
	ctx.r9.s64 = ctx.r30.s64 + -16;
	// vcsxwfp128 v38,v43,15
	simd::store_f32_aligned(ctx.v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v43.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// addi r8,r30,16
	ctx.r8.s64 = ctx.r30.s64 + 16;
	// vcsxwfp128 v6,v41,15
	simd::store_f32_aligned(ctx.v6.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v41.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vadduwm v0,v0,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vcsxwfp128 v37,v40,15
	simd::store_f32_aligned(ctx.v37.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v40.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vadduwm v12,v12,v13
	simd::store_u32(ctx.v12.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v13.u32)));
	// vcuxwfp128 v4,v51,31
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v51.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vadduwm v11,v11,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v13.u32)));
	// vcuxwfp128 v2,v49,31
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v49.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vadduwm v10,v10,v13
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v13.u32)));
	// vcuxwfp128 v31,v47,31
	simd::store_f32_aligned(ctx.v31.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v47.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// addic. r28,r28,-1
	ctx.xer.ca = ctx.r28.u32 > 0;
	ctx.r28.s64 = ctx.r28.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// vcuxwfp128 v29,v45,31
	simd::store_f32_aligned(ctx.v29.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v45.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsubfp128 v5,v42,v9
	simd::store_f32_aligned(ctx.v5.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v42.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vsubfp128 v3,v39,v8
	simd::store_f32_aligned(ctx.v3.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v39.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vsubfp128 v1,v38,v7
	simd::store_f32_aligned(ctx.v1.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v38.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vsubfp128 v30,v37,v6
	simd::store_f32_aligned(ctx.v30.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v37.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// vmaddfp v9,v5,v4,v9
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v4.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vmaddfp v8,v3,v2,v8
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v2.f32)), simd::load_f32_aligned(ctx.v8.f32)));
	// vmaddfp v7,v1,v31,v7
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v31.f32)), simd::load_f32_aligned(ctx.v7.f32)));
	// vmaddfp v6,v30,v29,v6
	simd::store_f32_aligned(ctx.v6.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v30.f32), simd::load_f32_aligned(ctx.v29.f32)), simd::load_f32_aligned(ctx.v6.f32)));
	// stvlx v9,0,r4
{
	uint32_t addr = 
ctx.r4.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v9), 15 - i));
}
	// stvrx v9,r4,r29
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r29.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v9), i));
}
	// addi r4,r4,64
	ctx.r4.s64 = ctx.r4.s64 + 64;
	// stvlx v8,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v8), 15 - i));
}
	// stvrx v8,r9,r29
{
	uint32_t addr = 
ctx.r9.u32 + ctx.r29.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v8), i));
}
	// stvlx v7,0,r30
{
	uint32_t addr = 
ctx.r30.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v7), 15 - i));
}
	// stvrx v7,r30,r29
{
	uint32_t addr = 
ctx.r30.u32 + ctx.r29.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v7), i));
}
	// addi r30,r30,64
	ctx.r30.s64 = ctx.r30.s64 + 64;
	// stvlx v6,0,r8
{
	uint32_t addr = 
ctx.r8.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v6), 15 - i));
}
	// stvrx v6,r8,r29
{
	uint32_t addr = 
ctx.r8.u32 + ctx.r29.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v6), i));
}
	// bne 0x822d88ac
	if (!ctx.cr0.eq) goto loc_822D88AC;
loc_822D8AA4:
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// beq cr6,0x822d8b8c
	if (ctx.cr6.eq) goto loc_822D8B8C;
	// addi r9,r25,-1
	ctx.r9.s64 = ctx.r25.s64 + -1;
	// vspltisw128 v63,1
	simd::store_i32(ctx.v63.u32, simd::set1_i32(int32_t(0x1)));
	// li r7,32
	ctx.r7.s64 = 32;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822D8AC4:
	// rldicl r8,r3,32,32
	ctx.r8.u64 = rotl64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// add r9,r11,r3
	ctx.r9.u64 = ctx.r11.u64 + ctx.r3.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rldicl r6,r9,32,32
	ctx.r6.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// add r9,r11,r9
	ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rldicl r5,r9,32,32
	ctx.r5.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// add r9,r11,r9
	ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lvlx128 v36,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v35,r8,r10
	temp.u32 = ctx.r8.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r8,r6,1,0,30
	ctx.r8.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rldicl r6,r9,32,32
	ctx.r6.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v34,v58,v36,2
	simd::store_i8(ctx.v34.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v36.u8), 14));
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// vsldoi128 v33,v57,v35,2
	simd::store_i8(ctx.v33.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v35.u8), 14));
	// add r9,r11,r9
	ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rldicl r30,r9,32,32
	ctx.r30.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// clrldi r3,r9,32
	ctx.r3.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lvlx128 v32,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r9,r30,1,0,30
	ctx.r9.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v62,r8,r10
	temp.u32 = ctx.r8.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r8,r5,1,0,30
	ctx.r8.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// vsldoi128 v61,v34,v32,2
	simd::store_i8(ctx.v61.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v34.u8), simd::load_i8(ctx.v32.u8), 14));
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// vsldoi128 v60,v33,v62,2
	simd::store_i8(ctx.v60.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v33.u8), simd::load_i8(ctx.v62.u8), 14));
	// lvlx128 v58,r8,r10
	temp.u32 = ctx.r8.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v59,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r8,r6,1,0,30
	ctx.r8.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// vsldoi128 v56,v60,v58,2
	simd::store_i8(ctx.v56.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v58.u8), 14));
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// vsldoi128 v57,v61,v59,2
	simd::store_i8(ctx.v57.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v59.u8), 14));
	// add r31,r9,r31
	ctx.r31.u64 = ctx.r9.u64 + ctx.r31.u64;
	// lvlx128 v55,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v54,r8,r10
	temp.u32 = ctx.r8.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v58,v57,v55,2
	simd::store_i8(ctx.v58.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v55.u8), 14));
	// vsldoi128 v57,v56,v54,2
	simd::store_i8(ctx.v57.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v54.u8), 14));
	// dcbt r31,r7
	// vupklsb128 v53,v58,v96
	simd::store_i32(ctx.v53.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v58.s16)));
	// vsrw128 v52,v0,v63
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(ctx.v52, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// vupklsb128 v51,v57,v96
	simd::store_i32(ctx.v51.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v57.s16)));
	// vadduwm v0,v0,v28
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v28.u32)));
	// vcsxwfp128 v13,v53,15
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v53.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v50,v51,15
	simd::store_f32_aligned(ctx.v50.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v51.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcuxwfp128 v11,v52,31
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v52.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsubfp128 v12,v50,v13
	simd::store_f32_aligned(ctx.v12.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v50.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v13,v12,v11,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// stvlx v13,0,r4
{
	uint32_t addr = 
ctx.r4.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v13), 15 - i));
}
	// stvrx v13,r4,r29
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r29.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v13), i));
}
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// bdnz 0x822d8ac4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D8AC4;
loc_822D8B8C:
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x822d8bf8
	if (ctx.cr6.eq) goto loc_822D8BF8;
	// vspltisw128 v63,1
	simd::store_i32(ctx.v63.u32, simd::set1_i32(int32_t(0x1)));
	// mtctr r24
	ctx.ctr.u64 = ctx.r24.u64;
loc_822D8B9C:
	// rldicl r8,r3,32,32
	ctx.r8.u64 = rotl64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vsrw128 v49,v0,v63
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(ctx.v49, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// add r9,r11,r3
	ctx.r9.u64 = ctx.r11.u64 + ctx.r3.u64;
	// vadduwm v0,v0,v27
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v27.u32)));
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rldicl r7,r9,32,32
	ctx.r7.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// vcuxwfp128 v11,v49,31
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v49.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// clrldi r3,r9,32
	ctx.r3.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// rlwinm r9,r7,1,0,30
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r9,r31
	ctx.r31.u64 = ctx.r9.u64 + ctx.r31.u64;
	// lvlx128 v48,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v47,r8,r10
	temp.u32 = ctx.r8.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v46,v48,v96
	simd::store_i32(ctx.v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v48.s16)));
	// vupkhsb128 v45,v47,v96
	simd::store_i32(ctx.v45.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v47.s16)));
	// vcsxwfp128 v13,v46,15
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v46.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v44,v45,15
	simd::store_f32_aligned(ctx.v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v45.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vsubfp128 v12,v44,v13
	simd::store_f32_aligned(ctx.v12.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v44.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v13,v12,v11,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vspltw128 v43,v13,0
	simd::store_i32(ctx.v43.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 3));
	// stvewx128 v43,r0,r4
	PPC_STORE_U32((ctx.r4.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v43.u32), 3 - ((ctx.r4.u32) & 0xF) >> 2));
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// bdnz 0x822d8b9c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D8B9C;
loc_822D8BF8:
	// bl 0x823413b0
	ctx.lr = 0x822D8BFC;
	sub_823413B0(ctx, base);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// subf r10,r22,r31
	ctx.r10.s64 = ctx.r31.s64 - ctx.r22.s64;
	// srawi r3,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r10.s32 >> 1;
	// lfd f0,-1472(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -1472);
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(ctx.f0.f64));
	// stfs f13,0(r23)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r23.u32 + 0, temp.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8233e498
	__restgprlr_20(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D8C20"))) PPC_WEAK_FUNC(sub_822D8C20);
PPC_FUNC_IMPL(__imp__sub_822D8C20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822D8C28;
	__restfpr_27(ctx, base);
	// stfd f30,-64(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.f30.u64);
	// stfd f31,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f31.u64);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lwz r7,16(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r30,24(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r27,0(r3)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r28,r11,8
	ctx.r28.s64 = ctx.r11.s64 + 8;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// fctiwz f13,f1
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f1.f64)));
	// stfd f13,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f13.u64);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r6,r9
	ctx.r6.s64 = ctx.r9.s32;
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// lfs f30,-1496(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -1496);
	ctx.f30.f64 = double(temp.f32);
	// lwz r5,28(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lwz r31,36(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// fsubs f13,f1,f10
	ctx.f13.f64 = static_cast<float>(ctx.f1.f64 - ctx.f10.f64);
	// beq cr6,0x822d8cc8
	if (ctx.cr6.eq) goto loc_822D8CC8;
	// lhz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r27.u32 + 0);
	// extsh r7,r11
	ctx.r7.s64 = ctx.r11.s16;
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// lfd f12,88(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f30
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f30.f64));
	// stfs f9,0(r28)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r28.u32 + 0, temp.u32);
loc_822D8CC8:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// cmpwi cr6,r31,1
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 1, ctx.xer);
	// lfs f31,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f31.f64 = double(temp.f32);
	// bge cr6,0x822d8d34
	if (!ctx.cr6.lt) goto loc_822D8D34;
	// rlwinm r11,r31,2,0,29
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r4,-4
	ctx.r8.s64 = ctx.r4.s64 + -4;
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + ctx.r28.u64;
loc_822D8CE8:
	// cmplw cr6,r10,r5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, ctx.xer);
	// bge cr6,0x822d8d30
	if (!ctx.cr6.lt) goto loc_822D8D30;
	// lfs f12,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lfs f11,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// add r31,r9,r31
	ctx.r31.u64 = ctx.r9.u64 + ctx.r31.u64;
	// fsubs f10,f11,f12
	ctx.f10.f64 = static_cast<float>(ctx.f11.f64 - ctx.f12.f64);
	// add r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 + ctx.r11.u64;
	// fmadds f9,f10,f0,f12
	ctx.f9.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfsu f9,4(r8)
	temp.f32 = float(ctx.f9.f64);
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r8.u32 = ea;
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// blt cr6,0x822d8d28
	if (ctx.cr6.lt) goto loc_822D8D28;
	// fsubs f0,f0,f31
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f31.f64);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
loc_822D8D28:
	// cmpwi cr6,r31,1
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 1, ctx.xer);
	// blt cr6,0x822d8ce8
	if (ctx.cr6.lt) goto loc_822D8CE8;
loc_822D8D30:
	// stfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
loc_822D8D34:
	// rlwinm r11,r31,1,0,30
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// subf r5,r10,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r10.s64;
	// add r4,r9,r4
	ctx.r4.u64 = ctx.r9.u64 + ctx.r4.u64;
	// addi r3,r11,-2
	ctx.r3.s64 = ctx.r11.s64 + -2;
	// bl 0x822d87d0
	ctx.lr = 0x822D8D54;
	sub_822D87D0(ctx, base);
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// add r11,r3,r31
	ctx.r11.u64 = ctx.r3.u64 + ctx.r31.u64;
	// blt cr6,0x822d8d70
	if (ctx.cr6.lt) goto loc_822D8D70;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// lfs f0,-28948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
loc_822D8D70:
	// addi r10,r30,-1
	ctx.r10.s64 = ctx.r30.s64 + -1;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x822d8d80
	if (!ctx.cr6.lt) goto loc_822D8D80;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822D8D80:
	// subf r10,r30,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r30.s64;
	// stfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// neg r8,r30
	ctx.r8.s64 = -ctx.r30.s64;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r10,-1
	ctx.r11.s64 = ctx.r10.s64 + -1;
	// stw r9,12(r29)
	PPC_STORE_U32(ctx.r29.u32 + 12, ctx.r9.u32);
	// stw r10,36(r29)
	PPC_STORE_U32(ctx.r29.u32 + 36, ctx.r10.u32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d8e50
	if (!ctx.cr6.lt) goto loc_822D8E50;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d8e14
	if (ctx.cr6.lt) goto loc_822D8E14;
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r9,r28
	ctx.r10.u64 = ctx.r9.u64 + ctx.r28.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r30,2
	ctx.r7.s64 = ctx.r30.s64 + 2;
	// addi r6,r30,3
	ctx.r6.s64 = ctx.r30.s64 + 3;
	// addi r5,r8,-3
	ctx.r5.s64 = ctx.r8.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + ctx.r28.u64;
loc_822D8DD8:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r4,r7,r11
	ctx.r4.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r3,r6,r11
	ctx.r3.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r4,r28
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r28.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f12,r3,r28
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r28.u32);
	ctx.f12.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d8dd8
	if (ctx.cr6.lt) goto loc_822D8DD8;
loc_822D8E14:
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d8e50
	if (!ctx.cr6.lt) goto loc_822D8E50;
	// add r7,r11,r30
	ctx.r7.u64 = ctx.r11.u64 + ctx.r30.u64;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + ctx.r28.u64;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + ctx.r28.u64;
	// addi r7,r9,-4
	ctx.r7.s64 = ctx.r9.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_822D8E44:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d8e44
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D8E44;
loc_822D8E50:
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d8f2c
	if (ctx.cr6.lt) goto loc_822D8F2C;
	// subfic r9,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r9.s64 = -4 - ctx.r11.s64;
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r8,r28
	ctx.r10.u64 = ctx.r8.u64 + ctx.r28.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r30,2
	ctx.r7.s64 = ctx.r30.s64 + 2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r6,r30,3
	ctx.r6.s64 = ctx.r30.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r8,r27
	ctx.r9.u64 = ctx.r8.u64 + ctx.r27.u64;
loc_822D8E90:
	// add r5,r6,r11
	ctx.r5.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lhz r4,6(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// add r3,r7,r11
	ctx.r3.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lhzu r8,8(r9)
	ea = 8 + ctx.r9.u32;
	ctx.r8.u64 = PPC_LOAD_U16(ea);
	ctx.r9.u32 = ea;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// std r4,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r4.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lhzx r5,r5,r27
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r27.u32);
	// fcfid f10,f0
	ctx.f10.f64 = double(ctx.f0.s64);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// lhzx r3,r3,r27
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r3.u32 + ctx.r27.u32);
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// lfd f12,96(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// extsh r4,r3
	ctx.r4.s64 = ctx.r3.s16;
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// extsw r3,r4
	ctx.r3.s64 = ctx.r4.s32;
	// fcfid f9,f13
	ctx.f9.f64 = double(ctx.f13.s64);
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// std r3,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r3.u64);
	// lfd f4,104(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f2,f4
	ctx.f2.f64 = double(ctx.f4.s64);
	// frsp f7,f10
	ctx.f7.f64 = double(float(ctx.f10.f64));
	// frsp f0,f2
	ctx.f0.f64 = double(float(ctx.f2.f64));
	// frsp f6,f9
	ctx.f6.f64 = double(float(ctx.f9.f64));
	// fmuls f5,f8,f30
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f30.f64));
	// stfs f5,8(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// fmuls f3,f7,f30
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f30.f64));
	// stfs f3,4(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// fmuls f13,f0,f30
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f30.f64));
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// fmuls f1,f6,f30
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f30.f64));
	// stfsu f1,16(r10)
	temp.f32 = float(ctx.f1.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d8e90
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D8E90;
loc_822D8F2C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822d8f80
	if (!ctx.cr6.lt) goto loc_822D8F80;
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r8,r11
	ctx.r8.s64 = -ctx.r11.s64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r9,r28
	ctx.r11.u64 = ctx.r9.u64 + ctx.r28.u64;
	// add r9,r10,r27
	ctx.r9.u64 = ctx.r10.u64 + ctx.r27.u64;
	// addi r10,r11,-4
	ctx.r10.s64 = ctx.r11.s64 + -4;
	// addi r11,r9,-2
	ctx.r11.s64 = ctx.r9.s64 + -2;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D8F58:
	// lhzu r9,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f30
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d8f58
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D8F58;
loc_822D8F80:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lfd f30,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// lfd f31,-56(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D8F90"))) PPC_WEAK_FUNC(sub_822D8F90);
PPC_FUNC_IMPL(__imp__sub_822D8F90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e44c
	ctx.lr = 0x822D8F98;
	__restfpr_21(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r24,r6
	ctx.r24.u64 = ctx.r6.u64;
	// mr r23,r3
	ctx.r23.u64 = ctx.r3.u64;
	// dcbt r0,r3
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f13,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// li r11,4
	ctx.r11.s64 = 4;
	// vspltisw128 v59,0
	simd::store_i32(ctx.v59.u32, simd::set1_i32(int32_t(0x0)));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// rlwinm r9,r5,0,0,27
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFF0;
	// lfd f0,-1464(r10)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + -1464);
	// li r7,16
	ctx.r7.s64 = 16;
	// fmul f12,f1,f0
	ctx.f12.f64 = ctx.f1.f64 * ctx.f0.f64;
	// rlwinm r26,r5,0,28,29
	ctx.r26.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xC;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// clrlwi r25,r5,30
	ctx.r25.u64 = ctx.r5.u32 & 0x3;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f10.u64);
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fctidz f9,f11
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// stfd f9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f9.u64);
	// lvlx128 v56,r11,r8
	temp.u32 = ctx.r11.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r3,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r3.u64);
	// rldicr r28,r10,2,61
	ctx.r28.u64 = rotl64(ctx.r10.u64, 2) & 0xFFFFFFFFFFFFFFFC;
	// vspltw128 v27,v56,0
	simd::store_i32(ctx.v27.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v56.u32), 3));
	// lvlx128 v55,r11,r6
	temp.u32 = ctx.r11.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v55,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v55.u32), 3));
	// vsldoi128 v54,v59,v27,4
	simd::store_i8(ctx.v54.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v59.u8), simd::load_i8(ctx.v27.u8), 12));
	// vadduwm v28,v27,v27
	simd::store_u32(ctx.v28.u32, simd::add_u32(simd::load_u32(ctx.v27.u32), simd::load_u32(ctx.v27.u32)));
	// vsldoi128 v0,v54,v27,4
	simd::store_i8(ctx.v0.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v54.u8), simd::load_i8(ctx.v27.u8), 12));
	// vadduwm v10,v28,v28
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v28.u32), simd::load_u32(ctx.v28.u32)));
	// vadduwm v12,v0,v28
	simd::store_u32(ctx.v12.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v28.u32)));
	// vadduwm v0,v0,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vadduwm v12,v12,v13
	simd::store_u32(ctx.v12.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v13.u32)));
	// vadduwm v13,v10,v10
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v10.u32)));
	// vadduwm v11,v0,v10
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v10.u32)));
	// vadduwm v10,v12,v10
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v10.u32)));
	// beq cr6,0x822d9248
	if (ctx.cr6.eq) goto loc_822D9248;
	// addi r8,r9,-1
	ctx.r8.s64 = ctx.r9.s64 + -1;
	// vspltisw128 v59,1
	simd::store_i32(ctx.v59.u32, simd::set1_i32(int32_t(0x1)));
	// addi r9,r4,32
	ctx.r9.s64 = ctx.r4.s64 + 32;
	// rlwinm r8,r8,28,4,31
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0xFFFFFFF;
	// li r27,128
	ctx.r27.s64 = 128;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D9060:
	// add r8,r28,r3
	ctx.r8.u64 = ctx.r28.u64 + ctx.r3.u64;
	// rldicl r30,r3,32,32
	ctx.r30.u64 = rotl64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// clrldi r5,r8,32
	ctx.r5.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// rldicl r8,r8,32,32
	ctx.r8.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF;
	// add r6,r10,r3
	ctx.r6.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rldicl r29,r5,32,32
	ctx.r29.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// rlwinm r3,r30,2,0,29
	ctx.r3.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + ctx.r31.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// rldicl r22,r5,32,32
	ctx.r22.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// rlwinm r30,r29,2,0,29
	ctx.r30.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r29,r22,2,0,29
	ctx.r29.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r30,r8
	ctx.r30.u64 = ctx.r30.u64 + ctx.r8.u64;
	// lvlx128 v53,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v52,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r3,r29,r8
	ctx.r3.u64 = ctx.r29.u64 + ctx.r8.u64;
	// rldicl r22,r6,32,32
	ctx.r22.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v51,v61,v53,4
	simd::store_i8(ctx.v51.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v61.u8), simd::load_i8(ctx.v53.u8), 12));
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// vsldoi128 v50,v60,v52,4
	simd::store_i8(ctx.v50.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v60.u8), simd::load_i8(ctx.v52.u8), 12));
	// lvlx128 v49,r29,r8
	temp.u32 = ctx.r29.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// lvlx128 v48,r0,r30
	temp.u32 = ctx.r0.u32 + ctx.r30.u32;
	simd::store_shuffled(ctx.v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rldicl r29,r5,32,32
	ctx.r29.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// lvlx128 v47,r30,r11
	temp.u32 = ctx.r30.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r30,r22,2,0,29
	ctx.r30.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v46,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// add r3,r30,r31
	ctx.r3.u64 = ctx.r30.u64 + ctx.r31.u64;
	// vsldoi128 v45,v63,v48,4
	simd::store_i8(ctx.v45.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v63.u8), simd::load_i8(ctx.v48.u8), 12));
	// rldicl r22,r5,32,32
	ctx.r22.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v44,v62,v47,4
	simd::store_i8(ctx.v44.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v62.u8), simd::load_i8(ctx.v47.u8), 12));
	// rldicl r21,r6,32,32
	ctx.r21.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFF;
	// rlwinm r29,r29,2,0,29
	ctx.r29.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v43,v45,v49,4
	simd::store_i8(ctx.v43.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v45.u8), simd::load_i8(ctx.v49.u8), 12));
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lvlx128 v42,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r30,r29,r8
	ctx.r30.u64 = ctx.r29.u64 + ctx.r8.u64;
	// lvlx128 v41,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r3,r22,2,0,29
	ctx.r3.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v40,v44,v46,4
	simd::store_i8(ctx.v40.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v44.u8), simd::load_i8(ctx.v46.u8), 12));
	// rldicl r22,r5,32,32
	ctx.r22.u64 = rotl64(ctx.r5.u64, 32) & 0xFFFFFFFF;
	// add r3,r3,r8
	ctx.r3.u64 = ctx.r3.u64 + ctx.r8.u64;
	// lvlx128 v37,r29,r8
	temp.u32 = ctx.r29.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v37,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v39,v51,v42,4
	simd::store_i8(ctx.v39.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v51.u8), simd::load_i8(ctx.v42.u8), 12));
	// vsldoi128 v38,v50,v41,4
	simd::store_i8(ctx.v38.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v50.u8), simd::load_i8(ctx.v41.u8), 12));
	// lvlx128 v36,r30,r11
	temp.u32 = ctx.r30.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v35,v43,v37,4
	simd::store_i8(ctx.v35.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v43.u8), simd::load_i8(ctx.v37.u8), 12));
	// rlwinm r30,r22,2,0,29
	ctx.r30.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v34,v40,v36,4
	simd::store_i8(ctx.v34.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v40.u8), simd::load_i8(ctx.v36.u8), 12));
	// lvlx128 v33,r0,r3
	temp.u32 = ctx.r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v33,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v32,r3,r11
	temp.u32 = ctx.r3.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r3,r10,r6
	ctx.r3.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r6,r21,2,0,29
	ctx.r6.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v63,v35,v33,4
	simd::store_i8(ctx.v63.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v35.u8), simd::load_i8(ctx.v33.u8), 12));
	// rldicl r29,r3,32,32
	ctx.r29.u64 = rotl64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vsldoi128 v62,v34,v32,4
	simd::store_i8(ctx.v62.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v34.u8), simd::load_i8(ctx.v32.u8), 12));
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + ctx.r31.u64;
	// clrldi r3,r5,32
	ctx.r3.u64 = ctx.r5.u64 & 0xFFFFFFFF;
	// lvlx128 v61,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v60,r6,r11
	temp.u32 = ctx.r6.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r6,r29,2,0,29
	ctx.r6.u64 = rotl64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v56,v39,v61,4
	simd::store_i8(ctx.v56.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v39.u8), simd::load_i8(ctx.v61.u8), 12));
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + ctx.r31.u64;
	// vsldoi128 v55,v38,v60,4
	simd::store_i8(ctx.v55.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v38.u8), simd::load_i8(ctx.v60.u8), 12));
	// add r31,r30,r8
	ctx.r31.u64 = ctx.r30.u64 + ctx.r8.u64;
	// lvlx128 v54,r0,r6
	temp.u32 = ctx.r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r6,r11
	temp.u32 = ctx.r6.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v61,v56,v54,4
	simd::store_i8(ctx.v61.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v56.u8), simd::load_i8(ctx.v54.u8), 12));
	// vsldoi128 v60,v55,v53,4
	simd::store_i8(ctx.v60.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v55.u8), simd::load_i8(ctx.v53.u8), 12));
	// dcbt r31,r27
	// vupkhsb128 v52,v61,v96
	simd::store_i32(ctx.v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v61.s16)));
	// vsrw128 v51,v0,v59
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(ctx.v51, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(ctx.v59), simd::set1_i32(0x1F))));
	// vupkhsb128 v50,v60,v96
	simd::store_i32(ctx.v50.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v60.s16)));
	// vsrw128 v49,v12,v59
simd::store_shuffled(ctx.v49, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v12), simd::and_u32(simd::to_vec128i(ctx.v59), simd::set1_i32(0x1F))));
	// vupklsb128 v48,v61,v96
	simd::store_i32(ctx.v48.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v61.s16)));
	// vsrw128 v47,v11,v59
simd::store_shuffled(ctx.v47, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v11), simd::and_u32(simd::to_vec128i(ctx.v59), simd::set1_i32(0x1F))));
	// vupklsb128 v46,v60,v96
	simd::store_i32(ctx.v46.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v60.s16)));
	// vcsxwfp128 v9,v52,15
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v52.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v45,v63,v96
	simd::store_i32(ctx.v45.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v63.s16)));
	// vcsxwfp128 v44,v50,15
	simd::store_f32_aligned(ctx.v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v50.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v43,v62,v96
	simd::store_i32(ctx.v43.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v62.s16)));
	// vcsxwfp128 v8,v48,15
	simd::store_f32_aligned(ctx.v8.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v42,v63,v96
	simd::store_i32(ctx.v42.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v63.s16)));
	// vcsxwfp128 v41,v46,15
	simd::store_f32_aligned(ctx.v41.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v46.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v40,v62,v96
	simd::store_i32(ctx.v40.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v62.s16)));
	// vcuxwfp128 v4,v51,31
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v51.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v7,v45,15
	simd::store_f32_aligned(ctx.v7.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v45.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vsrw128 v39,v10,v59
simd::store_shuffled(ctx.v39, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v10), simd::and_u32(simd::to_vec128i(ctx.v59), simd::set1_i32(0x1F))));
	// vcsxwfp128 v38,v43,15
	simd::store_f32_aligned(ctx.v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v43.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// addi r8,r9,-16
	ctx.r8.s64 = ctx.r9.s64 + -16;
	// vcsxwfp128 v6,v42,15
	simd::store_f32_aligned(ctx.v6.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v42.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// vcsxwfp128 v37,v40,15
	simd::store_f32_aligned(ctx.v37.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v40.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vadduwm v0,v0,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vcuxwfp128 v2,v49,31
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v49.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vadduwm v12,v12,v13
	simd::store_u32(ctx.v12.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v13.u32)));
	// vcuxwfp128 v31,v47,31
	simd::store_f32_aligned(ctx.v31.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v47.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vadduwm v11,v11,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v13.u32)));
	// vcuxwfp128 v29,v39,31
	simd::store_f32_aligned(ctx.v29.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v39.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vadduwm v10,v10,v13
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v13.u32)));
	// vsubfp128 v5,v44,v9
	simd::store_f32_aligned(ctx.v5.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v44.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vsubfp128 v3,v41,v8
	simd::store_f32_aligned(ctx.v3.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v41.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vsubfp128 v1,v38,v7
	simd::store_f32_aligned(ctx.v1.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v38.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vsubfp128 v30,v37,v6
	simd::store_f32_aligned(ctx.v30.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v37.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// vmaddfp v9,v5,v4,v9
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v4.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vmaddfp v8,v3,v2,v8
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v2.f32)), simd::load_f32_aligned(ctx.v8.f32)));
	// vmaddfp v7,v1,v31,v7
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v31.f32)), simd::load_f32_aligned(ctx.v7.f32)));
	// vmaddfp v6,v30,v29,v6
	simd::store_f32_aligned(ctx.v6.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v30.f32), simd::load_f32_aligned(ctx.v29.f32)), simd::load_f32_aligned(ctx.v6.f32)));
	// stvlx v9,0,r4
{
	uint32_t addr = 
ctx.r4.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v9), 15 - i));
}
	// stvrx v9,r4,r7
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v9), i));
}
	// addi r4,r4,64
	ctx.r4.s64 = ctx.r4.s64 + 64;
	// stvlx v8,0,r8
{
	uint32_t addr = 
ctx.r8.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v8), 15 - i));
}
	// stvrx v8,r8,r7
{
	uint32_t addr = 
ctx.r8.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v8), i));
}
	// stvlx v7,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v7), 15 - i));
}
	// stvrx v7,r9,r7
{
	uint32_t addr = 
ctx.r9.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v7), i));
}
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// stvlx v6,0,r6
{
	uint32_t addr = 
ctx.r6.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v6), 15 - i));
}
	// stvrx v6,r6,r7
{
	uint32_t addr = 
ctx.r6.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v6), i));
}
	// bdnz 0x822d9060
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D9060;
loc_822D9248:
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// beq cr6,0x822d92f0
	if (ctx.cr6.eq) goto loc_822D92F0;
	// addi r9,r26,-1
	ctx.r9.s64 = ctx.r26.s64 + -1;
	// vspltisw128 v63,1
	simd::store_i32(ctx.v63.u32, simd::set1_i32(int32_t(0x1)));
	// li r6,32
	ctx.r6.s64 = 32;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822D9268:
	// rldicl r8,r3,32,32
	ctx.r8.u64 = rotl64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rldicl r5,r9,32,32
	ctx.r5.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rldicl r30,r9,32,32
	ctx.r30.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// clrldi r3,r9,32
	ctx.r3.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lvlx128 v36,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r9,r30,2,0,29
	ctx.r9.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v35,r8,r11
	temp.u32 = ctx.r8.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v34,v58,v36,4
	simd::store_i8(ctx.v34.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v58.u8), simd::load_i8(ctx.v36.u8), 12));
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// vsldoi128 v33,v57,v35,4
	simd::store_i8(ctx.v33.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v57.u8), simd::load_i8(ctx.v35.u8), 12));
	// add r31,r9,r31
	ctx.r31.u64 = ctx.r9.u64 + ctx.r31.u64;
	// lvlx128 v32,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v62,r8,r11
	temp.u32 = ctx.r8.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v58,v34,v32,4
	simd::store_i8(ctx.v58.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v34.u8), simd::load_i8(ctx.v32.u8), 12));
	// vsldoi128 v57,v33,v62,4
	simd::store_i8(ctx.v57.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v33.u8), simd::load_i8(ctx.v62.u8), 12));
	// dcbt r31,r6
	// vupklsb128 v61,v58,v96
	simd::store_i32(ctx.v61.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v58.s16)));
	// vsrw128 v60,v0,v63
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(ctx.v60, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// vupklsb128 v59,v57,v96
	simd::store_i32(ctx.v59.s32, simd::extend_i16_lo_to_i32(simd::load_i16(ctx.v57.s16)));
	// vadduwm v0,v0,v28
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v28.u32)));
	// vcsxwfp128 v13,v61,15
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v61.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v56,v59,15
	simd::store_f32_aligned(ctx.v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v59.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcuxwfp128 v11,v60,31
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v60.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsubfp128 v12,v56,v13
	simd::store_f32_aligned(ctx.v12.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v56.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v13,v12,v11,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// stvlx v13,0,r4
{
	uint32_t addr = 
ctx.r4.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v13), 15 - i));
}
	// stvrx v13,r4,r7
{
	uint32_t addr = 
ctx.r4.u32 + ctx.r7.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v13), i));
}
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// bdnz 0x822d9268
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D9268;
loc_822D92F0:
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// beq cr6,0x822d9370
	if (ctx.cr6.eq) goto loc_822D9370;
	// addi r9,r25,-1
	ctx.r9.s64 = ctx.r25.s64 + -1;
	// vspltisw128 v63,1
	simd::store_i32(ctx.v63.u32, simd::set1_i32(int32_t(0x1)));
	// rlwinm r9,r9,31,1,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822D930C:
	// rldicl r8,r3,32,32
	ctx.r8.u64 = rotl64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vsrw128 v55,v0,v63
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(ctx.v55, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(ctx.v63), simd::set1_i32(0x1F))));
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vadduwm v0,v0,v27
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v27.u32)));
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rldicl r7,r9,32,32
	ctx.r7.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// vcuxwfp128 v11,v55,31
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v55.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// clrldi r3,r9,32
	ctx.r3.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r9,r31
	ctx.r31.u64 = ctx.r9.u64 + ctx.r31.u64;
	// lvlx128 v54,r0,r8
	temp.u32 = ctx.r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r8,r11
	temp.u32 = ctx.r8.u32 + ctx.r11.u32;
	simd::store_shuffled(ctx.v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v52,v54,v96
	simd::store_i32(ctx.v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v54.s16)));
	// vupkhsb128 v51,v53,v96
	simd::store_i32(ctx.v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(ctx.v53.s16)));
	// vcsxwfp128 v13,v52,15
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v52.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v50,v51,15
	simd::store_f32_aligned(ctx.v50.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v51.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vsubfp128 v12,v50,v13
	simd::store_f32_aligned(ctx.v12.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v50.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v13,v12,v11,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vspltw128 v49,v13,0
	simd::store_i32(ctx.v49.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 3));
	// vspltw128 v48,v13,1
	simd::store_i32(ctx.v48.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 2));
	// stvewx128 v49,r0,r4
	PPC_STORE_U32((ctx.r4.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v49.u32), 3 - ((ctx.r4.u32) & 0xF) >> 2));
	// stvewx128 v48,r4,r11
	PPC_STORE_U32((ctx.r4.u32 + ctx.r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&ctx.v48.u32), 3 - ((ctx.r4.u32 + ctx.r11.u32) & 0xF) >> 2));
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// bdnz 0x822d930c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D930C;
loc_822D9370:
	// bl 0x823413b0
	ctx.lr = 0x822D9374;
	sub_823413B0(ctx, base);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// subf r10,r23,r31
	ctx.r10.s64 = ctx.r31.s64 - ctx.r23.s64;
	// srawi r3,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r10.s32 >> 1;
	// lfd f0,-1472(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -1472);
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(ctx.f0.f64));
	// stfs f13,0(r24)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r24.u32 + 0, temp.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8233e49c
	__restgprlr_21(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D9398"))) PPC_WEAK_FUNC(sub_822D9398);
PPC_FUNC_IMPL(__imp__sub_822D9398) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822D93A0;
	__restfpr_27(ctx, base);
	// stfd f30,-64(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.f30.u64);
	// stfd f31,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f31.u64);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r7,16(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r29,24(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r27,0(r3)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r30,r11,16
	ctx.r30.s64 = ctx.r11.s64 + 16;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// fctiwz f13,f1
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f1.f64)));
	// stfd f13,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f13.u64);
	// lwz r6,84(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r5,r6
	ctx.r5.s64 = ctx.r6.s32;
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// lwz r5,28(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r31,36(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lfs f30,-1496(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1496);
	ctx.f30.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// fsubs f12,f1,f10
	ctx.f12.f64 = static_cast<float>(ctx.f1.f64 - ctx.f10.f64);
	// beq cr6,0x822d9464
	if (ctx.cr6.eq) goto loc_822D9464;
	// lhz r11,2(r27)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r27.u32 + 2);
	// lhz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r27.u32 + 0);
	// extsh r3,r11
	ctx.r3.s64 = ctx.r11.s16;
	// extsh r11,r10
	ctx.r11.s64 = ctx.r10.s16;
	// std r3,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r3.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f13
	ctx.f9.f64 = double(ctx.f13.s64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// frsp f8,f10
	ctx.f8.f64 = double(float(ctx.f10.f64));
	// fmuls f5,f7,f30
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f30.f64));
	// stfs f5,4(r30)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r30.u32 + 4, temp.u32);
	// fmuls f6,f8,f30
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f30.f64));
	// stfs f6,0(r30)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r30.u32 + 0, temp.u32);
loc_822D9464:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// cmpwi cr6,r31,2
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 2, ctx.xer);
	// lfs f31,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f31.f64 = double(temp.f32);
	// bge cr6,0x822d94fc
	if (!ctx.cr6.lt) goto loc_822D94FC;
	// addi r11,r31,-2
	ctx.r11.s64 = ctx.r31.s64 + -2;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r31,2,0,29
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r7,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r7.s64;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + ctx.r30.u64;
loc_822D9494:
	// cmplw cr6,r8,r5
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r5.u32, ctx.xer);
	// bge cr6,0x822d94f8
	if (!ctx.cr6.lt) goto loc_822D94F8;
	// lfs f11,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f11.f64 = double(temp.f32);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// lfsux f13,r9,r7
	ea = ctx.r9.u32 + ctx.r7.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// add r31,r6,r31
	ctx.r31.u64 = ctx.r6.u64 + ctx.r31.u64;
	// lfs f10,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f8,f10,f13
	ctx.f8.f64 = static_cast<float>(ctx.f10.f64 - ctx.f13.f64);
	// fsubs f7,f9,f11
	ctx.f7.f64 = static_cast<float>(ctx.f9.f64 - ctx.f11.f64);
	// add r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 + ctx.r11.u64;
	// fmadds f6,f8,f0,f13
	ctx.f6.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfs f6,0(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmadds f5,f7,f0,f11
	ctx.f5.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// stfsu f5,4(r10)
	temp.f32 = float(ctx.f5.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// blt cr6,0x822d94f0
	if (ctx.cr6.lt) goto loc_822D94F0;
	// fsubs f0,f0,f31
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f31.f64);
	// addi r31,r31,2
	ctx.r31.s64 = ctx.r31.s64 + 2;
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
loc_822D94F0:
	// cmpwi cr6,r31,2
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 2, ctx.xer);
	// blt cr6,0x822d9494
	if (ctx.cr6.lt) goto loc_822D9494;
loc_822D94F8:
	// stfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
loc_822D94FC:
	// addi r10,r31,-2
	ctx.r10.s64 = ctx.r31.s64 + -2;
	// rlwinm r11,r8,2,0,29
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// subf r5,r8,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r8.s64;
	// add r4,r11,r4
	ctx.r4.u64 = ctx.r11.u64 + ctx.r4.u64;
	// add r3,r10,r27
	ctx.r3.u64 = ctx.r10.u64 + ctx.r27.u64;
	// bl 0x822d8f90
	ctx.lr = 0x822D951C;
	sub_822D8F90(ctx, base);
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// add r11,r3,r31
	ctx.r11.u64 = ctx.r3.u64 + ctx.r31.u64;
	// blt cr6,0x822d9538
	if (ctx.cr6.lt) goto loc_822D9538;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
	// lfs f0,-28948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
loc_822D9538:
	// addi r10,r29,-2
	ctx.r10.s64 = ctx.r29.s64 + -2;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x822d9548
	if (!ctx.cr6.lt) goto loc_822D9548;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822D9548:
	// subf r10,r29,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r29.s64;
	// stfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// neg r8,r29
	ctx.r8.s64 = -ctx.r29.s64;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r10,-2
	ctx.r11.s64 = ctx.r10.s64 + -2;
	// stw r9,12(r28)
	PPC_STORE_U32(ctx.r28.u32 + 12, ctx.r9.u32);
	// stw r10,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r10.u32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d9618
	if (!ctx.cr6.lt) goto loc_822D9618;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d95dc
	if (ctx.cr6.lt) goto loc_822D95DC;
	// add r10,r11,r29
	ctx.r10.u64 = ctx.r11.u64 + ctx.r29.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r9,r30
	ctx.r10.u64 = ctx.r9.u64 + ctx.r30.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r29,2
	ctx.r7.s64 = ctx.r29.s64 + 2;
	// addi r6,r29,3
	ctx.r6.s64 = ctx.r29.s64 + 3;
	// addi r5,r8,-3
	ctx.r5.s64 = ctx.r8.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + ctx.r30.u64;
loc_822D95A0:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r4,r7,r11
	ctx.r4.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r3,r6,r11
	ctx.r3.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r4,r30
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r30.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f12,r3,r30
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r30.u32);
	ctx.f12.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d95a0
	if (ctx.cr6.lt) goto loc_822D95A0;
loc_822D95DC:
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bge cr6,0x822d9618
	if (!ctx.cr6.lt) goto loc_822D9618;
	// add r7,r11,r29
	ctx.r7.u64 = ctx.r11.u64 + ctx.r29.u64;
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + ctx.r30.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + ctx.r30.u64;
	// addi r7,r9,-4
	ctx.r7.s64 = ctx.r9.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_822D960C:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d960c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D960C;
loc_822D9618:
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d96f4
	if (ctx.cr6.lt) goto loc_822D96F4;
	// subfic r9,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r9.s64 = -4 - ctx.r11.s64;
	// add r10,r11,r29
	ctx.r10.u64 = ctx.r11.u64 + ctx.r29.u64;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r10,-3
	ctx.r7.s64 = ctx.r10.s64 + -3;
	// add r10,r8,r30
	ctx.r10.u64 = ctx.r8.u64 + ctx.r30.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r29,2
	ctx.r7.s64 = ctx.r29.s64 + 2;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r6,r29,3
	ctx.r6.s64 = ctx.r29.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r8,r27
	ctx.r9.u64 = ctx.r8.u64 + ctx.r27.u64;
loc_822D9658:
	// add r5,r7,r11
	ctx.r5.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lhz r4,6(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// lhzu r8,8(r9)
	ea = 8 + ctx.r9.u32;
	ctx.r8.u64 = PPC_LOAD_U16(ea);
	ctx.r9.u32 = ea;
	// add r3,r6,r11
	ctx.r3.u64 = ctx.r6.u64 + ctx.r11.u64;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f0
	ctx.f9.f64 = double(ctx.f0.s64);
	// lhzx r5,r5,r27
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r27.u32);
	// frsp f5,f9
	ctx.f5.f64 = double(float(ctx.f9.f64));
	// lhzx r3,r3,r27
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r3.u32 + ctx.r27.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsh r8,r5
	ctx.r8.s64 = ctx.r5.s16;
	// std r4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r4.u64);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// fcfid f10,f13
	ctx.f10.f64 = double(ctx.f13.s64);
	// extsw r5,r3
	ctx.r5.s64 = ctx.r3.s32;
	// lfd f12,96(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f8,f12
	ctx.f8.f64 = double(ctx.f12.s64);
	// std r5,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r5.u64);
	// lfd f11,104(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f7,f11
	ctx.f7.f64 = double(ctx.f11.s64);
	// frsp f6,f10
	ctx.f6.f64 = double(float(ctx.f10.f64));
	// frsp f4,f8
	ctx.f4.f64 = double(float(ctx.f8.f64));
	// frsp f3,f7
	ctx.f3.f64 = double(float(ctx.f7.f64));
	// fmuls f1,f5,f30
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f30.f64));
	// stfs f1,8(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// fmuls f2,f6,f30
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f30.f64));
	// stfs f2,4(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// fmuls f0,f4,f30
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// stfs f0,12(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// fmuls f13,f3,f30
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// stfsu f13,16(r10)
	temp.f32 = float(ctx.f13.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d9658
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D9658;
loc_822D96F4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822d9748
	if (!ctx.cr6.lt) goto loc_822D9748;
	// add r9,r11,r29
	ctx.r9.u64 = ctx.r11.u64 + ctx.r29.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r8,r11
	ctx.r8.s64 = -ctx.r11.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r30
	ctx.r11.u64 = ctx.r10.u64 + ctx.r30.u64;
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + ctx.r27.u64;
	// addi r10,r11,-4
	ctx.r10.s64 = ctx.r11.s64 + -4;
	// addi r11,r9,-2
	ctx.r11.s64 = ctx.r9.s64 + -2;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822D9720:
	// lhzu r9,2(r11)
	ea = 2 + ctx.r11.u32;
	ctx.r9.u64 = PPC_LOAD_U16(ea);
	ctx.r11.u32 = ea;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f30
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// stfsu f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// bdnz 0x822d9720
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D9720;
loc_822D9748:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lfd f30,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// lfd f31,-56(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822D9758"))) PPC_WEAK_FUNC(sub_822D9758);
PPC_FUNC_IMPL(__imp__sub_822D9758) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lbz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfd f0,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// lfs f0,11260(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 11260);
	ctx.f0.f64 = double(temp.f32);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfs f13,-1492(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1492);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f11,f12,f0
	ctx.f11.f64 = static_cast<float>(ctx.f12.f64 - ctx.f0.f64);
	// fmuls f1,f11,f13
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D9788"))) PPC_WEAK_FUNC(sub_822D9788);
PPC_FUNC_IMPL(__imp__sub_822D9788) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lbz r11,2(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lbz r9,1(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// rotlwi r8,r11,8
	ctx.r8.u64 = rotl32(ctx.r11.u32, 8);
	// lbz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// or r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 | ctx.r9.u64;
	// lfs f0,-1484(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1484);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r5,r6,8,0,23
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// or r4,r5,r7
	ctx.r4.u64 = ctx.r5.u64 | ctx.r7.u64;
	// rlwinm r3,r4,8,0,23
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r11,r3,8
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFF) != 0);
	ctx.r11.s64 = ctx.r3.s32 >> 8;
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
	// lfd f13,-16(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f1,f11,f0
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D97D0"))) PPC_WEAK_FUNC(sub_822D97D0);
PPC_FUNC_IMPL(__imp__sub_822D97D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// extsw r9,r11
	ctx.r9.s64 = ctx.r11.s32;
	// std r9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r9.u64);
	// lfd f0,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// lfs f0,-1480(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1480);
	ctx.f0.f64 = double(temp.f32);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f1,f12,f0
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822D97F8"))) PPC_WEAK_FUNC(sub_822D97F8);
PPC_FUNC_IMPL(__imp__sub_822D97F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x822D9800;
	__restfpr_14(ctx, base);
	// stfd f30,-168(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.f30.u64);
	// stfd f31,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.f31.u64);
	// stwu r1,-448(r1)
	ea = -448 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r9,16(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// mr r17,r3
	ctx.r17.u64 = ctx.r3.u64;
	// lwz r31,32(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// li r26,0
	ctx.r26.s64 = 0;
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r10,r31,3,0,28
	ctx.r10.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r22,4(r3)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f13,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f13.u64);
	// lwz r7,100(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// std r6,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r6.u64);
	// lfd f12,96(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lwz r15,24(r3)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// lwz r20,28(r3)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// add r16,r10,r11
	ctx.r16.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r8,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
	// mullw r18,r7,r31
	ctx.r18.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r31.s32);
	// lfs f31,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f31.f64 = double(temp.f32);
	// lwz r23,0(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r30,36(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stw r3,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r3.u32);
	// stw r22,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r22.u32);
	// stw r15,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r15.u32);
	// stw r20,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r20.u32);
	// stw r16,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r16.u32);
	// fsubs f30,f0,f10
	ctx.f30.f64 = static_cast<float>(ctx.f0.f64 - ctx.f10.f64);
	// stw r18,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r18.u32);
	// mtctr r5
	ctx.ctr.u64 = ctx.r5.u64;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bdzf 4*cr6+eq,0x822d98c4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D98C4;
	// bdzf 4*cr6+eq,0x822d98d4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D98D4;
	// bdzf 4*cr6+eq,0x822d98f4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D98F4;
	// bdzf 4*cr6+eq,0x822d98e4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D98E4;
	// bdzf 4*cr6+eq,0x822d9900
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D9900;
	// bne cr6,0x822d990c
	if (!ctx.cr6.eq) goto loc_822D990C;
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// li r14,1
	ctx.r14.s64 = 1;
	// addi r11,r11,-26792
	ctx.r11.s64 = ctx.r11.s64 + -26792;
	// b 0x822d9918
	goto loc_822D9918;
loc_822D98C4:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// li r14,2
	ctx.r14.s64 = 2;
	// addi r11,r11,21048
	ctx.r11.s64 = ctx.r11.s64 + 21048;
	// b 0x822d9918
	goto loc_822D9918;
loc_822D98D4:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// li r14,3
	ctx.r14.s64 = 3;
	// addi r11,r11,21088
	ctx.r11.s64 = ctx.r11.s64 + 21088;
	// b 0x822d9918
	goto loc_822D9918;
loc_822D98E4:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// li r14,3
	ctx.r14.s64 = 3;
	// addi r11,r11,-26744
	ctx.r11.s64 = ctx.r11.s64 + -26744;
	// b 0x822d9918
	goto loc_822D9918;
loc_822D98F4:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,21160
	ctx.r11.s64 = ctx.r11.s64 + 21160;
	// b 0x822d9914
	goto loc_822D9914;
loc_822D9900:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,21208
	ctx.r11.s64 = ctx.r11.s64 + 21208;
	// b 0x822d9914
	goto loc_822D9914;
loc_822D990C:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r11,r11,-26672
	ctx.r11.s64 = ctx.r11.s64 + -26672;
loc_822D9914:
	// li r14,4
	ctx.r14.s64 = 4;
loc_822D9918:
	// stw r14,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r14.u32);
	// cmplwi cr6,r15,0
	ctx.cr6.compare<uint32_t>(ctx.r15.u32, 0, ctx.xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// beq cr6,0x822d995c
	if (ctx.cr6.eq) goto loc_822D995C;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x822d995c
	if (ctx.cr6.eq) goto loc_822D995C;
	// addi r27,r16,-4
	ctx.r27.s64 = ctx.r16.s64 + -4;
	// mr r28,r23
	ctx.r28.u64 = ctx.r23.u64;
	// mr r29,r31
	ctx.r29.u64 = ctx.r31.u64;
	// rotlwi r25,r11,0
	ctx.r25.u64 = rotl32(ctx.r11.u32, 0);
loc_822D9940:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// mtctr r25
	ctx.ctr.u64 = ctx.r25.u64;
	// bctrl 
	ctx.lr = 0x822D994C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stfsu f1,4(r27)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r27.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r27.u32 = ea;
	// addic. r29,r29,-1
	ctx.xer.ca = ctx.r29.u32 > 0;
	ctx.r29.s64 = ctx.r29.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// add r28,r28,r14
	ctx.r28.u64 = ctx.r28.u64 + ctx.r14.u64;
	// bne 0x822d9940
	if (!ctx.cr0.eq) goto loc_822D9940;
loc_822D995C:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// cmpw cr6,r30,r31
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r31.s32, ctx.xer);
	// lfs f6,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f6.f64 = double(temp.f32);
	// bge cr6,0x822d9ac4
	if (!ctx.cr6.lt) goto loc_822D9AC4;
	// subf r11,r31,r30
	ctx.r11.s64 = ctx.r30.s64 - ctx.r31.s64;
	// addi r10,r30,2
	ctx.r10.s64 = ctx.r30.s64 + 2;
	// addi r9,r11,2
	ctx.r9.s64 = ctx.r11.s64 + 2;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r30,r31
	ctx.r11.u64 = ctx.r30.u64 + ctx.r31.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r30,r31
	ctx.r6.u64 = ctx.r30.u64 + ctx.r31.u64;
	// rlwinm r29,r31,2,0,29
	ctx.r29.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r18,2,0,29
	ctx.r28.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r10,r16
	ctx.r4.u64 = ctx.r10.u64 + ctx.r16.u64;
	// addi r5,r11,-3
	ctx.r5.s64 = ctx.r11.s64 + -3;
	// add r3,r9,r16
	ctx.r3.u64 = ctx.r9.u64 + ctx.r16.u64;
loc_822D999C:
	// cmplw cr6,r26,r20
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r20.u32, ctx.xer);
	// bge cr6,0x822d9f08
	if (!ctx.cr6.lt) goto loc_822D9F08;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// cmpw cr6,r30,r6
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r6.s32, ctx.xer);
	// bge cr6,0x822d9a84
	if (!ctx.cr6.lt) goto loc_822D9A84;
	// subf r10,r30,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r30.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d9a30
	if (ctx.cr6.lt) goto loc_822D9A30;
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r4,-12
	ctx.r9.s64 = ctx.r4.s64 + -12;
	// add r8,r10,r22
	ctx.r8.u64 = ctx.r10.u64 + ctx.r22.u64;
	// addi r10,r3,-12
	ctx.r10.s64 = ctx.r3.s64 + -12;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
loc_822D99D0:
	// lfs f12,4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// lfs f9,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// lfs f11,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fsubs f7,f9,f12
	ctx.f7.f64 = static_cast<float>(ctx.f9.f64 - ctx.f12.f64);
	// lfs f10,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// lfs f8,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f5,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// fsubs f4,f8,f11
	ctx.f4.f64 = static_cast<float>(ctx.f8.f64 - ctx.f11.f64);
	// lfsu f0,16(r10)
	ea = 16 + ctx.r10.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r10.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// fsubs f3,f5,f10
	ctx.f3.f64 = static_cast<float>(ctx.f5.f64 - ctx.f10.f64);
	// lfsu f13,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fsubs f2,f13,f0
	ctx.f2.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// fmadds f1,f7,f31,f12
	ctx.f1.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f31.f64), float(ctx.f12.f64)));
	// stfs f1,4(r8)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// fmadds f13,f4,f31,f11
	ctx.f13.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f31.f64), float(ctx.f11.f64)));
	// stfs f13,8(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// fmadds f12,f3,f31,f10
	ctx.f12.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f31.f64), float(ctx.f10.f64)));
	// stfs f12,12(r8)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 12, temp.u32);
	// fmadds f11,f2,f31,f0
	ctx.f11.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f31.f64), float(ctx.f0.f64)));
	// stfsu f11,16(r8)
	temp.f32 = float(ctx.f11.f64);
	ea = 16 + ctx.r8.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r8.u32 = ea;
	// blt cr6,0x822d99d0
	if (ctx.cr6.lt) goto loc_822D99D0;
loc_822D9A30:
	// cmpw cr6,r11,r6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r6.s32, ctx.xer);
	// bge cr6,0x822d9a84
	if (!ctx.cr6.lt) goto loc_822D9A84;
	// subf r9,r31,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r31.s64;
	// rlwinm r8,r26,2,0,29
	ctx.r8.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r11,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r11.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r27,r9,-1
	ctx.r27.s64 = ctx.r9.s64 + -1;
	// add r11,r8,r22
	ctx.r11.u64 = ctx.r8.u64 + ctx.r22.u64;
	// add r9,r7,r16
	ctx.r9.u64 = ctx.r7.u64 + ctx.r16.u64;
	// rlwinm r8,r27,2,0,29
	ctx.r8.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// addi r7,r11,-4
	ctx.r7.s64 = ctx.r11.s64 + -4;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// add r11,r8,r16
	ctx.r11.u64 = ctx.r8.u64 + ctx.r16.u64;
	// add r26,r10,r26
	ctx.r26.u64 = ctx.r10.u64 + ctx.r26.u64;
loc_822D9A6C:
	// lfsu f0,4(r11)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r11.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r11.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// lfsu f13,4(r9)
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// fsubs f13,f13,f0
	ctx.f13.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// fmadds f12,f13,f31,f0
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f31.f64), float(ctx.f0.f64)));
	// stfsu f12,4(r7)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d9a6c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D9A6C;
loc_822D9A84:
	// fadds f31,f30,f31
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = double(float(ctx.f30.f64 + ctx.f31.f64));
	// add r30,r18,r30
	ctx.r30.u64 = ctx.r18.u64 + ctx.r30.u64;
	// add r6,r6,r18
	ctx.r6.u64 = ctx.r6.u64 + ctx.r18.u64;
	// add r5,r5,r18
	ctx.r5.u64 = ctx.r5.u64 + ctx.r18.u64;
	// add r4,r28,r4
	ctx.r4.u64 = ctx.r28.u64 + ctx.r4.u64;
	// add r3,r28,r3
	ctx.r3.u64 = ctx.r28.u64 + ctx.r3.u64;
	// fcmpu cr6,f31,f6
	ctx.cr6.compare(ctx.f31.f64, ctx.f6.f64);
	// blt cr6,0x822d9abc
	if (ctx.cr6.lt) goto loc_822D9ABC;
	// fsubs f31,f31,f6
	ctx.f31.f64 = static_cast<float>(ctx.f31.f64 - ctx.f6.f64);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + ctx.r31.u64;
	// add r30,r30,r31
	ctx.r30.u64 = ctx.r30.u64 + ctx.r31.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + ctx.r31.u64;
	// add r4,r29,r4
	ctx.r4.u64 = ctx.r29.u64 + ctx.r4.u64;
	// add r3,r29,r3
	ctx.r3.u64 = ctx.r29.u64 + ctx.r3.u64;
loc_822D9ABC:
	// cmpw cr6,r30,r31
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r31.s32, ctx.xer);
	// blt cr6,0x822d999c
	if (ctx.cr6.lt) goto loc_822D999C;
loc_822D9AC4:
	// cmplw cr6,r26,r20
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r20.u32, ctx.xer);
	// bge cr6,0x822d9f08
	if (!ctx.cr6.lt) goto loc_822D9F08;
	// rlwinm r11,r31,1,0,30
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r18,1,0,30
	ctx.r9.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r5,r31,r30
	ctx.r5.s64 = ctx.r30.s64 - ctx.r31.s64;
	// add r4,r18,r9
	ctx.r4.u64 = ctx.r18.u64 + ctx.r9.u64;
	// add r21,r31,r11
	ctx.r21.u64 = ctx.r31.u64 + ctx.r11.u64;
	// rlwinm r6,r5,1,0,30
	ctx.r6.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r4,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r4.u32);
	// rlwinm r10,r30,1,0,30
	ctx.r10.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r30,1,0,30
	ctx.r8.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r30,2,0,29
	ctx.r7.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r30,r31
	ctx.r11.u64 = ctx.r30.u64 + ctx.r31.u64;
	// add r29,r30,r10
	ctx.r29.u64 = ctx.r30.u64 + ctx.r10.u64;
	// add r25,r8,r23
	ctx.r25.u64 = ctx.r8.u64 + ctx.r23.u64;
	// add r28,r7,r23
	ctx.r28.u64 = ctx.r7.u64 + ctx.r23.u64;
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// add r24,r6,r23
	ctx.r24.u64 = ctx.r6.u64 + ctx.r23.u64;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// lfs f7,-1480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -1480);
	ctx.f7.f64 = double(temp.f32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f11,-1484(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1484);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-1488(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1488);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r19,r31,1,0,30
	ctx.r19.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lfs f8,-1496(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -1496);
	ctx.f8.f64 = double(temp.f32);
	// add r27,r5,r23
	ctx.r27.u64 = ctx.r5.u64 + ctx.r23.u64;
	// lfs f9,-1492(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -1492);
	ctx.f9.f64 = double(temp.f32);
	// lfs f10,11260(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 11260);
	ctx.f10.f64 = double(temp.f32);
loc_822D9B48:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// cmpw cr6,r30,r4
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r4.s32, ctx.xer);
	// bge cr6,0x822d9ea8
	if (!ctx.cr6.lt) goto loc_822D9EA8;
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r20,20(r17)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r17.u32 + 20);
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + ctx.r22.u64;
	// mr r8,r28
	ctx.r8.u64 = ctx.r28.u64;
	// addi r5,r10,-4
	ctx.r5.s64 = ctx.r10.s64 + -4;
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// mr r9,r27
	ctx.r9.u64 = ctx.r27.u64;
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// subf r22,r31,r23
	ctx.r22.s64 = ctx.r23.s64 - ctx.r31.s64;
loc_822D9B7C:
	// mtctr r20
	ctx.ctr.u64 = ctx.r20.u64;
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// bdzf 4*cr6+eq,0x822d9bd8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D9BD8;
	// bdzf 4*cr6+eq,0x822d9c14
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D9C14;
	// bdzf 4*cr6+eq,0x822d9dac
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D9DAC;
	// bdzf 4*cr6+eq,0x822d9ce0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D9CE0;
	// bdzf 4*cr6+eq,0x822d9df0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822D9DF0;
	// bne cr6,0x822d9e34
	if (!ctx.cr6.eq) goto loc_822D9E34;
	// lbzx r14,r22,r3
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r22.u32 + ctx.r3.u32);
	// lbzx r10,r3,r23
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + ctx.r23.u32);
	// std r14,200(r1)
	PPC_STORE_U64(ctx.r1.u32 + 200, ctx.r14.u64);
	// lfd f13,200(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 200);
	// std r10,264(r1)
	PPC_STORE_U64(ctx.r1.u32 + 264, ctx.r10.u64);
	// lfd f0,264(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 264);
	// fcfid f4,f0
	ctx.f4.f64 = double(ctx.f0.s64);
	// fcfid f5,f13
	ctx.f5.f64 = double(ctx.f13.s64);
	// frsp f2,f4
	ctx.f2.f64 = double(float(ctx.f4.f64));
	// frsp f3,f5
	ctx.f3.f64 = double(float(ctx.f5.f64));
	// fsubs f13,f2,f10
	ctx.f13.f64 = static_cast<float>(ctx.f2.f64 - ctx.f10.f64);
	// fsubs f1,f3,f10
	ctx.f1.f64 = static_cast<float>(ctx.f3.f64 - ctx.f10.f64);
	// fmuls f13,f13,f9
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fmuls f0,f1,f9
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// b 0x822d9e6c
	goto loc_822D9E6C;
loc_822D9BD8:
	// lhz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// lhz r14,0(r6)
	ctx.r14.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r14,r14
	ctx.r14.s64 = ctx.r14.s16;
	// std r10,168(r1)
	PPC_STORE_U64(ctx.r1.u32 + 168, ctx.r10.u64);
	// std r14,216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 216, ctx.r14.u64);
	// lfd f0,168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 168);
	// lfd f13,216(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 216);
	// fcfid f4,f0
	ctx.f4.f64 = double(ctx.f0.s64);
	// fcfid f5,f13
	ctx.f5.f64 = double(ctx.f13.s64);
	// frsp f2,f4
	ctx.f2.f64 = double(float(ctx.f4.f64));
	// frsp f3,f5
	ctx.f3.f64 = double(float(ctx.f5.f64));
	// fmuls f0,f2,f8
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
	// fmuls f13,f3,f8
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f8.f64));
	// b 0x822d9e6c
	goto loc_822D9E6C;
loc_822D9C14:
	// add r17,r11,r23
	ctx.r17.u64 = ctx.r11.u64 + ctx.r23.u64;
	// std r7,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r7.u64);
	// add r18,r11,r23
	ctx.r18.u64 = ctx.r11.u64 + ctx.r23.u64;
	// lbzx r14,r11,r23
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r23.u32);
	// stw r17,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r17.u32);
	// subf r10,r21,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r21.s64;
	// lwz r7,112(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r18,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r18.u32);
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + ctx.r23.u64;
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r11.u64);
	// std r8,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r8.u64);
	// lwz r18,124(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r15,128(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lbz r8,1(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lwz r16,88(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r17,120(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r11,104(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// std r9,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r9.u64);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rotlwi r9,r9,8
	ctx.r9.u64 = rotl32(ctx.r9.u32, 8);
	// lbz r11,2(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// ld r8,160(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// rotlwi r11,r11,8
	ctx.r11.u64 = rotl32(ctx.r11.u32, 8);
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// ld r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r10,r10,12
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 12;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r10.u64);
	// lfd f0,184(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 184);
	// lbz r7,1(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// fcfid f5,f0
	ctx.f5.f64 = double(ctx.f0.s64);
	// or r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 | ctx.r7.u64;
	// frsp f3,f5
	ctx.f3.f64 = double(float(ctx.f5.f64));
	// ld r7,136(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// rlwinm r11,r11,8,0,23
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFFFFFF00;
	// or r14,r11,r14
	ctx.r14.u64 = ctx.r11.u64 | ctx.r14.u64;
	// ld r11,144(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// rlwinm r14,r14,8,0,23
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r14,r14,12
	ctx.xer.ca = (ctx.r14.s32 < 0) & ((ctx.r14.u32 & 0xFFF) != 0);
	ctx.r14.s64 = ctx.r14.s32 >> 12;
	// extsw r14,r14
	ctx.r14.s64 = ctx.r14.s32;
	// fmuls f0,f3,f12
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// std r14,232(r1)
	PPC_STORE_U64(ctx.r1.u32 + 232, ctx.r14.u64);
	// lfd f13,232(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 232);
	// fcfid f4,f13
	ctx.f4.f64 = double(ctx.f13.s64);
	// frsp f2,f4
	ctx.f2.f64 = double(float(ctx.f4.f64));
	// fmuls f13,f2,f12
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// b 0x822d9e6c
	goto loc_822D9E6C;
loc_822D9CE0:
	// add r18,r11,r23
	ctx.r18.u64 = ctx.r11.u64 + ctx.r23.u64;
	// lbzx r14,r11,r23
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r23.u32);
	// add r17,r11,r23
	ctx.r17.u64 = ctx.r11.u64 + ctx.r23.u64;
	// std r11,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r11.u64);
	// subf r10,r21,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r21.s64;
	// stw r18,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r18.u32);
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// std r7,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r7.u64);
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + ctx.r23.u64;
	// stw r17,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r17.u32);
	// lwz r7,104(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// std r9,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r9.u64);
	// std r8,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r8.u64);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r8,1(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// rotlwi r9,r9,8
	ctx.r9.u64 = rotl32(ctx.r9.u32, 8);
	// lbz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lwz r18,124(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// lwz r15,128(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r16,88(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r17,120(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// ld r8,144(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// ld r9,160(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r10,r10,8
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,248(r1)
	PPC_STORE_U64(ctx.r1.u32 + 248, ctx.r10.u64);
	// lbz r11,2(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// rotlwi r11,r11,8
	ctx.r11.u64 = rotl32(ctx.r11.u32, 8);
	// lfd f0,248(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 248);
	// lbz r7,1(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// or r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 | ctx.r7.u64;
	// ld r7,112(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// rlwinm r11,r11,8,0,23
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFFFFFF00;
	// or r14,r11,r14
	ctx.r14.u64 = ctx.r11.u64 | ctx.r14.u64;
	// ld r11,136(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// rlwinm r14,r14,8,0,23
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r14,r14,8
	ctx.xer.ca = (ctx.r14.s32 < 0) & ((ctx.r14.u32 & 0xFF) != 0);
	ctx.r14.s64 = ctx.r14.s32 >> 8;
	// frsp f4,f13
	ctx.f4.f64 = double(float(ctx.f13.f64));
	// extsw r14,r14
	ctx.r14.s64 = ctx.r14.s32;
	// std r14,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r14.u64);
	// lfd f5,176(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f3,f5
	ctx.f3.f64 = double(ctx.f5.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fmuls f0,f4,f11
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f13,f2,f11
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// b 0x822d9e6c
	goto loc_822D9E6C;
loc_822D9DAC:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r14,0(r8)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// srawi r10,r10,12
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 12;
	// srawi r14,r14,12
	ctx.xer.ca = (ctx.r14.s32 < 0) & ((ctx.r14.u32 & 0xFFF) != 0);
	ctx.r14.s64 = ctx.r14.s32 >> 12;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// extsw r14,r14
	ctx.r14.s64 = ctx.r14.s32;
	// std r10,208(r1)
	PPC_STORE_U64(ctx.r1.u32 + 208, ctx.r10.u64);
	// lfd f5,208(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 208);
	// std r14,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r14.u64);
	// lfd f0,192(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// fcfid f3,f5
	ctx.f3.f64 = double(ctx.f5.s64);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// frsp f4,f13
	ctx.f4.f64 = double(float(ctx.f13.f64));
	// fmuls f0,f2,f12
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// fmuls f13,f4,f12
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// b 0x822d9e6c
	goto loc_822D9E6C;
loc_822D9DF0:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r14,0(r8)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// srawi r10,r10,8
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// srawi r14,r14,8
	ctx.xer.ca = (ctx.r14.s32 < 0) & ((ctx.r14.u32 & 0xFF) != 0);
	ctx.r14.s64 = ctx.r14.s32 >> 8;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// extsw r14,r14
	ctx.r14.s64 = ctx.r14.s32;
	// std r10,224(r1)
	PPC_STORE_U64(ctx.r1.u32 + 224, ctx.r10.u64);
	// std r14,240(r1)
	PPC_STORE_U64(ctx.r1.u32 + 240, ctx.r14.u64);
	// lfd f13,240(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 240);
	// fcfid f4,f13
	ctx.f4.f64 = double(ctx.f13.s64);
	// lfd f0,224(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 224);
	// frsp f2,f4
	ctx.f2.f64 = double(float(ctx.f4.f64));
	// fcfid f5,f0
	ctx.f5.f64 = double(ctx.f0.s64);
	// fmuls f13,f2,f11
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// frsp f3,f5
	ctx.f3.f64 = double(float(ctx.f5.f64));
	// fmuls f0,f3,f11
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// b 0x822d9e6c
	goto loc_822D9E6C;
loc_822D9E34:
	// lwz r14,0(r8)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// extsw r14,r14
	ctx.r14.s64 = ctx.r14.s32;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r14,272(r1)
	PPC_STORE_U64(ctx.r1.u32 + 272, ctx.r14.u64);
	// std r10,256(r1)
	PPC_STORE_U64(ctx.r1.u32 + 256, ctx.r10.u64);
	// lfd f0,256(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// fcfid f5,f0
	ctx.f5.f64 = double(ctx.f0.s64);
	// lfd f13,272(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 272);
	// frsp f3,f5
	ctx.f3.f64 = double(float(ctx.f5.f64));
	// fcfid f4,f13
	ctx.f4.f64 = double(ctx.f13.s64);
	// fmuls f0,f3,f7
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// frsp f2,f4
	ctx.f2.f64 = double(float(ctx.f4.f64));
	// fmuls f13,f2,f7
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f7.f64));
loc_822D9E6C:
	// fsubs f13,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// cmpw cr6,r3,r4
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r4.s32, ctx.xer);
	// fmadds f5,f13,f31,f0
	ctx.f5.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f31.f64), float(ctx.f0.f64)));
	// stfsu f5,4(r5)
	temp.f32 = float(ctx.f5.f64);
	ea = 4 + ctx.r5.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r5.u32 = ea;
	// blt cr6,0x822d9b7c
	if (ctx.cr6.lt) goto loc_822D9B7C;
	// lwz r22,152(r1)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r14,84(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r20,132(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
loc_822D9EA8:
	// fadds f31,f30,f31
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = double(float(ctx.f30.f64 + ctx.f31.f64));
	// rlwinm r11,r18,2,0,29
	ctx.r11.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// add r30,r18,r30
	ctx.r30.u64 = ctx.r18.u64 + ctx.r30.u64;
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// add r27,r11,r27
	ctx.r27.u64 = ctx.r11.u64 + ctx.r27.u64;
	// rlwinm r11,r18,1,0,30
	ctx.r11.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r4,r18
	ctx.r4.u64 = ctx.r4.u64 + ctx.r18.u64;
	// add r29,r10,r29
	ctx.r29.u64 = ctx.r10.u64 + ctx.r29.u64;
	// add r25,r11,r25
	ctx.r25.u64 = ctx.r11.u64 + ctx.r25.u64;
	// add r24,r11,r24
	ctx.r24.u64 = ctx.r11.u64 + ctx.r24.u64;
	// fcmpu cr6,f31,f6
	ctx.cr6.compare(ctx.f31.f64, ctx.f6.f64);
	// blt cr6,0x822d9f00
	if (ctx.cr6.lt) goto loc_822D9F00;
	// rlwinm r11,r31,2,0,29
	ctx.r11.u64 = rotl64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// fsubs f31,f31,f6
	ctx.f31.f64 = static_cast<float>(ctx.f31.f64 - ctx.f6.f64);
	// add r4,r4,r31
	ctx.r4.u64 = ctx.r4.u64 + ctx.r31.u64;
	// add r30,r30,r31
	ctx.r30.u64 = ctx.r30.u64 + ctx.r31.u64;
	// add r29,r21,r29
	ctx.r29.u64 = ctx.r21.u64 + ctx.r29.u64;
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// add r27,r11,r27
	ctx.r27.u64 = ctx.r11.u64 + ctx.r27.u64;
	// add r25,r19,r25
	ctx.r25.u64 = ctx.r19.u64 + ctx.r25.u64;
	// add r24,r19,r24
	ctx.r24.u64 = ctx.r19.u64 + ctx.r24.u64;
loc_822D9F00:
	// cmplw cr6,r26,r20
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r20.u32, ctx.xer);
	// blt cr6,0x822d9b48
	if (ctx.cr6.lt) goto loc_822D9B48;
loc_822D9F08:
	// subf r11,r31,r15
	ctx.r11.s64 = ctx.r15.s64 - ctx.r31.s64;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x822d9f18
	if (!ctx.cr6.lt) goto loc_822D9F18;
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
loc_822D9F18:
	// stfs f31,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lwz r11,96(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r11,12(r17)
	PPC_STORE_U32(ctx.r17.u32 + 12, ctx.r11.u32);
	// cmpw cr6,r30,r15
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r15.s32, ctx.xer);
	// li r11,0
	ctx.r11.s64 = 0;
	// beq cr6,0x822d9f34
	if (ctx.cr6.eq) goto loc_822D9F34;
	// neg r11,r31
	ctx.r11.s64 = -ctx.r31.s64;
loc_822D9F34:
	// stw r11,36(r17)
	PPC_STORE_U32(ctx.r17.u32 + 36, ctx.r11.u32);
	// rotlwi r11,r11,0
	ctx.r11.u64 = rotl32(ctx.r11.u32, 0);
	// neg r5,r15
	ctx.r5.s64 = -ctx.r15.s64;
	// subf r11,r31,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r31.s64;
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// bge cr6,0x822d9ff8
	if (!ctx.cr6.lt) goto loc_822D9FF8;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r11.s64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// blt cr6,0x822d9fbc
	if (ctx.cr6.lt) goto loc_822D9FBC;
	// add r10,r11,r15
	ctx.r10.u64 = ctx.r11.u64 + ctx.r15.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r10,-3
	ctx.r8.s64 = ctx.r10.s64 + -3;
	// add r10,r9,r16
	ctx.r10.u64 = ctx.r9.u64 + ctx.r16.u64;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r15,2
	ctx.r8.s64 = ctx.r15.s64 + 2;
	// addi r7,r15,3
	ctx.r7.s64 = ctx.r15.s64 + 3;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// add r9,r9,r16
	ctx.r9.u64 = ctx.r9.u64 + ctx.r16.u64;
loc_822D9F80:
	// lfs f0,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// add r4,r11,r8
	ctx.r4.u64 = ctx.r11.u64 + ctx.r8.u64;
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// add r3,r11,r7
	ctx.r3.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmpw cr6,r11,r6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r6.s32, ctx.xer);
	// lfsu f0,16(r9)
	ea = 16 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// lfsx f13,r4,r16
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r16.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfsx f12,r3,r16
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r16.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfsu f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	ea = 16 + ctx.r10.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x822d9f80
	if (ctx.cr6.lt) goto loc_822D9F80;
loc_822D9FBC:
	// cmpw cr6,r11,r5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r5.s32, ctx.xer);
	// bge cr6,0x822d9ff8
	if (!ctx.cr6.lt) goto loc_822D9FF8;
	// add r8,r11,r15
	ctx.r8.u64 = ctx.r11.u64 + ctx.r15.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r11.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r16
	ctx.r9.u64 = ctx.r9.u64 + ctx.r16.u64;
	// add r8,r8,r16
	ctx.r8.u64 = ctx.r8.u64 + ctx.r16.u64;
	// addi r7,r9,-4
	ctx.r7.s64 = ctx.r9.s64 + -4;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
loc_822D9FEC:
	// lfsu f0,4(r9)
	ctx.fpscr.disableFlushMode();
	ea = 4 + ctx.r9.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r9.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsu f0,4(r7)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r7.u32 = ea;
	// bdnz 0x822d9fec
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822D9FEC;
loc_822D9FF8:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x822da03c
	if (!ctx.cr6.lt) goto loc_822DA03C;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r28,80(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r9,r11,r15
	ctx.r9.u64 = ctx.r11.u64 + ctx.r15.u64;
	// add r10,r10,r16
	ctx.r10.u64 = ctx.r10.u64 + ctx.r16.u64;
	// mullw r9,r9,r14
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r14.s32);
	// addi r29,r10,-4
	ctx.r29.s64 = ctx.r10.s64 + -4;
	// add r30,r9,r23
	ctx.r30.u64 = ctx.r9.u64 + ctx.r23.u64;
	// neg r31,r11
	ctx.r31.s64 = -ctx.r11.s64;
loc_822DA020:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// mtctr r28
	ctx.ctr.u64 = ctx.r28.u64;
	// bctrl 
	ctx.lr = 0x822DA02C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stfsu f1,4(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	ea = 4 + ctx.r29.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r29.u32 = ea;
	// addic. r31,r31,-1
	ctx.xer.ca = ctx.r31.u32 > 0;
	ctx.r31.s64 = ctx.r31.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// add r30,r30,r14
	ctx.r30.u64 = ctx.r30.u64 + ctx.r14.u64;
	// bne 0x822da020
	if (!ctx.cr0.eq) goto loc_822DA020;
loc_822DA03C:
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// lfd f30,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lfd f31,-160(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DA04C"))) PPC_WEAK_FUNC(sub_822DA04C);
PPC_FUNC_IMPL(__imp__sub_822DA04C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DA050"))) PPC_WEAK_FUNC(sub_822DA050);
PPC_FUNC_IMPL(__imp__sub_822DA050) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// blt cr6,0x822da124
	if (ctx.cr6.lt) goto loc_822DA124;
	// beq cr6,0x822da0c8
	if (ctx.cr6.eq) goto loc_822DA0C8;
	// cmplwi cr6,r3,7
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 7, ctx.xer);
	// blt cr6,0x822da124
	if (ctx.cr6.lt) goto loc_822DA124;
	// bne cr6,0x822da12c
	if (!ctx.cr6.eq) goto loc_822DA12C;
	// cmplwi cr6,r4,1
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 1, ctx.xer);
	// bne cr6,0x822da080
	if (!ctx.cr6.eq) goto loc_822DA080;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,28680
	ctx.r11.s64 = ctx.r11.s64 + 28680;
	// b 0x822da12c
	goto loc_822DA12C;
loc_822DA080:
	// cmplwi cr6,r4,2
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 2, ctx.xer);
	// bne cr6,0x822da094
	if (!ctx.cr6.eq) goto loc_822DA094;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,24192
	ctx.r11.s64 = ctx.r11.s64 + 24192;
	// b 0x822da12c
	goto loc_822DA12C;
loc_822DA094:
	// cmplwi cr6,r4,4
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 4, ctx.xer);
	// bne cr6,0x822da0a8
	if (!ctx.cr6.eq) goto loc_822DA0A8;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,25000
	ctx.r11.s64 = ctx.r11.s64 + 25000;
	// b 0x822da12c
	goto loc_822DA12C;
loc_822DA0A8:
	// cmplwi cr6,r4,6
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 6, ctx.xer);
	// bne cr6,0x822da0bc
	if (!ctx.cr6.eq) goto loc_822DA0BC;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,26064
	ctx.r11.s64 = ctx.r11.s64 + 26064;
	// b 0x822da12c
	goto loc_822DA12C;
loc_822DA0BC:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,22928
	ctx.r11.s64 = ctx.r11.s64 + 22928;
	// b 0x822da12c
	goto loc_822DA12C;
loc_822DA0C8:
	// cmplwi cr6,r4,1
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 1, ctx.xer);
	// bne cr6,0x822da0dc
	if (!ctx.cr6.eq) goto loc_822DA0DC;
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r11,r11,-29664
	ctx.r11.s64 = ctx.r11.s64 + -29664;
	// b 0x822da12c
	goto loc_822DA12C;
loc_822DA0DC:
	// cmplwi cr6,r4,2
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 2, ctx.xer);
	// bne cr6,0x822da0f0
	if (!ctx.cr6.eq) goto loc_822DA0F0;
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r11,r11,-27752
	ctx.r11.s64 = ctx.r11.s64 + -27752;
	// b 0x822da12c
	goto loc_822DA12C;
loc_822DA0F0:
	// cmplwi cr6,r4,4
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 4, ctx.xer);
	// bne cr6,0x822da104
	if (!ctx.cr6.eq) goto loc_822DA104;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,31224
	ctx.r11.s64 = ctx.r11.s64 + 31224;
	// b 0x822da12c
	goto loc_822DA12C;
loc_822DA104:
	// cmplwi cr6,r4,6
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 6, ctx.xer);
	// bne cr6,0x822da118
	if (!ctx.cr6.eq) goto loc_822DA118;
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,32760
	ctx.r11.s64 = ctx.r11.s64 + 32760;
	// b 0x822da12c
	goto loc_822DA12C;
loc_822DA118:
	// lis r11,-32211
	ctx.r11.s64 = -2110980096;
	// addi r11,r11,29376
	ctx.r11.s64 = ctx.r11.s64 + 29376;
	// b 0x822da12c
	goto loc_822DA12C;
loc_822DA124:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r11,r11,-26632
	ctx.r11.s64 = ctx.r11.s64 + -26632;
loc_822DA12C:
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DA134"))) PPC_WEAK_FUNC(sub_822DA134);
PPC_FUNC_IMPL(__imp__sub_822DA134) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DA138"))) PPC_WEAK_FUNC(sub_822DA138);
PPC_FUNC_IMPL(__imp__sub_822DA138) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lfs f12,5256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5256);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,5280(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5280);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x822da208
	if (ctx.cr6.lt) goto loc_822DA208;
	// addi r8,r5,-3
	ctx.r8.s64 = ctx.r5.s64 + -3;
	// addi r10,r4,12
	ctx.r10.s64 = ctx.r4.s64 + 12;
	// addi r11,r3,4
	ctx.r11.s64 = ctx.r3.s64 + 4;
	// subf r7,r3,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r3.s64;
loc_822DA164:
	// lfs f0,-12(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da178
	if (!ctx.cr6.lt) goto loc_822DA178;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da184
	goto loc_822DA184;
loc_822DA178:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da184
	if (!ctx.cr6.gt) goto loc_822DA184;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA184:
	// stfs f0,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lfsx f0,r7,r11
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da19c
	if (!ctx.cr6.lt) goto loc_822DA19C;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da1a8
	goto loc_822DA1A8;
loc_822DA19C:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da1a8
	if (!ctx.cr6.gt) goto loc_822DA1A8;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA1A8:
	// stfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfs f0,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da1c0
	if (!ctx.cr6.lt) goto loc_822DA1C0;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da1cc
	goto loc_822DA1CC;
loc_822DA1C0:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da1cc
	if (!ctx.cr6.gt) goto loc_822DA1CC;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA1CC:
	// stfs f0,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da1e4
	if (!ctx.cr6.lt) goto loc_822DA1E4;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da1f0
	goto loc_822DA1F0;
loc_822DA1E4:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da1f0
	if (!ctx.cr6.gt) goto loc_822DA1F0;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA1F0:
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stfs f0,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x822da164
	if (ctx.cr6.lt) goto loc_822DA164;
loc_822DA208:
	// cmplw cr6,r9,r5
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r5.u32, ctx.xer);
	// bgelr cr6
	if (!ctx.cr6.lt) return;
	// subf r8,r9,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r9.s64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r3,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r3.s64;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822DA224:
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da238
	if (!ctx.cr6.lt) goto loc_822DA238;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da244
	goto loc_822DA244;
loc_822DA238:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da244
	if (!ctx.cr6.gt) goto loc_822DA244;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA244:
	// stfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822da224
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DA224;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DA254"))) PPC_WEAK_FUNC(sub_822DA254);
PPC_FUNC_IMPL(__imp__sub_822DA254) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DA258"))) PPC_WEAK_FUNC(sub_822DA258);
PPC_FUNC_IMPL(__imp__sub_822DA258) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lfs f11,-1452(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1452);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-1456(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1456);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,5280(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5280);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x822da368
	if (ctx.cr6.lt) goto loc_822DA368;
	// addi r8,r5,-3
	ctx.r8.s64 = ctx.r5.s64 + -3;
	// addi r9,r3,-2
	ctx.r9.s64 = ctx.r3.s64 + -2;
	// addi r10,r4,8
	ctx.r10.s64 = ctx.r4.s64 + 8;
loc_822DA288:
	// lfs f0,-8(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da29c
	if (!ctx.cr6.lt) goto loc_822DA29C;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da2a8
	goto loc_822DA2A8;
loc_822DA29C:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da2a8
	if (!ctx.cr6.gt) goto loc_822DA2A8;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA2A8:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lhz r7,-10(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + -10);
	// sth r7,2(r9)
	PPC_STORE_U16(ctx.r9.u32 + 2, ctx.r7.u16);
	// bge cr6,0x822da2d0
	if (!ctx.cr6.lt) goto loc_822DA2D0;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da2dc
	goto loc_822DA2DC;
loc_822DA2D0:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da2dc
	if (!ctx.cr6.gt) goto loc_822DA2DC;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA2DC:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lhz r7,-10(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + -10);
	// sth r7,4(r9)
	PPC_STORE_U16(ctx.r9.u32 + 4, ctx.r7.u16);
	// bge cr6,0x822da304
	if (!ctx.cr6.lt) goto loc_822DA304;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da310
	goto loc_822DA310;
loc_822DA304:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da310
	if (!ctx.cr6.gt) goto loc_822DA310;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA310:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lhz r7,-10(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + -10);
	// sth r7,6(r9)
	PPC_STORE_U16(ctx.r9.u32 + 6, ctx.r7.u16);
	// bge cr6,0x822da338
	if (!ctx.cr6.lt) goto loc_822DA338;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da344
	goto loc_822DA344;
loc_822DA338:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da344
	if (!ctx.cr6.gt) goto loc_822DA344;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA344:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r11,r8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r8.u32, ctx.xer);
	// fctiwz f10,f0
	ctx.f10.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f10.u64);
	// lhz r7,-10(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + -10);
	// sthu r7,8(r9)
	ea = 8 + ctx.r9.u32;
	PPC_STORE_U16(ea, ctx.r7.u16);
	ctx.r9.u32 = ea;
	// blt cr6,0x822da288
	if (ctx.cr6.lt) goto loc_822DA288;
loc_822DA368:
	// cmplw cr6,r11,r5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r5.u32, ctx.xer);
	// bgelr cr6
	if (!ctx.cr6.lt) return;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r11.s64;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822DA38C:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da3a0
	if (!ctx.cr6.lt) goto loc_822DA3A0;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da3ac
	goto loc_822DA3AC;
loc_822DA3A0:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da3ac
	if (!ctx.cr6.gt) goto loc_822DA3AC;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA3AC:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fctiwz f10,f0
	ctx.f10.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f10.u64);
	// lhz r9,-10(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + -10);
	// sthu r9,2(r10)
	ea = 2 + ctx.r10.u32;
	PPC_STORE_U16(ea, ctx.r9.u16);
	ctx.r10.u32 = ea;
	// bdnz 0x822da38c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DA38C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DA3CC"))) PPC_WEAK_FUNC(sub_822DA3CC);
PPC_FUNC_IMPL(__imp__sub_822DA3CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DA3D0"))) PPC_WEAK_FUNC(sub_822DA3D0);
PPC_FUNC_IMPL(__imp__sub_822DA3D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// li r11,0
	ctx.r11.s64 = 0;
	// lfs f11,11260(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 11260);
	ctx.f11.f64 = double(temp.f32);
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lfs f12,5256(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5256);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,-1448(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -1448);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,5280(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 5280);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x822da504
	if (ctx.cr6.lt) goto loc_822DA504;
	// addi r7,r3,1
	ctx.r7.s64 = ctx.r3.s64 + 1;
	// addi r6,r3,2
	ctx.r6.s64 = ctx.r3.s64 + 2;
	// addi r9,r3,3
	ctx.r9.s64 = ctx.r3.s64 + 3;
	// addi r8,r5,-3
	ctx.r8.s64 = ctx.r5.s64 + -3;
	// addi r10,r4,8
	ctx.r10.s64 = ctx.r4.s64 + 8;
loc_822DA414:
	// lfs f0,-8(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da428
	if (!ctx.cr6.lt) goto loc_822DA428;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da434
	goto loc_822DA434;
loc_822DA428:
	// fcmpu cr6,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// ble cr6,0x822da434
	if (!ctx.cr6.gt) goto loc_822DA434;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
loc_822DA434:
	// fadds f9,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// lfs f0,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fmuls f8,f9,f11
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f7,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f7.u64);
	// lbz r31,-9(r1)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -9);
	// stbx r31,r11,r3
	PPC_STORE_U8(ctx.r11.u32 + ctx.r3.u32, ctx.r31.u8);
	// bge cr6,0x822da460
	if (!ctx.cr6.lt) goto loc_822DA460;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da46c
	goto loc_822DA46C;
loc_822DA460:
	// fcmpu cr6,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// ble cr6,0x822da46c
	if (!ctx.cr6.gt) goto loc_822DA46C;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
loc_822DA46C:
	// fadds f9,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fmuls f8,f9,f11
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f7,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f7.u64);
	// lbz r31,-9(r1)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -9);
	// stbx r31,r7,r11
	PPC_STORE_U8(ctx.r7.u32 + ctx.r11.u32, ctx.r31.u8);
	// bge cr6,0x822da498
	if (!ctx.cr6.lt) goto loc_822DA498;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da4a4
	goto loc_822DA4A4;
loc_822DA498:
	// fcmpu cr6,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// ble cr6,0x822da4a4
	if (!ctx.cr6.gt) goto loc_822DA4A4;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
loc_822DA4A4:
	// fadds f9,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// lfs f0,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fmuls f8,f9,f11
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f7,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f7.u64);
	// lbz r31,-9(r1)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -9);
	// stbx r31,r6,r11
	PPC_STORE_U8(ctx.r6.u32 + ctx.r11.u32, ctx.r31.u8);
	// bge cr6,0x822da4d0
	if (!ctx.cr6.lt) goto loc_822DA4D0;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da4dc
	goto loc_822DA4DC;
loc_822DA4D0:
	// fcmpu cr6,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// ble cr6,0x822da4dc
	if (!ctx.cr6.gt) goto loc_822DA4DC;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
loc_822DA4DC:
	// fadds f0,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// fmuls f9,f0,f11
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f8.u64);
	// lbz r31,-9(r1)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -9);
	// stbx r31,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + ctx.r11.u32, ctx.r31.u8);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplw cr6,r11,r8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x822da414
	if (ctx.cr6.lt) goto loc_822DA414;
loc_822DA504:
	// cmplw cr6,r11,r5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r5.u32, ctx.xer);
	// bge cr6,0x822da560
	if (!ctx.cr6.lt) goto loc_822DA560;
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r11.s64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822DA51C:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da530
	if (!ctx.cr6.lt) goto loc_822DA530;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da53c
	goto loc_822DA53C;
loc_822DA530:
	// fcmpu cr6,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// ble cr6,0x822da53c
	if (!ctx.cr6.gt) goto loc_822DA53C;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
loc_822DA53C:
	// fadds f0,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fmuls f9,f0,f11
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f8.u64);
	// lbz r9,-9(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -9);
	// stbx r9,r11,r3
	PPC_STORE_U8(ctx.r11.u32 + ctx.r3.u32, ctx.r9.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// bdnz 0x822da51c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DA51C;
loc_822DA560:
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DA568"))) PPC_WEAK_FUNC(sub_822DA568);
PPC_FUNC_IMPL(__imp__sub_822DA568) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lfs f11,-1440(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -1440);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-1444(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1444);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,5280(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5280);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x822da6d8
	if (ctx.cr6.lt) goto loc_822DA6D8;
	// addi r8,r5,-3
	ctx.r8.s64 = ctx.r5.s64 + -3;
	// addi r9,r4,8
	ctx.r9.s64 = ctx.r4.s64 + 8;
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
loc_822DA59C:
	// lfs f0,-8(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da5b0
	if (!ctx.cr6.lt) goto loc_822DA5B0;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da5bc
	goto loc_822DA5BC;
loc_822DA5B0:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da5bc
	if (!ctx.cr6.gt) goto loc_822DA5BC;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA5BC:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,-4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r7,-12(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r7,4,0,27
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// srawi r7,r6,8
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 8;
	// stb r6,1(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1, ctx.r6.u8);
	// srawi r31,r6,16
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r31.s64 = ctx.r6.s32 >> 16;
	// stb r7,2(r11)
	PPC_STORE_U8(ctx.r11.u32 + 2, ctx.r7.u8);
	// stb r31,3(r11)
	PPC_STORE_U8(ctx.r11.u32 + 3, ctx.r31.u8);
	// bge cr6,0x822da5f8
	if (!ctx.cr6.lt) goto loc_822DA5F8;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da604
	goto loc_822DA604;
loc_822DA5F8:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da604
	if (!ctx.cr6.gt) goto loc_822DA604;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA604:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r7,-12(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r7,4,0,27
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// srawi r7,r6,8
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 8;
	// stb r6,4(r11)
	PPC_STORE_U8(ctx.r11.u32 + 4, ctx.r6.u8);
	// srawi r31,r6,16
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r31.s64 = ctx.r6.s32 >> 16;
	// stb r7,5(r11)
	PPC_STORE_U8(ctx.r11.u32 + 5, ctx.r7.u8);
	// stb r31,6(r11)
	PPC_STORE_U8(ctx.r11.u32 + 6, ctx.r31.u8);
	// bge cr6,0x822da640
	if (!ctx.cr6.lt) goto loc_822DA640;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da64c
	goto loc_822DA64C;
loc_822DA640:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da64c
	if (!ctx.cr6.gt) goto loc_822DA64C;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA64C:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r7,-12(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r7,4,0,27
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// srawi r7,r6,8
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 8;
	// stb r6,7(r11)
	PPC_STORE_U8(ctx.r11.u32 + 7, ctx.r6.u8);
	// srawi r31,r6,16
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r31.s64 = ctx.r6.s32 >> 16;
	// stb r7,8(r11)
	PPC_STORE_U8(ctx.r11.u32 + 8, ctx.r7.u8);
	// stb r31,9(r11)
	PPC_STORE_U8(ctx.r11.u32 + 9, ctx.r31.u8);
	// bge cr6,0x822da688
	if (!ctx.cr6.lt) goto loc_822DA688;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da694
	goto loc_822DA694;
loc_822DA688:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da694
	if (!ctx.cr6.gt) goto loc_822DA694;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA694:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// fctiwz f10,f0
	ctx.f10.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f10.u64);
	// lwz r7,-12(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r7,4,0,27
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// srawi r7,r6,8
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 8;
	// srawi r31,r6,16
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r31.s64 = ctx.r6.s32 >> 16;
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// extsb r7,r7
	ctx.r7.s64 = ctx.r7.s8;
	// extsb r31,r31
	ctx.r31.s64 = ctx.r31.s8;
	// stb r6,10(r11)
	PPC_STORE_U8(ctx.r11.u32 + 10, ctx.r6.u8);
	// stb r7,11(r11)
	PPC_STORE_U8(ctx.r11.u32 + 11, ctx.r7.u8);
	// stbu r31,12(r11)
	ea = 12 + ctx.r11.u32;
	PPC_STORE_U8(ea, ctx.r31.u8);
	ctx.r11.u32 = ea;
	// blt cr6,0x822da59c
	if (ctx.cr6.lt) goto loc_822DA59C;
loc_822DA6D8:
	// cmplw cr6,r10,r5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, ctx.xer);
	// bge cr6,0x822da75c
	if (!ctx.cr6.lt) goto loc_822DA75C;
	// rlwinm r11,r10,1,0,30
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r9,r10,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r10.s64;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
loc_822DA700:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da714
	if (!ctx.cr6.lt) goto loc_822DA714;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da720
	goto loc_822DA720;
loc_822DA714:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da720
	if (!ctx.cr6.gt) goto loc_822DA720;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA720:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fctiwz f10,f0
	ctx.f10.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f10.u64);
	// lwz r9,-12(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r8,r9,4,0,27
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// srawi r7,r8,8
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 8;
	// srawi r6,r8,16
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFF) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 16;
	// extsb r5,r8
	ctx.r5.s64 = ctx.r8.s8;
	// extsb r4,r7
	ctx.r4.s64 = ctx.r7.s8;
	// extsb r3,r6
	ctx.r3.s64 = ctx.r6.s8;
	// stb r5,1(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1, ctx.r5.u8);
	// stb r4,2(r11)
	PPC_STORE_U8(ctx.r11.u32 + 2, ctx.r4.u8);
	// stbu r3,3(r11)
	ea = 3 + ctx.r11.u32;
	PPC_STORE_U8(ea, ctx.r3.u8);
	ctx.r11.u32 = ea;
	// bdnz 0x822da700
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DA700;
loc_822DA75C:
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DA764"))) PPC_WEAK_FUNC(sub_822DA764);
PPC_FUNC_IMPL(__imp__sub_822DA764) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DA768"))) PPC_WEAK_FUNC(sub_822DA768);
PPC_FUNC_IMPL(__imp__sub_822DA768) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lfs f11,-1432(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -1432);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-1436(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1436);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,5280(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5280);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x822da8c8
	if (ctx.cr6.lt) goto loc_822DA8C8;
	// addi r8,r5,-3
	ctx.r8.s64 = ctx.r5.s64 + -3;
	// addi r9,r4,8
	ctx.r9.s64 = ctx.r4.s64 + 8;
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
loc_822DA79C:
	// lfs f0,-8(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da7b0
	if (!ctx.cr6.lt) goto loc_822DA7B0;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da7bc
	goto loc_822DA7BC;
loc_822DA7B0:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da7bc
	if (!ctx.cr6.gt) goto loc_822DA7BC;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA7BC:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,-4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r7,-12(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// srawi r6,r7,8
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 8;
	// srawi r31,r7,16
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r31.s64 = ctx.r7.s32 >> 16;
	// stb r7,1(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1, ctx.r7.u8);
	// stb r6,2(r11)
	PPC_STORE_U8(ctx.r11.u32 + 2, ctx.r6.u8);
	// stb r31,3(r11)
	PPC_STORE_U8(ctx.r11.u32 + 3, ctx.r31.u8);
	// bge cr6,0x822da7f4
	if (!ctx.cr6.lt) goto loc_822DA7F4;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da800
	goto loc_822DA800;
loc_822DA7F4:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da800
	if (!ctx.cr6.gt) goto loc_822DA800;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA800:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r7,-12(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// srawi r6,r7,8
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 8;
	// srawi r31,r7,16
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r31.s64 = ctx.r7.s32 >> 16;
	// stb r6,5(r11)
	PPC_STORE_U8(ctx.r11.u32 + 5, ctx.r6.u8);
	// stb r7,4(r11)
	PPC_STORE_U8(ctx.r11.u32 + 4, ctx.r7.u8);
	// stb r31,6(r11)
	PPC_STORE_U8(ctx.r11.u32 + 6, ctx.r31.u8);
	// bge cr6,0x822da838
	if (!ctx.cr6.lt) goto loc_822DA838;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da844
	goto loc_822DA844;
loc_822DA838:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da844
	if (!ctx.cr6.gt) goto loc_822DA844;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA844:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r7,-12(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// srawi r6,r7,8
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 8;
	// srawi r31,r7,16
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r31.s64 = ctx.r7.s32 >> 16;
	// stb r6,8(r11)
	PPC_STORE_U8(ctx.r11.u32 + 8, ctx.r6.u8);
	// stb r7,7(r11)
	PPC_STORE_U8(ctx.r11.u32 + 7, ctx.r7.u8);
	// stb r31,9(r11)
	PPC_STORE_U8(ctx.r11.u32 + 9, ctx.r31.u8);
	// bge cr6,0x822da87c
	if (!ctx.cr6.lt) goto loc_822DA87C;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da888
	goto loc_822DA888;
loc_822DA87C:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da888
	if (!ctx.cr6.gt) goto loc_822DA888;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA888:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// fctiwz f10,f0
	ctx.f10.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f10.u64);
	// lwz r7,-12(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// srawi r6,r7,8
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 8;
	// srawi r31,r7,16
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r31.s64 = ctx.r7.s32 >> 16;
	// extsb r7,r7
	ctx.r7.s64 = ctx.r7.s8;
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// extsb r31,r31
	ctx.r31.s64 = ctx.r31.s8;
	// stb r7,10(r11)
	PPC_STORE_U8(ctx.r11.u32 + 10, ctx.r7.u8);
	// stb r6,11(r11)
	PPC_STORE_U8(ctx.r11.u32 + 11, ctx.r6.u8);
	// stbu r31,12(r11)
	ea = 12 + ctx.r11.u32;
	PPC_STORE_U8(ea, ctx.r31.u8);
	ctx.r11.u32 = ea;
	// blt cr6,0x822da79c
	if (ctx.cr6.lt) goto loc_822DA79C;
loc_822DA8C8:
	// cmplw cr6,r10,r5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, ctx.xer);
	// bge cr6,0x822da948
	if (!ctx.cr6.lt) goto loc_822DA948;
	// rlwinm r11,r10,1,0,30
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r9,r10,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r10.s64;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
loc_822DA8F0:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da904
	if (!ctx.cr6.lt) goto loc_822DA904;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da910
	goto loc_822DA910;
loc_822DA904:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da910
	if (!ctx.cr6.gt) goto loc_822DA910;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA910:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fctiwz f10,f0
	ctx.f10.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f10.u64);
	// lwz r9,-12(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// srawi r8,r9,8
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 8;
	// srawi r7,r9,16
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 16;
	// extsb r6,r9
	ctx.r6.s64 = ctx.r9.s8;
	// extsb r5,r8
	ctx.r5.s64 = ctx.r8.s8;
	// extsb r4,r7
	ctx.r4.s64 = ctx.r7.s8;
	// stb r6,1(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1, ctx.r6.u8);
	// stb r5,2(r11)
	PPC_STORE_U8(ctx.r11.u32 + 2, ctx.r5.u8);
	// stbu r4,3(r11)
	ea = 3 + ctx.r11.u32;
	PPC_STORE_U8(ea, ctx.r4.u8);
	ctx.r11.u32 = ea;
	// bdnz 0x822da8f0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DA8F0;
loc_822DA948:
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DA950"))) PPC_WEAK_FUNC(sub_822DA950);
PPC_FUNC_IMPL(__imp__sub_822DA950) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lfs f11,-1440(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -1440);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-1444(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1444);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,5280(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5280);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x822daa78
	if (ctx.cr6.lt) goto loc_822DAA78;
	// addi r8,r5,-3
	ctx.r8.s64 = ctx.r5.s64 + -3;
	// addi r10,r4,12
	ctx.r10.s64 = ctx.r4.s64 + 12;
	// addi r11,r3,4
	ctx.r11.s64 = ctx.r3.s64 + 4;
	// subf r7,r3,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r3.s64;
loc_822DA984:
	// lfs f0,-12(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822da998
	if (!ctx.cr6.lt) goto loc_822DA998;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da9a4
	goto loc_822DA9A4;
loc_822DA998:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da9a4
	if (!ctx.cr6.gt) goto loc_822DA9A4;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA9A4:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfsx f0,r7,r11
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r6,-12(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r6,12,0,19
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFFFF000;
	// stw r6,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r6.u32);
	// bge cr6,0x822da9d0
	if (!ctx.cr6.lt) goto loc_822DA9D0;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822da9dc
	goto loc_822DA9DC;
loc_822DA9D0:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822da9dc
	if (!ctx.cr6.gt) goto loc_822DA9DC;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DA9DC:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r6,-12(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r6,12,0,19
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFFFF000;
	// stw r6,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r6.u32);
	// bge cr6,0x822daa08
	if (!ctx.cr6.lt) goto loc_822DAA08;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822daa14
	goto loc_822DAA14;
loc_822DAA08:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822daa14
	if (!ctx.cr6.gt) goto loc_822DAA14;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DAA14:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r6,-12(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r6,12,0,19
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFFFF000;
	// stw r6,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r6.u32);
	// bge cr6,0x822daa40
	if (!ctx.cr6.lt) goto loc_822DAA40;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822daa4c
	goto loc_822DAA4C;
loc_822DAA40:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822daa4c
	if (!ctx.cr6.gt) goto loc_822DAA4C;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DAA4C:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// fctiwz f10,f0
	ctx.f10.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f10.u64);
	// lwz r6,-12(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r6,12,0,19
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFFFF000;
	// stw r6,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r6.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// blt cr6,0x822da984
	if (ctx.cr6.lt) goto loc_822DA984;
loc_822DAA78:
	// cmplw cr6,r9,r5
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r5.u32, ctx.xer);
	// bgelr cr6
	if (!ctx.cr6.lt) return;
	// subf r8,r9,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r9.s64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r3,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r3.s64;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822DAA94:
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822daaa8
	if (!ctx.cr6.lt) goto loc_822DAAA8;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822daab4
	goto loc_822DAAB4;
loc_822DAAA8:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822daab4
	if (!ctx.cr6.gt) goto loc_822DAAB4;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DAAB4:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// fctiwz f10,f0
	ctx.f10.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f10.u64);
	// lwz r9,-12(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r8,r9,12,0,19
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFFFF000;
	// stw r8,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822daa94
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DAA94;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DAAD8"))) PPC_WEAK_FUNC(sub_822DAAD8);
PPC_FUNC_IMPL(__imp__sub_822DAAD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lfs f11,-1432(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -1432);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-1436(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1436);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,5280(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5280);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x822dac00
	if (ctx.cr6.lt) goto loc_822DAC00;
	// addi r8,r5,-3
	ctx.r8.s64 = ctx.r5.s64 + -3;
	// addi r10,r4,12
	ctx.r10.s64 = ctx.r4.s64 + 12;
	// addi r11,r3,4
	ctx.r11.s64 = ctx.r3.s64 + 4;
	// subf r7,r3,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r3.s64;
loc_822DAB0C:
	// lfs f0,-12(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822dab20
	if (!ctx.cr6.lt) goto loc_822DAB20;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dab2c
	goto loc_822DAB2C;
loc_822DAB20:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822dab2c
	if (!ctx.cr6.gt) goto loc_822DAB2C;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DAB2C:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfsx f0,r7,r11
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r6,-12(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// stw r6,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r6.u32);
	// bge cr6,0x822dab58
	if (!ctx.cr6.lt) goto loc_822DAB58;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dab64
	goto loc_822DAB64;
loc_822DAB58:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822dab64
	if (!ctx.cr6.gt) goto loc_822DAB64;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DAB64:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r6,-12(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// stw r6,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r6.u32);
	// bge cr6,0x822dab90
	if (!ctx.cr6.lt) goto loc_822DAB90;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dab9c
	goto loc_822DAB9C;
loc_822DAB90:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822dab9c
	if (!ctx.cr6.gt) goto loc_822DAB9C;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DAB9C:
	// fmuls f10,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f9.u64);
	// lwz r6,-12(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// stw r6,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r6.u32);
	// bge cr6,0x822dabc8
	if (!ctx.cr6.lt) goto loc_822DABC8;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dabd4
	goto loc_822DABD4;
loc_822DABC8:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822dabd4
	if (!ctx.cr6.gt) goto loc_822DABD4;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DABD4:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// fctiwz f10,f0
	ctx.f10.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f10.u64);
	// lwz r6,-12(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// stw r6,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r6.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// blt cr6,0x822dab0c
	if (ctx.cr6.lt) goto loc_822DAB0C;
loc_822DAC00:
	// cmplw cr6,r9,r5
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r5.u32, ctx.xer);
	// bgelr cr6
	if (!ctx.cr6.lt) return;
	// subf r8,r9,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r9.s64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r3,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r3.s64;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822DAC1C:
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822dac30
	if (!ctx.cr6.lt) goto loc_822DAC30;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dac3c
	goto loc_822DAC3C;
loc_822DAC30:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822dac3c
	if (!ctx.cr6.gt) goto loc_822DAC3C;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
loc_822DAC3C:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// fctiwz f10,f0
	ctx.f10.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfd f10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f10.u64);
	// lwz r9,-12(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// rlwinm r8,r9,8,0,23
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// stw r8,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822dac1c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DAC1C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DAC60"))) PPC_WEAK_FUNC(sub_822DAC60);
PPC_FUNC_IMPL(__imp__sub_822DAC60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// li r9,0
	ctx.r9.s64 = 0;
	// lfd f12,-1416(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r11.u32 + -1416);
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lfs f10,-1436(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1436);
	ctx.f10.f64 = double(temp.f32);
	// lfd f11,-1424(r8)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r8.u32 + -1424);
	// lfs f13,5280(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 5280);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x822dad74
	if (ctx.cr6.lt) goto loc_822DAD74;
	// addi r8,r5,-3
	ctx.r8.s64 = ctx.r5.s64 + -3;
	// addi r10,r4,12
	ctx.r10.s64 = ctx.r4.s64 + 12;
	// addi r11,r3,4
	ctx.r11.s64 = ctx.r3.s64 + 4;
	// subf r6,r3,r4
	ctx.r6.s64 = ctx.r4.s64 - ctx.r3.s64;
	// li r30,-4
	ctx.r30.s64 = -4;
	// li r31,4
	ctx.r31.s64 = 4;
	// li r7,8
	ctx.r7.s64 = 8;
loc_822DACB0:
	// lfs f0,-12(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822dacc4
	if (!ctx.cr6.lt) goto loc_822DACC4;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dacd0
	goto loc_822DACD0;
loc_822DACC4:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f11.f64);
	// ble cr6,0x822dacd0
	if (!ctx.cr6.gt) goto loc_822DACD0;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
loc_822DACD0:
	// fmul f9,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = ctx.f0.f64 * ctx.f12.f64;
	// lfsx f0,r6,r11
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f8,f9
	ctx.f8.u64 = uint64_t(int32_t(std::trunc(ctx.f9.f64)));
	// stfiwx f8,r11,r30
	PPC_STORE_U32(ctx.r11.u32 + ctx.r30.u32, ctx.f8.u32);
	// bge cr6,0x822dacf0
	if (!ctx.cr6.lt) goto loc_822DACF0;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dacfc
	goto loc_822DACFC;
loc_822DACF0:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f11.f64);
	// ble cr6,0x822dacfc
	if (!ctx.cr6.gt) goto loc_822DACFC;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
loc_822DACFC:
	// fmul f9,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = ctx.f0.f64 * ctx.f12.f64;
	// lfs f0,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f8,f9
	ctx.f8.u64 = uint64_t(int32_t(std::trunc(ctx.f9.f64)));
	// stfiwx f8,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f8.u32);
	// bge cr6,0x822dad1c
	if (!ctx.cr6.lt) goto loc_822DAD1C;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dad28
	goto loc_822DAD28;
loc_822DAD1C:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f11.f64);
	// ble cr6,0x822dad28
	if (!ctx.cr6.gt) goto loc_822DAD28;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
loc_822DAD28:
	// fmul f9,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = ctx.f0.f64 * ctx.f12.f64;
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// fctiwz f8,f9
	ctx.f8.u64 = uint64_t(int32_t(std::trunc(ctx.f9.f64)));
	// stfiwx f8,r11,r31
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, ctx.f8.u32);
	// bge cr6,0x822dad48
	if (!ctx.cr6.lt) goto loc_822DAD48;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dad54
	goto loc_822DAD54;
loc_822DAD48:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f11.f64);
	// ble cr6,0x822dad54
	if (!ctx.cr6.gt) goto loc_822DAD54;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
loc_822DAD54:
	// fmul f0,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f0.f64 * ctx.f12.f64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// fctiwz f9,f0
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfiwx f9,r11,r7
	PPC_STORE_U32(ctx.r11.u32 + ctx.r7.u32, ctx.f9.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// blt cr6,0x822dacb0
	if (ctx.cr6.lt) goto loc_822DACB0;
loc_822DAD74:
	// cmplw cr6,r9,r5
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r5.u32, ctx.xer);
	// bge cr6,0x822dadc4
	if (!ctx.cr6.lt) goto loc_822DADC4;
	// subf r8,r9,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r9.s64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r3,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r3.s64;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_822DAD90:
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x822dada4
	if (!ctx.cr6.lt) goto loc_822DADA4;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dadb0
	goto loc_822DADB0;
loc_822DADA4:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f11.f64);
	// ble cr6,0x822dadb0
	if (!ctx.cr6.gt) goto loc_822DADB0;
	// fmr f0,f10
	ctx.f0.f64 = ctx.f10.f64;
loc_822DADB0:
	// fmul f0,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f0.f64 * ctx.f12.f64;
	// fctiwz f9,f0
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f0.f64)));
	// stfiwx f9,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f9.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822dad90
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DAD90;
loc_822DADC4:
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DADD0"))) PPC_WEAK_FUNC(sub_822DADD0);
PPC_FUNC_IMPL(__imp__sub_822DADD0) {
	PPC_FUNC_PROLOGUE();
	// mtctr r3
	ctx.ctr.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x822dae48
	if (ctx.cr6.eq) goto loc_822DAE48;
	// bdz 0x822dae00
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822DAE00;
	// bdz 0x822dae0c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822DAE0C;
	// bdz 0x822dae18
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822DAE18;
	// bdz 0x822dae24
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822DAE24;
	// bdz 0x822dae30
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822DAE30;
	// bdz 0x822dae3c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0) goto loc_822DAE3C;
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-24264
	ctx.r3.s64 = ctx.r11.s64 + -24264;
	// blr 
	return;
loc_822DAE00:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-23976
	ctx.r3.s64 = ctx.r11.s64 + -23976;
	// blr 
	return;
loc_822DAE0C:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-23192
	ctx.r3.s64 = ctx.r11.s64 + -23192;
	// blr 
	return;
loc_822DAE18:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-22192
	ctx.r3.s64 = ctx.r11.s64 + -22192;
	// blr 
	return;
loc_822DAE24:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-22680
	ctx.r3.s64 = ctx.r11.s64 + -22680;
	// blr 
	return;
loc_822DAE30:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-21800
	ctx.r3.s64 = ctx.r11.s64 + -21800;
	// blr 
	return;
loc_822DAE3C:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-21408
	ctx.r3.s64 = ctx.r11.s64 + -21408;
	// blr 
	return;
loc_822DAE48:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-23600
	ctx.r3.s64 = ctx.r11.s64 + -23600;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DAE54"))) PPC_WEAK_FUNC(sub_822DAE54);
PPC_FUNC_IMPL(__imp__sub_822DAE54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DAE58"))) PPC_WEAK_FUNC(sub_822DAE58);
PPC_FUNC_IMPL(__imp__sub_822DAE58) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32202
	ctx.r11.s64 = -2110390272;
	// addi r3,r11,16952
	ctx.r3.s64 = ctx.r11.s64 + 16952;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DAE64"))) PPC_WEAK_FUNC(sub_822DAE64);
PPC_FUNC_IMPL(__imp__sub_822DAE64) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DAE68"))) PPC_WEAK_FUNC(sub_822DAE68);
PPC_FUNC_IMPL(__imp__sub_822DAE68) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x822DAE70;
	__restfpr_29(ctx, base);
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r8,32(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r7,0
	if (ctx.r7.u32 <= 0) __builtin_debugtrap();
	// stw r9,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, ctx.r9.u32);
	// lfs f11,-48(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	ctx.f11.f64 = double(temp.f32);
	// stw r8,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, ctx.r8.u32);
	// lfs f10,-44(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	ctx.f10.f64 = double(temp.f32);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// divwu. r10,r10,r7
	ctx.r10.u32 = ctx.r10.u32 / ctx.r7.u32;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r6,r11
	ctx.r6.u64 = ctx.r11.u64;
	// lwz r31,12(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r29,24(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// beq 0x822daf4c
	if (ctx.cr0.eq) goto loc_822DAF4C;
	// rlwinm r30,r7,2,0,29
	ctx.r30.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r3,r11,r8
	ctx.r3.s64 = ctx.r8.s64 - ctx.r11.s64;
loc_822DAEBC:
	// li r11,32
	ctx.r11.s64 = 32;
	// dcbt r6,r11
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822daf40
	if (ctx.cr6.eq) goto loc_822DAF40;
	// subf r8,r9,r3
	ctx.r8.s64 = ctx.r3.s64 - ctx.r9.s64;
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
	// add r4,r8,r6
	ctx.r4.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r8,r9,r31
	ctx.r8.s64 = ctx.r31.s64 - ctx.r9.s64;
	// subf r5,r9,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r9.s64;
loc_822DAEE4:
	// lfs f12,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// cmpwi cr6,r29,2
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 2, ctx.xer);
	// lfsx f0,r4,r11
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f0,f0,f12
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f12.f64);
	// lfsx f9,r8,r11
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r11.u32);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f13,f9,f10,f0
	ctx.f13.f64 = -double(std::fma(float(ctx.f9.f64), float(ctx.f10.f64), -float(ctx.f0.f64)));
	// fmadds f0,f13,f11,f9
	ctx.f0.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f9.f64)));
	// stfsx f0,r8,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + ctx.r11.u32, temp.u32);
	// fmadds f12,f0,f11,f12
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f11.f64), float(ctx.f12.f64)));
	// stfs f12,0(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// bne cr6,0x822daf18
	if (!ctx.cr6.eq) goto loc_822DAF18;
	// stfsx f13,r5,r11
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, temp.u32);
	// b 0x822daf38
	goto loc_822DAF38;
loc_822DAF18:
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// bne cr6,0x822daf28
	if (!ctx.cr6.eq) goto loc_822DAF28;
	// stfsx f12,r5,r11
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, temp.u32);
	// b 0x822daf38
	goto loc_822DAF38;
loc_822DAF28:
	// cmpwi cr6,r29,1
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 1, ctx.xer);
	// beq cr6,0x822daf34
	if (ctx.cr6.eq) goto loc_822DAF34;
	// fadds f0,f12,f13
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
loc_822DAF34:
	// stfsx f0,r5,r11
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, temp.u32);
loc_822DAF38:
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x822daee4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DAEE4;
loc_822DAF40:
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// add r6,r30,r6
	ctx.r6.u64 = ctx.r30.u64 + ctx.r6.u64;
	// bne 0x822daebc
	if (!ctx.cr0.eq) goto loc_822DAEBC;
loc_822DAF4C:
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DAF50"))) PPC_WEAK_FUNC(sub_822DAF50);
PPC_FUNC_IMPL(__imp__sub_822DAF50) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r10,32(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lwz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r6,12(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r11.u32);
	// lfs f12,-16(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// stw r10,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r10.u32);
	// lfs f11,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f11.f64 = double(temp.f32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f13,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lfs f0,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplwi cr6,r8,3
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 3, ctx.xer);
	// bgt cr6,0x822db0bc
	if (ctx.cr6.gt) goto loc_822DB0BC;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// bdzf 4*cr6+eq,0x822db034
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822DB034;
	// bdzf 4*cr6+eq,0x822dafec
	--ctx.ctr.u64;
	if (ctx.ctr.u32 == 0 && !ctx.cr6.eq) goto loc_822DAFEC;
	// bne cr6,0x822db07c
	if (!ctx.cr6.eq) goto loc_822DB07C;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822db0bc
	if (ctx.cr6.eq) goto loc_822DB0BC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
loc_822DAFB8:
	// lfsx f10,r9,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f9,f10,f13
	ctx.f9.f64 = static_cast<float>(ctx.f10.f64 - ctx.f13.f64);
	// fnmsubs f8,f11,f0,f9
	ctx.f8.f64 = -double(std::fma(float(ctx.f11.f64), float(ctx.f0.f64), -float(ctx.f9.f64)));
	// fmadds f0,f8,f12,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f12.f64), float(ctx.f0.f64)));
	// fmadds f13,f0,f12,f13
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f12.f64), float(ctx.f13.f64)));
	// dcbt r0,r10
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// bdnz 0x822dafb8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DAFB8;
	// stfs f13,0(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// stfs f0,0(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// blr 
	return;
loc_822DAFEC:
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822db0bc
	if (ctx.cr6.eq) goto loc_822DB0BC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
loc_822DB000:
	// lfsx f10,r11,r9
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f9,f10,f13
	ctx.f9.f64 = static_cast<float>(ctx.f10.f64 - ctx.f13.f64);
	// fnmsubs f8,f11,f0,f9
	ctx.f8.f64 = -double(std::fma(float(ctx.f11.f64), float(ctx.f0.f64), -float(ctx.f9.f64)));
	// fmadds f0,f8,f12,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f12.f64), float(ctx.f0.f64)));
	// dcbt r0,r10
	// stfs f8,0(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fmadds f13,f0,f12,f13
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f12.f64), float(ctx.f13.f64)));
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// bdnz 0x822db000
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DB000;
	// stfs f13,0(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// stfs f0,0(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// blr 
	return;
loc_822DB034:
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822db0bc
	if (ctx.cr6.eq) goto loc_822DB0BC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
loc_822DB048:
	// lfsx f10,r11,r9
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f9,f10,f13
	ctx.f9.f64 = static_cast<float>(ctx.f10.f64 - ctx.f13.f64);
	// fnmsubs f8,f11,f0,f9
	ctx.f8.f64 = -double(std::fma(float(ctx.f11.f64), float(ctx.f0.f64), -float(ctx.f9.f64)));
	// fmadds f0,f8,f12,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f12.f64), float(ctx.f0.f64)));
	// dcbt r0,r10
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fmadds f13,f0,f12,f13
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f12.f64), float(ctx.f13.f64)));
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// bdnz 0x822db048
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DB048;
	// stfs f13,0(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// stfs f0,0(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// blr 
	return;
loc_822DB07C:
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822db0bc
	if (ctx.cr6.eq) goto loc_822DB0BC;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
loc_822DB090:
	// lfsx f10,r11,r9
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f9,f10,f13
	ctx.f9.f64 = static_cast<float>(ctx.f10.f64 - ctx.f13.f64);
	// fnmsubs f8,f11,f0,f9
	ctx.f8.f64 = -double(std::fma(float(ctx.f11.f64), float(ctx.f0.f64), -float(ctx.f9.f64)));
	// fmadds f0,f8,f12,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f12.f64), float(ctx.f0.f64)));
	// fmadds f13,f0,f12,f13
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f12.f64), float(ctx.f13.f64)));
	// dcbt r0,r10
	// fadds f7,f13,f8
	ctx.f7.f64 = double(float(ctx.f13.f64 + ctx.f8.f64));
	// stfs f7,0(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// bdnz 0x822db090
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DB090;
loc_822DB0BC:
	// stfs f13,0(r7)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// stfs f0,0(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB0C8"))) PPC_WEAK_FUNC(sub_822DB0C8);
PPC_FUNC_IMPL(__imp__sub_822DB0C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r9,32(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lwz r6,8(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r5,12(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// stw r10,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r10.u32);
	// lfs f0,-16(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f0.f64 = double(temp.f32);
	// stw r9,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r9.u32);
	// lfs f7,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f7.f64 = double(temp.f32);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f11,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lfs f13,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lfs f12,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822db1a4
	if (ctx.cr6.eq) goto loc_822DB1A4;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// rlwinm r8,r11,31,1,31
	ctx.r8.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r11,r9,4
	ctx.r11.s64 = ctx.r9.s64 + 4;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// subf r9,r9,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r9.s64;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// li r8,12
	ctx.r8.s64 = 12;
loc_822DB12C:
	// lfs f9,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lfsx f8,r9,r11
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f8.f64 = double(temp.f32);
	// fsubs f6,f9,f11
	ctx.f6.f64 = static_cast<float>(ctx.f9.f64 - ctx.f11.f64);
	// fsubs f5,f8,f10
	ctx.f5.f64 = static_cast<float>(ctx.f8.f64 - ctx.f10.f64);
	// fnmsubs f9,f7,f13,f6
	ctx.f9.f64 = -double(std::fma(float(ctx.f7.f64), float(ctx.f13.f64), -float(ctx.f6.f64)));
	// fnmsubs f8,f7,f12,f5
	ctx.f8.f64 = -double(std::fma(float(ctx.f7.f64), float(ctx.f12.f64), -float(ctx.f5.f64)));
	// fmadds f13,f9,f0,f13
	ctx.f13.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// fmadds f12,f8,f0,f12
	ctx.f12.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// fmadds f11,f13,f0,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// fmadds f10,f12,f0,f10
	ctx.f10.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f0.f64), float(ctx.f10.f64)));
	// dcbt r11,r8
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// bne cr6,0x822db16c
	if (!ctx.cr6.eq) goto loc_822DB16C;
	// stfs f11,-4(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// stfs f10,0(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822db198
	goto loc_822DB198;
loc_822DB16C:
	// cmpwi cr6,r7,2
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 2, ctx.xer);
	// beq cr6,0x822db190
	if (ctx.cr6.eq) goto loc_822DB190;
	// cmpwi cr6,r7,1
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 1, ctx.xer);
	// bne cr6,0x822db188
	if (!ctx.cr6.eq) goto loc_822DB188;
	// stfs f13,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// stfs f12,0(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// b 0x822db198
	goto loc_822DB198;
loc_822DB188:
	// fadds f9,f11,f9
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// fadds f8,f10,f8
	ctx.f8.f64 = double(float(ctx.f10.f64 + ctx.f8.f64));
loc_822DB190:
	// stfs f8,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfs f9,-4(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
loc_822DB198:
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bdnz 0x822db12c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DB12C;
loc_822DB1A4:
	// stfs f11,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// stfs f10,4(r6)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// stfs f13,0(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// stfs f12,4(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB1B8"))) PPC_WEAK_FUNC(sub_822DB1B8);
PPC_FUNC_IMPL(__imp__sub_822DB1B8) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// bne cr6,0x822db1cc
	if (!ctx.cr6.eq) goto loc_822DB1CC;
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-20656
	ctx.r3.s64 = ctx.r11.s64 + -20656;
	// blr 
	return;
loc_822DB1CC:
	// cmplwi cr6,r3,2
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 2, ctx.xer);
	// bne cr6,0x822db1e0
	if (!ctx.cr6.eq) goto loc_822DB1E0;
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-20280
	ctx.r3.s64 = ctx.r11.s64 + -20280;
	// blr 
	return;
loc_822DB1E0:
	// lis r11,-32210
	ctx.r11.s64 = -2110914560;
	// addi r3,r11,-20888
	ctx.r3.s64 = ctx.r11.s64 + -20888;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB1EC"))) PPC_WEAK_FUNC(sub_822DB1EC);
PPC_FUNC_IMPL(__imp__sub_822DB1EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DB1F0"))) PPC_WEAK_FUNC(sub_822DB1F0);
PPC_FUNC_IMPL(__imp__sub_822DB1F0) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r4,8
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 8, ctx.xer);
	// bne cr6,0x822db230
	if (!ctx.cr6.eq) goto loc_822DB230;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822DB1FC:
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x822db20c
	if (ctx.cr6.eq) goto loc_822DB20C;
	// cmplwi cr6,r3,2
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 2, ctx.xer);
	// bne cr6,0x822db250
	if (!ctx.cr6.eq) goto loc_822DB250;
loc_822DB20C:
	// rlwinm r11,r3,1,0,30
	ctx.r11.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r9,-32199
	ctx.r9.s64 = -2110193664;
	// add r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 + ctx.r11.u64;
	// addi r8,r9,12260
	ctx.r8.s64 = ctx.r9.s64 + 12260;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r7,r11,-3
	ctx.r7.s64 = ctx.r11.s64 + -3;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r6,r8
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r8.u32);
	// blr 
	return;
loc_822DB230:
	// cmplwi cr6,r4,16
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 16, ctx.xer);
	// bne cr6,0x822db240
	if (!ctx.cr6.eq) goto loc_822DB240;
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x822db1fc
	goto loc_822DB1FC;
loc_822DB240:
	// cmplwi cr6,r4,32
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 32, ctx.xer);
	// bne cr6,0x822db250
	if (!ctx.cr6.eq) goto loc_822DB250;
	// li r10,2
	ctx.r10.s64 = 2;
	// b 0x822db1fc
	goto loc_822DB1FC;
loc_822DB250:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB258"))) PPC_WEAK_FUNC(sub_822DB258);
PPC_FUNC_IMPL(__imp__sub_822DB258) {
	PPC_FUNC_PROLOGUE();
	// clrlwi r11,r4,16
	ctx.r11.u64 = ctx.r4.u32 & 0xFFFF;
	// rlwinm r9,r4,3,13,28
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0x7FFF8;
	// clrlwi r10,r3,16
	ctx.r10.u64 = ctx.r3.u32 & 0xFFFF;
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r11.s64;
	// cmpw cr6,r10,r8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822db29c
	if (ctx.cr6.lt) goto loc_822DB29C;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// twllei r11,0
	if (ctx.r11.u32 <= 0) __builtin_debugtrap();
	// rotlwi r10,r9,1
	ctx.r10.u64 = rotl32(ctx.r9.u32, 1);
	// divw r8,r9,r11
	ctx.r8.s32 = ctx.r9.s32 / ctx.r11.s32;
	// addi r7,r10,-1
	ctx.r7.s64 = ctx.r10.s64 + -1;
	// addis r6,r8,1
	ctx.r6.s64 = ctx.r8.s64 + 65536;
	// andc r5,r11,r7
	ctx.r5.u64 = ctx.r11.u64 & ~ctx.r7.u64;
	// addi r6,r6,-12
	ctx.r6.s64 = ctx.r6.s64 + -12;
	// twlgei r5,-1
	if (ctx.r5.u32 >= 4294967295) __builtin_debugtrap();
	// clrlwi r3,r6,16
	ctx.r3.u64 = ctx.r6.u32 & 0xFFFF;
	// blr 
	return;
loc_822DB29C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB2A4"))) PPC_WEAK_FUNC(sub_822DB2A4);
PPC_FUNC_IMPL(__imp__sub_822DB2A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DB2A8"))) PPC_WEAK_FUNC(sub_822DB2A8);
PPC_FUNC_IMPL(__imp__sub_822DB2A8) {
	PPC_FUNC_PROLOGUE();
	// clrlwi r11,r4,16
	ctx.r11.u64 = ctx.r4.u32 & 0xFFFF;
	// rlwinm r9,r4,3,13,28
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0x7FFF8;
	// clrlwi r10,r3,16
	ctx.r10.u64 = ctx.r3.u32 & 0xFFFF;
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r11.s64;
	// cmpw cr6,r10,r8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822db2ec
	if (ctx.cr6.lt) goto loc_822DB2EC;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// twllei r11,0
	if (ctx.r11.u32 <= 0) __builtin_debugtrap();
	// divw r8,r9,r11
	ctx.r8.s32 = ctx.r9.s32 / ctx.r11.s32;
	// rotlwi r10,r9,1
	ctx.r10.u64 = rotl32(ctx.r9.u32, 1);
	// addis r4,r8,1
	ctx.r4.s64 = ctx.r8.s64 + 65536;
	// addi r7,r10,-1
	ctx.r7.s64 = ctx.r10.s64 + -1;
	// addi r4,r4,-12
	ctx.r4.s64 = ctx.r4.s64 + -12;
	// andc r3,r11,r7
	ctx.r3.u64 = ctx.r11.u64 & ~ctx.r7.u64;
	// clrlwi r11,r4,16
	ctx.r11.u64 = ctx.r4.u32 & 0xFFFF;
	// twlgei r3,-1
	if (ctx.r3.u32 >= 4294967295) __builtin_debugtrap();
	// b 0x822db2f0
	goto loc_822DB2F0;
loc_822DB2EC:
	// li r11,0
	ctx.r11.s64 = 0;
loc_822DB2F0:
	// clrlwi r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// add r10,r11,r5
	ctx.r10.u64 = ctx.r11.u64 + ctx.r5.u64;
	// twllei r11,0
	if (ctx.r11.u32 <= 0) __builtin_debugtrap();
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// divwu r3,r10,r11
	ctx.r3.u32 = ctx.r10.u32 / ctx.r11.u32;
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// mullw r11,r3,r11
	ctx.r11.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r11.s32);
	// subf r10,r5,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r5.s64;
	// stw r10,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB31C"))) PPC_WEAK_FUNC(sub_822DB31C);
PPC_FUNC_IMPL(__imp__sub_822DB31C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DB320"))) PPC_WEAK_FUNC(sub_822DB320);
PPC_FUNC_IMPL(__imp__sub_822DB320) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f13,28(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f11.f64 = double(temp.f32);
	// li r11,1
	ctx.r11.s64 = 1;
	// lfs f12,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lfs f0,5256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f10,f0,f13
	ctx.f10.f64 = static_cast<float>(ctx.f0.f64 - ctx.f13.f64);
	// fmuls f0,f10,f11
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// blt cr6,0x822db3f0
	if (ctx.cr6.lt) goto loc_822DB3F0;
	// addi r10,r5,-3
	ctx.r10.s64 = ctx.r5.s64 + -3;
loc_822DB34C:
	// lfs f11,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f9,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f12,0(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f5,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// lfs f4,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f2,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmadds f12,f9,f13,f10
	ctx.f12.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f13.f64), float(ctx.f10.f64)));
	// stfs f12,16(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// fmadds f10,f13,f5,f8
	ctx.f10.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f8.f64)));
	// stfs f10,20(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// stfs f10,4(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fmadds f8,f12,f13,f6
	ctx.f8.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f13.f64), float(ctx.f6.f64)));
	// stfs f8,16(r3)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmr f11,f12
	ctx.f11.f64 = ctx.f12.f64;
	// fmadds f4,f8,f13,f3
	ctx.f4.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f13.f64), float(ctx.f3.f64)));
	// stfs f4,16(r3)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// fmadds f6,f13,f10,f9
	ctx.f6.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f10.f64), float(ctx.f9.f64)));
	// stfs f6,20(r3)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// fmuls f5,f8,f0
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f6,8(r4)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fmr f7,f8
	ctx.f7.f64 = ctx.f8.f64;
	// fmuls f12,f4,f0
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmadds f11,f4,f13,f1
	ctx.f11.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f13.f64), float(ctx.f1.f64)));
	// stfs f11,16(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// fmadds f2,f13,f6,f5
	ctx.f2.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f6.f64), float(ctx.f5.f64)));
	// stfs f2,20(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// stfs f2,12(r4)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmr f3,f4
	ctx.f3.f64 = ctx.f4.f64;
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// fmadds f12,f13,f2,f12
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f2.f64), float(ctx.f12.f64)));
	// stfs f12,20(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// ble cr6,0x822db34c
	if (!ctx.cr6.gt) goto loc_822DB34C;
loc_822DB3F0:
	// cmplw cr6,r11,r5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r5.u32, ctx.xer);
	// bgtlr cr6
	if (ctx.cr6.gt) return;
	// subf r11,r11,r5
	ctx.r11.s64 = ctx.r5.s64 - ctx.r11.s64;
	// lfs f13,28(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// addi r11,r4,-4
	ctx.r11.s64 = ctx.r4.s64 + -4;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_822DB40C:
	// lfs f11,16(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f7,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// stfsu f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r11.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r11.u32 = ea;
	// fmadds f12,f13,f7,f9
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f9.f64)));
	// stfs f12,20(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// fmadds f6,f13,f11,f8
	ctx.f6.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f8.f64)));
	// stfs f6,16(r3)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// bdnz 0x822db40c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DB40C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB43C"))) PPC_WEAK_FUNC(sub_822DB43C);
PPC_FUNC_IMPL(__imp__sub_822DB43C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DB440"))) PPC_WEAK_FUNC(sub_822DB440);
PPC_FUNC_IMPL(__imp__sub_822DB440) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r12,r1,-16
	ctx.r12.s64 = ctx.r1.s64 + -16;
	// bl 0x8233fa2c
	ctx.lr = 0x822DB454;
	sub_8233FA2C(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lfs f0,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// lfs f13,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// fadds f31,f13,f0
	ctx.f31.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f25,-28948(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f25.f64 = double(temp.f32);
	// lfs f28,5256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f28.f64 = double(temp.f32);
	// fcmpu cr6,f31,f25
	ctx.cr6.compare(ctx.f31.f64, ctx.f25.f64);
	// bge cr6,0x822db488
	if (!ctx.cr6.lt) goto loc_822DB488;
	// fdivs f27,f0,f31
	ctx.f27.f64 = double(float(ctx.f0.f64 / ctx.f31.f64));
	// b 0x822db48c
	goto loc_822DB48C;
loc_822DB488:
	// fmr f27,f28
	ctx.fpscr.disableFlushMode();
	ctx.f27.f64 = ctx.f28.f64;
loc_822DB48C:
	// fsubs f12,f28,f27
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = static_cast<float>(ctx.f28.f64 - ctx.f27.f64);
	// lfs f11,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// fdivs f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 / ctx.f10.f64));
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfd f30,-17064(r11)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r11.u32 + -17064);
	// lfs f0,5284(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5284);
	ctx.f0.f64 = double(temp.f32);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// lfs f13,-1644(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1644);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f12,f31
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f26,f9,f0
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// bl 0x8233c318
	ctx.lr = 0x822DB4C8;
	sub_8233C318(ctx, base);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// lfd f29,18456(r8)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r8.u32 + 18456);
	// fmr f2,f29
	ctx.f2.f64 = ctx.f29.f64;
	// bl 0x8233c318
	ctx.lr = 0x822DB4DC;
	sub_8233C318(ctx, base);
	// fmuls f7,f27,f31
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// frsp f31,f1
	ctx.f31.f64 = double(float(ctx.f1.f64));
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// lfs f0,-1640(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -1640);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f2,f7,f0
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// bl 0x8233c318
	ctx.lr = 0x822DB4F8;
	sub_8233C318(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// stfs f1,24(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r31.u32 + 24, temp.u32);
	// fmr f2,f29
	ctx.f2.f64 = ctx.f29.f64;
	// bl 0x8233c318
	ctx.lr = 0x822DB508;
	sub_8233C318(ctx, base);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// frsp f6,f1
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = double(float(ctx.f1.f64));
	// stfs f6,24(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + 24, temp.u32);
	// lfs f0,-1596(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -1596);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f31,f0
	ctx.cr6.compare(ctx.f31.f64, ctx.f0.f64);
	// bge cr6,0x822db568
	if (!ctx.cr6.lt) goto loc_822DB568;
	// fmr f1,f26
	ctx.f1.f64 = ctx.f26.f64;
	// bl 0x8233c950
	ctx.lr = 0x822DB528;
	sub_8233C950(ctx, base);
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// fsubs f12,f28,f31
	ctx.f12.f64 = static_cast<float>(ctx.f28.f64 - ctx.f31.f64);
	// lfs f0,5260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5260);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f11,f13,f13,f28
	ctx.f11.f64 = -double(std::fma(float(ctx.f13.f64), float(ctx.f13.f64), -float(ctx.f28.f64)));
	// fsubs f10,f28,f13
	ctx.f10.f64 = static_cast<float>(ctx.f28.f64 - ctx.f13.f64);
	// fnmsubs f9,f13,f31,f28
	ctx.f9.f64 = -double(std::fma(float(ctx.f13.f64), float(ctx.f31.f64), -float(ctx.f28.f64)));
	// fmuls f8,f11,f31
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fmuls f7,f10,f31
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// fmuls f6,f8,f31
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fmsubs f5,f7,f0,f6
	ctx.f5.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f0.f64), -float(ctx.f6.f64)));
	// fsqrts f4,f5
	ctx.f4.f64 = double(simd::sqrt_f32(float(ctx.f5.f64)));
	// fsubs f3,f9,f4
	ctx.f3.f64 = static_cast<float>(ctx.f9.f64 - ctx.f4.f64);
	// fdivs f2,f3,f12
	ctx.f2.f64 = double(float(ctx.f3.f64 / ctx.f12.f64));
	// stfs f2,28(r31)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r31.u32 + 28, temp.u32);
	// b 0x822db56c
	goto loc_822DB56C;
loc_822DB568:
	// stfs f25,28(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r31.u32 + 28, temp.u32);
loc_822DB56C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// addi r12,r1,-16
	ctx.r12.s64 = ctx.r1.s64 + -16;
	// bl 0x8233fa78
	ctx.lr = 0x822DB578;
	__savefpr_25(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB588"))) PPC_WEAK_FUNC(sub_822DB588);
PPC_FUNC_IMPL(__imp__sub_822DB588) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f13,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// li r11,1
	ctx.r11.s64 = 1;
	// lfs f12,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f12.f64 = double(temp.f32);
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// lfs f11,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,5256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f9,f0,f13
	ctx.f9.f64 = static_cast<float>(ctx.f0.f64 - ctx.f13.f64);
	// fmuls f0,f9,f10
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// blt cr6,0x822db6f0
	if (ctx.cr6.lt) goto loc_822DB6F0;
	// addi r10,r5,-3
	ctx.r10.s64 = ctx.r5.s64 + -3;
loc_822DB5B8:
	// lfs f10,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stfs f12,0(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfsu f12,4(r4)
	ea = 4 + ctx.r4.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fmuls f8,f12,f0
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f6,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// fmuls f3,f6,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f7,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f4,f6,f13,f9
	ctx.f4.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f13.f64), float(ctx.f9.f64)));
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmadds f5,f7,f13,f8
	ctx.f5.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f13.f64), float(ctx.f8.f64)));
	// lfs f9,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// lfsu f12,4(r4)
	ea = 4 + ctx.r4.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fmuls f1,f7,f0
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fmuls f8,f12,f0
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f5,20(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// fmadds f7,f13,f9,f3
	ctx.f7.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f3.f64)));
	// stfs f7,0(r4)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmr f2,f4
	ctx.f2.f64 = ctx.f4.f64;
	// lfsu f12,4(r4)
	ea = 4 + ctx.r4.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fmuls f6,f12,f0
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f4,16(r3)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// fmr f10,f5
	ctx.f10.f64 = ctx.f5.f64;
	// lfs f9,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f5,f4,f13,f8
	ctx.f5.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f13.f64), float(ctx.f8.f64)));
	// stfs f5,16(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f7,24(r3)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// fmadds f8,f13,f9,f1
	ctx.f8.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f1.f64)));
	// stfs f8,0(r4)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfsu f12,4(r4)
	ea = 4 + ctx.r4.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// stfs f8,28(r3)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// fmadds f4,f10,f13,f6
	ctx.f4.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f13.f64), float(ctx.f6.f64)));
	// stfs f4,20(r3)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// fmr f3,f5
	ctx.f3.f64 = ctx.f5.f64;
	// fmadds f5,f13,f7,f2
	ctx.f5.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f2.f64)));
	// stfs f5,0(r4)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmuls f6,f12,f0
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfsu f12,4(r4)
	ea = 4 + ctx.r4.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fmr f11,f4
	ctx.f11.f64 = ctx.f4.f64;
	// stfs f5,24(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// fmuls f4,f12,f0
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmadds f2,f3,f13,f6
	ctx.f2.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f13.f64), float(ctx.f6.f64)));
	// stfs f2,16(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmadds f1,f11,f13,f4
	ctx.f1.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f13.f64), float(ctx.f4.f64)));
	// stfs f1,20(r3)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// fmuls f6,f3,f0
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmr f9,f2
	ctx.f9.f64 = ctx.f2.f64;
	// fmuls f2,f11,f0
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmadds f3,f13,f8,f10
	ctx.f3.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f8.f64), float(ctx.f10.f64)));
	// stfs f3,0(r4)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmr f4,f1
	ctx.f4.f64 = ctx.f1.f64;
	// lfsu f12,4(r4)
	ea = 4 + ctx.r4.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f12.f64 = double(temp.f32);
	// fmuls f1,f12,f0
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f3,28(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// fmadds f12,f13,f5,f6
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f5.f64), float(ctx.f6.f64)));
	// stfs f12,0(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfsu f11,4(r4)
	ea = 4 + ctx.r4.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f11.f64 = double(temp.f32);
	// fmadds f10,f13,f3,f2
	ctx.f10.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f3.f64), float(ctx.f2.f64)));
	// fmuls f6,f11,f0
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f12,24(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// fmuls f2,f9,f0
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f10,0(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmuls f11,f4,f0
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f10,28(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// fmadds f9,f9,f13,f1
	ctx.f9.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f13.f64), float(ctx.f1.f64)));
	// stfs f9,16(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// fmadds f8,f4,f13,f6
	ctx.f8.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f13.f64), float(ctx.f6.f64)));
	// stfs f8,20(r3)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// fmadds f12,f13,f12,f2
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f12.f64), float(ctx.f2.f64)));
	// stfs f12,24(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// fmadds f11,f13,f10,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f10.f64), float(ctx.f11.f64)));
	// stfs f11,28(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// ble cr6,0x822db5b8
	if (!ctx.cr6.gt) goto loc_822DB5B8;
loc_822DB6F0:
	// cmplw cr6,r11,r5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r5.u32, ctx.xer);
	// bgtlr cr6
	if (ctx.cr6.gt) return;
	// subf r11,r11,r5
	ctx.r11.s64 = ctx.r5.s64 - ctx.r11.s64;
	// lfs f13,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
loc_822DB708:
	// lfs f10,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f7,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f12,0(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmuls f5,f7,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfsu f10,4(r4)
	ea = 4 + ctx.r4.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	ctx.f10.f64 = double(temp.f32);
	// fmuls f4,f10,f0
	ctx.f4.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f3,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f2,f13,f9,f8
	ctx.f2.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f8.f64)));
	// lfs f1,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f12,f13,f3,f6
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f3.f64), float(ctx.f6.f64)));
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmadds f11,f13,f1,f5
	ctx.f11.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f5.f64)));
	// fmadds f10,f13,f7,f4
	ctx.f10.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f4.f64)));
	// stfs f2,16(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// stfs f12,24(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// stfs f11,28(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// stfs f10,20(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// bdnz 0x822db708
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DB708;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB764"))) PPC_WEAK_FUNC(sub_822DB764);
PPC_FUNC_IMPL(__imp__sub_822DB764) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DB768"))) PPC_WEAK_FUNC(sub_822DB768);
PPC_FUNC_IMPL(__imp__sub_822DB768) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r12,r1,-16
	ctx.r12.s64 = ctx.r1.s64 + -16;
	// bl 0x8233fa2c
	ctx.lr = 0x822DB77C;
	sub_8233FA2C(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lfs f0,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// lfs f13,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// fadds f31,f13,f0
	ctx.f31.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f25,-28948(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f25.f64 = double(temp.f32);
	// lfs f28,5256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f28.f64 = double(temp.f32);
	// fcmpu cr6,f31,f25
	ctx.cr6.compare(ctx.f31.f64, ctx.f25.f64);
	// bge cr6,0x822db7b0
	if (!ctx.cr6.lt) goto loc_822DB7B0;
	// fdivs f27,f0,f31
	ctx.f27.f64 = double(float(ctx.f0.f64 / ctx.f31.f64));
	// b 0x822db7b4
	goto loc_822DB7B4;
loc_822DB7B0:
	// fmr f27,f28
	ctx.fpscr.disableFlushMode();
	ctx.f27.f64 = ctx.f28.f64;
loc_822DB7B4:
	// fsubs f12,f28,f27
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = static_cast<float>(ctx.f28.f64 - ctx.f27.f64);
	// lfs f11,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// fdivs f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 / ctx.f10.f64));
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfd f30,-17064(r11)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r11.u32 + -17064);
	// lfs f0,5284(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5284);
	ctx.f0.f64 = double(temp.f32);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// lfs f13,-1644(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1644);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f12,f31
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f26,f9,f0
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// bl 0x8233c318
	ctx.lr = 0x822DB7F0;
	sub_8233C318(ctx, base);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// lfd f29,18456(r8)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r8.u32 + 18456);
	// fmr f2,f29
	ctx.f2.f64 = ctx.f29.f64;
	// bl 0x8233c318
	ctx.lr = 0x822DB804;
	sub_8233C318(ctx, base);
	// fmuls f7,f27,f31
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// frsp f31,f1
	ctx.f31.f64 = double(float(ctx.f1.f64));
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// lfs f0,-1640(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -1640);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f2,f7,f0
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// bl 0x8233c318
	ctx.lr = 0x822DB820;
	sub_8233C318(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// stfs f1,32(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r31.u32 + 32, temp.u32);
	// fmr f2,f29
	ctx.f2.f64 = ctx.f29.f64;
	// bl 0x8233c318
	ctx.lr = 0x822DB830;
	sub_8233C318(ctx, base);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// frsp f6,f1
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = double(float(ctx.f1.f64));
	// stfs f6,32(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + 32, temp.u32);
	// lfs f0,-1596(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -1596);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f31,f0
	ctx.cr6.compare(ctx.f31.f64, ctx.f0.f64);
	// bge cr6,0x822db890
	if (!ctx.cr6.lt) goto loc_822DB890;
	// fmr f1,f26
	ctx.f1.f64 = ctx.f26.f64;
	// bl 0x8233c950
	ctx.lr = 0x822DB850;
	sub_8233C950(ctx, base);
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// fsubs f12,f28,f31
	ctx.f12.f64 = static_cast<float>(ctx.f28.f64 - ctx.f31.f64);
	// lfs f0,5260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5260);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f11,f13,f13,f28
	ctx.f11.f64 = -double(std::fma(float(ctx.f13.f64), float(ctx.f13.f64), -float(ctx.f28.f64)));
	// fsubs f10,f28,f13
	ctx.f10.f64 = static_cast<float>(ctx.f28.f64 - ctx.f13.f64);
	// fnmsubs f9,f13,f31,f28
	ctx.f9.f64 = -double(std::fma(float(ctx.f13.f64), float(ctx.f31.f64), -float(ctx.f28.f64)));
	// fmuls f8,f11,f31
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fmuls f7,f10,f31
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// fmuls f6,f8,f31
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fmsubs f5,f7,f0,f6
	ctx.f5.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f0.f64), -float(ctx.f6.f64)));
	// fsqrts f4,f5
	ctx.f4.f64 = double(simd::sqrt_f32(float(ctx.f5.f64)));
	// fsubs f3,f9,f4
	ctx.f3.f64 = static_cast<float>(ctx.f9.f64 - ctx.f4.f64);
	// fdivs f2,f3,f12
	ctx.f2.f64 = double(float(ctx.f3.f64 / ctx.f12.f64));
	// stfs f2,36(r31)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r31.u32 + 36, temp.u32);
	// b 0x822db894
	goto loc_822DB894;
loc_822DB890:
	// stfs f25,36(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r31.u32 + 36, temp.u32);
loc_822DB894:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// addi r12,r1,-16
	ctx.r12.s64 = ctx.r1.s64 + -16;
	// bl 0x8233fa78
	ctx.lr = 0x822DB8A0;
	__savefpr_25(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB8B0"))) PPC_WEAK_FUNC(sub_822DB8B0);
PPC_FUNC_IMPL(__imp__sub_822DB8B0) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r4,1
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 1, ctx.xer);
	// bne cr6,0x822db8c4
	if (!ctx.cr6.eq) goto loc_822DB8C4;
	// lis r3,3
	ctx.r3.s64 = 196608;
	// ori r3,r3,23248
	ctx.r3.u64 = ctx.r3.u64 | 23248;
	// blr 
	return;
loc_822DB8C4:
	// cmplwi cr6,r4,2
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 2, ctx.xer);
	// bne cr6,0x822db8d8
	if (!ctx.cr6.eq) goto loc_822DB8D8;
	// lis r3,5
	ctx.r3.s64 = 327680;
	// ori r3,r3,45680
	ctx.r3.u64 = ctx.r3.u64 | 45680;
	// blr 
	return;
loc_822DB8D8:
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// bne cr6,0x822db8ec
	if (!ctx.cr6.eq) goto loc_822DB8EC;
	// lis r3,3
	ctx.r3.s64 = 196608;
	// ori r3,r3,24320
	ctx.r3.u64 = ctx.r3.u64 | 24320;
	// blr 
	return;
loc_822DB8EC:
	// lis r3,5
	ctx.r3.s64 = 327680;
	// ori r3,r3,51936
	ctx.r3.u64 = ctx.r3.u64 | 51936;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DB8F8"))) PPC_WEAK_FUNC(sub_822DB8F8);
PPC_FUNC_IMPL(__imp__sub_822DB8F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e440
	ctx.lr = 0x822DB900;
	__restfpr_18(ctx, base);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addis r27,r3,1
	ctx.r27.s64 = ctx.r3.s64 + 65536;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// addi r27,r27,36
	ctx.r27.s64 = ctx.r27.s64 + 36;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,1800(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1800);
	ctx.f11.f64 = double(temp.f32);
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lis r7,1
	ctx.r7.s64 = 65536;
	// lis r31,1
	ctx.r31.s64 = 65536;
	// lfs f12,-28948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lfs f0,0(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,5256(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5256);
	ctx.f13.f64 = double(temp.f32);
	// ori r23,r8,32
	ctx.r23.u64 = ctx.r8.u64 | 32;
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// ori r24,r7,20
	ctx.r24.u64 = ctx.r7.u64 | 20;
	// ori r25,r31,16
	ctx.r25.u64 = ctx.r31.u64 | 16;
	// lis r19,1
	ctx.r19.s64 = 65536;
	// ori r20,r11,24
	ctx.r20.u64 = ctx.r11.u64 | 24;
	// li r26,1
	ctx.r26.s64 = 1;
	// cmpwi cr6,r6,4
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 4, ctx.xer);
	// blt cr6,0x822dbbb0
	if (ctx.cr6.lt) goto loc_822DBBB0;
	// add r10,r3,r23
	ctx.r10.u64 = ctx.r3.u64 + ctx.r23.u64;
	// add r8,r3,r24
	ctx.r8.u64 = ctx.r3.u64 + ctx.r24.u64;
	// add r7,r3,r25
	ctx.r7.u64 = ctx.r3.u64 + ctx.r25.u64;
	// add r11,r3,r19
	ctx.r11.u64 = ctx.r3.u64 + ctx.r19.u64;
	// addi r22,r6,-3
	ctx.r22.s64 = ctx.r6.s64 + -3;
	// addi r29,r4,8
	ctx.r29.s64 = ctx.r4.s64 + 8;
	// addi r31,r5,12
	ctx.r31.s64 = ctx.r5.s64 + 12;
	// subf r21,r5,r4
	ctx.r21.s64 = ctx.r4.s64 - ctx.r5.s64;
loc_822DB978:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r28,0(r8)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// fsubs f10,f13,f0
	ctx.f10.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// lwz r30,0(r7)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// subf r28,r28,r9
	ctx.r28.s64 = ctx.r9.s64 - ctx.r28.s64;
	// subf r18,r30,r9
	ctx.r18.s64 = ctx.r9.s64 - ctx.r30.s64;
	// rlwinm r28,r28,2,16,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFC;
	// rlwinm r18,r18,2,16,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFC;
	// lfsx f9,r28,r3
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r3.u32);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfsx f7,r18,r3
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r3.u32);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f7,f10,f8
	ctx.f6.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f10.f64), float(ctx.f8.f64)));
	// stfs f6,-8(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + -8, temp.u32);
	// stfs f6,0(r27)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// ble cr6,0x822db9c4
	if (!ctx.cr6.gt) goto loc_822DB9C4;
	// fsubs f0,f0,f11
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f11.f64);
	// b 0x822db9e4
	goto loc_822DB9E4;
loc_822DB9C4:
	// lwzx r28,r3,r20
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r20.u32);
	// cmplw cr6,r28,r30
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x822db9d8
	if (ctx.cr6.eq) goto loc_822DB9D8;
	// fmr f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822db9dc
	goto loc_822DB9DC;
loc_822DB9D8:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f12.f64;
loc_822DB9DC:
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r30.u32);
	// stw r28,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r28.u32);
loc_822DB9E4:
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfs f0,-8(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r9,r3
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r3.u32, temp.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r30,0(r7)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r28,0(r8)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// fsubs f10,f13,f0
	ctx.f10.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// clrlwi r9,r9,18
	ctx.r9.u64 = ctx.r9.u32 & 0x3FFF;
	// subf r18,r30,r9
	ctx.r18.s64 = ctx.r9.s64 - ctx.r30.s64;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// subf r28,r28,r9
	ctx.r28.s64 = ctx.r9.s64 - ctx.r28.s64;
	// rlwinm r18,r18,2,16,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFC;
	// rlwinm r28,r28,2,16,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFC;
	// lfsx f9,r18,r3
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r3.u32);
	ctx.f9.f64 = double(temp.f32);
	// lfsx f8,r28,r3
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r3.u32);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmadds f6,f9,f10,f7
	ctx.f6.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f10.f64), float(ctx.f7.f64)));
	// stfs f6,-4(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + -4, temp.u32);
	// stfs f6,0(r27)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// ble cr6,0x822dba4c
	if (!ctx.cr6.gt) goto loc_822DBA4C;
	// fsubs f0,f0,f11
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f11.f64);
	// b 0x822dba6c
	goto loc_822DBA6C;
loc_822DBA4C:
	// lwzx r28,r3,r20
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r20.u32);
	// cmplw cr6,r28,r30
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x822dba60
	if (ctx.cr6.eq) goto loc_822DBA60;
	// fmr f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dba64
	goto loc_822DBA64;
loc_822DBA60:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f12.f64;
loc_822DBA64:
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r30.u32);
	// stw r28,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r28.u32);
loc_822DBA6C:
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfs f0,-4(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r9,r3
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r3.u32, temp.u32);
	// lwz r28,0(r8)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f10,f13,f0
	ctx.f10.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// lwz r30,0(r7)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// clrlwi r9,r9,18
	ctx.r9.u64 = ctx.r9.u32 & 0x3FFF;
	// subf r18,r30,r9
	ctx.r18.s64 = ctx.r9.s64 - ctx.r30.s64;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// subf r28,r28,r9
	ctx.r28.s64 = ctx.r9.s64 - ctx.r28.s64;
	// rlwinm r18,r18,2,16,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFC;
	// rlwinm r28,r28,2,16,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFC;
	// lfsx f9,r18,r3
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r3.u32);
	ctx.f9.f64 = double(temp.f32);
	// lfsx f8,r28,r3
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r3.u32);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmadds f6,f9,f10,f7
	ctx.f6.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f10.f64), float(ctx.f7.f64)));
	// stfs f6,0(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
	// stfs f6,0(r27)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// ble cr6,0x822dbad4
	if (!ctx.cr6.gt) goto loc_822DBAD4;
	// fsubs f0,f0,f11
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f11.f64);
	// b 0x822dbaf4
	goto loc_822DBAF4;
loc_822DBAD4:
	// lwzx r28,r3,r20
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r20.u32);
	// cmplw cr6,r28,r30
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x822dbae8
	if (ctx.cr6.eq) goto loc_822DBAE8;
	// fmr f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dbaec
	goto loc_822DBAEC;
loc_822DBAE8:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f12.f64;
loc_822DBAEC:
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r30.u32);
	// stw r28,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r28.u32);
loc_822DBAF4:
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfs f0,0(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r9,r3
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r3.u32, temp.u32);
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f10,f13,f0
	ctx.f10.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// lwz r28,0(r8)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r30,0(r7)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// clrlwi r9,r9,18
	ctx.r9.u64 = ctx.r9.u32 & 0x3FFF;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// subf r28,r28,r9
	ctx.r28.s64 = ctx.r9.s64 - ctx.r28.s64;
	// subf r18,r30,r9
	ctx.r18.s64 = ctx.r9.s64 - ctx.r30.s64;
	// rlwinm r28,r28,2,16,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFC;
	// rlwinm r18,r18,2,16,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFC;
	// lfsx f9,r28,r3
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r3.u32);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfsx f7,r18,r3
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r3.u32);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f7,f10,f8
	ctx.f6.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f10.f64), float(ctx.f8.f64)));
	// stfs f6,4(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + 4, temp.u32);
	// stfs f6,0(r27)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// ble cr6,0x822dbb5c
	if (!ctx.cr6.gt) goto loc_822DBB5C;
	// fsubs f0,f0,f11
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f11.f64);
	// b 0x822dbb7c
	goto loc_822DBB7C;
loc_822DBB5C:
	// lwzx r28,r3,r20
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r20.u32);
	// cmplw cr6,r28,r30
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x822dbb70
	if (ctx.cr6.eq) goto loc_822DBB70;
	// fmr f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dbb74
	goto loc_822DBB74;
loc_822DBB70:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f12.f64;
loc_822DBB74:
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r30.u32);
	// stw r28,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r28.u32);
loc_822DBB7C:
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfsx f0,r21,r31
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r31.u32);
	ctx.f0.f64 = double(temp.f32);
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// addi r29,r29,16
	ctx.r29.s64 = ctx.r29.s64 + 16;
	// addi r31,r31,16
	ctx.r31.s64 = ctx.r31.s64 + 16;
	// cmplw cr6,r26,r22
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r22.u32, ctx.xer);
	// stfsx f0,r9,r3
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r3.u32, temp.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// clrlwi r9,r9,18
	ctx.r9.u64 = ctx.r9.u32 & 0x3FFF;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// ble cr6,0x822db978
	if (!ctx.cr6.gt) goto loc_822DB978;
loc_822DBBB0:
	// cmplw cr6,r26,r6
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r6.u32, ctx.xer);
	// bgt cr6,0x822dbc78
	if (ctx.cr6.gt) goto loc_822DBC78;
	// subf r10,r26,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r26.s64;
	// rlwinm r11,r26,2,0,29
	ctx.r11.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r7,r11,r4
	ctx.r7.u64 = ctx.r11.u64 + ctx.r4.u64;
	// add r9,r11,r5
	ctx.r9.u64 = ctx.r11.u64 + ctx.r5.u64;
	// add r6,r3,r23
	ctx.r6.u64 = ctx.r3.u64 + ctx.r23.u64;
	// add r4,r3,r24
	ctx.r4.u64 = ctx.r3.u64 + ctx.r24.u64;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// add r31,r3,r25
	ctx.r31.u64 = ctx.r3.u64 + ctx.r25.u64;
	// add r8,r3,r19
	ctx.r8.u64 = ctx.r3.u64 + ctx.r19.u64;
	// addi r5,r7,-8
	ctx.r5.s64 = ctx.r7.s64 + -8;
loc_822DBBE4:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lfs f0,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// fsubs f10,f13,f0
	ctx.f10.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// subf r7,r7,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r7.s64;
	// subf r30,r11,r10
	ctx.r30.s64 = ctx.r10.s64 - ctx.r11.s64;
	// rlwinm r7,r7,2,16,29
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFC;
	// rlwinm r30,r30,2,16,29
	ctx.r30.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFC;
	// lfsx f9,r7,r3
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r3.u32);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfsx f7,r30,r3
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r3.u32);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f7,f10,f8
	ctx.f6.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f10.f64), float(ctx.f8.f64)));
	// stfs f6,0(r9)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f6,0(r27)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// ble cr6,0x822dbc30
	if (!ctx.cr6.gt) goto loc_822DBC30;
	// fsubs f0,f0,f11
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f11.f64);
	// b 0x822dbc50
	goto loc_822DBC50;
loc_822DBC30:
	// lwzx r7,r3,r20
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r20.u32);
	// cmplw cr6,r7,r11
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x822dbc44
	if (ctx.cr6.eq) goto loc_822DBC44;
	// fmr f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dbc48
	goto loc_822DBC48;
loc_822DBC44:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f12.f64;
loc_822DBC48:
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// stw r7,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r7.u32);
loc_822DBC50:
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// lfsu f0,4(r5)
	ea = 4 + ctx.r5.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r5.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stfsx f0,r11,r3
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r3.u32, temp.u32);
	// lwz r11,0(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// clrlwi r7,r10,18
	ctx.r7.u64 = ctx.r10.u32 & 0x3FFF;
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// bdnz 0x822dbbe4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DBBE4;
loc_822DBC78:
	// b 0x8233e490
	__restgprlr_18(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DBC7C"))) PPC_WEAK_FUNC(sub_822DBC7C);
PPC_FUNC_IMPL(__imp__sub_822DBC7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DBC80"))) PPC_WEAK_FUNC(sub_822DBC80);
PPC_FUNC_IMPL(__imp__sub_822DBC80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e45c
	ctx.lr = 0x822DBC88;
	__restfpr_25(ctx, base);
	// addis r28,r3,2
	ctx.r28.s64 = ctx.r3.s64 + 131072;
	// addis r27,r3,2
	ctx.r27.s64 = ctx.r3.s64 + 131072;
	// addi r28,r28,36
	ctx.r28.s64 = ctx.r28.s64 + 36;
	// addi r27,r27,40
	ctx.r27.s64 = ctx.r27.s64 + 40;
	// cmplwi cr6,r6,1
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 1, ctx.xer);
	// lfs f0,0(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// stfs f13,4(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// blt cr6,0x822dbdd0
	if (ctx.cr6.lt) goto loc_822DBDD0;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// addis r31,r3,2
	ctx.r31.s64 = ctx.r3.s64 + 131072;
	// addis r30,r3,2
	ctx.r30.s64 = ctx.r3.s64 + 131072;
	// addis r29,r3,2
	ctx.r29.s64 = ctx.r3.s64 + 131072;
	// lfs f11,1800(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1800);
	ctx.f11.f64 = double(temp.f32);
	// lis r7,2
	ctx.r7.s64 = 131072;
	// lfs f12,-28948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f12.f64 = double(temp.f32);
	// addi r6,r5,8
	ctx.r6.s64 = ctx.r5.s64 + 8;
	// lfs f13,5256(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5256);
	ctx.f13.f64 = double(temp.f32);
	// addi r5,r4,-4
	ctx.r5.s64 = ctx.r4.s64 + -4;
	// addi r31,r31,32
	ctx.r31.s64 = ctx.r31.s64 + 32;
	// addi r30,r30,20
	ctx.r30.s64 = ctx.r30.s64 + 20;
	// addi r29,r29,16
	ctx.r29.s64 = ctx.r29.s64 + 16;
	// addis r9,r3,2
	ctx.r9.s64 = ctx.r3.s64 + 131072;
	// ori r4,r7,24
	ctx.r4.u64 = ctx.r7.u64 | 24;
loc_822DBCF8:
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lfs f0,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// fsubs f10,f13,f0
	ctx.f10.f64 = static_cast<float>(ctx.f13.f64 - ctx.f0.f64);
	// lwz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// subf r11,r11,r8
	ctx.r11.s64 = ctx.r8.s64 - ctx.r11.s64;
	// subf r10,r7,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r7.s64;
	// addi r26,r11,-1
	ctx.r26.s64 = ctx.r11.s64 + -1;
	// rlwinm r11,r11,2,15,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0x1FFFC;
	// rlwinm r26,r26,2,15,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0x1FFFC;
	// addi r25,r10,-1
	ctx.r25.s64 = ctx.r10.s64 + -1;
	// rlwinm r10,r10,2,15,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x1FFFC;
	// rlwinm r25,r25,2,15,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0x1FFFC;
	// lfsx f9,r11,r3
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
	ctx.f9.f64 = double(temp.f32);
	// lfsx f8,r26,r3
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r3.u32);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f9,f0
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f6,f0,f8
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// lfsx f5,r10,r3
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	ctx.f5.f64 = double(temp.f32);
	// lfsx f4,r25,r3
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r3.u32);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f3,f5,f10,f7
	ctx.f3.f64 = double(std::fma(float(ctx.f5.f64), float(ctx.f10.f64), float(ctx.f7.f64)));
	// stfs f3,0(r6)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// fmadds f2,f10,f4,f6
	ctx.f2.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f4.f64), float(ctx.f6.f64)));
	// stfs f3,0(r28)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r28.u32 + 0, temp.u32);
	// stfs f2,4(r6)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// stfs f2,0(r27)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// ble cr6,0x822dbd6c
	if (!ctx.cr6.gt) goto loc_822DBD6C;
	// fsubs f0,f0,f11
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f11.f64);
	// b 0x822dbd8c
	goto loc_822DBD8C;
loc_822DBD6C:
	// lwzx r11,r3,r4
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r4.u32);
	// cmplw cr6,r11,r7
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r7.u32, ctx.xer);
	// beq cr6,0x822dbd80
	if (ctx.cr6.eq) goto loc_822DBD80;
	// fmr f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f13.f64;
	// b 0x822dbd84
	goto loc_822DBD84;
loc_822DBD80:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f12.f64;
loc_822DBD84:
	// stw r7,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r7.u32);
	// stw r11,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r11.u32);
loc_822DBD8C:
	// rlwinm r11,r8,2,0,29
	ctx.r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f10,4(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// stfs f0,0(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// lfsu f0,8(r5)
	ea = 8 + ctx.r5.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r5.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// stfsx f10,r11,r3
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r3.u32, temp.u32);
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// clrlwi r8,r10,17
	ctx.r8.u64 = ctx.r10.u32 & 0x7FFF;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// stfsx f0,r7,r3
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + ctx.r3.u32, temp.u32);
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// clrlwi r10,r11,17
	ctx.r10.u64 = ctx.r11.u32 & 0x7FFF;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// bdnz 0x822dbcf8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DBCF8;
loc_822DBDD0:
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DBDD4"))) PPC_WEAK_FUNC(sub_822DBDD4);
PPC_FUNC_IMPL(__imp__sub_822DBDD4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DBDD8"))) PPC_WEAK_FUNC(sub_822DBDD8);
PPC_FUNC_IMPL(__imp__sub_822DBDD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e438
	ctx.lr = 0x822DBDE0;
	__restfpr_16(ctx, base);
	// addi r12,r1,-136
	ctx.r12.s64 = ctx.r1.s64 + -136;
	// bl 0x8233fa1c
	ctx.lr = 0x822DBDE8;
	sub_8233FA1C(ctx, base);
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lwz r11,4(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f21,1904(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1904);
	ctx.f21.f64 = double(temp.f32);
	// bne cr6,0x822dbe10
	if (!ctx.cr6.eq) goto loc_822DBE10;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x822dbe34
	goto loc_822DBE34;
loc_822DBE10:
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f21
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f21.f64));
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f10.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_822DBE34:
	// bl 0x822fcaf8
	ctx.lr = 0x822DBE38;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,4095
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 4095, ctx.xer);
	// ble cr6,0x822dbe44
	if (!ctx.cr6.gt) goto loc_822DBE44;
	// li r3,4095
	ctx.r3.s64 = 4095;
loc_822DBE44:
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lwz r10,80(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 80);
	// lis r9,1
	ctx.r9.s64 = 65536;
	// ori r5,r11,38904
	ctx.r5.u64 = ctx.r11.u64 | 38904;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r8,2
	ctx.r8.s64 = 131072;
	// addi r30,r11,-1408
	ctx.r30.s64 = ctx.r11.s64 + -1408;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lis r7,3
	ctx.r7.s64 = 196608;
	// lfs f13,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// stwx r3,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r3.u32);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// ori r4,r9,59592
	ctx.r4.u64 = ctx.r9.u64 | 59592;
	// ori r10,r8,39304
	ctx.r10.u64 = ctx.r8.u64 | 39304;
	// ori r9,r7,19016
	ctx.r9.u64 = ctx.r7.u64 | 19016;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// addi r11,r11,-17732
	ctx.r11.s64 = ctx.r11.s64 + -17732;
	// lfs f11,-1556(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -1556);
	ctx.f11.f64 = double(temp.f32);
	// fcmpu cr6,f13,f11
	ctx.cr6.compare(ctx.f13.f64, ctx.f11.f64);
	// stwx r3,r31,r4
	PPC_STORE_U32(ctx.r31.u32 + ctx.r4.u32, ctx.r3.u32);
	// lfs f23,5256(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5256);
	ctx.f23.f64 = double(temp.f32);
	// stwx r3,r31,r10
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, ctx.r3.u32);
	// lfs f22,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f22.f64 = double(temp.f32);
	// stwx r3,r31,r9
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, ctx.r3.u32);
	// blt cr6,0x822dbeb8
	if (ctx.cr6.lt) goto loc_822DBEB8;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f0,-1580(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1580);
	ctx.f0.f64 = double(temp.f32);
	// b 0x822dbed0
	goto loc_822DBED0;
loc_822DBEB8:
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f0,4(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfs f12,-17908(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -17908);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f13,f13,f0,f12
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// lfs f0,-1580(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1580);
	ctx.f0.f64 = double(temp.f32);
loc_822DBED0:
	// fmuls f25,f13,f22
	ctx.fpscr.disableFlushMode();
	ctx.f25.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// fcmpu cr6,f25,f0
	ctx.cr6.compare(ctx.f25.f64, ctx.f0.f64);
	// bge cr6,0x822dbee0
	if (!ctx.cr6.lt) goto loc_822DBEE0;
	// fmr f25,f0
	ctx.f25.f64 = ctx.f0.f64;
loc_822DBEE0:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,5260(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5260);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f30,f25,f13
	ctx.f30.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// fcmpu cr6,f30,f0
	ctx.cr6.compare(ctx.f30.f64, ctx.f0.f64);
	// bge cr6,0x822dbefc
	if (!ctx.cr6.lt) goto loc_822DBEFC;
	// fmr f30,f0
	ctx.f30.f64 = ctx.f0.f64;
	// b 0x822dbf08
	goto loc_822DBF08;
loc_822DBEFC:
	// fcmpu cr6,f30,f23
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f30.f64, ctx.f23.f64);
	// ble cr6,0x822dbf08
	if (!ctx.cr6.gt) goto loc_822DBF08;
	// fmr f30,f23
	ctx.f30.f64 = ctx.f23.f64;
loc_822DBF08:
	// lwz r11,76(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 76);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f13,1900(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1900);
	ctx.f13.f64 = double(temp.f32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// blt cr6,0x822dbf44
	if (ctx.cr6.lt) goto loc_822DBF44;
	// fsubs f13,f0,f13
	ctx.f13.f64 = static_cast<float>(ctx.f0.f64 - ctx.f13.f64);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// fmr f29,f23
	ctx.f29.f64 = ctx.f23.f64;
	// fmr f28,f23
	ctx.f28.f64 = ctx.f23.f64;
	// fmr f27,f23
	ctx.f27.f64 = ctx.f23.f64;
	// lfs f0,1896(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1896);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f13,f0
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// b 0x822dbfac
	goto loc_822DBFAC;
loc_822DBF44:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f11.f64);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// blt cr6,0x822dbf70
	if (ctx.cr6.lt) goto loc_822DBF70;
	// fsubs f13,f0,f11
	ctx.f13.f64 = static_cast<float>(ctx.f0.f64 - ctx.f11.f64);
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// lfs f0,1896(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1896);
	ctx.f0.f64 = double(temp.f32);
	// fmr f28,f23
	ctx.f28.f64 = ctx.f23.f64;
	// fmr f27,f23
	ctx.f27.f64 = ctx.f23.f64;
	// lfs f31,-28948(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f29,f13,f0
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// b 0x822dbfac
	goto loc_822DBFAC;
loc_822DBF70:
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lfs f13,1892(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1892);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// lfs f31,-28948(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// fmr f29,f31
	ctx.f29.f64 = ctx.f31.f64;
	// blt cr6,0x822dbfa0
	if (ctx.cr6.lt) goto loc_822DBFA0;
	// fsubs f13,f0,f13
	ctx.f13.f64 = static_cast<float>(ctx.f0.f64 - ctx.f13.f64);
	// lfs f0,1896(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1896);
	ctx.f0.f64 = double(temp.f32);
	// fmr f27,f23
	ctx.f27.f64 = ctx.f23.f64;
	// fmuls f28,f13,f0
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// b 0x822dbfac
	goto loc_822DBFAC;
loc_822DBFA0:
	// lfs f13,1896(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1896);
	ctx.f13.f64 = double(temp.f32);
	// fmr f28,f31
	ctx.f28.f64 = ctx.f31.f64;
	// fmuls f27,f0,f13
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
loc_822DBFAC:
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lfs f0,1888(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1888);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f30,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DBFC8;
	sub_822FCAF8(ctx, base);
	// lis r10,1
	ctx.r10.s64 = 65536;
	// lis r9,1
	ctx.r9.s64 = 65536;
	// ori r8,r10,2160
	ctx.r8.u64 = ctx.r10.u64 | 2160;
	// ori r7,r9,2164
	ctx.r7.u64 = ctx.r9.u64 | 2164;
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// stwx r3,r31,r8
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, ctx.r3.u32);
	// stwx r11,r31,r7
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, ctx.r11.u32);
	// lfs f0,1884(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 1884);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f30,f0
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f11.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC000;
	sub_822FCAF8(ctx, base);
	// lwz r5,4(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// subfic r4,r5,0
	ctx.xer.ca = ctx.r5.u32 <= 0;
	ctx.r4.s64 = 0 - ctx.r5.s64;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// subfe r9,r4,r4
	temp.u8 = (~ctx.r4.u32 + ctx.r4.u32 < ~ctx.r4.u32) | (~ctx.r4.u32 + ctx.r4.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r4.u64 + ctx.r4.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// lis r8,1
	ctx.r8.s64 = 65536;
	// rlwinm r11,r9,0,31,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// ori r7,r10,10960
	ctx.r7.u64 = ctx.r10.u64 | 10960;
	// rlwinm r11,r11,0,28,21
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFC0F;
	// ori r5,r8,10964
	ctx.r5.u64 = ctx.r8.u64 | 10964;
	// addi r11,r11,1020
	ctx.r11.s64 = ctx.r11.s64 + 1020;
	// addi r10,r3,-1
	ctx.r10.s64 = ctx.r3.s64 + -1;
	// clrldi r6,r11,32
	ctx.r6.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stwx r3,r31,r7
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, ctx.r3.u32);
	// std r6,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r6.u64);
	// lfd f10,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// stwx r10,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r10.u32);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fmuls f7,f8,f30
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f30.f64));
	// fctidz f6,f7
	ctx.f6.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// stfd f6,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f6.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC05C;
	sub_822FCAF8(ctx, base);
	// lis r4,1
	ctx.r4.s64 = 65536;
	// fmuls f26,f31,f30
	ctx.fpscr.disableFlushMode();
	ctx.f26.f64 = double(float(ctx.f31.f64 * ctx.f30.f64));
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// ori r10,r4,18244
	ctx.r10.u64 = ctx.r4.u64 | 18244;
	// lfs f0,1880(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1880);
	ctx.f0.f64 = double(temp.f32);
	// stwx r3,r31,r10
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, ctx.r3.u32);
	// fmuls f5,f26,f0
	ctx.f5.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// fctidz f4,f5
	ctx.f4.s64 = (ctx.f5.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f5.f64);
	// stfd f4,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f4.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC088;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc094
	if (!ctx.cr6.eq) goto loc_822DC094;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC094:
	// addis r21,r31,1
	ctx.r21.s64 = ctx.r31.s64 + 65536;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addi r21,r21,18272
	ctx.r21.s64 = ctx.r21.s64 + 18272;
	// lfs f0,1876(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1876);
	ctx.f0.f64 = double(temp.f32);
	// lwz r10,2048(r21)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r21.u32 + 2048);
	// fmuls f0,f30,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stw r3,2056(r21)
	PPC_STORE_U32(ctx.r21.u32 + 2056, ctx.r3.u32);
	// subf r9,r3,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r3.s64;
	// clrlwi r8,r9,23
	ctx.r8.u64 = ctx.r9.u32 & 0x1FF;
	// stw r8,2052(r21)
	PPC_STORE_U32(ctx.r21.u32 + 2052, ctx.r8.u32);
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC0CC;
	sub_822FCAF8(ctx, base);
	// addis r20,r31,1
	ctx.r20.s64 = ctx.r31.s64 + 65536;
	// addi r20,r20,20352
	ctx.r20.s64 = ctx.r20.s64 + 20352;
	// fmuls f12,f31,f25
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f25.f64));
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// lwz r6,2048(r20)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2048);
	// stw r3,2056(r20)
	PPC_STORE_U32(ctx.r20.u32 + 2056, ctx.r3.u32);
	// lfs f0,1872(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1872);
	ctx.f0.f64 = double(temp.f32);
	// subf r5,r3,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r3.s64;
	// clrlwi r4,r5,23
	ctx.r4.u64 = ctx.r5.u32 & 0x1FF;
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stw r4,2052(r20)
	PPC_STORE_U32(ctx.r20.u32 + 2052, ctx.r4.u32);
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f10.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC108;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc114
	if (!ctx.cr6.eq) goto loc_822DC114;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC114:
	// addis r26,r31,2
	ctx.r26.s64 = ctx.r31.s64 + 131072;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addi r26,r26,-26636
	ctx.r26.s64 = ctx.r26.s64 + -26636;
	// lfs f0,1868(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1868);
	ctx.f0.f64 = double(temp.f32);
	// stw r3,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r3.u32);
	// fmuls f0,f26,f0
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC13C;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc148
	if (!ctx.cr6.eq) goto loc_822DC148;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC148:
	// addis r23,r31,2
	ctx.r23.s64 = ctx.r31.s64 + 131072;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addi r23,r23,-26576
	ctx.r23.s64 = ctx.r23.s64 + -26576;
	// lfs f0,1864(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1864);
	ctx.f0.f64 = double(temp.f32);
	// lwz r10,2048(r23)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r23.u32 + 2048);
	// fmuls f0,f30,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stw r3,2056(r23)
	PPC_STORE_U32(ctx.r23.u32 + 2056, ctx.r3.u32);
	// subf r9,r3,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r3.s64;
	// clrlwi r8,r9,23
	ctx.r8.u64 = ctx.r9.u32 & 0x1FF;
	// stw r8,2052(r23)
	PPC_STORE_U32(ctx.r23.u32 + 2052, ctx.r8.u32);
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC180;
	sub_822FCAF8(ctx, base);
	// addis r22,r31,2
	ctx.r22.s64 = ctx.r31.s64 + 131072;
	// addi r22,r22,-24496
	ctx.r22.s64 = ctx.r22.s64 + -24496;
	// fmuls f12,f29,f25
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f25.f64));
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lwz r7,2048(r22)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2048);
	// stw r3,2056(r22)
	PPC_STORE_U32(ctx.r22.u32 + 2056, ctx.r3.u32);
	// lfs f0,1860(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 1860);
	ctx.f0.f64 = double(temp.f32);
	// subf r5,r3,r7
	ctx.r5.s64 = ctx.r7.s64 - ctx.r3.s64;
	// clrlwi r4,r5,23
	ctx.r4.u64 = ctx.r5.u32 & 0x1FF;
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stw r4,2052(r22)
	PPC_STORE_U32(ctx.r22.u32 + 2052, ctx.r4.u32);
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f10.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC1BC;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc1c8
	if (!ctx.cr6.eq) goto loc_822DC1C8;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC1C8:
	// fmuls f28,f28,f30
	ctx.fpscr.disableFlushMode();
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addis r27,r31,2
	ctx.r27.s64 = ctx.r31.s64 + 131072;
	// addi r27,r27,-5948
	ctx.r27.s64 = ctx.r27.s64 + -5948;
	// lfs f0,1856(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1856);
	ctx.f0.f64 = double(temp.f32);
	// stw r3,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r3.u32);
	// fmuls f0,f28,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC1F4;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc200
	if (!ctx.cr6.eq) goto loc_822DC200;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC200:
	// fmuls f31,f29,f30
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f30.f64));
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// addis r11,r31,2
	ctx.r11.s64 = ctx.r31.s64 + 131072;
	// addi r11,r11,-5888
	ctx.r11.s64 = ctx.r11.s64 + -5888;
	// lfs f0,1852(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1852);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,8192(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// stw r3,8200(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8200, ctx.r3.u32);
	// fmuls f0,f31,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// subf r8,r3,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r3.s64;
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// stw r7,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r7.u32);
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC23C;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc248
	if (!ctx.cr6.eq) goto loc_822DC248;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC248:
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// addis r10,r31,2
	ctx.r10.s64 = ctx.r31.s64 + 131072;
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
	// addi r10,r10,2336
	ctx.r10.s64 = ctx.r10.s64 + 2336;
	// lfs f0,1848(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 1848);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f30,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lwz r8,8192(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// stw r11,8200(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8200, ctx.r11.u32);
	// subf r7,r11,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r11.s64;
	// clrlwi r6,r7,21
	ctx.r6.u64 = ctx.r7.u32 & 0x7FF;
	// stw r6,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r6.u32);
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC284;
	sub_822FCAF8(ctx, base);
	// addis r5,r31,2
	ctx.r5.s64 = ctx.r31.s64 + 131072;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addi r5,r5,10560
	ctx.r5.s64 = ctx.r5.s64 + 10560;
	// lfs f0,1844(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1844);
	ctx.f0.f64 = double(temp.f32);
	// lwz r4,8192(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8192);
	// fmuls f12,f26,f0
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// stw r3,8200(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8200, ctx.r3.u32);
	// subf r10,r3,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r3.s64;
	// clrlwi r9,r10,21
	ctx.r9.u64 = ctx.r10.u32 & 0x7FF;
	// stw r9,8196(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8196, ctx.r9.u32);
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f11.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC2BC;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc2c8
	if (!ctx.cr6.eq) goto loc_822DC2C8;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC2C8:
	// addis r24,r31,2
	ctx.r24.s64 = ctx.r31.s64 + 131072;
	// fmuls f13,f27,f25
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f25.f64));
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addi r24,r24,18784
	ctx.r24.s64 = ctx.r24.s64 + 18784;
	// lfs f0,1840(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1840);
	ctx.f0.f64 = double(temp.f32);
	// lwz r10,4096(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4096);
	// stw r3,4104(r24)
	PPC_STORE_U32(ctx.r24.u32 + 4104, ctx.r3.u32);
	// subf r9,r3,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r3.s64;
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// clrlwi r8,r9,22
	ctx.r8.u64 = ctx.r9.u32 & 0x3FF;
	// stw r8,4100(r24)
	PPC_STORE_U32(ctx.r24.u32 + 4100, ctx.r8.u32);
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f11.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC304;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc310
	if (!ctx.cr6.eq) goto loc_822DC310;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC310:
	// addis r28,r31,3
	ctx.r28.s64 = ctx.r31.s64 + 196608;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addi r28,r28,-26236
	ctx.r28.s64 = ctx.r28.s64 + -26236;
	// lfs f0,1836(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1836);
	ctx.f0.f64 = double(temp.f32);
	// stw r3,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r3.u32);
	// fmuls f0,f28,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC338;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc344
	if (!ctx.cr6.eq) goto loc_822DC344;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC344:
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// addis r11,r31,3
	ctx.r11.s64 = ctx.r31.s64 + 196608;
	// addi r11,r11,-26176
	ctx.r11.s64 = ctx.r11.s64 + -26176;
	// lfs f0,1832(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1832);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f31,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lwz r9,8192(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// stw r3,8200(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8200, ctx.r3.u32);
	// subf r8,r3,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r3.s64;
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// stw r7,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r7.u32);
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC37C;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc388
	if (!ctx.cr6.eq) goto loc_822DC388;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC388:
	// addis r10,r31,3
	ctx.r10.s64 = ctx.r31.s64 + 196608;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addi r10,r10,-17952
	ctx.r10.s64 = ctx.r10.s64 + -17952;
	// lfs f0,1828(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1828);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// fmuls f0,f26,f0
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// stw r3,8200(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8200, ctx.r3.u32);
	// subf r8,r3,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r3.s64;
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// stw r7,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r7.u32);
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC3C0;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc3cc
	if (!ctx.cr6.eq) goto loc_822DC3CC;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC3CC:
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addis r10,r31,3
	ctx.r10.s64 = ctx.r31.s64 + 196608;
	// addi r10,r10,-9728
	ctx.r10.s64 = ctx.r10.s64 + -9728;
	// lfs f0,1824(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1824);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f26,f0
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// stw r3,8200(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8200, ctx.r3.u32);
	// subf r8,r3,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r3.s64;
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// stw r7,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r7.u32);
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC404;
	sub_822FCAF8(ctx, base);
	// addis r25,r31,3
	ctx.r25.s64 = ctx.r31.s64 + 196608;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// addi r25,r25,-1504
	ctx.r25.s64 = ctx.r25.s64 + -1504;
	// lfs f0,1820(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 1820);
	ctx.f0.f64 = double(temp.f32);
	// lwz r5,4096(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 4096);
	// fmuls f12,f25,f0
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// stw r3,4104(r25)
	PPC_STORE_U32(ctx.r25.u32 + 4104, ctx.r3.u32);
	// subf r4,r3,r5
	ctx.r4.s64 = ctx.r5.s64 - ctx.r3.s64;
	// clrlwi r3,r4,22
	ctx.r3.u64 = ctx.r4.u32 & 0x3FF;
	// stw r3,4100(r25)
	PPC_STORE_U32(ctx.r25.u32 + 4100, ctx.r3.u32);
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f11.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// bl 0x822fcaf8
	ctx.lr = 0x822DC43C;
	sub_822FCAF8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x822dc448
	if (!ctx.cr6.eq) goto loc_822DC448;
	// li r3,1
	ctx.r3.s64 = 1;
loc_822DC448:
	// lis r11,2
	ctx.r11.s64 = 131072;
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// lwz r6,0(r27)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// ori r9,r11,64008
	ctx.r9.u64 = ctx.r11.u64 | 64008;
	// lwz r7,72(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 72);
	// ori r8,r10,55784
	ctx.r8.u64 = ctx.r10.u64 | 55784;
	// lwz r4,0(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// lis r11,2
	ctx.r11.s64 = 131072;
	// lis r28,1
	ctx.r28.s64 = 65536;
	// ori r27,r11,22888
	ctx.r27.u64 = ctx.r11.u64 | 22888;
	// lwzx r10,r31,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r9.u32);
	// ori r28,r28,22408
	ctx.r28.u64 = ctx.r28.u64 | 22408;
	// lwzx r11,r31,r8
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r8.u32);
	// lis r8,1
	ctx.r8.s64 = 65536;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// lis r26,2
	ctx.r26.s64 = 131072;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// ori r7,r8,20328
	ctx.r7.u64 = ctx.r8.u64 | 20328;
	// lwzx r9,r31,r27
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r27.u32);
	// lwzx r10,r31,r28
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r28.u32);
	// ori r28,r26,18760
	ctx.r28.u64 = ctx.r26.u64 | 18760;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwzx r8,r31,r7
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r7.u32);
	// ori r7,r9,47560
	ctx.r7.u64 = ctx.r9.u64 | 47560;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// lwzx r9,r31,r28
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r28.u32);
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// ori r8,r10,10536
	ctx.r8.u64 = ctx.r10.u64 | 10536;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwzx r7,r31,r7
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r7.u32);
	// lis r28,1
	ctx.r28.s64 = 65536;
	// ori r9,r10,43096
	ctx.r9.u64 = ctx.r10.u64 | 43096;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// lwzx r10,r31,r8
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r8.u32);
	// ori r7,r28,41016
	ctx.r7.u64 = ctx.r28.u64 | 41016;
	// lis r28,3
	ctx.r28.s64 = 196608;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwzx r8,r31,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r9.u32);
	// ori r10,r28,2600
	ctx.r10.u64 = ctx.r28.u64 | 2600;
	// lis r28,2
	ctx.r28.s64 = 131072;
	// lfs f12,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// lwzx r9,r31,r7
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r7.u32);
	// ori r8,r28,2312
	ctx.r8.u64 = ctx.r28.u64 | 2312;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwzx r7,r31,r10
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r10.u32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r28,3
	ctx.r28.s64 = 196608;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// lwzx r10,r31,r8
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r8.u32);
	// ori r8,r28,19012
	ctx.r8.u64 = ctx.r28.u64 | 19012;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lfs f0,1816(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 1816);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// stwx r3,r31,r8
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, ctx.r3.u32);
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// lfd f30,-17064(r7)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r7.u32 + -17064);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// lfs f13,1812(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1812);
	ctx.f13.f64 = double(temp.f32);
	// add r9,r11,r4
	ctx.r9.u64 = ctx.r11.u64 + ctx.r4.u64;
	// rlwinm r8,r9,31,1,31
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fdivs f7,f12,f8
	ctx.f7.f64 = double(float(ctx.f12.f64 / ctx.f8.f64));
	// fdivs f2,f13,f7
	ctx.f2.f64 = double(float(ctx.f13.f64 / ctx.f7.f64));
	// bl 0x8233c318
	ctx.lr = 0x822DC578;
	sub_8233C318(ctx, base);
	// lwz r7,16(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 16);
	// frsp f6,f1
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = double(float(ctx.f1.f64));
	// lwz r6,8(r29)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	// addi r5,r30,888
	ctx.r5.s64 = ctx.r30.s64 + 888;
	// rlwinm r4,r7,2,0,29
	ctx.r4.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r3,-32256
	ctx.r3.s64 = -2113929216;
	// lwz r11,32(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 32);
	// lwz r28,40(r29)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r29.u32 + 40);
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,44(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 44);
	// lwz r8,36(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 36);
	// lis r27,-32255
	ctx.r27.s64 = -2113863680;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lfsx f28,r4,r5
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r5.u32);
	ctx.f28.f64 = double(temp.f32);
	// lfs f11,5268(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 5268);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r3,r28,4,0,27
	ctx.r3.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 4) & 0xFFFFFFF0;
	// fmuls f5,f6,f11
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r9,r28,r3
	ctx.r9.s64 = ctx.r3.s64 - ctx.r28.s64;
	// lfs f10,1808(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1808);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,-1572(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + -1572);
	ctx.f9.f64 = double(temp.f32);
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r9,r10
	ctx.r3.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r19,24(r29)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r29.u32 + 24);
	// add r9,r11,r8
	ctx.r9.u64 = ctx.r11.u64 + ctx.r8.u64;
	// lwz r7,28(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 28);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r26,r30,376
	ctx.r26.s64 = ctx.r30.s64 + 376;
	// addi r5,r30,192
	ctx.r5.s64 = ctx.r30.s64 + 192;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// fsqrts f0,f5
	ctx.f0.f64 = double(simd::sqrt_f32(float(ctx.f5.f64)));
	// lfs f12,1804(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 1804);
	ctx.f12.f64 = double(temp.f32);
	// addi r8,r30,504
	ctx.r8.s64 = ctx.r30.s64 + 504;
	// lfsx f26,r4,r26
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r26.u32);
	ctx.f26.f64 = double(temp.f32);
	// lfsx f25,r4,r5
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r5.u32);
	ctx.f25.f64 = double(temp.f32);
	// addi r5,r30,632
	ctx.r5.s64 = ctx.r30.s64 + 632;
	// addi r4,r30,760
	ctx.r4.s64 = ctx.r30.s64 + 760;
	// addi r28,r30,1016
	ctx.r28.s64 = ctx.r30.s64 + 1016;
	// addi r26,r30,1536
	ctx.r26.s64 = ctx.r30.s64 + 1536;
	// lfsx f27,r6,r8
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r8.u32);
	ctx.f27.f64 = double(temp.f32);
	// addi r8,r30,2056
	ctx.r8.s64 = ctx.r30.s64 + 2056;
	// lfsx f2,r6,r5
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r5.u32);
	ctx.f2.f64 = double(temp.f32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f1,r6,r4
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	ctx.f1.f64 = double(temp.f32);
	// addi r27,r30,316
	ctx.r27.s64 = ctx.r30.s64 + 316;
	// lfsx f8,r9,r28
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r28.u32);
	ctx.f8.f64 = double(temp.f32);
	// addi r5,r30,2600
	ctx.r5.s64 = ctx.r30.s64 + 2600;
	// lfsx f6,r9,r26
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r26.u32);
	ctx.f6.f64 = double(temp.f32);
	// addi r4,r30,132
	ctx.r4.s64 = ctx.r30.s64 + 132;
	// lfsx f5,r3,r8
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r8.u32);
	ctx.f5.f64 = double(temp.f32);
	// addi r6,r30,3144
	ctx.r6.s64 = ctx.r30.s64 + 3144;
	// addi r28,r30,3144
	ctx.r28.s64 = ctx.r30.s64 + 3144;
	// rlwinm r9,r19,2,0,29
	ctx.r9.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f7,r10,r27
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r27.u32);
	ctx.f7.f64 = double(temp.f32);
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f4,r3,r5
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r5.u32);
	ctx.f4.f64 = double(temp.f32);
	// lfsx f3,r10,r4
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r4.u32);
	ctx.f3.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfsx f13,r9,r6
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r6.u32);
	ctx.f13.f64 = double(temp.f32);
	// lfsx f31,r8,r28
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r28.u32);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f24,f0,f10,f9
	ctx.f24.f64 = -double(std::fma(float(ctx.f0.f64), float(ctx.f10.f64), -float(ctx.f9.f64)));
	// fmuls f11,f0,f11
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// fmuls f29,f28,f12
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f12.f64));
	// fmuls f10,f26,f24
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmuls f24,f25,f24
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// fneg f9,f11
	ctx.f9.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// fmuls f26,f10,f12
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// fmuls f25,f10,f12
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// bne cr6,0x822dc6a0
	if (!ctx.cr6.eq) goto loc_822DC6A0;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x822dc6c4
	goto loc_822DC6C4;
loc_822DC6A0:
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lfd f12,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmuls f12,f12,f21
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f21.f64));
	// fctidz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f12,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f12.u64);
	// lwz r11,92(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_822DC6C4:
	// lis r9,1
	ctx.r9.s64 = 65536;
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// ori r8,r9,20
	ctx.r8.u64 = ctx.r9.u64 | 20;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// lwzx r6,r31,r8
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r8.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r6,r7
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r7.u32, ctx.xer);
	// bne cr6,0x822dc704
	if (!ctx.cr6.eq) goto loc_822DC704;
	// lis r9,1
	ctx.r9.s64 = 65536;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r11.u32);
	// lis r8,1
	ctx.r8.s64 = 65536;
	// ori r7,r9,24
	ctx.r7.u64 = ctx.r9.u64 | 24;
	// ori r6,r8,32
	ctx.r6.u64 = ctx.r8.u64 | 32;
	// stwx r11,r31,r7
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, ctx.r11.u32);
	// stfsx f23,r31,r6
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, temp.u32);
	// b 0x822dc710
	goto loc_822DC710;
loc_822DC704:
	// lis r10,1
	ctx.r10.s64 = 65536;
	// ori r9,r10,24
	ctx.r9.u64 = ctx.r10.u64 | 24;
	// stwx r11,r31,r9
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, ctx.r11.u32);
loc_822DC710:
	// stfs f2,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lis r11,1
	ctx.r11.s64 = 65536;
	// stfs f1,88(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lis r10,1
	ctx.r10.s64 = 65536;
	// ori r5,r11,2172
	ctx.r5.u64 = ctx.r11.u64 | 2172;
	// lfs f12,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,1
	ctx.r9.s64 = 65536;
	// lfs f2,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// addis r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 65536;
	// fneg f23,f13
	ctx.f23.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lfs f1,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f1.f64 = double(temp.f32);
	// ori r4,r10,2176
	ctx.r4.u64 = ctx.r10.u64 | 2176;
	// fneg f21,f31
	ctx.f21.u64 = ctx.f31.u64 ^ 0x8000000000000000;
	// ori r3,r9,2180
	ctx.r3.u64 = ctx.r9.u64 | 2180;
	// stfsx f12,r31,r5
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, temp.u32);
	// lis r6,1
	ctx.r6.s64 = 65536;
	// addi r11,r11,2208
	ctx.r11.s64 = ctx.r11.s64 + 2208;
	// ori r10,r8,10972
	ctx.r10.u64 = ctx.r8.u64 | 10972;
	// lis r7,1
	ctx.r7.s64 = 65536;
	// stfsx f2,r31,r4
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r4.u32, temp.u32);
	// ori r8,r6,10980
	ctx.r8.u64 = ctx.r6.u64 | 10980;
	// stfsx f27,r31,r3
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r3.u32, temp.u32);
	// ori r9,r7,10976
	ctx.r9.u64 = ctx.r7.u64 | 10976;
	// stfs f13,524(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 524, temp.u32);
	// lis r6,1
	ctx.r6.s64 = 65536;
	// stfs f23,528(r11)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r11.u32 + 528, temp.u32);
	// addis r3,r31,1
	ctx.r3.s64 = ctx.r31.s64 + 65536;
	// lis r7,1
	ctx.r7.s64 = 65536;
	// stfsx f12,r31,r10
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, temp.u32);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// stfsx f29,r31,r8
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, temp.u32);
	// addis r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 65536;
	// stfsx f1,r31,r9
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, temp.u32);
	// lis r4,1
	ctx.r4.s64 = 65536;
	// ori r8,r6,18256
	ctx.r8.u64 = ctx.r6.u64 | 18256;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r3,r3,11008
	ctx.r3.s64 = ctx.r3.s64 + 11008;
	// ori r10,r7,18252
	ctx.r10.u64 = ctx.r7.u64 | 18252;
	// lwz r7,88(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// ori r6,r5,80
	ctx.r6.u64 = ctx.r5.u64 | 80;
	// addi r11,r11,12064
	ctx.r11.s64 = ctx.r11.s64 + 12064;
	// ori r5,r4,84
	ctx.r5.u64 = ctx.r4.u64 | 84;
	// stfs f13,1036(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 1036, temp.u32);
	// lis r4,1
	ctx.r4.s64 = 65536;
	// stfs f23,1040(r3)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r3.u32 + 1040, temp.u32);
	// lis r3,1
	ctx.r3.s64 = 65536;
	// lis r28,1
	ctx.r28.s64 = 65536;
	// stfs f13,2060(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2060, temp.u32);
	// lis r27,1
	ctx.r27.s64 = 65536;
	// stfs f23,2064(r11)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2064, temp.u32);
	// stwx r9,r31,r6
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, ctx.r9.u32);
	// stwx r7,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r7.u32);
	// stfsx f24,r31,r10
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, temp.u32);
	// stfsx f10,r31,r8
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, temp.u32);
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// stfs f31,2060(r21)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r21.u32 + 2060, temp.u32);
	// lis r8,1
	ctx.r8.s64 = 65536;
	// stfs f21,2064(r21)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r21.u32 + 2064, temp.u32);
	// lis r6,1
	ctx.r6.s64 = 65536;
	// stfs f31,2060(r20)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r20.u32 + 2060, temp.u32);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// stfs f21,2064(r20)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r20.u32 + 2064, temp.u32);
	// lis r9,1
	ctx.r9.s64 = 65536;
	// lis r7,1
	ctx.r7.s64 = 65536;
	// lis r26,1
	ctx.r26.s64 = 65536;
	// lis r21,1
	ctx.r21.s64 = 65536;
	// lis r20,1
	ctx.r20.s64 = 65536;
	// ori r4,r4,22444
	ctx.r4.u64 = ctx.r4.u64 | 22444;
	// ori r3,r3,22464
	ctx.r3.u64 = ctx.r3.u64 | 22464;
	// ori r11,r11,22468
	ctx.r11.u64 = ctx.r11.u64 | 22468;
	// ori r28,r28,22472
	ctx.r28.u64 = ctx.r28.u64 | 22472;
	// ori r10,r10,22492
	ctx.r10.u64 = ctx.r10.u64 | 22492;
	// ori r8,r8,22496
	ctx.r8.u64 = ctx.r8.u64 | 22496;
	// ori r6,r6,38940
	ctx.r6.u64 = ctx.r6.u64 | 38940;
	// ori r5,r5,43132
	ctx.r5.u64 = ctx.r5.u64 | 43132;
	// ori r9,r9,43152
	ctx.r9.u64 = ctx.r9.u64 | 43152;
	// ori r7,r7,43156
	ctx.r7.u64 = ctx.r7.u64 | 43156;
	// ori r27,r27,43160
	ctx.r27.u64 = ctx.r27.u64 | 43160;
	// ori r26,r26,43180
	ctx.r26.u64 = ctx.r26.u64 | 43180;
	// lis r19,2
	ctx.r19.s64 = 131072;
	// stfsx f8,r31,r4
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r4.u32, temp.u32);
	// lis r18,2
	ctx.r18.s64 = 131072;
	// stfsx f7,r31,r3
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r3.u32, temp.u32);
	// ori r21,r21,43184
	ctx.r21.u64 = ctx.r21.u64 | 43184;
	// stfsx f6,r31,r11
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r11.u32, temp.u32);
	// ori r20,r20,59628
	ctx.r20.u64 = ctx.r20.u64 | 59628;
	// stfsx f0,r31,r6
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, temp.u32);
	// lis r4,3
	ctx.r4.s64 = 196608;
	// stfsx f4,r31,r10
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, temp.u32);
	// lis r16,3
	ctx.r16.s64 = 196608;
	// stfsx f3,r31,r8
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, temp.u32);
	// ori r3,r19,39340
	ctx.r3.u64 = ctx.r19.u64 | 39340;
	// stfsx f5,r31,r28
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r28.u32, temp.u32);
	// ori r11,r18,39344
	ctx.r11.u64 = ctx.r18.u64 | 39344;
	// stfs f31,2060(r23)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r23.u32 + 2060, temp.u32);
	// stfs f21,2064(r23)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r23.u32 + 2064, temp.u32);
	// ori r10,r16,19052
	ctx.r10.u64 = ctx.r16.u64 | 19052;
	// ori r8,r4,19056
	ctx.r8.u64 = ctx.r4.u64 | 19056;
	// stfs f31,2060(r22)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r22.u32 + 2060, temp.u32);
	// stfs f21,2064(r22)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r22.u32 + 2064, temp.u32);
	// lwz r17,68(r29)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r29.u32 + 68);
	// stfsx f0,r31,r20
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r20.u32, temp.u32);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// stfsx f8,r31,r5
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, temp.u32);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// stfsx f7,r31,r9
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, temp.u32);
	// stfsx f6,r31,r7
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, temp.u32);
	// stfsx f5,r31,r27
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r27.u32, temp.u32);
	// stw r17,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r17.u32);
	// stfsx f4,r31,r26
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r26.u32, temp.u32);
	// stfsx f3,r31,r21
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r21.u32, temp.u32);
	// stfs f31,4108(r24)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r24.u32 + 4108, temp.u32);
	// stfs f21,4112(r24)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r24.u32 + 4112, temp.u32);
	// stfsx f11,r31,r3
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r3.u32, temp.u32);
	// stfsx f11,r31,r11
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r11.u32, temp.u32);
	// stfs f31,4108(r25)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 4108, temp.u32);
	// stfs f21,4112(r25)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r25.u32 + 4112, temp.u32);
	// stfsx f9,r31,r10
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, temp.u32);
	// stfsx f9,r31,r8
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, temp.u32);
	// lfs f29,-1640(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -1640);
	ctx.f29.f64 = double(temp.f32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f2,f0,f29
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f29.f64));
	// bl 0x8233c318
	ctx.lr = 0x822DC904;
	sub_8233C318(ctx, base);
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// lis r5,1
	ctx.r5.s64 = 65536;
	// lwz r4,68(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 68);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// ori r3,r5,38912
	ctx.r3.u64 = ctx.r5.u64 | 38912;
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// lfs f12,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f2,f12,f29
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f29.f64));
	// fmuls f11,f13,f28
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f28.f64));
	// stfsx f11,r31,r3
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r3.u32, temp.u32);
	// bl 0x8233c318
	ctx.lr = 0x822DC930;
	sub_8233C318(ctx, base);
	// frsp f10,f1
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lwz r10,68(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 68);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// ori r9,r11,59600
	ctx.r9.u64 = ctx.r11.u64 | 59600;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lfs f9,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f9,f29
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f29.f64));
	// fmuls f8,f10,f28
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f28.f64));
	// stfsx f8,r31,r9
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, temp.u32);
	// bl 0x8233c318
	ctx.lr = 0x822DC95C;
	sub_8233C318(ctx, base);
	// frsp f7,f1
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f1.f64));
	// lis r8,2
	ctx.r8.s64 = 131072;
	// lwz r7,68(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 68);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// ori r6,r8,39312
	ctx.r6.u64 = ctx.r8.u64 | 39312;
	// stw r7,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r7.u32);
	// lfs f6,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f2,f6,f29
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f29.f64));
	// fmuls f5,f7,f26
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// stfsx f5,r31,r6
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, temp.u32);
	// bl 0x8233c318
	ctx.lr = 0x822DC988;
	sub_8233C318(ctx, base);
	// frsp f4,f1
	ctx.fpscr.disableFlushMode();
	ctx.f4.f64 = double(float(ctx.f1.f64));
	// lwz r5,64(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 64);
	// lis r4,3
	ctx.r4.s64 = 196608;
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// ori r3,r4,19024
	ctx.r3.u64 = ctx.r4.u64 | 19024;
	// stw r5,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r5.u32);
	// fmuls f2,f4,f25
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f25.f64));
	// stfsx f2,r31,r3
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r3.u32, temp.u32);
	// lfs f3,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f3,f29
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// bl 0x8233c318
	ctx.lr = 0x822DC9B4;
	sub_8233C318(ctx, base);
	// lwz r10,52(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 52);
	// lfs f0,20(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,3
	ctx.r11.s64 = 196608;
	// fmuls f0,f31,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// addis r8,r31,3
	ctx.r8.s64 = ctx.r31.s64 + 196608;
	// addis r7,r31,3
	ctx.r7.s64 = ctx.r31.s64 + 196608;
	// frsp f1,f1
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// ori r9,r11,23236
	ctx.r9.u64 = ctx.r11.u64 | 23236;
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// addis r30,r31,1
	ctx.r30.s64 = ctx.r31.s64 + 65536;
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r8,r8,19072
	ctx.r8.s64 = ctx.r8.s64 + 19072;
	// addi r7,r7,21152
	ctx.r7.s64 = ctx.r7.s64 + 21152;
	// addi r30,r30,48
	ctx.r30.s64 = ctx.r30.s64 + 48;
	// stwx r6,r31,r9
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, ctx.r6.u32);
	// fneg f12,f0
	ctx.f12.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f0,2060(r8)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 2060, temp.u32);
	// stfs f12,2064(r8)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 2064, temp.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stfs f0,2060(r7)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 2060, temp.u32);
	// stfs f12,2064(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 2064, temp.u32);
	// stfs f13,4(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r30.u32 + 4, temp.u32);
	// bl 0x822db440
	ctx.lr = 0x822DCA18;
	sub_822DB440(ctx, base);
	// lwz r5,56(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 56);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stw r5,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r5.u32);
	// lfs f11,88(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r30)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r30.u32 + 8, temp.u32);
	// bl 0x822db440
	ctx.lr = 0x822DCA30;
	sub_822DB440(ctx, base);
	// lwz r4,60(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 60);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// lfs f10,88(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r30)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r30.u32 + 12, temp.u32);
	// bl 0x822db440
	ctx.lr = 0x822DCA48;
	sub_822DB440(ctx, base);
	// lwz r8,84(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// lis r3,3
	ctx.r3.s64 = 196608;
	// lwz r7,88(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 88);
	// lis r11,3
	ctx.r11.s64 = 196608;
	// ori r10,r3,23232
	ctx.r10.u64 = ctx.r3.u64 | 23232;
	// ori r9,r11,23240
	ctx.r9.u64 = ctx.r11.u64 | 23240;
	// stw r8,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
	// lfs f9,88(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f9,f22
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f22.f64));
	// stfs f8,88(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lwz r6,88(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// stwx r7,r31,r9
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, ctx.r7.u32);
	// stwx r6,r31,r10
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, ctx.r6.u32);
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-136
	ctx.r12.s64 = ctx.r1.s64 + -136;
	// bl 0x8233fa68
	ctx.lr = 0x822DCA88;
	__savefpr_21(ctx, base);
	// b 0x8233e488
	__restgprlr_16(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DCA8C"))) PPC_WEAK_FUNC(sub_822DCA8C);
PPC_FUNC_IMPL(__imp__sub_822DCA8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DCA90"))) PPC_WEAK_FUNC(sub_822DCA90);
PPC_FUNC_IMPL(__imp__sub_822DCA90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// bl 0x822dbdd8
	ctx.lr = 0x822DCAB0;
	sub_822DBDD8(ctx, base);
	// lwz r11,48(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 48);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822dcac4
	if (!ctx.cr6.eq) goto loc_822DCAC4;
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x822dcaf0
	goto loc_822DCAF0;
loc_822DCAC4:
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfs f0,1904(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1904);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f10.u64);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_822DCAF0:
	// addis r11,r31,3
	ctx.r11.s64 = ctx.r31.s64 + 196608;
	// addi r11,r11,23248
	ctx.r11.s64 = ctx.r11.s64 + 23248;
	// lwz r9,1044(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1044);
	// lwz r8,1040(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1040);
	// stw r10,1048(r11)
	PPC_STORE_U32(ctx.r11.u32 + 1048, ctx.r10.u32);
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// bne cr6,0x822dcb1c
	if (!ctx.cr6.eq) goto loc_822DCB1C;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// stw r10,1040(r11)
	PPC_STORE_U32(ctx.r11.u32 + 1040, ctx.r10.u32);
	// lfs f0,5256(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,1056(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 1056, temp.u32);
loc_822DCB1C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DCB34"))) PPC_WEAK_FUNC(sub_822DCB34);
PPC_FUNC_IMPL(__imp__sub_822DCB34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DCB38"))) PPC_WEAK_FUNC(sub_822DCB38);
PPC_FUNC_IMPL(__imp__sub_822DCB38) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e444
	ctx.lr = 0x822DCB40;
	__restfpr_19(ctx, base);
	// addi r12,r1,-112
	ctx.r12.s64 = ctx.r1.s64 + -112;
	// bl 0x8233fa2c
	ctx.lr = 0x822DCB48;
	sub_8233FA2C(ctx, base);
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// bl 0x822dbdd8
	ctx.lr = 0x822DCB58;
	sub_822DBDD8(ctx, base);
	// lwz r8,80(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 80);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addi r29,r11,-1408
	ctx.r29.s64 = ctx.r11.s64 + -1408;
	// lfs f13,-1556(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1556);
	ctx.f13.f64 = double(temp.f32);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// lfs f25,5256(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5256);
	ctx.f25.f64 = double(temp.f32);
	// blt cr6,0x822dcb9c
	if (ctx.cr6.lt) goto loc_822DCB9C;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f13,-17732(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -17732);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f0,-1580(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1580);
	ctx.f0.f64 = double(temp.f32);
	// b 0x822dcbc0
	goto loc_822DCBC0;
loc_822DCB9C:
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lfs f13,4(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfs f12,-17908(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -17908);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f12,f0,f13,f12
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f13.f64), float(ctx.f12.f64)));
	// lfs f13,-17732(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -17732);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-1580(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1580);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
loc_822DCBC0:
	// fcmpu cr6,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// bge cr6,0x822dcbcc
	if (!ctx.cr6.lt) goto loc_822DCBCC;
	// fmr f13,f0
	ctx.f13.f64 = ctx.f0.f64;
loc_822DCBCC:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f12,5260(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 5260);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f31,f13,f12
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fcmpu cr6,f31,f0
	ctx.cr6.compare(ctx.f31.f64, ctx.f0.f64);
	// bge cr6,0x822dcbe8
	if (!ctx.cr6.lt) goto loc_822DCBE8;
	// fmr f31,f0
	ctx.f31.f64 = ctx.f0.f64;
	// b 0x822dcbf4
	goto loc_822DCBF4;
loc_822DCBE8:
	// fcmpu cr6,f31,f25
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f31.f64, ctx.f25.f64);
	// ble cr6,0x822dcbf4
	if (!ctx.cr6.gt) goto loc_822DCBF4;
	// fmr f31,f25
	ctx.f31.f64 = ctx.f25.f64;
loc_822DCBF4:
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lfs f0,1888(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1888);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f31,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f13.u64);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x822fcaf8
	ctx.lr = 0x822DCC10;
	sub_822FCAF8(ctx, base);
	// lis r10,5
	ctx.r10.s64 = 327680;
	// lis r9,5
	ctx.r9.s64 = 327680;
	// ori r8,r10,25408
	ctx.r8.u64 = ctx.r10.u64 | 25408;
	// ori r7,r9,25412
	ctx.r7.u64 = ctx.r9.u64 | 25412;
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// stwx r3,r31,r8
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, ctx.r3.u32);
	// stwx r11,r31,r7
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, ctx.r11.u32);
	// lfs f0,1908(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 1908);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f31,f0
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f11.u64);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x822fcaf8
	ctx.lr = 0x822DCC48;
	sub_822FCAF8(ctx, base);
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// subfic r4,r5,0
	ctx.xer.ca = ctx.r5.u32 <= 0;
	ctx.r4.s64 = 0 - ctx.r5.s64;
	// lis r10,5
	ctx.r10.s64 = 327680;
	// subfe r9,r4,r4
	temp.u8 = (~ctx.r4.u32 + ctx.r4.u32 < ~ctx.r4.u32) | (~ctx.r4.u32 + ctx.r4.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r9.u64 = ~ctx.r4.u64 + ctx.r4.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// lis r8,5
	ctx.r8.s64 = 327680;
	// rlwinm r11,r9,0,31,29
	ctx.r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// ori r7,r10,34208
	ctx.r7.u64 = ctx.r10.u64 | 34208;
	// rlwinm r11,r11,0,28,21
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFC0F;
	// ori r5,r8,34212
	ctx.r5.u64 = ctx.r8.u64 | 34212;
	// addi r11,r11,1020
	ctx.r11.s64 = ctx.r11.s64 + 1020;
	// addi r10,r3,-1
	ctx.r10.s64 = ctx.r3.s64 + -1;
	// clrldi r6,r11,32
	ctx.r6.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stwx r3,r31,r7
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, ctx.r3.u32);
	// std r6,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r6.u64);
	// lfd f10,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// stwx r10,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r10.u32);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fmuls f7,f8,f31
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fctidz f6,f7
	ctx.f6.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// stfd f6,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f6.u64);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x822fcaf8
	ctx.lr = 0x822DCCA4;
	sub_822FCAF8(ctx, base);
	// lis r4,2
	ctx.r4.s64 = 131072;
	// lwz r7,12(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	// lis r6,1
	ctx.r6.s64 = 65536;
	// lwz r5,8(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// ori r4,r4,64008
	ctx.r4.u64 = ctx.r4.u64 | 64008;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// ori r11,r6,59588
	ctx.r11.u64 = ctx.r6.u64 | 59588;
	// ori r8,r10,55784
	ctx.r8.u64 = ctx.r10.u64 | 55784;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// lwzx r10,r31,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r4.u32);
	// lis r4,1
	ctx.r4.s64 = 65536;
	// ori r6,r9,22888
	ctx.r6.u64 = ctx.r9.u64 | 22888;
	// lwzx r11,r31,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r11.u32);
	// lis r28,5
	ctx.r28.s64 = 327680;
	// lwzx r9,r31,r8
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r8.u32);
	// ori r8,r4,22408
	ctx.r8.u64 = ctx.r4.u64 | 22408;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lis r4,1
	ctx.r4.s64 = 65536;
	// lwzx r10,r31,r6
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r6.u32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// ori r4,r4,20328
	ctx.r4.u64 = ctx.r4.u64 | 20328;
	// lis r6,3
	ctx.r6.s64 = 196608;
	// lwzx r9,r31,r8
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r8.u32);
	// lis r27,2
	ctx.r27.s64 = 131072;
	// ori r28,r28,41492
	ctx.r28.u64 = ctx.r28.u64 | 41492;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// ori r6,r6,19012
	ctx.r6.u64 = ctx.r6.u64 | 19012;
	// lwzx r8,r31,r4
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r4.u32);
	// ori r27,r27,18760
	ctx.r27.u64 = ctx.r27.u64 | 18760;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lis r26,2
	ctx.r26.s64 = 131072;
	// stwx r3,r31,r28
	PPC_STORE_U32(ctx.r31.u32 + ctx.r28.u32, ctx.r3.u32);
	// rlwinm r3,r7,2,0,29
	ctx.r3.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r31,r6
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r6.u32);
	// ori r4,r26,47560
	ctx.r4.u64 = ctx.r26.u64 | 47560;
	// lwzx r9,r31,r27
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r27.u32);
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// lis r7,2
	ctx.r7.s64 = 131072;
	// lwzx r8,r31,r4
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r4.u32);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r6,72(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 72);
	// ori r4,r7,10536
	ctx.r4.u64 = ctx.r7.u64 | 10536;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// lis r10,1
	ctx.r10.s64 = 65536;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// ori r9,r10,43096
	ctx.r9.u64 = ctx.r10.u64 | 43096;
	// lis r7,1
	ctx.r7.s64 = 65536;
	// add r10,r11,r8
	ctx.r10.u64 = ctx.r11.u64 + ctx.r8.u64;
	// lwzx r11,r31,r4
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r4.u32);
	// ori r6,r7,41016
	ctx.r6.u64 = ctx.r7.u64 | 41016;
	// lis r4,2
	ctx.r4.s64 = 131072;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwzx r9,r31,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r9.u32);
	// ori r8,r4,39300
	ctx.r8.u64 = ctx.r4.u64 | 39300;
	// lis r7,1
	ctx.r7.s64 = 65536;
	// add r10,r11,r9
	ctx.r10.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwzx r11,r31,r6
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r6.u32);
	// ori r6,r7,38900
	ctx.r6.u64 = ctx.r7.u64 | 38900;
	// lis r4,3
	ctx.r4.s64 = 196608;
	// lwzx r9,r31,r8
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r8.u32);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// ori r8,r4,2600
	ctx.r8.u64 = ctx.r4.u64 | 2600;
	// lis r7,2
	ctx.r7.s64 = 131072;
	// lwzx r10,r31,r6
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r6.u32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// ori r6,r7,2312
	ctx.r6.u64 = ctx.r7.u64 | 2312;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwzx r9,r31,r8
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r8.u32);
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// lfs f5,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f5.f64 = double(temp.f32);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwzx r10,r31,r6
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r6.u32);
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// addi r6,r29,504
	ctx.r6.s64 = ctx.r29.s64 + 504;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lfs f0,1816(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1816);
	ctx.f0.f64 = double(temp.f32);
	// lfd f31,-17064(r8)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r8.u32 + -17064);
	// addi r10,r29,632
	ctx.r10.s64 = ctx.r29.s64 + 632;
	// rlwinm r9,r11,31,1,31
	ctx.r9.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// fmr f1,f31
	ctx.f1.f64 = ctx.f31.f64;
	// lfs f13,1812(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1812);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r29,760
	ctx.r8.s64 = ctx.r29.s64 + 760;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// lfd f4,96(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// rlwinm r7,r5,2,0,29
	ctx.r7.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// lfsx f29,r3,r6
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r6.u32);
	ctx.f29.f64 = double(temp.f32);
	// lfsx f28,r7,r10
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	ctx.f28.f64 = double(temp.f32);
	// lfsx f27,r3,r8
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r8.u32);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f0,f2,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fdivs f12,f5,f0
	ctx.f12.f64 = double(float(ctx.f5.f64 / ctx.f0.f64));
	// fdivs f2,f13,f12
	ctx.f2.f64 = double(float(ctx.f13.f64 / ctx.f12.f64));
	// bl 0x8233c318
	ctx.lr = 0x822DCE20;
	sub_8233C318(ctx, base);
	// frsp f11,f1
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// lwz r6,20(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r4,12(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	// rlwinm r3,r6,2,0,29
	ctx.r3.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r8,r29,376
	ctx.r8.s64 = ctx.r29.s64 + 376;
	// lfs f0,5268(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 5268);
	ctx.f0.f64 = double(temp.f32);
	// addi r7,r29,888
	ctx.r7.s64 = ctx.r29.s64 + 888;
	// addi r6,r29,192
	ctx.r6.s64 = ctx.r29.s64 + 192;
	// lfs f13,1808(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1808);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-1572(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1572);
	ctx.f12.f64 = double(temp.f32);
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// addi r10,r29,632
	ctx.r10.s64 = ctx.r29.s64 + 632;
	// lfsx f9,r3,r8
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r8.u32);
	ctx.f9.f64 = double(temp.f32);
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfsx f30,r3,r7
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r7.u32);
	ctx.f30.f64 = double(temp.f32);
	// lfsx f7,r3,r6
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r6.u32);
	ctx.f7.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// fsqrts f6,f8
	ctx.f6.f64 = double(simd::sqrt_f32(float(ctx.f8.f64)));
	// fnmsubs f5,f6,f13,f12
	ctx.f5.f64 = -double(std::fma(float(ctx.f6.f64), float(ctx.f13.f64), -float(ctx.f12.f64)));
	// lfs f0,1804(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1804);
	ctx.f0.f64 = double(temp.f32);
	// lfsx f10,r9,r10
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f11,f30,f0
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f13,f9,f5
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// fmuls f12,f7,f5
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fmuls f26,f13,f0
	ctx.f26.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// bne cr6,0x822dcea4
	if (!ctx.cr6.eq) goto loc_822DCEA4;
	// li r9,1
	ctx.r9.s64 = 1;
	// b 0x822dced0
	goto loc_822DCED0;
loc_822DCEA4:
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f9,f0
	ctx.f9.f64 = double(ctx.f0.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// lfs f0,1904(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1904);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fctidz f6,f7
	ctx.f6.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// stfd f6,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f6.u64);
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
loc_822DCED0:
	// addis r11,r31,3
	ctx.r11.s64 = ctx.r31.s64 + 196608;
	// lis r8,2
	ctx.r8.s64 = 131072;
	// addi r11,r11,23248
	ctx.r11.s64 = ctx.r11.s64 + 23248;
	// ori r7,r8,20
	ctx.r7.u64 = ctx.r8.u64 | 20;
	// addis r10,r11,2
	ctx.r10.s64 = ctx.r11.s64 + 131072;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// lwzx r5,r11,r7
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r7.u32);
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r5,r6
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r6.u32, ctx.xer);
	// bne cr6,0x822dcf1c
	if (!ctx.cr6.eq) goto loc_822DCF1C;
	// lis r8,2
	ctx.r8.s64 = 131072;
	// lis r7,2
	ctx.r7.s64 = 131072;
	// ori r6,r8,24
	ctx.r6.u64 = ctx.r8.u64 | 24;
	// ori r5,r7,32
	ctx.r5.u64 = ctx.r7.u64 | 32;
	// rlwinm r4,r9,1,0,30
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r4,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
	// stwx r4,r11,r6
	PPC_STORE_U32(ctx.r11.u32 + ctx.r6.u32, ctx.r4.u32);
	// stfsx f25,r11,r5
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r11.u32 + ctx.r5.u32, temp.u32);
	// b 0x822dcf2c
	goto loc_822DCF2C;
loc_822DCF1C:
	// lis r10,2
	ctx.r10.s64 = 131072;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// ori r8,r10,24
	ctx.r8.u64 = ctx.r10.u64 | 24;
	// stwx r9,r11,r8
	PPC_STORE_U32(ctx.r11.u32 + ctx.r8.u32, ctx.r9.u32);
loc_822DCF2C:
	// stfs f10,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lwz r7,68(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 68);
	// stfs f28,84(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lis r11,1
	ctx.r11.s64 = 65536;
	// stfs f27,88(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	// ori r5,r11,80
	ctx.r5.u64 = ctx.r11.u64 | 80;
	// lfs f0,0(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lis r9,5
	ctx.r9.s64 = 327680;
	// lfs f10,4(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// lis r8,5
	ctx.r8.s64 = 327680;
	// stw r7,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r7.u32);
	// lis r6,5
	ctx.r6.s64 = 327680;
	// lfs f9,20(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// rlwinm r3,r10,2,0,29
	ctx.r3.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// fmr f1,f31
	ctx.f1.f64 = ctx.f31.f64;
	// addi r4,r29,3144
	ctx.r4.s64 = ctx.r29.s64 + 3144;
	// addis r28,r31,5
	ctx.r28.s64 = ctx.r31.s64 + 327680;
	// lis r11,5
	ctx.r11.s64 = 327680;
	// lis r10,5
	ctx.r10.s64 = 327680;
	// lis r7,5
	ctx.r7.s64 = 327680;
	// ori r9,r9,25420
	ctx.r9.u64 = ctx.r9.u64 | 25420;
	// ori r8,r8,25424
	ctx.r8.u64 = ctx.r8.u64 | 25424;
	// ori r6,r6,25428
	ctx.r6.u64 = ctx.r6.u64 | 25428;
	// addis r21,r31,6
	ctx.r21.s64 = ctx.r31.s64 + 393216;
	// addi r28,r28,25456
	ctx.r28.s64 = ctx.r28.s64 + 25456;
	// addis r19,r31,6
	ctx.r19.s64 = ctx.r31.s64 + 393216;
	// ori r11,r11,34220
	ctx.r11.u64 = ctx.r11.u64 | 34220;
	// lwz r25,80(r1)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// ori r10,r10,34224
	ctx.r10.u64 = ctx.r10.u64 | 34224;
	// ori r7,r7,34228
	ctx.r7.u64 = ctx.r7.u64 | 34228;
	// lwz r22,84(r1)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lis r27,5
	ctx.r27.s64 = 327680;
	// lwz r20,88(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lis r26,5
	ctx.r26.s64 = 327680;
	// lis r24,5
	ctx.r24.s64 = 327680;
	// lis r23,5
	ctx.r23.s64 = 327680;
	// stwx r25,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r25.u32);
	// addi r21,r21,-31280
	ctx.r21.s64 = ctx.r21.s64 + -31280;
	// lfsx f8,r3,r4
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r4.u32);
	ctx.f8.f64 = double(temp.f32);
	// addi r19,r19,-30224
	ctx.r19.s64 = ctx.r19.s64 + -30224;
	// stfsx f29,r31,r6
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, temp.u32);
	// ori r5,r26,41504
	ctx.r5.u64 = ctx.r26.u64 | 41504;
	// stfsx f0,r31,r9
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, temp.u32);
	// ori r4,r24,23336
	ctx.r4.u64 = ctx.r24.u64 | 23336;
	// stfsx f10,r31,r8
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, temp.u32);
	// ori r3,r23,23340
	ctx.r3.u64 = ctx.r23.u64 | 23340;
	// fneg f7,f8
	ctx.f7.u64 = ctx.f8.u64 ^ 0x8000000000000000;
	// ori r27,r27,41500
	ctx.r27.u64 = ctx.r27.u64 | 41500;
	// stfs f8,524(r28)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r28.u32 + 524, temp.u32);
	// stfs f7,528(r28)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r28.u32 + 528, temp.u32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// stfsx f0,r31,r11
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r11.u32, temp.u32);
	// stfsx f9,r31,r10
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, temp.u32);
	// stfsx f11,r31,r7
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, temp.u32);
	// stfs f8,1036(r21)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r21.u32 + 1036, temp.u32);
	// stfs f7,1040(r21)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r21.u32 + 1040, temp.u32);
	// stfs f8,2060(r19)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r19.u32 + 2060, temp.u32);
	// stfs f7,2064(r19)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r19.u32 + 2064, temp.u32);
	// stwx r22,r31,r4
	PPC_STORE_U32(ctx.r31.u32 + ctx.r4.u32, ctx.r22.u32);
	// stfsx f12,r31,r27
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r27.u32, temp.u32);
	// stwx r20,r31,r3
	PPC_STORE_U32(ctx.r31.u32 + ctx.r3.u32, ctx.r20.u32);
	// stfsx f13,r31,r5
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, temp.u32);
	// lfs f6,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f6.f64 = double(temp.f32);
	// lfs f29,-1640(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1640);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f2,f6,f29
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f29.f64));
	// bl 0x8233c318
	ctx.lr = 0x822DD038;
	sub_8233C318(ctx, base);
	// frsp f5,f1
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = double(float(ctx.f1.f64));
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lwz r7,68(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 68);
	// fmr f1,f31
	ctx.f1.f64 = ctx.f31.f64;
	// ori r6,r8,59600
	ctx.r6.u64 = ctx.r8.u64 | 59600;
	// stw r7,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r7.u32);
	// lfs f4,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f2,f4,f29
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// fmuls f3,f5,f30
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f30.f64));
	// stfsx f3,r31,r6
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, temp.u32);
	// bl 0x8233c318
	ctx.lr = 0x822DD064;
	sub_8233C318(ctx, base);
	// lwz r4,28(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 28);
	// lfs f0,20(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// lis r5,3
	ctx.r5.s64 = 196608;
	// lwz r9,52(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 52);
	// frsp f2,f1
	ctx.f2.f64 = double(float(ctx.f1.f64));
	// ori r3,r5,19024
	ctx.r3.u64 = ctx.r5.u64 | 19024;
	// addi r11,r29,3144
	ctx.r11.s64 = ctx.r29.s64 + 3144;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addis r8,r31,6
	ctx.r8.s64 = ctx.r31.s64 + 393216;
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// addis r7,r31,6
	ctx.r7.s64 = ctx.r31.s64 + 393216;
	// addi r8,r8,-24016
	ctx.r8.s64 = ctx.r8.s64 + -24016;
	// addi r7,r7,-21936
	ctx.r7.s64 = ctx.r7.s64 + -21936;
	// fmuls f1,f2,f26
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f26.f64));
	// stfsx f1,r31,r3
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r3.u32, temp.u32);
	// lfsx f12,r10,r11
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// addis r31,r31,5
	ctx.r31.s64 = ctx.r31.s64 + 327680;
	// fneg f10,f11
	ctx.f10.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// stfs f11,2060(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 2060, temp.u32);
	// addi r31,r31,23296
	ctx.r31.s64 = ctx.r31.s64 + 23296;
	// stfs f10,2064(r8)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r8.u32 + 2064, temp.u32);
	// stfs f11,2060(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 2060, temp.u32);
	// stfs f10,2064(r7)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r7.u32 + 2064, temp.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stfs f13,4(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 4, temp.u32);
	// bl 0x822db768
	ctx.lr = 0x822DD0D4;
	sub_822DB768(ctx, base);
	// lwz r6,56(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r6,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r6.u32);
	// lfs f9,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,8(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r31.u32 + 8, temp.u32);
	// bl 0x822db768
	ctx.lr = 0x822DD0EC;
	sub_822DB768(ctx, base);
	// lwz r5,60(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 60);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r5,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r5.u32);
	// lfs f8,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,12(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r31.u32 + 12, temp.u32);
	// bl 0x822db768
	ctx.lr = 0x822DD104;
	sub_822DB768(ctx, base);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// addi r12,r1,-112
	ctx.r12.s64 = ctx.r1.s64 + -112;
	// bl 0x8233fa78
	ctx.lr = 0x822DD110;
	__savefpr_25(ctx, base);
	// b 0x8233e494
	__restgprlr_19(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DD114"))) PPC_WEAK_FUNC(sub_822DD114);
PPC_FUNC_IMPL(__imp__sub_822DD114) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DD118"))) PPC_WEAK_FUNC(sub_822DD118);
PPC_FUNC_IMPL(__imp__sub_822DD118) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// bl 0x822dcb38
	ctx.lr = 0x822DD138;
	sub_822DCB38(ctx, base);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lwz r6,28(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 28);
	// addis r9,r31,6
	ctx.r9.s64 = ctx.r31.s64 + 393216;
	// addi r11,r10,-1388
	ctx.r11.s64 = ctx.r10.s64 + -1388;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r11,3124
	ctx.r7.s64 = ctx.r11.s64 + 3124;
	// lfs f0,-1388(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -1388);
	ctx.f0.f64 = double(temp.f32);
	// addis r8,r31,6
	ctx.r8.s64 = ctx.r31.s64 + 393216;
	// addi r9,r9,-19856
	ctx.r9.s64 = ctx.r9.s64 + -19856;
	// addi r8,r8,-17776
	ctx.r8.s64 = ctx.r8.s64 + -17776;
	// lfsx f13,r5,r7
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r7.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f12,2060(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2060, temp.u32);
	// lwz r11,48(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 48);
	// fneg f11,f12
	ctx.f11.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// stfs f11,2064(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2064, temp.u32);
	// stfs f12,2060(r8)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 2060, temp.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stfs f11,2064(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 2064, temp.u32);
	// bne cr6,0x822dd190
	if (!ctx.cr6.eq) goto loc_822DD190;
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x822dd1bc
	goto loc_822DD1BC;
loc_822DD190:
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfs f0,1904(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1904);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f10.u64);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_822DD1BC:
	// addis r11,r31,6
	ctx.r11.s64 = ctx.r31.s64 + 393216;
	// addi r11,r11,-15696
	ctx.r11.s64 = ctx.r11.s64 + -15696;
	// lwz r9,2068(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2068);
	// lwz r8,2064(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2064);
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// bne cr6,0x822dd1f0
	if (!ctx.cr6.eq) goto loc_822DD1F0;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r8,2064(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2064, ctx.r8.u32);
	// stw r8,2072(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2072, ctx.r8.u32);
	// lfs f0,5256(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5256);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,2080(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2080, temp.u32);
	// b 0x822dd1f8
	goto loc_822DD1F8;
loc_822DD1F0:
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,2072(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2072, ctx.r10.u32);
loc_822DD1F8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DD210"))) PPC_WEAK_FUNC(sub_822DD210);
PPC_FUNC_IMPL(__imp__sub_822DD210) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e46c
	ctx.lr = 0x822DD218;
	__restfpr_29(ctx, base);
	// stfd f30,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f30.u64);
	// stfd f31,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addis r11,r3,1
	ctx.r11.s64 = ctx.r3.s64 + 65536;
	// addis r10,r3,1
	ctx.r10.s64 = ctx.r3.s64 + 65536;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// addi r10,r10,28
	ctx.r10.s64 = ctx.r10.s64 + 28;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addis r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 65536;
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// ori r6,r8,20
	ctx.r6.u64 = ctx.r8.u64 | 20;
	// fdivs f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 / ctx.f0.f64));
	// lfs f31,5268(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5268);
	ctx.f31.f64 = double(temp.f32);
	// addi r3,r3,48
	ctx.r3.s64 = ctx.r3.s64 + 48;
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// std r4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r4.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// lfs f0,1816(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1816);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f30,f1,f0
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmadds f9,f10,f13,f31
	ctx.f9.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f13.f64), float(ctx.f31.f64)));
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f8.u64);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// stwx r10,r31,r6
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, ctx.r10.u32);
	// stfs f1,0(r3)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// bl 0x822db440
	ctx.lr = 0x822DD29C;
	sub_822DB440(ctx, base);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfs f0,1888(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 1888);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f7,f30,f0,f31
	ctx.f7.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// fctidz f6,f7
	ctx.f6.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD2B8;
	sub_822FCAF8(ctx, base);
	// fadds f5,f30,f31
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = double(float(ctx.f30.f64 + ctx.f31.f64));
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lis r7,1
	ctx.r7.s64 = 65536;
	// lis r6,1
	ctx.r6.s64 = 65536;
	// ori r5,r8,2160
	ctx.r5.u64 = ctx.r8.u64 | 2160;
	// ori r4,r7,2164
	ctx.r4.u64 = ctx.r7.u64 | 2164;
	// ori r10,r6,2168
	ctx.r10.u64 = ctx.r6.u64 | 2168;
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// stwx r3,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r3.u32);
	// stwx r11,r31,r4
	PPC_STORE_U32(ctx.r31.u32 + ctx.r4.u32, ctx.r11.u32);
	// fctidz f4,f5
	ctx.f4.s64 = (ctx.f5.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f5.f64);
	// stfd f4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f4.u64);
	// lwz r30,84(r1)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stwx r30,r31,r10
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, ctx.r30.u32);
	// lfs f0,1936(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 1936);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f30,f0,f31
	ctx.f3.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// fctidz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f3.f64);
	// stfd f2,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f2.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD30C;
	sub_822FCAF8(ctx, base);
	// addis r8,r31,1
	ctx.r8.s64 = ctx.r31.s64 + 65536;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// addi r8,r8,2208
	ctx.r8.s64 = ctx.r8.s64 + 2208;
	// lfs f0,1884(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1884);
	ctx.f0.f64 = double(temp.f32);
	// lwz r6,512(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 512);
	// fmadds f1,f30,f0,f31
	ctx.f1.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// stw r3,520(r8)
	PPC_STORE_U32(ctx.r8.u32 + 520, ctx.r3.u32);
	// subf r5,r3,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r3.s64;
	// clrlwi r4,r5,25
	ctx.r4.u64 = ctx.r5.u32 & 0x7F;
	// stw r4,516(r8)
	PPC_STORE_U32(ctx.r8.u32 + 516, ctx.r4.u32);
	// fctidz f0,f1
	ctx.f0.s64 = (ctx.f1.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f1.f64);
	// stfd f0,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f0.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD344;
	sub_822FCAF8(ctx, base);
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// ori r9,r11,10960
	ctx.r9.u64 = ctx.r11.u64 | 10960;
	// lis r8,1
	ctx.r8.s64 = 65536;
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
	// ori r7,r8,10964
	ctx.r7.u64 = ctx.r8.u64 | 10964;
	// lfs f0,1932(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1932);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f13,f30,f0,f31
	ctx.f13.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// stwx r3,r31,r9
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, ctx.r3.u32);
	// stwx r11,r31,r7
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, ctx.r11.u32);
	// fctidz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f13.f64);
	// stfd f12,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f12.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD37C;
	sub_822FCAF8(ctx, base);
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// lfs f0,1928(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1928);
	ctx.f0.f64 = double(temp.f32);
	// lis r6,1
	ctx.r6.s64 = 65536;
	// fmadds f11,f30,f0,f31
	ctx.f11.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// ori r5,r6,10968
	ctx.r5.u64 = ctx.r6.u64 | 10968;
	// stwx r3,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r3.u32);
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f10.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD3A4;
	sub_822FCAF8(ctx, base);
	// addis r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 65536;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// addi r11,r11,11008
	ctx.r11.s64 = ctx.r11.s64 + 11008;
	// lfs f0,1924(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 1924);
	ctx.f0.f64 = double(temp.f32);
	// lwz r10,1024(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1024);
	// fmadds f9,f30,f0,f31
	ctx.f9.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// stw r3,1032(r11)
	PPC_STORE_U32(ctx.r11.u32 + 1032, ctx.r3.u32);
	// subf r8,r3,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r3.s64;
	// clrlwi r7,r8,24
	ctx.r7.u64 = ctx.r8.u32 & 0xFF;
	// stw r7,1028(r11)
	PPC_STORE_U32(ctx.r11.u32 + 1028, ctx.r7.u32);
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f8.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD3DC;
	sub_822FCAF8(ctx, base);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lfs f0,1880(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 1880);
	ctx.f0.f64 = double(temp.f32);
	// addis r4,r31,1
	ctx.r4.s64 = ctx.r31.s64 + 65536;
	// fmadds f7,f30,f0,f31
	ctx.f7.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// addi r4,r4,12064
	ctx.r4.s64 = ctx.r4.s64 + 12064;
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// ori r9,r11,18244
	ctx.r9.u64 = ctx.r11.u64 | 18244;
	// lfs f0,1920(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1920);
	ctx.f0.f64 = double(temp.f32);
	// ori r8,r10,18248
	ctx.r8.u64 = ctx.r10.u64 | 18248;
	// lwz r7,2048(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// fmadds f6,f30,f0,f31
	ctx.f6.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// stw r3,2056(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2056, ctx.r3.u32);
	// subf r6,r3,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r3.s64;
	// fctidz f5,f7
	ctx.f5.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// stfd f5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f5.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// clrlwi r5,r6,23
	ctx.r5.u64 = ctx.r6.u32 & 0x1FF;
	// stw r5,2052(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2052, ctx.r5.u32);
	// stwx r30,r31,r8
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, ctx.r30.u32);
	// fctidz f4,f6
	ctx.f4.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f6.f64);
	// stfd f4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f4.u64);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stwx r4,r31,r9
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, ctx.r4.u32);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD444;
	sub_822FCAF8(ctx, base);
	// addis r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 65536;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// addi r11,r11,18272
	ctx.r11.s64 = ctx.r11.s64 + 18272;
	// lfs f0,1876(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1876);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,2048(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// fmadds f3,f30,f0,f31
	ctx.f3.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// stw r3,2056(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2056, ctx.r3.u32);
	// subf r8,r3,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r3.s64;
	// clrlwi r7,r8,23
	ctx.r7.u64 = ctx.r8.u32 & 0x1FF;
	// stw r7,2052(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2052, ctx.r7.u32);
	// fctidz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f3.f64);
	// stfd f2,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f2.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD47C;
	sub_822FCAF8(ctx, base);
	// addis r5,r31,1
	ctx.r5.s64 = ctx.r31.s64 + 65536;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// addi r5,r5,20352
	ctx.r5.s64 = ctx.r5.s64 + 20352;
	// lis r4,1
	ctx.r4.s64 = 65536;
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// lfs f0,1872(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 1872);
	ctx.f0.f64 = double(temp.f32);
	// ori r8,r4,22440
	ctx.r8.u64 = ctx.r4.u64 | 22440;
	// lwz r9,2048(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2048);
	// fmadds f1,f30,f0,f31
	ctx.f1.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// ori r7,r11,22460
	ctx.r7.u64 = ctx.r11.u64 | 22460;
	// ori r6,r10,22488
	ctx.r6.u64 = ctx.r10.u64 | 22488;
	// subf r4,r3,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r3.s64;
	// stw r3,2056(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2056, ctx.r3.u32);
	// fctidz f0,f1
	ctx.f0.s64 = (ctx.f1.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f1.f64);
	// stfd f0,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f0.u64);
	// clrlwi r11,r4,23
	ctx.r11.u64 = ctx.r4.u32 & 0x1FF;
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r11,2052(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2052, ctx.r11.u32);
	// stwx r30,r31,r8
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, ctx.r30.u32);
	// stwx r30,r31,r7
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, ctx.r30.u32);
	// stwx r30,r31,r6
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, ctx.r30.u32);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD4D8;
	sub_822FCAF8(ctx, base);
	// addis r29,r31,2
	ctx.r29.s64 = ctx.r31.s64 + 131072;
	// addi r29,r29,-26632
	ctx.r29.s64 = ctx.r29.s64 + -26632;
	// lis r9,1
	ctx.r9.s64 = 65536;
	// ori r7,r9,38900
	ctx.r7.u64 = ctx.r9.u64 | 38900;
	// lwz r8,0(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// stwx r3,r31,r7
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, ctx.r3.u32);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// lfd f13,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmadds f10,f11,f30,f31
	ctx.f10.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f30.f64), float(ctx.f31.f64)));
	// fctidz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f9.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD514;
	sub_822FCAF8(ctx, base);
	// lis r6,1
	ctx.r6.s64 = 65536;
	// stw r3,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r3.u32);
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// ori r4,r6,38936
	ctx.r4.u64 = ctx.r6.u64 | 38936;
	// lfs f0,1868(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1868);
	ctx.f0.f64 = double(temp.f32);
	// stwx r30,r31,r4
	PPC_STORE_U32(ctx.r31.u32 + ctx.r4.u32, ctx.r30.u32);
	// fmadds f8,f30,f0,f31
	ctx.f8.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f7.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD540;
	sub_822FCAF8(ctx, base);
	// addis r10,r31,2
	ctx.r10.s64 = ctx.r31.s64 + 131072;
	// addi r10,r10,-26576
	ctx.r10.s64 = ctx.r10.s64 + -26576;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lwz r9,2048(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 2048);
	// stw r3,2056(r10)
	PPC_STORE_U32(ctx.r10.u32 + 2056, ctx.r3.u32);
	// lfs f0,1864(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1864);
	ctx.f0.f64 = double(temp.f32);
	// subf r8,r3,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r3.s64;
	// fmadds f6,f30,f0,f31
	ctx.f6.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r7,r8,23
	ctx.r7.u64 = ctx.r8.u32 & 0x1FF;
	// stw r7,2052(r10)
	PPC_STORE_U32(ctx.r10.u32 + 2052, ctx.r7.u32);
	// fctidz f5,f6
	ctx.f5.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f6.f64);
	// stfd f5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f5.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD578;
	sub_822FCAF8(ctx, base);
	// addis r5,r31,2
	ctx.r5.s64 = ctx.r31.s64 + 131072;
	// addi r5,r5,-24496
	ctx.r5.s64 = ctx.r5.s64 + -24496;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lis r4,1
	ctx.r4.s64 = 65536;
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// lwz r9,2048(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2048);
	// ori r8,r4,43128
	ctx.r8.u64 = ctx.r4.u64 | 43128;
	// lfs f0,1860(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 1860);
	ctx.f0.f64 = double(temp.f32);
	// ori r6,r11,43148
	ctx.r6.u64 = ctx.r11.u64 | 43148;
	// subf r7,r3,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r3.s64;
	// stw r3,2056(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2056, ctx.r3.u32);
	// ori r4,r10,43176
	ctx.r4.u64 = ctx.r10.u64 | 43176;
	// fmadds f4,f30,f0,f31
	ctx.f4.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r3,r7,23
	ctx.r3.u64 = ctx.r7.u32 & 0x1FF;
	// stw r3,2052(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2052, ctx.r3.u32);
	// stwx r30,r31,r8
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, ctx.r30.u32);
	// stwx r30,r31,r6
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, ctx.r30.u32);
	// stwx r30,r31,r4
	PPC_STORE_U32(ctx.r31.u32 + ctx.r4.u32, ctx.r30.u32);
	// fctidz f3,f4
	ctx.f3.s64 = (ctx.f4.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f4.f64);
	// stfd f3,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f3.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD5D4;
	sub_822FCAF8(ctx, base);
	// addis r29,r31,2
	ctx.r29.s64 = ctx.r31.s64 + 131072;
	// addi r29,r29,-5944
	ctx.r29.s64 = ctx.r29.s64 + -5944;
	// lis r11,1
	ctx.r11.s64 = 65536;
	// ori r10,r11,59588
	ctx.r10.u64 = ctx.r11.u64 | 59588;
	// lwz r8,0(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// stwx r3,r31,r10
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, ctx.r3.u32);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// lfd f2,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f0,f1
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// fmadds f13,f0,f30,f31
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f30.f64), float(ctx.f31.f64)));
	// fctidz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f13.f64);
	// stfd f12,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f12.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD610;
	sub_822FCAF8(ctx, base);
	// lis r7,1
	ctx.r7.s64 = 65536;
	// stw r3,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r3.u32);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// ori r5,r7,59624
	ctx.r5.u64 = ctx.r7.u64 | 59624;
	// lfs f0,1856(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 1856);
	ctx.f0.f64 = double(temp.f32);
	// stwx r30,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r30.u32);
	// fmadds f11,f30,f0,f31
	ctx.f11.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f10.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD63C;
	sub_822FCAF8(ctx, base);
	// addis r4,r31,2
	ctx.r4.s64 = ctx.r31.s64 + 131072;
	// addi r4,r4,-5888
	ctx.r4.s64 = ctx.r4.s64 + -5888;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lwz r11,8192(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8192);
	// stw r3,8200(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8200, ctx.r3.u32);
	// lfs f0,1852(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1852);
	ctx.f0.f64 = double(temp.f32);
	// subf r9,r3,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r3.s64;
	// fmadds f9,f30,f0,f31
	ctx.f9.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r8,r9,21
	ctx.r8.u64 = ctx.r9.u32 & 0x7FF;
	// stw r8,8196(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8196, ctx.r8.u32);
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f8.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD674;
	sub_822FCAF8(ctx, base);
	// addis r7,r31,2
	ctx.r7.s64 = ctx.r31.s64 + 131072;
	// addi r7,r7,2336
	ctx.r7.s64 = ctx.r7.s64 + 2336;
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// lwz r6,8192(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8192);
	// stw r3,8200(r7)
	PPC_STORE_U32(ctx.r7.u32 + 8200, ctx.r3.u32);
	// lfs f0,1848(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1848);
	ctx.f0.f64 = double(temp.f32);
	// subf r4,r3,r6
	ctx.r4.s64 = ctx.r6.s64 - ctx.r3.s64;
	// fmadds f7,f30,f0,f31
	ctx.f7.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r3,r4,21
	ctx.r3.u64 = ctx.r4.u32 & 0x7FF;
	// stw r3,8196(r7)
	PPC_STORE_U32(ctx.r7.u32 + 8196, ctx.r3.u32);
	// fctidz f6,f7
	ctx.f6.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD6AC;
	sub_822FCAF8(ctx, base);
	// addis r11,r31,2
	ctx.r11.s64 = ctx.r31.s64 + 131072;
	// addi r11,r11,10560
	ctx.r11.s64 = ctx.r11.s64 + 10560;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// stw r3,8200(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8200, ctx.r3.u32);
	// lfs f0,1844(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 1844);
	ctx.f0.f64 = double(temp.f32);
	// subf r8,r3,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r3.s64;
	// fmadds f5,f30,f0,f31
	ctx.f5.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// stw r7,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r7.u32);
	// fctidz f4,f5
	ctx.f4.s64 = (ctx.f5.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f5.f64);
	// stfd f4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f4.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD6E4;
	sub_822FCAF8(ctx, base);
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// addis r6,r31,2
	ctx.r6.s64 = ctx.r31.s64 + 131072;
	// lfs f0,1840(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1840);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f30,f0,f31
	ctx.f3.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// addi r6,r6,18784
	ctx.r6.s64 = ctx.r6.s64 + 18784;
	// lwz r5,4096(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4096);
	// stw r3,4104(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4104, ctx.r3.u32);
	// subf r11,r3,r5
	ctx.r11.s64 = ctx.r5.s64 - ctx.r3.s64;
	// clrlwi r10,r11,22
	ctx.r10.u64 = ctx.r11.u32 & 0x3FF;
	// fctidz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f3.f64);
	// stfd f2,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f2.u64);
	// stw r10,4100(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4100, ctx.r10.u32);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD71C;
	sub_822FCAF8(ctx, base);
	// addis r29,r31,3
	ctx.r29.s64 = ctx.r31.s64 + 196608;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// addi r29,r29,-26232
	ctx.r29.s64 = ctx.r29.s64 + -26232;
	// ori r7,r9,39300
	ctx.r7.u64 = ctx.r9.u64 | 39300;
	// lwz r6,0(r29)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// stwx r3,r31,r7
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, ctx.r3.u32);
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// lfd f1,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f1
	ctx.f0.f64 = double(ctx.f1.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(ctx.f0.f64));
	// fmadds f12,f13,f30,f31
	ctx.f12.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f30.f64), float(ctx.f31.f64)));
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f11.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD758;
	sub_822FCAF8(ctx, base);
	// lis r5,2
	ctx.r5.s64 = 131072;
	// stw r3,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r3.u32);
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// ori r3,r5,39336
	ctx.r3.u64 = ctx.r5.u64 | 39336;
	// lfs f0,1836(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1836);
	ctx.f0.f64 = double(temp.f32);
	// stwx r30,r31,r3
	PPC_STORE_U32(ctx.r31.u32 + ctx.r3.u32, ctx.r30.u32);
	// fmadds f10,f30,f0,f31
	ctx.f10.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// fctidz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f9.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD784;
	sub_822FCAF8(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = ctx.r31.s64 + 196608;
	// addi r11,r11,-26176
	ctx.r11.s64 = ctx.r11.s64 + -26176;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// stw r3,8200(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8200, ctx.r3.u32);
	// lfs f0,1832(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 1832);
	ctx.f0.f64 = double(temp.f32);
	// subf r8,r3,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r3.s64;
	// fmadds f8,f30,f0,f31
	ctx.f8.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// stw r7,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r7.u32);
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f7.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD7BC;
	sub_822FCAF8(ctx, base);
	// addis r5,r31,3
	ctx.r5.s64 = ctx.r31.s64 + 196608;
	// addi r5,r5,-17952
	ctx.r5.s64 = ctx.r5.s64 + -17952;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lwz r4,8192(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8192);
	// stw r3,8200(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8200, ctx.r3.u32);
	// lfs f0,1828(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 1828);
	ctx.f0.f64 = double(temp.f32);
	// subf r3,r3,r4
	ctx.r3.s64 = ctx.r4.s64 - ctx.r3.s64;
	// fmadds f6,f30,f0,f31
	ctx.f6.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r11,r3,21
	ctx.r11.u64 = ctx.r3.u32 & 0x7FF;
	// stw r11,8196(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8196, ctx.r11.u32);
	// fctidz f5,f6
	ctx.f5.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f6.f64);
	// stfd f5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f5.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD7F4;
	sub_822FCAF8(ctx, base);
	// addis r10,r31,3
	ctx.r10.s64 = ctx.r31.s64 + 196608;
	// addi r10,r10,-9728
	ctx.r10.s64 = ctx.r10.s64 + -9728;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lwz r8,8192(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// stw r3,8200(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8200, ctx.r3.u32);
	// lfs f0,1824(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 1824);
	ctx.f0.f64 = double(temp.f32);
	// subf r7,r3,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r3.s64;
	// fmadds f4,f30,f0,f31
	ctx.f4.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r6,r7,21
	ctx.r6.u64 = ctx.r7.u32 & 0x7FF;
	// stw r6,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r6.u32);
	// fctidz f3,f4
	ctx.f3.s64 = (ctx.f4.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f4.f64);
	// stfd f3,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f3.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD82C;
	sub_822FCAF8(ctx, base);
	// addis r5,r31,3
	ctx.r5.s64 = ctx.r31.s64 + 196608;
	// addi r5,r5,-1504
	ctx.r5.s64 = ctx.r5.s64 + -1504;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lwz r4,4096(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4096);
	// stw r3,4104(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4104, ctx.r3.u32);
	// lfs f0,1820(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1820);
	ctx.f0.f64 = double(temp.f32);
	// subf r10,r3,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r3.s64;
	// fmadds f2,f30,f0,f31
	ctx.f2.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r9,r10,22
	ctx.r9.u64 = ctx.r10.u32 & 0x3FF;
	// stw r9,4100(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4100, ctx.r9.u32);
	// fctidz f1,f2
	ctx.f1.s64 = (ctx.f2.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f2.f64);
	// stfd f1,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f1.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD864;
	sub_822FCAF8(ctx, base);
	// lis r8,3
	ctx.r8.s64 = 196608;
	// ori r7,r8,19012
	ctx.r7.u64 = ctx.r8.u64 | 19012;
	// addis r29,r31,3
	ctx.r29.s64 = ctx.r31.s64 + 196608;
	// addi r29,r29,19016
	ctx.r29.s64 = ctx.r29.s64 + 19016;
	// stwx r3,r31,r7
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, ctx.r3.u32);
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmadds f11,f12,f30,f31
	ctx.f11.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f30.f64), float(ctx.f31.f64)));
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f10.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD8A0;
	sub_822FCAF8(ctx, base);
	// stw r3,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r3.u32);
	// lis r3,3
	ctx.r3.s64 = 196608;
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// ori r11,r3,19048
	ctx.r11.u64 = ctx.r3.u64 | 19048;
	// lfs f0,1916(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1916);
	ctx.f0.f64 = double(temp.f32);
	// stwx r30,r31,r11
	PPC_STORE_U32(ctx.r31.u32 + ctx.r11.u32, ctx.r30.u32);
	// fmadds f9,f30,f0,f31
	ctx.f9.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f8.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD8CC;
	sub_822FCAF8(ctx, base);
	// addis r9,r31,3
	ctx.r9.s64 = ctx.r31.s64 + 196608;
	// addi r9,r9,19072
	ctx.r9.s64 = ctx.r9.s64 + 19072;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lwz r8,2048(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2048);
	// stw r3,2056(r9)
	PPC_STORE_U32(ctx.r9.u32 + 2056, ctx.r3.u32);
	// lfs f0,1912(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1912);
	ctx.f0.f64 = double(temp.f32);
	// subf r7,r3,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r3.s64;
	// fmadds f7,f30,f0,f31
	ctx.f7.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r6,r7,23
	ctx.r6.u64 = ctx.r7.u32 & 0x1FF;
	// stw r6,2052(r9)
	PPC_STORE_U32(ctx.r9.u32 + 2052, ctx.r6.u32);
	// fctidz f6,f7
	ctx.f6.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DD904;
	sub_822FCAF8(ctx, base);
	// addis r5,r31,3
	ctx.r5.s64 = ctx.r31.s64 + 196608;
	// addi r5,r5,21152
	ctx.r5.s64 = ctx.r5.s64 + 21152;
	// lwz r4,2048(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2048);
	// stw r3,2056(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2056, ctx.r3.u32);
	// subf r3,r3,r4
	ctx.r3.s64 = ctx.r4.s64 - ctx.r3.s64;
	// clrlwi r11,r3,23
	ctx.r11.u64 = ctx.r3.u32 & 0x1FF;
	// stw r11,2052(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2052, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f30,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f31,-40(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8233e4bc
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DD930"))) PPC_WEAK_FUNC(sub_822DD930);
PPC_FUNC_IMPL(__imp__sub_822DD930) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// bl 0x822dd210
	ctx.lr = 0x822DD950;
	sub_822DD210(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = ctx.r31.s64 + 196608;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r11,r11,23248
	ctx.r11.s64 = ctx.r11.s64 + 23248;
	// lfs f0,5268(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5268);
	ctx.f0.f64 = double(temp.f32);
	// lwz r8,1040(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1040);
	// lfs f13,1052(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1052);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f31,f13
	ctx.f12.f64 = double(float(ctx.f31.f64 / ctx.f13.f64));
	// stfs f31,1052(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 1052, temp.u32);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmadds f8,f12,f9,f0
	ctx.f8.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f9.f64), float(ctx.f0.f64)));
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f7.u64);
	// lwz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r7,1040(r11)
	PPC_STORE_U32(ctx.r11.u32 + 1040, ctx.r7.u32);
	// stw r7,1044(r11)
	PPC_STORE_U32(ctx.r11.u32 + 1044, ctx.r7.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DD9B0"))) PPC_WEAK_FUNC(sub_822DD9B0);
PPC_FUNC_IMPL(__imp__sub_822DD9B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f29,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f29.u64);
	// stfd f30,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f30.u64);
	// stfd f31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// fmr f30,f1
	ctx.f30.f64 = ctx.f1.f64;
	// bl 0x822dd210
	ctx.lr = 0x822DD9DC;
	sub_822DD210(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = ctx.r31.s64 + 196608;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r11,r11,23248
	ctx.r11.s64 = ctx.r11.s64 + 23248;
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// addis r9,r11,2
	ctx.r9.s64 = ctx.r11.s64 + 131072;
	// addis r8,r11,2
	ctx.r8.s64 = ctx.r11.s64 + 131072;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// lfs f31,5268(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5268);
	ctx.f31.f64 = double(temp.f32);
	// addi r8,r8,28
	ctx.r8.s64 = ctx.r8.s64 + 28;
	// lfs f0,1816(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1816);
	ctx.f0.f64 = double(temp.f32);
	// lis r7,2
	ctx.r7.s64 = 131072;
	// fmuls f29,f30,f0
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// addis r3,r31,5
	ctx.r3.s64 = ctx.r31.s64 + 327680;
	// ori r6,r7,20
	ctx.r6.u64 = ctx.r7.u64 | 20;
	// addi r3,r3,23296
	ctx.r3.s64 = ctx.r3.s64 + 23296;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lfs f0,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fdivs f13,f30,f0
	ctx.f13.f64 = double(float(ctx.f30.f64 / ctx.f0.f64));
	// stfs f30,0(r8)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmadds f9,f10,f13,f31
	ctx.f9.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f13.f64), float(ctx.f31.f64)));
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f8.u64);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// stwx r7,r11,r6
	PPC_STORE_U32(ctx.r11.u32 + ctx.r6.u32, ctx.r7.u32);
	// stfs f30,0(r3)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// bl 0x822db768
	ctx.lr = 0x822DDA5C;
	sub_822DB768(ctx, base);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lfs f0,1888(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 1888);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f7,f29,f0,f31
	ctx.f7.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// fctidz f6,f7
	ctx.f6.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DDA78;
	sub_822FCAF8(ctx, base);
	// fadds f5,f29,f31
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = double(float(ctx.f29.f64 + ctx.f31.f64));
	// lis r5,5
	ctx.r5.s64 = 327680;
	// lis r4,5
	ctx.r4.s64 = 327680;
	// lis r11,5
	ctx.r11.s64 = 327680;
	// ori r10,r5,25408
	ctx.r10.u64 = ctx.r5.u64 | 25408;
	// ori r8,r11,25416
	ctx.r8.u64 = ctx.r11.u64 | 25416;
	// ori r9,r4,25412
	ctx.r9.u64 = ctx.r4.u64 | 25412;
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// stwx r3,r31,r10
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, ctx.r3.u32);
	// fctidz f4,f5
	ctx.f4.s64 = (ctx.f5.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f5.f64);
	// stfd f4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f4.u64);
	// lwz r30,84(r1)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stwx r11,r31,r9
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, ctx.r11.u32);
	// lfs f0,1952(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1952);
	ctx.f0.f64 = double(temp.f32);
	// stwx r30,r31,r8
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, ctx.r30.u32);
	// fmadds f3,f29,f0,f31
	ctx.f3.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// fctidz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f3.f64);
	// stfd f2,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f2.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DDACC;
	sub_822FCAF8(ctx, base);
	// addis r6,r31,5
	ctx.r6.s64 = ctx.r31.s64 + 327680;
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// addi r6,r6,25456
	ctx.r6.s64 = ctx.r6.s64 + 25456;
	// lfs f0,1908(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1908);
	ctx.f0.f64 = double(temp.f32);
	// lwz r4,512(r6)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 512);
	// fmadds f1,f29,f0,f31
	ctx.f1.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// stw r3,520(r6)
	PPC_STORE_U32(ctx.r6.u32 + 520, ctx.r3.u32);
	// subf r3,r3,r4
	ctx.r3.s64 = ctx.r4.s64 - ctx.r3.s64;
	// clrlwi r11,r3,25
	ctx.r11.u64 = ctx.r3.u32 & 0x7F;
	// stw r11,516(r6)
	PPC_STORE_U32(ctx.r6.u32 + 516, ctx.r11.u32);
	// fctidz f0,f1
	ctx.f0.s64 = (ctx.f1.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f1.f64);
	// stfd f0,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f0.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DDB04;
	sub_822FCAF8(ctx, base);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,5
	ctx.r9.s64 = 327680;
	// lfs f0,1932(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1932);
	ctx.f0.f64 = double(temp.f32);
	// ori r8,r9,34208
	ctx.r8.u64 = ctx.r9.u64 | 34208;
	// lis r7,5
	ctx.r7.s64 = 327680;
	// stwx r3,r31,r8
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, ctx.r3.u32);
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
	// fmadds f13,f29,f0,f31
	ctx.f13.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// ori r6,r7,34212
	ctx.r6.u64 = ctx.r7.u64 | 34212;
	// stwx r11,r31,r6
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, ctx.r11.u32);
	// fctidz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f13.f64);
	// stfd f12,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f12.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DDB3C;
	sub_822FCAF8(ctx, base);
	// lis r5,5
	ctx.r5.s64 = 327680;
	// ori r4,r5,34216
	ctx.r4.u64 = ctx.r5.u64 | 34216;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// stwx r3,r31,r4
	PPC_STORE_U32(ctx.r31.u32 + ctx.r4.u32, ctx.r3.u32);
	// lfs f0,1948(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1948);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f11,f29,f0,f31
	ctx.f11.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f10.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DDB64;
	sub_822FCAF8(ctx, base);
	// addis r10,r31,6
	ctx.r10.s64 = ctx.r31.s64 + 393216;
	// addi r10,r10,-31280
	ctx.r10.s64 = ctx.r10.s64 + -31280;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lwz r9,1024(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 1024);
	// stw r3,1032(r10)
	PPC_STORE_U32(ctx.r10.u32 + 1032, ctx.r3.u32);
	// lfs f0,1944(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 1944);
	ctx.f0.f64 = double(temp.f32);
	// subf r7,r3,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r3.s64;
	// fmadds f9,f29,f0,f31
	ctx.f9.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// clrlwi r6,r7,24
	ctx.r6.u64 = ctx.r7.u32 & 0xFF;
	// stw r6,1028(r10)
	PPC_STORE_U32(ctx.r10.u32 + 1028, ctx.r6.u32);
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f8.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DDB9C;
	sub_822FCAF8(ctx, base);
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// lfs f0,1940(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1940);
	ctx.f0.f64 = double(temp.f32);
	// addis r11,r31,6
	ctx.r11.s64 = ctx.r31.s64 + 393216;
	// fmadds f7,f29,f0,f31
	ctx.f7.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// addi r11,r11,-30224
	ctx.r11.s64 = ctx.r11.s64 + -30224;
	// lis r10,5
	ctx.r10.s64 = 327680;
	// lis r9,5
	ctx.r9.s64 = 327680;
	// ori r8,r10,41492
	ctx.r8.u64 = ctx.r10.u64 | 41492;
	// lfs f0,1920(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1920);
	ctx.f0.f64 = double(temp.f32);
	// ori r6,r9,41496
	ctx.r6.u64 = ctx.r9.u64 | 41496;
	// lwz r7,2048(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// fmadds f6,f29,f0,f31
	ctx.f6.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// stw r3,2056(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2056, ctx.r3.u32);
	// subf r5,r3,r7
	ctx.r5.s64 = ctx.r7.s64 - ctx.r3.s64;
	// fctidz f5,f7
	ctx.f5.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// stfd f5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f5.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// clrlwi r4,r5,23
	ctx.r4.u64 = ctx.r5.u32 & 0x1FF;
	// stw r4,2052(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2052, ctx.r4.u32);
	// stwx r30,r31,r6
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, ctx.r30.u32);
	// fctidz f4,f6
	ctx.f4.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f6.f64);
	// stfd f4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f4.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stwx r11,r31,r8
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, ctx.r11.u32);
	// bl 0x822fcaf8
	ctx.lr = 0x822DDC04;
	sub_822FCAF8(ctx, base);
	// addis r10,r31,6
	ctx.r10.s64 = ctx.r31.s64 + 393216;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// addi r10,r10,-24016
	ctx.r10.s64 = ctx.r10.s64 + -24016;
	// lfs f0,11184(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 11184);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,2048(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 2048);
	// fmadds f3,f29,f0,f31
	ctx.f3.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// stw r3,2056(r10)
	PPC_STORE_U32(ctx.r10.u32 + 2056, ctx.r3.u32);
	// subf r7,r3,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r3.s64;
	// clrlwi r6,r7,23
	ctx.r6.u64 = ctx.r7.u32 & 0x1FF;
	// stw r6,2052(r10)
	PPC_STORE_U32(ctx.r10.u32 + 2052, ctx.r6.u32);
	// fctidz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f3.f64);
	// stfd f2,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f2.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DDC3C;
	sub_822FCAF8(ctx, base);
	// addis r5,r31,6
	ctx.r5.s64 = ctx.r31.s64 + 393216;
	// addi r5,r5,-21936
	ctx.r5.s64 = ctx.r5.s64 + -21936;
	// lwz r4,2048(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2048);
	// stw r3,2056(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2056, ctx.r3.u32);
	// subf r11,r3,r4
	ctx.r11.s64 = ctx.r4.s64 - ctx.r3.s64;
	// clrlwi r10,r11,23
	ctx.r10.u64 = ctx.r11.u32 & 0x1FF;
	// stw r10,2052(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2052, ctx.r10.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f29,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f30,-40(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f31,-32(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DDC7C"))) PPC_WEAK_FUNC(sub_822DDC7C);
PPC_FUNC_IMPL(__imp__sub_822DDC7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DDC80"))) PPC_WEAK_FUNC(sub_822DDC80);
PPC_FUNC_IMPL(__imp__sub_822DDC80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f29,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f29.u64);
	// stfd f30,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f30.u64);
	// stfd f31,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// fmr f30,f1
	ctx.f30.f64 = ctx.f1.f64;
	// bl 0x822dd9b0
	ctx.lr = 0x822DDCA8;
	sub_822DD9B0(ctx, base);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f0,1816(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1816);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f29,f30,f0
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f0,1956(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1956);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,5268(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5268);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f0,f29,f0,f31
	ctx.f0.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// fctidz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f0.f64);
	// stfd f13,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f13.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DDCD8;
	sub_822FCAF8(ctx, base);
	// addis r8,r31,6
	ctx.r8.s64 = ctx.r31.s64 + 393216;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// addi r8,r8,-19856
	ctx.r8.s64 = ctx.r8.s64 + -19856;
	// lfs f0,1952(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1952);
	ctx.f0.f64 = double(temp.f32);
	// lwz r6,2048(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 2048);
	// fmadds f12,f29,f0,f31
	ctx.f12.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f0.f64), float(ctx.f31.f64)));
	// stw r3,2056(r8)
	PPC_STORE_U32(ctx.r8.u32 + 2056, ctx.r3.u32);
	// subf r5,r3,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r3.s64;
	// clrlwi r4,r5,23
	ctx.r4.u64 = ctx.r5.u32 & 0x1FF;
	// stw r4,2052(r8)
	PPC_STORE_U32(ctx.r8.u32 + 2052, ctx.r4.u32);
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f11.u64);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822fcaf8
	ctx.lr = 0x822DDD10;
	sub_822FCAF8(ctx, base);
	// addis r11,r31,6
	ctx.r11.s64 = ctx.r31.s64 + 393216;
	// addi r11,r11,-17776
	ctx.r11.s64 = ctx.r11.s64 + -17776;
	// addis r9,r31,6
	ctx.r9.s64 = ctx.r31.s64 + 393216;
	// addi r9,r9,-15696
	ctx.r9.s64 = ctx.r9.s64 + -15696;
	// lwz r10,2048(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// stw r3,2056(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2056, ctx.r3.u32);
	// subf r8,r3,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r3.s64;
	// clrlwi r7,r8,23
	ctx.r7.u64 = ctx.r8.u32 & 0x1FF;
	// stw r7,2052(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2052, ctx.r7.u32);
	// lfs f10,2076(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2076);
	ctx.f10.f64 = double(temp.f32);
	// lwz r5,2064(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2064);
	// fdivs f9,f30,f10
	ctx.f9.f64 = double(float(ctx.f30.f64 / ctx.f10.f64));
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// lfd f8,80(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// stfs f30,2076(r9)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2076, temp.u32);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmadds f5,f9,f6,f31
	ctx.f5.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f6.f64), float(ctx.f31.f64)));
	// fctidz f4,f5
	ctx.f4.s64 = (ctx.f5.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f5.f64);
	// stfd f4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f4.u64);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rlwinm r3,r4,1,0,30
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r3,2064(r9)
	PPC_STORE_U32(ctx.r9.u32 + 2064, ctx.r3.u32);
	// stw r3,2068(r9)
	PPC_STORE_U32(ctx.r9.u32 + 2068, ctx.r3.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f29,-40(r1)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f30,-32(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// lfd f31,-24(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DDD90"))) PPC_WEAK_FUNC(sub_822DDD90);
PPC_FUNC_IMPL(__imp__sub_822DDD90) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r5,1
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 1, ctx.xer);
	// bne cr6,0x822ddd9c
	if (!ctx.cr6.eq) goto loc_822DDD9C;
	// b 0x822dd210
	sub_822DD210(ctx, base);
	return;
loc_822DDD9C:
	// cmplwi cr6,r5,2
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 2, ctx.xer);
	// bne cr6,0x822ddda8
	if (!ctx.cr6.eq) goto loc_822DDDA8;
	// b 0x822dd9b0
	sub_822DD9B0(ctx, base);
	return;
loc_822DDDA8:
	// cmplwi cr6,r4,1
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 1, ctx.xer);
	// bne cr6,0x822dddb4
	if (!ctx.cr6.eq) goto loc_822DDDB4;
	// b 0x822dd930
	sub_822DD930(ctx, base);
	return;
loc_822DDDB4:
	// b 0x822ddc80
	sub_822DDC80(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DDDB8"))) PPC_WEAK_FUNC(sub_822DDDB8);
PPC_FUNC_IMPL(__imp__sub_822DDDB8) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// cmplwi cr6,r5,1
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 1, ctx.xer);
	// bne cr6,0x822dddcc
	if (!ctx.cr6.eq) goto loc_822DDDCC;
	// b 0x822dbdd8
	sub_822DBDD8(ctx, base);
	return;
loc_822DDDCC:
	// cmplwi cr6,r5,2
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 2, ctx.xer);
	// bne cr6,0x822dddd8
	if (!ctx.cr6.eq) goto loc_822DDDD8;
	// b 0x822dcb38
	sub_822DCB38(ctx, base);
	return;
loc_822DDDD8:
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x822ddde4
	if (!ctx.cr6.eq) goto loc_822DDDE4;
	// b 0x822dca90
	sub_822DCA90(ctx, base);
	return;
loc_822DDDE4:
	// b 0x822dd118
	sub_822DD118(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DDDE8"))) PPC_WEAK_FUNC(sub_822DDDE8);
PPC_FUNC_IMPL(__imp__sub_822DDDE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DDE18;
	sub_8233EAF0(ctx, base);
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// lis r9,1
	ctx.r9.s64 = 65536;
	// ori r3,r11,16
	ctx.r3.u64 = ctx.r11.u64 | 16;
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lis r7,1
	ctx.r7.s64 = 65536;
	// ori r11,r10,20
	ctx.r11.u64 = ctx.r10.u64 | 20;
	// lis r6,1
	ctx.r6.s64 = 65536;
	// ori r10,r9,24
	ctx.r10.u64 = ctx.r9.u64 | 24;
	// stwx r30,r31,r3
	PPC_STORE_U32(ctx.r31.u32 + ctx.r3.u32, ctx.r30.u32);
	// ori r9,r8,28
	ctx.r9.u64 = ctx.r8.u64 | 28;
	// ori r8,r7,32
	ctx.r8.u64 = ctx.r7.u64 | 32;
	// lis r5,-32249
	ctx.r5.s64 = -2113470464;
	// stwx r30,r31,r11
	PPC_STORE_U32(ctx.r31.u32 + ctx.r11.u32, ctx.r30.u32);
	// ori r7,r6,36
	ctx.r7.u64 = ctx.r6.u64 | 36;
	// lis r4,1
	ctx.r4.s64 = 65536;
	// stwx r30,r31,r10
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, ctx.r30.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// stfsx f31,r31,r9
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, temp.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lfs f0,-28948(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r31,r8
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, temp.u32);
	// stfsx f0,r31,r7
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, temp.u32);
	// stwx r6,r31,r4
	PPC_STORE_U32(ctx.r31.u32 + ctx.r4.u32, ctx.r6.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DDE94"))) PPC_WEAK_FUNC(sub_822DDE94);
PPC_FUNC_IMPL(__imp__sub_822DDE94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DDE98"))) PPC_WEAK_FUNC(sub_822DDE98);
PPC_FUNC_IMPL(__imp__sub_822DDE98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DDEC8;
	sub_8233EAF0(ctx, base);
	// lis r4,-32249
	ctx.r4.s64 = -2113470464;
	// lis r11,2
	ctx.r11.s64 = 131072;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// lis r8,2
	ctx.r8.s64 = 131072;
	// lis r7,2
	ctx.r7.s64 = 131072;
	// lfs f0,-28948(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// lis r3,2
	ctx.r3.s64 = 131072;
	// ori r11,r11,16
	ctx.r11.u64 = ctx.r11.u64 | 16;
	// ori r10,r10,20
	ctx.r10.u64 = ctx.r10.u64 | 20;
	// ori r9,r9,24
	ctx.r9.u64 = ctx.r9.u64 | 24;
	// ori r8,r8,28
	ctx.r8.u64 = ctx.r8.u64 | 28;
	// ori r7,r7,32
	ctx.r7.u64 = ctx.r7.u64 | 32;
	// ori r6,r6,36
	ctx.r6.u64 = ctx.r6.u64 | 36;
	// ori r5,r5,40
	ctx.r5.u64 = ctx.r5.u64 | 40;
	// rlwinm r4,r30,1,0,30
	ctx.r4.u64 = rotl64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// li r30,0
	ctx.r30.s64 = 0;
	// stfsx f31,r31,r8
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r8.u32, temp.u32);
	// stfsx f0,r31,r7
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r7.u32, temp.u32);
	// stwx r4,r31,r11
	PPC_STORE_U32(ctx.r31.u32 + ctx.r11.u32, ctx.r4.u32);
	// stwx r30,r31,r3
	PPC_STORE_U32(ctx.r31.u32 + ctx.r3.u32, ctx.r30.u32);
	// stfsx f0,r31,r6
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r6.u32, temp.u32);
	// stfsx f0,r31,r5
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, temp.u32);
	// stwx r4,r31,r10
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, ctx.r4.u32);
	// stwx r4,r31,r9
	PPC_STORE_U32(ctx.r31.u32 + ctx.r9.u32, ctx.r4.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DDF54"))) PPC_WEAK_FUNC(sub_822DDF54);
PPC_FUNC_IMPL(__imp__sub_822DDF54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DDF58"))) PPC_WEAK_FUNC(sub_822DDF58);
PPC_FUNC_IMPL(__imp__sub_822DDF58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822DDF60;
	__restfpr_28(ctx, base);
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DDF78;
	sub_8233EAF0(ctx, base);
	// lis r11,1
	ctx.r11.s64 = 65536;
	// addis r7,r30,1
	ctx.r7.s64 = ctx.r30.s64 + 65536;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// addis r29,r30,1
	ctx.r29.s64 = ctx.r30.s64 + 65536;
	// lis r9,1
	ctx.r9.s64 = 65536;
	// ori r8,r11,36
	ctx.r8.u64 = ctx.r11.u64 | 36;
	// addi r7,r7,48
	ctx.r7.s64 = ctx.r7.s64 + 48;
	// addi r29,r29,96
	ctx.r29.s64 = ctx.r29.s64 + 96;
	// lfs f31,-28948(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// li r31,0
	ctx.r31.s64 = 0;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stfsx f31,r30,r8
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r30.u32 + ctx.r8.u32, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stwx r31,r30,r9
	PPC_STORE_U32(ctx.r30.u32 + ctx.r9.u32, ctx.r31.u32);
	// stfs f31,16(r7)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r7.u32 + 16, temp.u32);
	// stfs f31,20(r7)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r7.u32 + 20, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DDFC0;
	sub_8233EAF0(ctx, base);
	// addis r28,r30,1
	ctx.r28.s64 = ctx.r30.s64 + 65536;
	// stfs f31,2088(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2088, temp.u32);
	// li r5,512
	ctx.r5.s64 = 512;
	// stfs f31,2092(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2092, temp.u32);
	// addi r28,r28,2208
	ctx.r28.s64 = ctx.r28.s64 + 2208;
	// stfs f31,2096(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2096, temp.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r31,2048(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2048, ctx.r31.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DDFE8;
	sub_8233EAF0(ctx, base);
	// lwz r6,520(r28)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r28.u32 + 520);
	// addis r29,r30,1
	ctx.r29.s64 = ctx.r30.s64 + 65536;
	// stfs f31,532(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 532, temp.u32);
	// neg r11,r6
	ctx.r11.s64 = -ctx.r6.s64;
	// stw r31,512(r28)
	PPC_STORE_U32(ctx.r28.u32 + 512, ctx.r31.u32);
	// addi r29,r29,2752
	ctx.r29.s64 = ctx.r29.s64 + 2752;
	// clrlwi r10,r11,25
	ctx.r10.u64 = ctx.r11.u32 & 0x7F;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r10,516(r28)
	PPC_STORE_U32(ctx.r28.u32 + 516, ctx.r10.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE018;
	sub_8233EAF0(ctx, base);
	// addis r28,r30,1
	ctx.r28.s64 = ctx.r30.s64 + 65536;
	// li r5,1024
	ctx.r5.s64 = 1024;
	// stfs f31,8232(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 8232, temp.u32);
	// addi r28,r28,11008
	ctx.r28.s64 = ctx.r28.s64 + 11008;
	// stfs f31,8236(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 8236, temp.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stfs f31,8240(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 8240, temp.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stw r31,8192(r29)
	PPC_STORE_U32(ctx.r29.u32 + 8192, ctx.r31.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE040;
	sub_8233EAF0(ctx, base);
	// lwz r9,1032(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1032);
	// addis r29,r30,1
	ctx.r29.s64 = ctx.r30.s64 + 65536;
	// stfs f31,1044(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 1044, temp.u32);
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// stw r31,1024(r28)
	PPC_STORE_U32(ctx.r28.u32 + 1024, ctx.r31.u32);
	// addi r29,r29,12064
	ctx.r29.s64 = ctx.r29.s64 + 12064;
	// clrlwi r7,r8,24
	ctx.r7.u64 = ctx.r8.u32 & 0xFF;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r7,1028(r28)
	PPC_STORE_U32(ctx.r28.u32 + 1028, ctx.r7.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE070;
	sub_8233EAF0(ctx, base);
	// lwz r6,2056(r29)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r29.u32 + 2056);
	// addis r28,r30,1
	ctx.r28.s64 = ctx.r30.s64 + 65536;
	// stfs f31,2068(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2068, temp.u32);
	// neg r11,r6
	ctx.r11.s64 = -ctx.r6.s64;
	// stw r31,2048(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2048, ctx.r31.u32);
	// addi r28,r28,14144
	ctx.r28.s64 = ctx.r28.s64 + 14144;
	// clrlwi r10,r11,23
	ctx.r10.u64 = ctx.r11.u32 & 0x1FF;
	// li r5,4096
	ctx.r5.s64 = 4096;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r10,2052(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2052, ctx.r10.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE0A0;
	sub_8233EAF0(ctx, base);
	// addis r29,r30,1
	ctx.r29.s64 = ctx.r30.s64 + 65536;
	// stfs f31,4116(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 4116, temp.u32);
	// addi r29,r29,18272
	ctx.r29.s64 = ctx.r29.s64 + 18272;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stfs f31,4120(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 4120, temp.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r31,4096(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4096, ctx.r31.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE0C4;
	sub_8233EAF0(ctx, base);
	// lwz r9,2056(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 2056);
	// addis r28,r30,1
	ctx.r28.s64 = ctx.r30.s64 + 65536;
	// stfs f31,2068(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2068, temp.u32);
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// stw r31,2048(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2048, ctx.r31.u32);
	// addi r28,r28,20352
	ctx.r28.s64 = ctx.r28.s64 + 20352;
	// clrlwi r7,r8,23
	ctx.r7.u64 = ctx.r8.u32 & 0x1FF;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r7,2052(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2052, ctx.r7.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE0F4;
	sub_8233EAF0(ctx, base);
	// lwz r9,2056(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// addis r6,r30,1
	ctx.r6.s64 = ctx.r30.s64 + 65536;
	// stfs f31,2068(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// addis r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 65536;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// addis r10,r30,1
	ctx.r10.s64 = ctx.r30.s64 + 65536;
	// addi r6,r6,22432
	ctx.r6.s64 = ctx.r6.s64 + 22432;
	// addis r29,r30,1
	ctx.r29.s64 = ctx.r30.s64 + 65536;
	// addi r11,r11,22452
	ctx.r11.s64 = ctx.r11.s64 + 22452;
	// addi r10,r10,22480
	ctx.r10.s64 = ctx.r10.s64 + 22480;
	// clrlwi r7,r8,23
	ctx.r7.u64 = ctx.r8.u32 & 0x1FF;
	// addi r29,r29,22512
	ctx.r29.s64 = ctx.r29.s64 + 22512;
	// stw r7,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r7.u32);
	// stfs f31,0(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// stfs f31,4(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// li r5,16384
	ctx.r5.s64 = 16384;
	// stfs f31,16(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 16, temp.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stfs f31,0(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfs f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stfs f31,24(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// stfs f31,0(r10)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f31,4(r10)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// stfs f31,20(r10)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 20, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE160;
	sub_8233EAF0(ctx, base);
	// addis r6,r30,2
	ctx.r6.s64 = ctx.r30.s64 + 131072;
	// stfs f31,16404(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 16404, temp.u32);
	// addis r28,r30,2
	ctx.r28.s64 = ctx.r30.s64 + 131072;
	// stfs f31,16408(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 16408, temp.u32);
	// addi r6,r6,-26608
	ctx.r6.s64 = ctx.r6.s64 + -26608;
	// stw r31,16384(r29)
	PPC_STORE_U32(ctx.r29.u32 + 16384, ctx.r31.u32);
	// addi r28,r28,-26576
	ctx.r28.s64 = ctx.r28.s64 + -26576;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f31,0(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// stfs f31,4(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// stfs f31,16(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 16, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE198;
	sub_8233EAF0(ctx, base);
	// lwz r11,2056(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// addis r29,r30,2
	ctx.r29.s64 = ctx.r30.s64 + 131072;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stfs f31,2068(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// addi r29,r29,-24496
	ctx.r29.s64 = ctx.r29.s64 + -24496;
	// clrlwi r9,r10,23
	ctx.r9.u64 = ctx.r10.u32 & 0x1FF;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stw r9,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r9.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE1C8;
	sub_8233EAF0(ctx, base);
	// addis r8,r30,2
	ctx.r8.s64 = ctx.r30.s64 + 131072;
	// addis r7,r30,2
	ctx.r7.s64 = ctx.r30.s64 + 131072;
	// stfs f31,2068(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2068, temp.u32);
	// addis r6,r30,2
	ctx.r6.s64 = ctx.r30.s64 + 131072;
	// stw r31,2048(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2048, ctx.r31.u32);
	// addis r28,r30,2
	ctx.r28.s64 = ctx.r30.s64 + 131072;
	// addi r8,r8,-22416
	ctx.r8.s64 = ctx.r8.s64 + -22416;
	// addi r7,r7,-22396
	ctx.r7.s64 = ctx.r7.s64 + -22396;
	// addi r6,r6,-22368
	ctx.r6.s64 = ctx.r6.s64 + -22368;
	// addi r28,r28,-22336
	ctx.r28.s64 = ctx.r28.s64 + -22336;
	// lwz r11,2056(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 2056);
	// li r5,16384
	ctx.r5.s64 = 16384;
	// li r4,0
	ctx.r4.s64 = 0;
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// clrlwi r9,r10,23
	ctx.r9.u64 = ctx.r10.u32 & 0x1FF;
	// stw r9,2052(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2052, ctx.r9.u32);
	// stfs f31,0(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f31,4(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// stfs f31,16(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 16, temp.u32);
	// stfs f31,0(r7)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// stfs f31,4(r7)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// stfs f31,24(r7)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r7.u32 + 24, temp.u32);
	// stfs f31,0(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// stfs f31,4(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// stfs f31,20(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 20, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE234;
	sub_8233EAF0(ctx, base);
	// addis r8,r30,2
	ctx.r8.s64 = ctx.r30.s64 + 131072;
	// stfs f31,16404(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16404, temp.u32);
	// addis r29,r30,2
	ctx.r29.s64 = ctx.r30.s64 + 131072;
	// stfs f31,16408(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16408, temp.u32);
	// addi r8,r8,-5920
	ctx.r8.s64 = ctx.r8.s64 + -5920;
	// stw r31,16384(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16384, ctx.r31.u32);
	// addi r29,r29,-5888
	ctx.r29.s64 = ctx.r29.s64 + -5888;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stfs f31,0(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f31,4(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// stfs f31,16(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 16, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE26C;
	sub_8233EAF0(ctx, base);
	// lwz r7,8200(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8200);
	// addis r28,r30,2
	ctx.r28.s64 = ctx.r30.s64 + 131072;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// stfs f31,8212(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 8212, temp.u32);
	// neg r6,r7
	ctx.r6.s64 = -ctx.r7.s64;
	// stw r31,8192(r29)
	PPC_STORE_U32(ctx.r29.u32 + 8192, ctx.r31.u32);
	// addi r28,r28,2336
	ctx.r28.s64 = ctx.r28.s64 + 2336;
	// clrlwi r11,r6,21
	ctx.r11.u64 = ctx.r6.u32 & 0x7FF;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stw r11,8196(r29)
	PPC_STORE_U32(ctx.r29.u32 + 8196, ctx.r11.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE29C;
	sub_8233EAF0(ctx, base);
	// lwz r10,8200(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8200);
	// addis r29,r30,2
	ctx.r29.s64 = ctx.r30.s64 + 131072;
	// stfs f31,8216(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8216, temp.u32);
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// stw r31,8192(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8192, ctx.r31.u32);
	// addi r29,r29,10560
	ctx.r29.s64 = ctx.r29.s64 + 10560;
	// clrlwi r8,r9,21
	ctx.r8.u64 = ctx.r9.u32 & 0x7FF;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r8,8196(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8196, ctx.r8.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE2CC;
	sub_8233EAF0(ctx, base);
	// lwz r7,8200(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8200);
	// addis r28,r30,2
	ctx.r28.s64 = ctx.r30.s64 + 131072;
	// stfs f31,8212(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 8212, temp.u32);
	// neg r6,r7
	ctx.r6.s64 = -ctx.r7.s64;
	// stw r31,8192(r29)
	PPC_STORE_U32(ctx.r29.u32 + 8192, ctx.r31.u32);
	// addi r28,r28,18784
	ctx.r28.s64 = ctx.r28.s64 + 18784;
	// clrlwi r11,r6,21
	ctx.r11.u64 = ctx.r6.u32 & 0x7FF;
	// li r5,4096
	ctx.r5.s64 = 4096;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,8196(r29)
	PPC_STORE_U32(ctx.r29.u32 + 8196, ctx.r11.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE2FC;
	sub_8233EAF0(ctx, base);
	// lwz r10,4104(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4104);
	// addis r29,r30,2
	ctx.r29.s64 = ctx.r30.s64 + 131072;
	// stfs f31,4116(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 4116, temp.u32);
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// stw r31,4096(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4096, ctx.r31.u32);
	// addi r29,r29,22912
	ctx.r29.s64 = ctx.r29.s64 + 22912;
	// clrlwi r8,r9,22
	ctx.r8.u64 = ctx.r9.u32 & 0x3FF;
	// li r5,16384
	ctx.r5.s64 = 16384;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r8,4100(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4100, ctx.r8.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE32C;
	sub_8233EAF0(ctx, base);
	// addis r7,r30,3
	ctx.r7.s64 = ctx.r30.s64 + 196608;
	// addi r7,r7,-26208
	ctx.r7.s64 = ctx.r7.s64 + -26208;
	// addis r28,r30,3
	ctx.r28.s64 = ctx.r30.s64 + 196608;
	// stfs f31,16404(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 16404, temp.u32);
	// li r5,8192
	ctx.r5.s64 = 8192;
	// stfs f31,16408(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 16408, temp.u32);
	// addi r28,r28,-26176
	ctx.r28.s64 = ctx.r28.s64 + -26176;
	// stw r31,16384(r29)
	PPC_STORE_U32(ctx.r29.u32 + 16384, ctx.r31.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stfs f31,0(r7)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f31,4(r7)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// stfs f31,20(r7)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r7.u32 + 20, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE364;
	sub_8233EAF0(ctx, base);
	// lwz r6,8200(r28)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8200);
	// addis r29,r30,3
	ctx.r29.s64 = ctx.r30.s64 + 196608;
	// stfs f31,8212(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8212, temp.u32);
	// neg r11,r6
	ctx.r11.s64 = -ctx.r6.s64;
	// stw r31,8192(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8192, ctx.r31.u32);
	// addi r29,r29,-17952
	ctx.r29.s64 = ctx.r29.s64 + -17952;
	// clrlwi r10,r11,21
	ctx.r10.u64 = ctx.r11.u32 & 0x7FF;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r10,8196(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8196, ctx.r10.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE394;
	sub_8233EAF0(ctx, base);
	// lwz r9,8200(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8200);
	// addis r28,r30,3
	ctx.r28.s64 = ctx.r30.s64 + 196608;
	// stfs f31,8216(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 8216, temp.u32);
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// stw r31,8192(r29)
	PPC_STORE_U32(ctx.r29.u32 + 8192, ctx.r31.u32);
	// addi r28,r28,-9728
	ctx.r28.s64 = ctx.r28.s64 + -9728;
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r7,8196(r29)
	PPC_STORE_U32(ctx.r29.u32 + 8196, ctx.r7.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE3C4;
	sub_8233EAF0(ctx, base);
	// lwz r6,8200(r28)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8200);
	// addis r29,r30,3
	ctx.r29.s64 = ctx.r30.s64 + 196608;
	// stfs f31,8212(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8212, temp.u32);
	// neg r11,r6
	ctx.r11.s64 = -ctx.r6.s64;
	// stw r31,8192(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8192, ctx.r31.u32);
	// addi r29,r29,-1504
	ctx.r29.s64 = ctx.r29.s64 + -1504;
	// clrlwi r10,r11,21
	ctx.r10.u64 = ctx.r11.u32 & 0x7FF;
	// li r5,4096
	ctx.r5.s64 = 4096;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r10,8196(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8196, ctx.r10.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE3F4;
	sub_8233EAF0(ctx, base);
	// lwz r9,4104(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4104);
	// addis r28,r30,3
	ctx.r28.s64 = ctx.r30.s64 + 196608;
	// stfs f31,4116(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 4116, temp.u32);
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// stw r31,4096(r29)
	PPC_STORE_U32(ctx.r29.u32 + 4096, ctx.r31.u32);
	// addi r28,r28,2624
	ctx.r28.s64 = ctx.r28.s64 + 2624;
	// clrlwi r7,r8,22
	ctx.r7.u64 = ctx.r8.u32 & 0x3FF;
	// li r5,16384
	ctx.r5.s64 = 16384;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r7,4100(r29)
	PPC_STORE_U32(ctx.r29.u32 + 4100, ctx.r7.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE424;
	sub_8233EAF0(ctx, base);
	// addis r6,r30,3
	ctx.r6.s64 = ctx.r30.s64 + 196608;
	// addis r29,r30,3
	ctx.r29.s64 = ctx.r30.s64 + 196608;
	// stfs f31,16404(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16404, temp.u32);
	// addi r6,r6,19040
	ctx.r6.s64 = ctx.r6.s64 + 19040;
	// stfs f31,16408(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16408, temp.u32);
	// addi r29,r29,19072
	ctx.r29.s64 = ctx.r29.s64 + 19072;
	// stw r31,16384(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16384, ctx.r31.u32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stfs f31,0(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// stfs f31,4(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// stfs f31,20(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 20, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE45C;
	sub_8233EAF0(ctx, base);
	// addis r30,r30,3
	ctx.r30.s64 = ctx.r30.s64 + 196608;
	// stfs f31,2068(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2068, temp.u32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stw r31,2048(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2048, ctx.r31.u32);
	// addi r30,r30,21152
	ctx.r30.s64 = ctx.r30.s64 + 21152;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r11,2056(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 2056);
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// clrlwi r9,r10,23
	ctx.r9.u64 = ctx.r10.u32 & 0x1FF;
	// stw r9,2052(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2052, ctx.r9.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE48C;
	sub_8233EAF0(ctx, base);
	// lwz r8,2056(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 2056);
	// stfs f31,2068(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r30.u32 + 2068, temp.u32);
	// stw r31,2048(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2048, ctx.r31.u32);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// clrlwi r6,r7,23
	ctx.r6.u64 = ctx.r7.u32 & 0x1FF;
	// stw r6,2052(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2052, ctx.r6.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DE4B0"))) PPC_WEAK_FUNC(sub_822DE4B0);
PPC_FUNC_IMPL(__imp__sub_822DE4B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x822ddf58
	ctx.lr = 0x822DE4C8;
	sub_822DDF58(ctx, base);
	// addis r31,r31,3
	ctx.r31.s64 = ctx.r31.s64 + 196608;
	// li r5,1024
	ctx.r5.s64 = 1024;
	// addi r31,r31,23248
	ctx.r31.s64 = ctx.r31.s64 + 23248;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE4E0;
	sub_8233EAF0(ctx, base);
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,1024(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1024, ctx.r10.u32);
	// lfs f0,-28948(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,1060(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 1060, temp.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822DE508"))) PPC_WEAK_FUNC(sub_822DE508);
PPC_FUNC_IMPL(__imp__sub_822DE508) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822DE510;
	__restfpr_28(ctx, base);
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// bl 0x822ddf58
	ctx.lr = 0x822DE520;
	sub_822DDF58(ctx, base);
	// addis r29,r30,3
	ctx.r29.s64 = ctx.r30.s64 + 196608;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// addi r29,r29,23248
	ctx.r29.s64 = ctx.r29.s64 + 23248;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE538;
	sub_8233EAF0(ctx, base);
	// lis r11,2
	ctx.r11.s64 = 131072;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// ori r7,r11,36
	ctx.r7.u64 = ctx.r11.u64 | 36;
	// addis r11,r30,5
	ctx.r11.s64 = ctx.r30.s64 + 327680;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// addis r28,r30,5
	ctx.r28.s64 = ctx.r30.s64 + 327680;
	// lis r8,2
	ctx.r8.s64 = 131072;
	// ori r6,r10,40
	ctx.r6.u64 = ctx.r10.u64 | 40;
	// addi r11,r11,23296
	ctx.r11.s64 = ctx.r11.s64 + 23296;
	// addi r28,r28,23344
	ctx.r28.s64 = ctx.r28.s64 + 23344;
	// lfs f31,-28948(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// li r31,0
	ctx.r31.s64 = 0;
	// stfsx f31,r29,r7
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + ctx.r7.u32, temp.u32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stfsx f31,r29,r6
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + ctx.r6.u32, temp.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stwx r31,r29,r8
	PPC_STORE_U32(ctx.r29.u32 + ctx.r8.u32, ctx.r31.u32);
	// stfs f31,16(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f31,28(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 28, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE58C;
	sub_8233EAF0(ctx, base);
	// addis r29,r30,5
	ctx.r29.s64 = ctx.r30.s64 + 327680;
	// stfs f31,2088(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2088, temp.u32);
	// li r5,512
	ctx.r5.s64 = 512;
	// stfs f31,2092(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2092, temp.u32);
	// addi r29,r29,25456
	ctx.r29.s64 = ctx.r29.s64 + 25456;
	// stfs f31,2096(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2096, temp.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE5B4;
	sub_8233EAF0(ctx, base);
	// lwz r10,520(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 520);
	// addis r28,r30,5
	ctx.r28.s64 = ctx.r30.s64 + 327680;
	// stfs f31,532(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 532, temp.u32);
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// stw r31,512(r29)
	PPC_STORE_U32(ctx.r29.u32 + 512, ctx.r31.u32);
	// addi r28,r28,26000
	ctx.r28.s64 = ctx.r28.s64 + 26000;
	// clrlwi r8,r9,25
	ctx.r8.u64 = ctx.r9.u32 & 0x7F;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r8,516(r29)
	PPC_STORE_U32(ctx.r29.u32 + 516, ctx.r8.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE5E4;
	sub_8233EAF0(ctx, base);
	// addis r29,r30,6
	ctx.r29.s64 = ctx.r30.s64 + 393216;
	// li r5,1024
	ctx.r5.s64 = 1024;
	// stfs f31,8232(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8232, temp.u32);
	// addi r29,r29,-31280
	ctx.r29.s64 = ctx.r29.s64 + -31280;
	// stfs f31,8236(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8236, temp.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stfs f31,8240(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8240, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stw r31,8192(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8192, ctx.r31.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE60C;
	sub_8233EAF0(ctx, base);
	// lwz r7,1032(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1032);
	// addis r28,r30,6
	ctx.r28.s64 = ctx.r30.s64 + 393216;
	// stfs f31,1044(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 1044, temp.u32);
	// neg r6,r7
	ctx.r6.s64 = -ctx.r7.s64;
	// stw r31,1024(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1024, ctx.r31.u32);
	// addi r28,r28,-30224
	ctx.r28.s64 = ctx.r28.s64 + -30224;
	// clrlwi r11,r6,24
	ctx.r11.u64 = ctx.r6.u32 & 0xFF;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,1028(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1028, ctx.r11.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE63C;
	sub_8233EAF0(ctx, base);
	// addis r29,r30,6
	ctx.r29.s64 = ctx.r30.s64 + 393216;
	// lwz r10,2056(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// stfs f31,2068(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// addi r29,r29,-28144
	ctx.r29.s64 = ctx.r29.s64 + -28144;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// li r5,4096
	ctx.r5.s64 = 4096;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// clrlwi r8,r9,23
	ctx.r8.u64 = ctx.r9.u32 & 0x1FF;
	// stw r8,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r8.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE66C;
	sub_8233EAF0(ctx, base);
	// addis r28,r30,6
	ctx.r28.s64 = ctx.r30.s64 + 393216;
	// stfs f31,4116(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 4116, temp.u32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stfs f31,4120(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 4120, temp.u32);
	// addi r28,r28,-24016
	ctx.r28.s64 = ctx.r28.s64 + -24016;
	// stw r31,4096(r29)
	PPC_STORE_U32(ctx.r29.u32 + 4096, ctx.r31.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE690;
	sub_8233EAF0(ctx, base);
	// lwz r7,2056(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// addis r30,r30,6
	ctx.r30.s64 = ctx.r30.s64 + 393216;
	// stfs f31,2068(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// neg r6,r7
	ctx.r6.s64 = -ctx.r7.s64;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// addi r30,r30,-21936
	ctx.r30.s64 = ctx.r30.s64 + -21936;
	// clrlwi r11,r6,23
	ctx.r11.u64 = ctx.r6.u32 & 0x1FF;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r11.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE6C0;
	sub_8233EAF0(ctx, base);
	// lwz r10,2056(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 2056);
	// stfs f31,2068(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r30.u32 + 2068, temp.u32);
	// stw r31,2048(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2048, ctx.r31.u32);
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// clrlwi r8,r9,23
	ctx.r8.u64 = ctx.r9.u32 & 0x1FF;
	// stw r8,2052(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2052, ctx.r8.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DE6E4"))) PPC_WEAK_FUNC(sub_822DE6E4);
PPC_FUNC_IMPL(__imp__sub_822DE6E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DE6E8"))) PPC_WEAK_FUNC(sub_822DE6E8);
PPC_FUNC_IMPL(__imp__sub_822DE6E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822DE6F0;
	__restfpr_28(ctx, base);
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// bl 0x822de508
	ctx.lr = 0x822DE700;
	sub_822DE508(ctx, base);
	// addis r29,r30,6
	ctx.r29.s64 = ctx.r30.s64 + 393216;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// addi r29,r29,-19856
	ctx.r29.s64 = ctx.r29.s64 + -19856;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE718;
	sub_8233EAF0(ctx, base);
	// lwz r10,2056(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 2056);
	// addis r28,r30,6
	ctx.r28.s64 = ctx.r30.s64 + 393216;
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// addi r28,r28,-17776
	ctx.r28.s64 = ctx.r28.s64 + -17776;
	// clrlwi r8,r9,23
	ctx.r8.u64 = ctx.r9.u32 & 0x1FF;
	// li r31,0
	ctx.r31.s64 = 0;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// lfs f31,-28948(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stfs f31,2068(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2068, temp.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stw r31,2048(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2048, ctx.r31.u32);
	// stw r8,2052(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2052, ctx.r8.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822DE754;
	sub_8233EAF0(ctx, base);
	// lwz r7,2056(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// addis r30,r30,6
	ctx.r30.s64 = ctx.r30.s64 + 393216;
	// stfs f31,2068(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// neg r6,r7
	ctx.r6.s64 = -ctx.r7.s64;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// addi r30,r30,-15696
	ctx.r30.s64 = ctx.r30.s64 + -15696;
	// clrlwi r11,r6,23
	ctx.r11.u64 = ctx.r6.u32 & 0x1FF;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r11.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822DE784;
	sub_8233EAF0(ctx, base);
	// stfs f31,2084(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r30.u32 + 2084, temp.u32);
	// stfs f31,2088(r30)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r30.u32 + 2088, temp.u32);
	// stw r31,2048(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2048, ctx.r31.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DE79C"))) PPC_WEAK_FUNC(sub_822DE79C);
PPC_FUNC_IMPL(__imp__sub_822DE79C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822DE7A0"))) PPC_WEAK_FUNC(sub_822DE7A0);
PPC_FUNC_IMPL(__imp__sub_822DE7A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x822DE7A8;
	__restfpr_14(ctx, base);
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa30
	ctx.lr = 0x822DE7B0;
	sub_8233FA30(ctx, base);
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r6
	ctx.r24.u64 = ctx.r6.u64;
	// stw r5,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, ctx.r5.u32);
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = ctx.f1.f64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// bl 0x822db8f8
	ctx.lr = 0x822DE7DC;
	sub_822DB8F8(ctx, base);
	// addis r3,r31,1
	ctx.r3.s64 = ctx.r31.s64 + 65536;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// addi r3,r3,48
	ctx.r3.s64 = ctx.r3.s64 + 48;
	// bl 0x822db320
	ctx.lr = 0x822DE7F0;
	sub_822DB320(ctx, base);
	// lis r11,3
	ctx.r11.s64 = 196608;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// ori r9,r11,23232
	ctx.r9.u64 = ctx.r11.u64 | 23232;
	// lfs f10,-28948(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f10.f64 = double(temp.f32);
	// lwzx r8,r31,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r9.u32);
	// fcmpu cr6,f31,f10
	ctx.cr6.compare(ctx.f31.f64, ctx.f10.f64);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// ble cr6,0x822de818
	if (!ctx.cr6.gt) goto loc_822DE818;
	// fmr f12,f10
	ctx.f12.f64 = ctx.f10.f64;
loc_822DE818:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x822df628
	if (ctx.cr6.eq) goto loc_822DF628;
	// addis r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 65536;
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// stw r24,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r24.u32);
	// addis r9,r31,1
	ctx.r9.s64 = ctx.r31.s64 + 65536;
	// addi r11,r11,12052
	ctx.r11.s64 = ctx.r11.s64 + 12052;
	// addi r10,r10,14132
	ctx.r10.s64 = ctx.r10.s64 + 14132;
	// addi r9,r9,80
	ctx.r9.s64 = ctx.r9.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// addis r8,r31,1
	ctx.r8.s64 = ctx.r31.s64 + 65536;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// addis r3,r31,3
	ctx.r3.s64 = ctx.r31.s64 + 196608;
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// addi r8,r8,84
	ctx.r8.s64 = ctx.r8.s64 + 84;
	// addi r3,r3,23236
	ctx.r3.s64 = ctx.r3.s64 + 23236;
	// addis r7,r31,1
	ctx.r7.s64 = ctx.r31.s64 + 65536;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// addis r6,r31,1
	ctx.r6.s64 = ctx.r31.s64 + 65536;
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// addis r11,r31,3
	ctx.r11.s64 = ctx.r31.s64 + 196608;
	// addis r10,r31,3
	ctx.r10.s64 = ctx.r31.s64 + 196608;
	// addis r9,r31,3
	ctx.r9.s64 = ctx.r31.s64 + 196608;
	// subf r8,r28,r29
	ctx.r8.s64 = ctx.r29.s64 - ctx.r28.s64;
	// addi r7,r7,18260
	ctx.r7.s64 = ctx.r7.s64 + 18260;
	// addi r6,r6,18264
	ctx.r6.s64 = ctx.r6.s64 + 18264;
	// stw r8,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r8.u32);
	// addi r11,r11,23240
	ctx.r11.s64 = ctx.r11.s64 + 23240;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// addi r10,r10,21140
	ctx.r10.s64 = ctx.r10.s64 + 21140;
	// stw r6,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r6.u32);
	// addi r9,r9,23220
	ctx.r9.s64 = ctx.r9.s64 + 23220;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r11.u32);
	// lis r3,-32256
	ctx.r3.s64 = -2113929216;
	// stw r10,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r10.u32);
	// addis r23,r31,1
	ctx.r23.s64 = ctx.r31.s64 + 65536;
	// stw r9,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r9.u32);
	// addis r22,r31,1
	ctx.r22.s64 = ctx.r31.s64 + 65536;
	// addis r21,r31,1
	ctx.r21.s64 = ctx.r31.s64 + 65536;
	// addis r20,r31,1
	ctx.r20.s64 = ctx.r31.s64 + 65536;
	// addis r19,r31,1
	ctx.r19.s64 = ctx.r31.s64 + 65536;
	// lfs f6,5256(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 5256);
	ctx.f6.f64 = double(temp.f32);
	// addis r18,r31,1
	ctx.r18.s64 = ctx.r31.s64 + 65536;
	// addis r17,r31,1
	ctx.r17.s64 = ctx.r31.s64 + 65536;
	// addis r5,r31,3
	ctx.r5.s64 = ctx.r31.s64 + 196608;
	// addis r4,r31,3
	ctx.r4.s64 = ctx.r31.s64 + 196608;
	// addi r23,r23,2184
	ctx.r23.s64 = ctx.r23.s64 + 2184;
	// addi r22,r22,2188
	ctx.r22.s64 = ctx.r22.s64 + 2188;
	// addi r21,r21,2192
	ctx.r21.s64 = ctx.r21.s64 + 2192;
	// stw r23,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r23.u32);
	// addi r20,r20,2740
	ctx.r20.s64 = ctx.r20.s64 + 2740;
	// stw r22,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r22.u32);
	// addi r19,r19,10984
	ctx.r19.s64 = ctx.r19.s64 + 10984;
	// stw r21,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r21.u32);
	// addi r18,r18,10988
	ctx.r18.s64 = ctx.r18.s64 + 10988;
	// stw r20,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r20.u32);
	// addi r17,r17,10992
	ctx.r17.s64 = ctx.r17.s64 + 10992;
	// stw r19,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r19.u32);
	// addi r5,r5,19072
	ctx.r5.s64 = ctx.r5.s64 + 19072;
	// stw r18,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r18.u32);
	// addi r4,r4,21152
	ctx.r4.s64 = ctx.r4.s64 + 21152;
	// stw r17,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r17.u32);
	// addis r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 65536;
	// stw r5,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r5.u32);
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// stw r4,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r4.u32);
	// addis r9,r31,1
	ctx.r9.s64 = ctx.r31.s64 + 65536;
	// addis r8,r31,1
	ctx.r8.s64 = ctx.r31.s64 + 65536;
	// addis r7,r31,1
	ctx.r7.s64 = ctx.r31.s64 + 65536;
	// addis r6,r31,1
	ctx.r6.s64 = ctx.r31.s64 + 65536;
	// addi r11,r11,96
	ctx.r11.s64 = ctx.r11.s64 + 96;
	// addi r10,r10,2752
	ctx.r10.s64 = ctx.r10.s64 + 2752;
	// addi r9,r9,14144
	ctx.r9.s64 = ctx.r9.s64 + 14144;
	// addi r8,r8,2208
	ctx.r8.s64 = ctx.r8.s64 + 2208;
	// addi r7,r7,11008
	ctx.r7.s64 = ctx.r7.s64 + 11008;
	// addi r6,r6,12064
	ctx.r6.s64 = ctx.r6.s64 + 12064;
	// lis r30,-32256
	ctx.r30.s64 = -2113929216;
	// lis r29,-32255
	ctx.r29.s64 = -2113863680;
	// lis r28,3
	ctx.r28.s64 = 196608;
	// lis r26,1
	ctx.r26.s64 = 65536;
	// lis r25,1
	ctx.r25.s64 = 65536;
	// lis r3,1
	ctx.r3.s64 = 65536;
	// lis r16,2
	ctx.r16.s64 = 131072;
	// lfs f7,5260(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 5260);
	ctx.f7.f64 = double(temp.f32);
	// lfs f8,-1560(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -1560);
	ctx.f8.f64 = double(temp.f32);
	// ori r27,r28,19032
	ctx.r27.u64 = ctx.r28.u64 | 19032;
	// ori r30,r3,59648
	ctx.r30.u64 = ctx.r3.u64 | 59648;
	// ori r28,r26,18272
	ctx.r28.u64 = ctx.r26.u64 | 18272;
	// ori r29,r25,38960
	ctx.r29.u64 = ctx.r25.u64 | 38960;
	// ori r3,r16,39360
	ctx.r3.u64 = ctx.r16.u64 | 39360;
loc_822DE984:
	// lwz r26,2048(r11)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// lfs f9,0(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lwz r25,2064(r11)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2064);
	// lfs f5,0(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lwz r16,2068(r11)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2068);
	// rlwinm r15,r26,2,0,29
	ctx.r15.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r25,r25,r26
	ctx.r25.s64 = ctx.r26.s64 - ctx.r25.s64;
	// lwz r14,2072(r11)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2072);
	// subf r16,r16,r26
	ctx.r16.s64 = ctx.r26.s64 - ctx.r16.s64;
	// lfs f0,2076(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2076);
	ctx.f0.f64 = double(temp.f32);
	// subf r26,r14,r26
	ctx.r26.s64 = ctx.r26.s64 - ctx.r14.s64;
	// fadds f2,f5,f9
	ctx.f2.f64 = double(float(ctx.f5.f64 + ctx.f9.f64));
	// rlwinm r25,r25,2,21,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0x7FC;
	// lfs f13,2080(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2080);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r16,r16,2,21,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0x7FC;
	// lfs f11,2084(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2084);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r26,r26,2,21,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0x7FC;
	// lfs f4,0(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,0(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// lfsx f1,r25,r11
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r11.u32);
	ctx.f1.f64 = double(temp.f32);
	// lfsx f9,r16,r11
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r11.u32);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f5,f0,f1
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// lfsx f1,r26,r11
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r11.u32);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f0,f13,f9
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fmuls f13,f11,f1
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f1.f64));
	// stfs f5,2088(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2088, temp.u32);
	// stfs f0,2092(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2092, temp.u32);
	// stfs f13,2096(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2096, temp.u32);
	// stfsx f3,r15,r11
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r15.u32 + ctx.r11.u32, temp.u32);
	// lwz r26,2048(r11)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// clrlwi r26,r26,23
	ctx.r26.u64 = ctx.r26.u32 & 0x1FF;
	// stw r26,2048(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2048, ctx.r26.u32);
	// lfs f11,524(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 524);
	ctx.f11.f64 = double(temp.f32);
	// lwz r26,512(r8)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r8.u32 + 512);
	// lfs f9,0(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lwz r25,516(r8)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r8.u32 + 516);
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r26,r26,2,0,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f5,r25,r8
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r8.u32);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,528(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 528);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f2,f11,f5,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f5.f64), float(ctx.f2.f64)));
	// stfsx f2,r26,r8
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r26.u32 + ctx.r8.u32, temp.u32);
	// lwz r25,512(r8)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r8.u32 + 512);
	// lwz r26,516(r8)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r8.u32 + 516);
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// fmadds f1,f3,f2,f5
	ctx.f1.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f2.f64), float(ctx.f5.f64)));
	// clrlwi r26,r26,25
	ctx.r26.u64 = ctx.r26.u32 & 0x7F;
	// stfs f1,532(r8)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r8.u32 + 532, temp.u32);
	// clrlwi r25,r25,25
	ctx.r25.u64 = ctx.r25.u32 & 0x7F;
	// stw r26,516(r8)
	PPC_STORE_U32(ctx.r8.u32 + 516, ctx.r26.u32);
	// stw r25,512(r8)
	PPC_STORE_U32(ctx.r8.u32 + 512, ctx.r25.u32);
	// lfs f5,0(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,0(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// lwz r25,8192(r10)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r15,8208(r10)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8208);
	// subf r15,r15,r25
	ctx.r15.s64 = ctx.r25.s64 - ctx.r15.s64;
	// lwz r16,8216(r10)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8216);
	// rlwinm r15,r15,2,19,29
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0x1FFC;
	// fadds f1,f3,f5
	ctx.f1.f64 = double(float(ctx.f3.f64 + ctx.f5.f64));
	// lfs f0,8220(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8220);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,8224(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8224);
	ctx.f11.f64 = double(temp.f32);
	// lwz r26,8212(r10)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8212);
	// lfs f2,8228(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8228);
	ctx.f2.f64 = double(temp.f32);
	// subf r26,r26,r25
	ctx.r26.s64 = ctx.r25.s64 - ctx.r26.s64;
	// lfsx f3,r15,r10
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + ctx.r10.u32);
	ctx.f3.f64 = double(temp.f32);
	// subf r16,r16,r25
	ctx.r16.s64 = ctx.r25.s64 - ctx.r16.s64;
	// rlwinm r26,r26,2,19,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0x1FFC;
	// lfs f13,0(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r16,r16,2,19,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0x1FFC;
	// lfsx f5,r26,r10
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r10.u32);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f11,f5,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fmuls f0,f3,f0
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfsx f3,r16,r10
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r10.u32);
	ctx.f3.f64 = double(temp.f32);
	// rlwinm r26,r25,2,0,29
	ctx.r26.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f5,f2,f3
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// lwz r25,84(r1)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stfs f0,8232(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8232, temp.u32);
	// stfs f11,8236(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8236, temp.u32);
	// stfs f5,8240(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8240, temp.u32);
	// stfsx f9,r26,r10
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r26.u32 + ctx.r10.u32, temp.u32);
	// lwz r26,8192(r10)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// std r10,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r10.u64);
	// clrlwi r26,r26,21
	ctx.r26.u64 = ctx.r26.u32 & 0x7FF;
	// lwz r16,88(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r15,92(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// stw r26,8192(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8192, ctx.r26.u32);
	// lfs f3,1036(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1036);
	ctx.f3.f64 = double(temp.f32);
	// lwz r26,1024(r7)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r7.u32 + 1024);
	// lfs f2,1040(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1040);
	ctx.f2.f64 = double(temp.f32);
	// lwz r10,1028(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 1028);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r10,r7
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r7.u32);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f0,f3,f1
	ctx.f3.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f3.f64), float(ctx.f1.f64)));
	// rlwinm r26,r26,2,0,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f5,0(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f2,f2,f3,f0
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f3.f64), float(ctx.f0.f64)));
	// lwz r14,96(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// std r11,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r11.u64);
	// std r8,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r8.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// stfsx f3,r26,r7
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r26.u32 + ctx.r7.u32, temp.u32);
	// lwz r26,1028(r7)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r7.u32 + 1028);
	// stfs f2,1044(r7)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r7.u32 + 1044, temp.u32);
	// addi r25,r26,1
	ctx.r25.s64 = ctx.r26.s64 + 1;
	// lwz r26,1024(r7)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r7.u32 + 1024);
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// clrlwi r26,r26,24
	ctx.r26.u64 = ctx.r26.u32 & 0xFF;
	// lwz r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// clrlwi r25,r25,24
	ctx.r25.u64 = ctx.r25.u32 & 0xFF;
	// stw r26,1024(r7)
	PPC_STORE_U32(ctx.r7.u32 + 1024, ctx.r26.u32);
	// stw r25,1028(r7)
	PPC_STORE_U32(ctx.r7.u32 + 1028, ctx.r25.u32);
	// lfs f3,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// lwz r25,2052(r6)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r6.u32 + 2052);
	// lfs f2,2064(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 2064);
	ctx.f2.f64 = double(temp.f32);
	// lwz r26,2048(r6)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r6.u32 + 2048);
	// rlwinm r26,r26,2,0,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f1,2060(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 2060);
	ctx.f1.f64 = double(temp.f32);
	// lfsx f0,r25,r6
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r6.u32);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f11,f1,f0,f5
	ctx.f11.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f0.f64), float(ctx.f5.f64)));
	// stfsx f11,r26,r6
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r26.u32 + ctx.r6.u32, temp.u32);
	// lwz r26,2052(r6)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r6.u32 + 2052);
	// lwz r25,2048(r6)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r6.u32 + 2048);
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// fmadds f2,f11,f2,f0
	ctx.f2.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f2.f64), float(ctx.f0.f64)));
	// clrlwi r25,r25,23
	ctx.r25.u64 = ctx.r25.u32 & 0x1FF;
	// stfs f2,2068(r6)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r6.u32 + 2068, temp.u32);
	// clrlwi r26,r26,23
	ctx.r26.u64 = ctx.r26.u32 & 0x1FF;
	// stw r25,2048(r6)
	PPC_STORE_U32(ctx.r6.u32 + 2048, ctx.r25.u32);
	// stw r26,2052(r6)
	PPC_STORE_U32(ctx.r6.u32 + 2052, ctx.r26.u32);
	// lfs f1,4108(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4108);
	ctx.f1.f64 = double(temp.f32);
	// lwz r26,0(r15)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	// lfs f2,4112(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4112);
	ctx.f2.f64 = double(temp.f32);
	// lwz r15,0(r14)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	// lfs f0,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r25,4096(r9)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4096);
	// lfs f11,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lwz r14,4100(r9)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4100);
	// stw r15,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r15.u32);
	// stw r26,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r26.u32);
	// lfs f30,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f30.f64 = double(temp.f32);
	// subf r26,r14,r25
	ctx.r26.s64 = ctx.r25.s64 - ctx.r14.s64;
	// lwz r16,4104(r9)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4104);
	// subf r16,r16,r25
	ctx.r16.s64 = ctx.r25.s64 - ctx.r16.s64;
	// lfs f29,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f29.f64 = double(temp.f32);
	// rlwinm r26,r26,2,20,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFC;
	// fmuls f9,f29,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f9.f64));
	// rlwinm r16,r16,2,20,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0xFFC;
	// fmuls f5,f30,f5
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f5.f64));
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f29,r26,r9
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r9.u32);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f1,f29,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// lfsx f29,r16,r9
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r9.u32);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f2,f29,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f2.f64));
	// stfs f1,4116(r9)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4116, temp.u32);
	// lwz r16,116(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// stfs f2,4120(r9)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4120, temp.u32);
	// stfsx f3,r25,r9
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r25.u32 + ctx.r9.u32, temp.u32);
	// fadds f1,f5,f9
	ctx.f1.f64 = double(float(ctx.f5.f64 + ctx.f9.f64));
	// lwz r26,4096(r9)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4096);
	// lwz r25,120(r1)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// ld r11,176(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// ld r8,184(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 184);
	// clrlwi r26,r26,22
	ctx.r26.u64 = ctx.r26.u32 & 0x3FF;
	// ld r10,192(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// stw r26,4096(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4096, ctx.r26.u32);
	// lwz r26,0(r25)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// fadds f9,f1,f4
	ctx.f9.f64 = double(float(ctx.f1.f64 + ctx.f4.f64));
	// lwz r26,0(r16)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	// stw r26,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r26.u32);
	// lfs f5,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f9,f9,f5
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// bne cr6,0x822df534
	if (!ctx.cr6.eq) goto loc_822DF534;
	// addis r4,r31,1
	ctx.r4.s64 = ctx.r31.s64 + 65536;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// addi r4,r4,20352
	ctx.r4.s64 = ctx.r4.s64 + 20352;
	// lis r26,1
	ctx.r26.s64 = 65536;
	// ori r5,r5,20340
	ctx.r5.u64 = ctx.r5.u64 | 20340;
	// ori r26,r26,22420
	ctx.r26.u64 = ctx.r26.u64 | 22420;
	// addis r23,r31,1
	ctx.r23.s64 = ctx.r31.s64 + 65536;
	// lwz r25,2052(r4)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2052);
	// lfs f10,2060(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2060);
	ctx.f10.f64 = double(temp.f32);
	// lwz r24,2048(r4)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// lfs f5,2064(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2064);
	ctx.f5.f64 = double(temp.f32);
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f4,r31,r5
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r5.u32);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r24,r24,2,0,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f3,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f3.f64 = double(temp.f32);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// fadds f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 + ctx.f13.f64));
	// addis r26,r31,1
	ctx.r26.s64 = ctx.r31.s64 + 65536;
	// addi r23,r23,22452
	ctx.r23.s64 = ctx.r23.s64 + 22452;
	// lfsx f1,r25,r4
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r4.u32);
	ctx.f1.f64 = double(temp.f32);
	// ori r25,r5,22448
	ctx.r25.u64 = ctx.r5.u64 | 22448;
	// fmadds f10,f10,f1,f4
	ctx.f10.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f1.f64), float(ctx.f4.f64)));
	// stfsx f10,r24,r4
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r24.u32 + ctx.r4.u32, temp.u32);
	// addi r26,r26,22432
	ctx.r26.s64 = ctx.r26.s64 + 22432;
	// addis r22,r31,2
	ctx.r22.s64 = ctx.r31.s64 + 131072;
	// addi r22,r22,-26608
	ctx.r22.s64 = ctx.r22.s64 + -26608;
	// fmadds f5,f5,f10,f1
	ctx.f5.f64 = double(std::fma(float(ctx.f5.f64), float(ctx.f10.f64), float(ctx.f1.f64)));
	// stfs f5,2068(r4)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r4.u32 + 2068, temp.u32);
	// lwz r5,2048(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// addi r24,r5,1
	ctx.r24.s64 = ctx.r5.s64 + 1;
	// lwz r5,2052(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2052);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r5,r5,23
	ctx.r5.u64 = ctx.r5.u32 & 0x1FF;
	// clrlwi r24,r24,23
	ctx.r24.u64 = ctx.r24.u32 & 0x1FF;
	// stw r5,2052(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2052, ctx.r5.u32);
	// stw r24,2048(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2048, ctx.r24.u32);
	// lis r24,1
	ctx.r24.s64 = 65536;
	// lwz r4,8(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f5,0(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// ori r4,r24,22476
	ctx.r4.u64 = ctx.r24.u64 | 22476;
	// lfsx f3,r5,r26
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r26.u32);
	ctx.f3.f64 = double(temp.f32);
	// addis r5,r31,1
	ctx.r5.s64 = ctx.r31.s64 + 65536;
	// lfs f4,12(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f10,f3,f4,f2
	ctx.f10.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f4.f64), float(ctx.f2.f64)));
	// lfsx f1,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f1.f64 = double(temp.f32);
	// addi r5,r5,22480
	ctx.r5.s64 = ctx.r5.s64 + 22480;
	// stfs f10,16(r26)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16, temp.u32);
	// lis r25,1
	ctx.r25.s64 = 65536;
	// stfs f5,4(r26)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4, temp.u32);
	// lis r24,1
	ctx.r24.s64 = 65536;
	// stfs f2,0(r26)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r26.u32 + 0, temp.u32);
	// lwz r26,8(r23)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8);
	// lfs f4,20(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 20);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r26,r26,2,0,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f3,16(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 16);
	ctx.f3.f64 = double(temp.f32);
	// ori r25,r25,22500
	ctx.r25.u64 = ctx.r25.u64 | 22500;
	// lfs f2,12(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// lfsx f10,r26,r23
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r23.u32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f5,f4,f10
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// fmadds f4,f3,f10,f1
	ctx.f4.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f10.f64), float(ctx.f1.f64)));
	// lfsx f3,r31,r4
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r4.u32);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,0(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// ori r4,r24,38916
	ctx.r4.u64 = ctx.r24.u64 | 38916;
	// stfs f4,0(r23)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r23.u32 + 0, temp.u32);
	// lis r26,1
	ctx.r26.s64 = 65536;
	// stfs f1,4(r23)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r23.u32 + 4, temp.u32);
	// fmadds f10,f2,f4,f5
	ctx.f10.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f4.f64), float(ctx.f5.f64)));
	// stfs f10,24(r23)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r23.u32 + 24, temp.u32);
	// lfs f5,12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// lwz r24,8(r5)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	// rlwinm r24,r24,2,0,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f4,16(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	ctx.f4.f64 = double(temp.f32);
	// lfsx f2,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f2.f64 = double(temp.f32);
	// ori r26,r26,38944
	ctx.r26.u64 = ctx.r26.u64 | 38944;
	// fmuls f10,f2,f8
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
	// lfsx f2,r24,r5
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + ctx.r5.u32);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f4,f4,f2,f3
	ctx.f4.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f2.f64), float(ctx.f3.f64)));
	// lfs f1,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f3,f5,f4
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// stfs f3,20(r5)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
	// stfs f4,0(r5)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lis r25,1
	ctx.r25.s64 = 65536;
	// stfs f1,4(r5)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lis r24,1
	ctx.r24.s64 = 65536;
	// lfs f1,12(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 12);
	ctx.f1.f64 = double(temp.f32);
	// ori r25,r25,41028
	ctx.r25.u64 = ctx.r25.u64 | 41028;
	// lfsx f2,r31,r4
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r4.u32);
	ctx.f2.f64 = double(temp.f32);
	// ori r24,r24,43108
	ctx.r24.u64 = ctx.r24.u64 | 43108;
	// addis r23,r31,2
	ctx.r23.s64 = ctx.r31.s64 + 131072;
	// addis r20,r31,2
	ctx.r20.s64 = ctx.r31.s64 + 131072;
	// addi r23,r23,-22416
	ctx.r23.s64 = ctx.r23.s64 + -22416;
	// addis r18,r31,2
	ctx.r18.s64 = ctx.r31.s64 + 131072;
	// addi r20,r20,-22396
	ctx.r20.s64 = ctx.r20.s64 + -22396;
	// addi r18,r18,-22368
	ctx.r18.s64 = ctx.r18.s64 + -22368;
	// lfs f4,0(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// lwz r5,8(r22)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8);
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f3,r4,r22
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r22.u32);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f1,f1,f3
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// lfsx f5,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f5.f64 = double(temp.f32);
	// addis r26,r31,1
	ctx.r26.s64 = ctx.r31.s64 + 65536;
	// stfs f2,0(r22)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r22.u32 + 0, temp.u32);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// stfs f1,16(r22)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r22.u32 + 16, temp.u32);
	// addi r26,r26,22512
	ctx.r26.s64 = ctx.r26.s64 + 22512;
	// stfs f4,4(r22)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r22.u32 + 4, temp.u32);
	// ori r5,r5,38920
	ctx.r5.u64 = ctx.r5.u64 | 38920;
	// addis r4,r31,2
	ctx.r4.s64 = ctx.r31.s64 + 131072;
	// lis r22,1
	ctx.r22.s64 = 65536;
	// addi r4,r4,-24496
	ctx.r4.s64 = ctx.r4.s64 + -24496;
	// ori r22,r22,43164
	ctx.r22.u64 = ctx.r22.u64 | 43164;
	// lfsx f3,r31,r5
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r5.u32);
	ctx.f3.f64 = double(temp.f32);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// lfs f4,16396(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 16396);
	ctx.f4.f64 = double(temp.f32);
	// lwz r17,16384(r26)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16384);
	// ori r21,r5,43136
	ctx.r21.u64 = ctx.r5.u64 | 43136;
	// lwz r16,16392(r26)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16392);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// lfs f2,16400(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 16400);
	ctx.f2.f64 = double(temp.f32);
	// subf r16,r16,r17
	ctx.r16.s64 = ctx.r17.s64 - ctx.r16.s64;
	// ori r19,r5,43188
	ctx.r19.u64 = ctx.r5.u64 | 43188;
	// lwz r5,16388(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16388);
	// subf r5,r5,r17
	ctx.r5.s64 = ctx.r17.s64 - ctx.r5.s64;
	// rlwinm r16,r16,2,18,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0x3FFC;
	// rlwinm r5,r5,2,18,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0x3FFC;
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f1,r16,r26
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r26.u32);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f1,f2
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// lfsx f1,r5,r26
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r26.u32);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f4,f1,f4
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
	// stfs f2,16408(r26)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16408, temp.u32);
	// stfs f4,16404(r26)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16404, temp.u32);
	// stfsx f10,r17,r26
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r26.u32, temp.u32);
	// lwz r5,16384(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16384);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r5,r5,20
	ctx.r5.u64 = ctx.r5.u32 & 0xFFF;
	// stw r5,16384(r26)
	PPC_STORE_U32(ctx.r26.u32 + 16384, ctx.r5.u32);
	// lfsx f2,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,2060(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2060);
	ctx.f1.f64 = double(temp.f32);
	// lwz r5,2052(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2052);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r5,r4
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
	ctx.f30.f64 = double(temp.f32);
	// lwz r26,2048(r4)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// lfsx f10,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f10.f64 = double(temp.f32);
	// rlwinm r26,r26,2,0,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f4,2064(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2064);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f2,f1,f30,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f30.f64), float(ctx.f2.f64)));
	// stfsx f2,r26,r4
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r26.u32 + ctx.r4.u32, temp.u32);
	// lwz r5,2048(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// addi r26,r5,1
	ctx.r26.s64 = ctx.r5.s64 + 1;
	// fadds f10,f10,f13
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f13.f64));
	// lwz r5,2052(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2052);
	// clrlwi r26,r26,23
	ctx.r26.u64 = ctx.r26.u32 & 0x1FF;
	// fmadds f1,f4,f2,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f2.f64), float(ctx.f30.f64)));
	// stfs f1,2068(r4)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + 2068, temp.u32);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r26,2048(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2048, ctx.r26.u32);
	// clrlwi r5,r5,23
	ctx.r5.u64 = ctx.r5.u32 & 0x1FF;
	// stw r5,2052(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2052, ctx.r5.u32);
	// lwz r4,8(r23)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f2,r5,r23
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	ctx.f2.f64 = double(temp.f32);
	// addis r4,r31,2
	ctx.r4.s64 = ctx.r31.s64 + 131072;
	// lfs f4,12(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// fmadds f1,f2,f4,f10
	ctx.f1.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f4.f64), float(ctx.f10.f64)));
	// lfsx f4,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,0(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// addi r4,r4,-22336
	ctx.r4.s64 = ctx.r4.s64 + -22336;
	// stfs f1,16(r23)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r23.u32 + 16, temp.u32);
	// ori r5,r5,59604
	ctx.r5.u64 = ctx.r5.u64 | 59604;
	// stfs f2,4(r23)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r23.u32 + 4, temp.u32);
	// addis r26,r31,2
	ctx.r26.s64 = ctx.r31.s64 + 131072;
	// stfs f10,0(r23)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r23.u32 + 0, temp.u32);
	// lwz r16,8(r20)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r20.u32 + 8);
	// rlwinm r16,r16,2,0,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f10,20(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 20);
	ctx.f10.f64 = double(temp.f32);
	// addi r26,r26,-5920
	ctx.r26.s64 = ctx.r26.s64 + -5920;
	// lfsx f2,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f2.f64 = double(temp.f32);
	// lis r25,1
	ctx.r25.s64 = 65536;
	// lfsx f29,r16,r20
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r20.u32);
	ctx.f29.f64 = double(temp.f32);
	// lis r23,2
	ctx.r23.s64 = 131072;
	// lfs f1,16(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f1,f29,f4
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f29.f64), float(ctx.f4.f64)));
	// fmuls f10,f10,f29
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f29.f64));
	// lfs f4,0(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// stfs f1,0(r20)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r20.u32 + 0, temp.u32);
	// addis r24,r31,2
	ctx.r24.s64 = ctx.r31.s64 + 131072;
	// lfs f30,12(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// ori r25,r25,59632
	ctx.r25.u64 = ctx.r25.u64 | 59632;
	// stfs f4,4(r20)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r20.u32 + 4, temp.u32);
	// ori r23,r23,2324
	ctx.r23.u64 = ctx.r23.u64 | 2324;
	// fmadds f4,f30,f1,f10
	ctx.f4.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f1.f64), float(ctx.f10.f64)));
	// stfs f4,24(r20)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r20.u32 + 24, temp.u32);
	// lwz r20,8(r18)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r18.u32 + 8);
	// rlwinm r20,r20,2,0,29
	ctx.r20.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f10,r31,r19
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r19.u32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f4,f10,f8
	ctx.f4.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// lfsx f29,r20,r18
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + ctx.r18.u32);
	ctx.f29.f64 = double(temp.f32);
	// addi r24,r24,2336
	ctx.r24.s64 = ctx.r24.s64 + 2336;
	// lfs f10,16(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f2,f10,f29,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f29.f64), float(ctx.f2.f64)));
	// lfs f1,12(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 12);
	ctx.f1.f64 = double(temp.f32);
	// lis r22,2
	ctx.r22.s64 = 131072;
	// lfs f30,0(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f1,f1,f2
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// stfs f30,4(r18)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r18.u32 + 4, temp.u32);
	// lis r21,2
	ctx.r21.s64 = 131072;
	// stfs f1,20(r18)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r18.u32 + 20, temp.u32);
	// addis r17,r31,2
	ctx.r17.s64 = ctx.r31.s64 + 131072;
	// stfs f2,0(r18)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r18.u32 + 0, temp.u32);
	// lwz r20,16384(r4)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16384);
	// lfs f10,16396(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16396);
	ctx.f10.f64 = double(temp.f32);
	// ori r22,r22,10552
	ctx.r22.u64 = ctx.r22.u64 | 10552;
	// addi r17,r17,10560
	ctx.r17.s64 = ctx.r17.s64 + 10560;
	// ori r21,r21,18772
	ctx.r21.u64 = ctx.r21.u64 | 18772;
	// lwz r19,16388(r4)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16388);
	// lfsx f2,r31,r5
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r5.u32);
	ctx.f2.f64 = double(temp.f32);
	// rlwinm r5,r20,2,0,29
	ctx.r5.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r20,r19,r20
	ctx.r20.s64 = ctx.r20.s64 - ctx.r19.s64;
	// rlwinm r20,r20,2,18,29
	ctx.r20.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0x3FFC;
	// lfsx f1,r20,r4
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + ctx.r4.u32);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f10,f1,f10
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// stfs f10,16404(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16404, temp.u32);
	// stfsx f4,r5,r4
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r4.u32, temp.u32);
	// lwz r5,16384(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16384);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r5,r5,20
	ctx.r5.u64 = ctx.r5.u32 & 0xFFF;
	// stw r5,16384(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16384, ctx.r5.u32);
	// lwz r4,8(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f4,12(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// lfsx f30,r5,r26
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r26.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f4,f4,f30
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// lfsx f1,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f1.f64 = double(temp.f32);
	// lfs f10,0(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,4(r26)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4, temp.u32);
	// stfs f2,0(r26)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r26.u32 + 0, temp.u32);
	// stfs f4,16(r26)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16, temp.u32);
	// lwz r4,8196(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8196);
	// lfsx f2,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f2.f64 = double(temp.f32);
	// fadds f10,f2,f0
	ctx.f10.f64 = double(float(ctx.f2.f64 + ctx.f0.f64));
	// lwz r5,8192(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8192);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f2,8204(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 8204);
	ctx.f2.f64 = double(temp.f32);
	// addis r26,r31,2
	ctx.r26.s64 = ctx.r31.s64 + 131072;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f4,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f4.f64 = double(temp.f32);
	// lfs f30,8212(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 8212);
	ctx.f30.f64 = double(temp.f32);
	// addi r26,r26,18784
	ctx.r26.s64 = ctx.r26.s64 + 18784;
	// lfs f29,8208(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 8208);
	ctx.f29.f64 = double(temp.f32);
	// addis r25,r31,2
	ctx.r25.s64 = ctx.r31.s64 + 131072;
	// lfsx f28,r4,r24
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r24.u32);
	ctx.f28.f64 = double(temp.f32);
	// lis r4,2
	ctx.r4.s64 = 131072;
	// fmadds f2,f2,f28,f10
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f28.f64), float(ctx.f10.f64)));
	// addi r25,r25,22912
	ctx.r25.s64 = ctx.r25.s64 + 22912;
	// stfsx f2,r5,r24
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r24.u32, temp.u32);
	// fmuls f2,f30,f28
	ctx.f2.f64 = double(float(ctx.f30.f64 * ctx.f28.f64));
	// lis r5,2
	ctx.r5.s64 = 131072;
	// ori r22,r4,39316
	ctx.r22.u64 = ctx.r4.u64 | 39316;
	// ori r23,r5,22900
	ctx.r23.u64 = ctx.r5.u64 | 22900;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// addis r19,r31,3
	ctx.r19.s64 = ctx.r31.s64 + 196608;
	// ori r20,r5,39348
	ctx.r20.u64 = ctx.r5.u64 | 39348;
	// addi r19,r19,-26208
	ctx.r19.s64 = ctx.r19.s64 + -26208;
	// fmadds f10,f29,f10,f2
	ctx.f10.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f10.f64), float(ctx.f2.f64)));
	// stfs f10,8216(r24)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r24.u32 + 8216, temp.u32);
	// lwz r5,8192(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8192);
	// addi r4,r5,1
	ctx.r4.s64 = ctx.r5.s64 + 1;
	// lwz r5,8196(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8196);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r4,r4,21
	ctx.r4.u64 = ctx.r4.u32 & 0x7FF;
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// stw r4,8192(r24)
	PPC_STORE_U32(ctx.r24.u32 + 8192, ctx.r4.u32);
	// stw r5,8196(r24)
	PPC_STORE_U32(ctx.r24.u32 + 8196, ctx.r5.u32);
	// lfs f2,8204(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 8204);
	ctx.f2.f64 = double(temp.f32);
	// lfs f10,8208(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 8208);
	ctx.f10.f64 = double(temp.f32);
	// lwz r4,8196(r17)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8196);
	// lwz r5,8192(r17)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8192);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f30.f64 = double(temp.f32);
	// lfsx f29,r4,r17
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r17.u32);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f4,f2,f29,f4
	ctx.f4.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f29.f64), float(ctx.f4.f64)));
	// stfsx f4,r5,r17
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r17.u32, temp.u32);
	// lwz r4,8192(r17)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8192);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// fmadds f2,f10,f4,f29
	ctx.f2.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f4.f64), float(ctx.f29.f64)));
	// lwz r5,8196(r17)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8196);
	// clrlwi r4,r4,21
	ctx.r4.u64 = ctx.r4.u32 & 0x7FF;
	// stfs f2,8212(r17)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r17.u32 + 8212, temp.u32);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r4,8192(r17)
	PPC_STORE_U32(ctx.r17.u32 + 8192, ctx.r4.u32);
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// stw r5,8196(r17)
	PPC_STORE_U32(ctx.r17.u32 + 8196, ctx.r5.u32);
	// lwz r4,4100(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4100);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f29,r4,r26
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r26.u32);
	ctx.f29.f64 = double(temp.f32);
	// lwz r5,4096(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4096);
	// lfs f10,4108(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 4108);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f10,f10,f29,f30
	ctx.f10.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f29.f64), float(ctx.f30.f64)));
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// fadds f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// lfsx f4,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,4112(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 4112);
	ctx.f2.f64 = double(temp.f32);
	// stfsx f10,r5,r26
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r26.u32, temp.u32);
	// fmadds f2,f2,f10,f29
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f10.f64), float(ctx.f29.f64)));
	// stfs f2,4116(r26)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4116, temp.u32);
	// lwz r4,4096(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4096);
	// lwz r5,4100(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4100);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// fadds f4,f4,f13
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f13.f64));
	// clrlwi r5,r5,22
	ctx.r5.u64 = ctx.r5.u32 & 0x3FF;
	// clrlwi r4,r4,22
	ctx.r4.u64 = ctx.r4.u32 & 0x3FF;
	// stw r5,4100(r26)
	PPC_STORE_U32(ctx.r26.u32 + 4100, ctx.r5.u32);
	// stw r4,4096(r26)
	PPC_STORE_U32(ctx.r26.u32 + 4096, ctx.r4.u32);
	// lfs f10,16396(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 16396);
	ctx.f10.f64 = double(temp.f32);
	// lwz r4,16388(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16388);
	// lfsx f2,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f2.f64 = double(temp.f32);
	// lwz r5,16384(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16384);
	// rlwinm r26,r5,2,0,29
	ctx.r26.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r4,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r4.s64;
	// rlwinm r5,r5,2,18,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0x3FFC;
	// lis r24,2
	ctx.r24.s64 = 131072;
	// addis r23,r31,3
	ctx.r23.s64 = ctx.r31.s64 + 196608;
	// ori r24,r24,64020
	ctx.r24.u64 = ctx.r24.u64 | 64020;
	// lis r22,3
	ctx.r22.s64 = 196608;
	// lfsx f30,r5,r25
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r25.u32);
	ctx.f30.f64 = double(temp.f32);
	// lis r21,3
	ctx.r21.s64 = 196608;
	// fmuls f10,f30,f10
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// stfs f10,16404(r25)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r25.u32 + 16404, temp.u32);
	// stfsx f4,r26,r25
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r26.u32 + ctx.r25.u32, temp.u32);
	// lwz r5,16384(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16384);
	// addi r4,r5,1
	ctx.r4.s64 = ctx.r5.s64 + 1;
	// clrlwi r4,r4,20
	ctx.r4.u64 = ctx.r4.u32 & 0xFFF;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// stw r4,16384(r25)
	PPC_STORE_U32(ctx.r25.u32 + 16384, ctx.r4.u32);
	// lfs f4,16(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 16);
	ctx.f4.f64 = double(temp.f32);
	// lwz r17,8(r19)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8);
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f10,12(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// ori r5,r5,47572
	ctx.r5.u64 = ctx.r5.u64 | 47572;
	// lfsx f28,r17,r19
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + ctx.r19.u32);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f4,f4,f28
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// lfsx f30,r31,r20
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r20.u32);
	ctx.f30.f64 = double(temp.f32);
	// addis r26,r31,3
	ctx.r26.s64 = ctx.r31.s64 + 196608;
	// lfs f29,0(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	ctx.f29.f64 = double(temp.f32);
	// lis r4,2
	ctx.r4.s64 = 131072;
	// stfs f2,0(r19)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r19.u32 + 0, temp.u32);
	// fmadds f2,f10,f2,f4
	ctx.f2.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f2.f64), float(ctx.f4.f64)));
	// stfs f2,20(r19)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r19.u32 + 20, temp.u32);
	// addi r26,r26,-17952
	ctx.r26.s64 = ctx.r26.s64 + -17952;
	// stfs f29,4(r19)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r19.u32 + 4, temp.u32);
	// ori r4,r4,55800
	ctx.r4.u64 = ctx.r4.u64 | 55800;
	// addis r25,r31,3
	ctx.r25.s64 = ctx.r31.s64 + 196608;
	// addis r18,r31,3
	ctx.r18.s64 = ctx.r31.s64 + 196608;
	// addi r25,r25,-9728
	ctx.r25.s64 = ctx.r25.s64 + -9728;
	// addi r23,r23,-1504
	ctx.r23.s64 = ctx.r23.s64 + -1504;
	// ori r22,r22,2612
	ctx.r22.u64 = ctx.r22.u64 | 2612;
	// ori r21,r21,19028
	ctx.r21.u64 = ctx.r21.u64 | 19028;
	// addi r18,r18,19040
	ctx.r18.s64 = ctx.r18.s64 + 19040;
	// lfsx f10,r31,r5
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r5.u32);
	ctx.f10.f64 = double(temp.f32);
	// lwz r5,8196(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// fadds f4,f10,f0
	ctx.f4.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// lwz r20,8192(r26)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f2,8204(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8204);
	ctx.f2.f64 = double(temp.f32);
	// rlwinm r20,r20,2,0,29
	ctx.r20.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f10,r31,r4
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r4.u32);
	ctx.f10.f64 = double(temp.f32);
	// lfs f29,8212(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8212);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,8208(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8208);
	ctx.f28.f64 = double(temp.f32);
	// lfsx f27,r5,r26
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r26.u32);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f29,f29,f27
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f27.f64));
	// fmadds f2,f2,f27,f4
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f27.f64), float(ctx.f4.f64)));
	// stfsx f2,r20,r26
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r20.u32 + ctx.r26.u32, temp.u32);
	// lwz r4,8192(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lwz r5,8196(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r4,r4,21
	ctx.r4.u64 = ctx.r4.u32 & 0x7FF;
	// fmadds f4,f28,f4,f29
	ctx.f4.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f4.f64), float(ctx.f29.f64)));
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// stfs f4,8216(r26)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r26.u32 + 8216, temp.u32);
	// stw r4,8192(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8192, ctx.r4.u32);
	// stw r5,8196(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8196, ctx.r5.u32);
	// lwz r4,8196(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// lfs f2,8204(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8204);
	ctx.f2.f64 = double(temp.f32);
	// lfsx f29,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f29.f64 = double(temp.f32);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,8192(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// lfsx f4,r5,r25
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r25.u32);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// fmadds f2,f2,f4,f10
	ctx.f2.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f4.f64), float(ctx.f10.f64)));
	// lfs f10,8208(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8208);
	ctx.f10.f64 = double(temp.f32);
	// stfsx f2,r5,r25
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r25.u32, temp.u32);
	// fmadds f4,f10,f2,f4
	ctx.f4.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f2.f64), float(ctx.f4.f64)));
	// stfs f4,8212(r25)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8212, temp.u32);
	// lwz r4,8192(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// lwz r5,8196(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// clrlwi r4,r4,21
	ctx.r4.u64 = ctx.r4.u32 & 0x7FF;
	// stw r5,8196(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8196, ctx.r5.u32);
	// addis r26,r31,3
	ctx.r26.s64 = ctx.r31.s64 + 196608;
	// stw r4,8192(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8192, ctx.r4.u32);
	// add r24,r31,r28
	ctx.r24.u64 = ctx.r31.u64 + ctx.r28.u64;
	// addi r26,r26,2624
	ctx.r26.s64 = ctx.r26.s64 + 2624;
	// add r16,r31,r29
	ctx.r16.u64 = ctx.r31.u64 + ctx.r29.u64;
	// lwz r5,4100(r23)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r23.u32 + 4100);
	// lfs f10,4108(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 4108);
	ctx.f10.f64 = double(temp.f32);
	// lwz r4,4096(r23)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r23.u32 + 4096);
	// lfsx f2,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f2.f64 = double(temp.f32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// fadds f2,f2,f13
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f13.f64));
	// lfsx f4,r5,r23
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r23.u32);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f10,f10,f4,f29
	ctx.f10.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f4.f64), float(ctx.f29.f64)));
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f29,4112(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 4112);
	ctx.f29.f64 = double(temp.f32);
	// fadds f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// lis r5,3
	ctx.r5.s64 = 196608;
	// fadds f13,f1,f5
	ctx.f13.f64 = double(float(ctx.f1.f64 + ctx.f5.f64));
	// ori r25,r5,19060
	ctx.r25.u64 = ctx.r5.u64 | 19060;
	// stfsx f10,r4,r23
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r23.u32, temp.u32);
	// lwz r5,4100(r23)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r23.u32 + 4100);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwz r4,4096(r23)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r23.u32 + 4096);
	// clrlwi r5,r5,22
	ctx.r5.u64 = ctx.r5.u32 & 0x3FF;
	// fmadds f4,f29,f10,f4
	ctx.f4.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f10.f64), float(ctx.f4.f64)));
	// stfs f4,4116(r23)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r23.u32 + 4116, temp.u32);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stw r5,4100(r23)
	PPC_STORE_U32(ctx.r23.u32 + 4100, ctx.r5.u32);
	// clrlwi r5,r4,22
	ctx.r5.u64 = ctx.r4.u32 & 0x3FF;
	// stw r5,4096(r23)
	PPC_STORE_U32(ctx.r23.u32 + 4096, ctx.r5.u32);
	// lwz r4,8(r18)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r18.u32 + 8);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f28,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f28.f64 = double(temp.f32);
	// fadds f26,f28,f30
	ctx.f26.f64 = double(float(ctx.f28.f64 + ctx.f30.f64));
	// lfs f10,16(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f30,f28,f30
	ctx.f30.f64 = static_cast<float>(ctx.f28.f64 - ctx.f30.f64);
	// lfsx f28,r5,r18
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r18.u32);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f10,f10,f28
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f28.f64));
	// lfsx f4,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f4.f64 = double(temp.f32);
	// lfs f27,0(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 0);
	ctx.f27.f64 = double(temp.f32);
	// stfs f4,0(r18)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r18.u32 + 0, temp.u32);
	// lfs f29,12(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 12);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f4,f29,f4,f10
	ctx.f4.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f4.f64), float(ctx.f10.f64)));
	// stfs f4,20(r18)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r18.u32 + 20, temp.u32);
	// stfs f27,4(r18)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r18.u32 + 4, temp.u32);
	// lwz r4,16388(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16388);
	// lfs f29,16400(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 16400);
	ctx.f29.f64 = double(temp.f32);
	// lwz r5,16384(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16384);
	// lwz r23,16392(r26)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16392);
	// lfsx f10,r31,r27
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r27.u32);
	ctx.f10.f64 = double(temp.f32);
	// subf r4,r4,r5
	ctx.r4.s64 = ctx.r5.s64 - ctx.r4.s64;
	// fadds f10,f10,f3
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f3.f64));
	// subf r23,r23,r5
	ctx.r23.s64 = ctx.r5.s64 - ctx.r23.s64;
	// lfs f4,16396(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 16396);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r25,r5,2,0,29
	ctx.r25.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r4,2,18,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0x3FFC;
	// rlwinm r5,r23,2,18,29
	ctx.r5.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 2) & 0x3FFC;
	// lfsx f28,r4,r26
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r26.u32);
	ctx.f28.f64 = double(temp.f32);
	// lfsx f3,r5,r26
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r26.u32);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f28,f4
	ctx.f4.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// fmuls f3,f3,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// stfs f4,16404(r26)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16404, temp.u32);
	// stfs f3,16408(r26)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16408, temp.u32);
	// stfsx f2,r25,r26
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r25.u32 + ctx.r26.u32, temp.u32);
	// lwz r5,16384(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16384);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r4,r5,20
	ctx.r4.u64 = ctx.r5.u32 & 0xFFF;
	// stw r4,16384(r26)
	PPC_STORE_U32(ctx.r26.u32 + 16384, ctx.r4.u32);
	// lfs f2,2060(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 2060);
	ctx.f2.f64 = double(temp.f32);
	// lwz r5,2052(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2052);
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,2048(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2048);
	// lfsx f3,r4,r24
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r24.u32);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f4,f2,f3,f26
	ctx.f4.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f3.f64), float(ctx.f26.f64)));
	// lfs f2,2064(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 2064);
	ctx.f2.f64 = double(temp.f32);
	// fadds f4,f4,f11
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f11.f64));
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f4,r4,r24
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r24.u32, temp.u32);
	// fmadds f3,f2,f4,f3
	ctx.f3.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f4.f64), float(ctx.f3.f64)));
	// stfs f3,2068(r24)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r24.u32 + 2068, temp.u32);
	// lwz r4,2048(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2048);
	// fsubs f2,f1,f5
	ctx.f2.f64 = static_cast<float>(ctx.f1.f64 - ctx.f5.f64);
	// lwz r5,2052(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2052);
	// add r25,r31,r30
	ctx.r25.u64 = ctx.r31.u64 + ctx.r30.u64;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lwz r17,128(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwz r18,132(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// clrlwi r4,r4,23
	ctx.r4.u64 = ctx.r4.u32 & 0x1FF;
	// lwz r19,136(r1)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// clrlwi r5,r5,23
	ctx.r5.u64 = ctx.r5.u32 & 0x1FF;
	// lwz r20,140(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// stw r4,2048(r24)
	PPC_STORE_U32(ctx.r24.u32 + 2048, ctx.r4.u32);
	// add r15,r31,r3
	ctx.r15.u64 = ctx.r31.u64 + ctx.r3.u64;
	// stw r5,2052(r24)
	PPC_STORE_U32(ctx.r24.u32 + 2052, ctx.r5.u32);
	// lfs f5,2064(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 2064);
	ctx.f5.f64 = double(temp.f32);
	// lwz r24,80(r1)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r21,144(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r22,148(r1)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r23,152(r1)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r26,436(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// lfs f1,2060(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 2060);
	ctx.f1.f64 = double(temp.f32);
	// lwz r5,2048(r16)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r16.u32 + 2048);
	// lwz r4,2052(r16)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r16.u32 + 2052);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f4,r4,r16
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r16.u32);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f3,f1,f4,f30
	ctx.f3.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f4.f64), float(ctx.f30.f64)));
	// fadds f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 + ctx.f11.f64));
	// stfsx f1,r5,r16
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r16.u32, temp.u32);
	// fmadds f11,f5,f1,f4
	ctx.f11.f64 = double(std::fma(float(ctx.f5.f64), float(ctx.f1.f64), float(ctx.f4.f64)));
	// stfs f11,2068(r16)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r16.u32 + 2068, temp.u32);
	// lwz r5,2048(r16)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r16.u32 + 2048);
	// lwz r4,2052(r16)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r16.u32 + 2052);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r4,r4,23
	ctx.r4.u64 = ctx.r4.u32 & 0x1FF;
	// clrlwi r5,r5,23
	ctx.r5.u64 = ctx.r5.u32 & 0x1FF;
	// stw r4,2052(r16)
	PPC_STORE_U32(ctx.r16.u32 + 2052, ctx.r4.u32);
	// stw r5,2048(r16)
	PPC_STORE_U32(ctx.r16.u32 + 2048, ctx.r5.u32);
	// lwz r4,8196(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// lfs f5,8204(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8204);
	ctx.f5.f64 = double(temp.f32);
	// lwz r5,8192(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// lfs f4,8208(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8208);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f3,r4,r25
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r25.u32);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f1,f5,f3,f13
	ctx.f1.f64 = double(std::fma(float(ctx.f5.f64), float(ctx.f3.f64), float(ctx.f13.f64)));
	// fadds f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 + ctx.f0.f64));
	// stfsx f13,r5,r25
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r25.u32, temp.u32);
	// fmadds f11,f4,f13,f3
	ctx.f11.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f13.f64), float(ctx.f3.f64)));
	// stfs f11,8212(r25)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8212, temp.u32);
	// lwz r4,8196(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// lwz r5,8192(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r4,r4,21
	ctx.r4.u64 = ctx.r4.u32 & 0x7FF;
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// stw r4,8196(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8196, ctx.r4.u32);
	// stw r5,8192(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8192, ctx.r5.u32);
	// lfs f5,8204(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 8204);
	ctx.f5.f64 = double(temp.f32);
	// lwz r4,8196(r15)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r15.u32 + 8196);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f3,r4,r15
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r15.u32);
	ctx.f3.f64 = double(temp.f32);
	// lwz r5,8192(r15)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r15.u32 + 8192);
	// fmadds f2,f5,f3,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f5.f64), float(ctx.f3.f64), float(ctx.f2.f64)));
	// lfs f4,8208(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 8208);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// fadds f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 + ctx.f0.f64));
	// stfsx f1,r5,r15
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r15.u32, temp.u32);
	// fmadds f0,f4,f1,f3
	ctx.f0.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f1.f64), float(ctx.f3.f64)));
	// stfs f0,8212(r15)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r15.u32 + 8212, temp.u32);
	// lwz r5,8196(r15)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r15.u32 + 8196);
	// lwz r4,8192(r15)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r15.u32 + 8192);
	// addi r16,r4,1
	ctx.r16.s64 = ctx.r4.s64 + 1;
	// addi r25,r5,1
	ctx.r25.s64 = ctx.r5.s64 + 1;
	// lwz r5,156(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// lwz r4,160(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// clrlwi r16,r16,21
	ctx.r16.u64 = ctx.r16.u32 & 0x7FF;
	// clrlwi r25,r25,21
	ctx.r25.u64 = ctx.r25.u32 & 0x7FF;
	// stw r25,8196(r15)
	PPC_STORE_U32(ctx.r15.u32 + 8196, ctx.r25.u32);
	// stw r16,8192(r15)
	PPC_STORE_U32(ctx.r15.u32 + 8192, ctx.r16.u32);
	// b 0x822df538
	goto loc_822DF538;
loc_822DF534:
	// lwz r26,436(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
loc_822DF538:
	// lwz r25,2052(r5)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2052);
	// lfs f0,2060(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 2060);
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f9,f10
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f10.f64));
	// lwz r16,2048(r5)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2048);
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,164(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// rlwinm r16,r16,2,0,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f11,2064(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 2064);
	ctx.f11.f64 = double(temp.f32);
	// lwz r14,168(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// fsubs f9,f6,f12
	ctx.f9.f64 = static_cast<float>(ctx.f6.f64 - ctx.f12.f64);
	// std r11,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r11.u64);
	// addi r24,r24,4
	ctx.r24.s64 = ctx.r24.s64 + 4;
	// std r10,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r10.u64);
	// addi r10,r26,4
	ctx.r10.s64 = ctx.r26.s64 + 4;
	// lfsx f5,r25,r5
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r5.u32);
	ctx.f5.f64 = double(temp.f32);
	// lwz r11,172(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// stw r10,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, ctx.r10.u32);
	// stw r24,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r24.u32);
	// lfsx f2,r11,r26
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r26.u32);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f1,f13,f7,f4
	ctx.f1.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f7.f64), float(ctx.f4.f64)));
	// stfsx f1,r16,r5
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r16.u32 + ctx.r5.u32, temp.u32);
	// ld r11,192(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// ld r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 184);
	// fmadds f0,f1,f11,f5
	ctx.f0.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f11.f64), float(ctx.f5.f64)));
	// stfs f0,2068(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 2068, temp.u32);
	// lwz r16,2048(r5)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2048);
	// lwz r25,2052(r5)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2052);
	// addi r16,r16,1
	ctx.r16.s64 = ctx.r16.s64 + 1;
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// clrlwi r16,r16,23
	ctx.r16.u64 = ctx.r16.u32 & 0x1FF;
	// clrlwi r25,r25,23
	ctx.r25.u64 = ctx.r25.u32 & 0x1FF;
	// stw r16,2048(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2048, ctx.r16.u32);
	// stw r25,2052(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2052, ctx.r25.u32);
	// lfs f11,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f4,f11,f12
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f13,2060(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2060);
	ctx.f13.f64 = double(temp.f32);
	// lwz r25,2052(r4)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2052);
	// lwz r16,2048(r4)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// rlwinm r16,r16,2,0,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f5,2064(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2064);
	ctx.f5.f64 = double(temp.f32);
	// fadds f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f31.f64));
	// lfsx f1,r25,r4
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r4.u32);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f0,f13,f1,f3
	ctx.f0.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f1.f64), float(ctx.f3.f64)));
	// stfsx f0,r16,r4
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r16.u32 + ctx.r4.u32, temp.u32);
	// fmadds f13,f9,f2,f4
	ctx.f13.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f2.f64), float(ctx.f4.f64)));
	// stfs f13,0(r26)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r26.u32 + 0, temp.u32);
	// fmadds f11,f0,f5,f1
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f5.f64), float(ctx.f1.f64)));
	// stfs f11,2068(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 2068, temp.u32);
	// lwz r25,2052(r4)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2052);
	// lwz r16,2048(r4)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// addi r26,r25,1
	ctx.r26.s64 = ctx.r25.s64 + 1;
	// addi r25,r16,1
	ctx.r25.s64 = ctx.r16.s64 + 1;
	// clrlwi r26,r26,23
	ctx.r26.u64 = ctx.r26.u32 & 0x1FF;
	// clrlwi r25,r25,23
	ctx.r25.u64 = ctx.r25.u32 & 0x1FF;
	// stw r26,2052(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2052, ctx.r26.u32);
	// stw r25,2048(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2048, ctx.r25.u32);
	// bdnz 0x822de984
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DE984;
loc_822DF628:
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa7c
	ctx.lr = 0x822DF634;
	__savefpr_26(ctx, base);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822DF638"))) PPC_WEAK_FUNC(sub_822DF638);
PPC_FUNC_IMPL(__imp__sub_822DF638) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x822DF640;
	__restfpr_14(ctx, base);
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa28
	ctx.lr = 0x822DF648;
	sub_8233FA28(ctx, base);
	// stwu r1,-416(r1)
	ea = -416 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r22,r6
	ctx.r22.u64 = ctx.r6.u64;
	// stw r5,452(r1)
	PPC_STORE_U32(ctx.r1.u32 + 452, ctx.r5.u32);
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = ctx.f1.f64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r5,r22
	ctx.r5.u64 = ctx.r22.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// bl 0x822db8f8
	ctx.lr = 0x822DF674;
	sub_822DB8F8(ctx, base);
	// addis r3,r31,1
	ctx.r3.s64 = ctx.r31.s64 + 65536;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// addi r3,r3,48
	ctx.r3.s64 = ctx.r3.s64 + 48;
	// bl 0x822db320
	ctx.lr = 0x822DF688;
	sub_822DB320(ctx, base);
	// lis r11,3
	ctx.r11.s64 = 196608;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// ori r9,r11,23232
	ctx.r9.u64 = ctx.r11.u64 | 23232;
	// lfs f12,-28948(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f12.f64 = double(temp.f32);
	// lwzx r8,r31,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r9.u32);
	// fcmpu cr6,f31,f12
	ctx.cr6.compare(ctx.f31.f64, ctx.f12.f64);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// lfs f10,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f10.f64 = double(temp.f32);
	// ble cr6,0x822df6b0
	if (!ctx.cr6.gt) goto loc_822DF6B0;
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
loc_822DF6B0:
	// fmr f6,f12
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = ctx.f12.f64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x822e0584
	if (ctx.cr6.eq) goto loc_822E0584;
	// addis r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 65536;
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// stw r22,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r22.u32);
	// addis r9,r31,1
	ctx.r9.s64 = ctx.r31.s64 + 65536;
	// addis r8,r31,1
	ctx.r8.s64 = ctx.r31.s64 + 65536;
	// addi r11,r11,10988
	ctx.r11.s64 = ctx.r11.s64 + 10988;
	// addi r10,r10,10992
	ctx.r10.s64 = ctx.r10.s64 + 10992;
	// addi r9,r9,12052
	ctx.r9.s64 = ctx.r9.s64 + 12052;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// addi r8,r8,14132
	ctx.r8.s64 = ctx.r8.s64 + 14132;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// addis r7,r31,1
	ctx.r7.s64 = ctx.r31.s64 + 65536;
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// addis r5,r31,1
	ctx.r5.s64 = ctx.r31.s64 + 65536;
	// stw r8,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r8.u32);
	// addis r4,r31,1
	ctx.r4.s64 = ctx.r31.s64 + 65536;
	// addi r7,r7,80
	ctx.r7.s64 = ctx.r7.s64 + 80;
	// addi r5,r5,18260
	ctx.r5.s64 = ctx.r5.s64 + 18260;
	// addi r4,r4,18264
	ctx.r4.s64 = ctx.r4.s64 + 18264;
	// stw r7,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r7.u32);
	// addis r6,r31,1
	ctx.r6.s64 = ctx.r31.s64 + 65536;
	// stw r5,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r5.u32);
	// addis r11,r31,3
	ctx.r11.s64 = ctx.r31.s64 + 196608;
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// addis r10,r31,3
	ctx.r10.s64 = ctx.r31.s64 + 196608;
	// addis r9,r31,3
	ctx.r9.s64 = ctx.r31.s64 + 196608;
	// addis r8,r31,3
	ctx.r8.s64 = ctx.r31.s64 + 196608;
	// addi r6,r6,84
	ctx.r6.s64 = ctx.r6.s64 + 84;
	// addi r11,r11,23240
	ctx.r11.s64 = ctx.r11.s64 + 23240;
	// addi r10,r10,24308
	ctx.r10.s64 = ctx.r10.s64 + 24308;
	// stw r6,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r6.u32);
	// addi r9,r9,21140
	ctx.r9.s64 = ctx.r9.s64 + 21140;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r11.u32);
	// addi r8,r8,23220
	ctx.r8.s64 = ctx.r8.s64 + 23220;
	// stw r10,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r10.u32);
	// addi r7,r29,-4
	ctx.r7.s64 = ctx.r29.s64 + -4;
	// stw r9,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r9.u32);
	// addis r3,r31,3
	ctx.r3.s64 = ctx.r31.s64 + 196608;
	// stw r8,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r8.u32);
	// addis r20,r31,1
	ctx.r20.s64 = ctx.r31.s64 + 65536;
	// stw r7,444(r1)
	PPC_STORE_U32(ctx.r1.u32 + 444, ctx.r7.u32);
	// addis r19,r31,1
	ctx.r19.s64 = ctx.r31.s64 + 65536;
	// addis r18,r31,1
	ctx.r18.s64 = ctx.r31.s64 + 65536;
	// addis r17,r31,1
	ctx.r17.s64 = ctx.r31.s64 + 65536;
	// addis r16,r31,1
	ctx.r16.s64 = ctx.r31.s64 + 65536;
	// addis r23,r31,3
	ctx.r23.s64 = ctx.r31.s64 + 196608;
	// addis r5,r31,3
	ctx.r5.s64 = ctx.r31.s64 + 196608;
	// addis r4,r31,3
	ctx.r4.s64 = ctx.r31.s64 + 196608;
	// addi r3,r3,23236
	ctx.r3.s64 = ctx.r3.s64 + 23236;
	// addi r20,r20,2184
	ctx.r20.s64 = ctx.r20.s64 + 2184;
	// addi r19,r19,2188
	ctx.r19.s64 = ctx.r19.s64 + 2188;
	// stw r3,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r3.u32);
	// addi r18,r18,2192
	ctx.r18.s64 = ctx.r18.s64 + 2192;
	// stw r20,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r20.u32);
	// addi r17,r17,2740
	ctx.r17.s64 = ctx.r17.s64 + 2740;
	// stw r19,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r19.u32);
	// addi r16,r16,10984
	ctx.r16.s64 = ctx.r16.s64 + 10984;
	// stw r18,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r18.u32);
	// addi r23,r23,23248
	ctx.r23.s64 = ctx.r23.s64 + 23248;
	// stw r17,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r17.u32);
	// addi r5,r5,19072
	ctx.r5.s64 = ctx.r5.s64 + 19072;
	// stw r16,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r16.u32);
	// addi r4,r4,21152
	ctx.r4.s64 = ctx.r4.s64 + 21152;
	// stw r23,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r23.u32);
	// addis r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 65536;
	// stw r5,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r5.u32);
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// stw r4,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r4.u32);
	// addis r9,r31,1
	ctx.r9.s64 = ctx.r31.s64 + 65536;
	// addis r8,r31,1
	ctx.r8.s64 = ctx.r31.s64 + 65536;
	// addis r7,r31,1
	ctx.r7.s64 = ctx.r31.s64 + 65536;
	// addis r6,r31,1
	ctx.r6.s64 = ctx.r31.s64 + 65536;
	// addi r11,r11,96
	ctx.r11.s64 = ctx.r11.s64 + 96;
	// addi r10,r10,2752
	ctx.r10.s64 = ctx.r10.s64 + 2752;
	// addi r9,r9,14144
	ctx.r9.s64 = ctx.r9.s64 + 14144;
	// addi r8,r8,2208
	ctx.r8.s64 = ctx.r8.s64 + 2208;
	// addi r7,r7,11008
	ctx.r7.s64 = ctx.r7.s64 + 11008;
	// addi r6,r6,12064
	ctx.r6.s64 = ctx.r6.s64 + 12064;
	// lis r3,-32255
	ctx.r3.s64 = -2113863680;
	// lis r30,-32256
	ctx.r30.s64 = -2113929216;
	// lis r29,-32256
	ctx.r29.s64 = -2113929216;
	// lfs f3,1800(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 1800);
	ctx.f3.f64 = double(temp.f32);
	// lis r28,-32255
	ctx.r28.s64 = -2113863680;
	// lfs f7,5256(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 5256);
	ctx.f7.f64 = double(temp.f32);
	// lis r3,1
	ctx.r3.s64 = 65536;
	// lis r27,3
	ctx.r27.s64 = 196608;
	// lis r25,1
	ctx.r25.s64 = 65536;
	// lis r30,1
	ctx.r30.s64 = 65536;
	// lfs f5,5260(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 5260);
	ctx.f5.f64 = double(temp.f32);
	// lis r24,2
	ctx.r24.s64 = 131072;
	// lfs f4,-1560(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + -1560);
	ctx.f4.f64 = double(temp.f32);
	// ori r29,r3,38960
	ctx.r29.u64 = ctx.r3.u64 | 38960;
	// ori r27,r27,19032
	ctx.r27.u64 = ctx.r27.u64 | 19032;
	// ori r28,r25,18272
	ctx.r28.u64 = ctx.r25.u64 | 18272;
	// ori r30,r30,59648
	ctx.r30.u64 = ctx.r30.u64 | 59648;
	// ori r3,r24,39360
	ctx.r3.u64 = ctx.r24.u64 | 39360;
loc_822DF840:
	// lwz r25,2048(r11)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// lfs f8,0(r19)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lwz r24,2064(r11)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2064);
	// lfs f9,0(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lwz r21,2068(r11)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2068);
	// rlwinm r15,r25,2,0,29
	ctx.r15.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r24,r24,r25
	ctx.r24.s64 = ctx.r25.s64 - ctx.r24.s64;
	// lwz r14,2072(r11)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2072);
	// subf r21,r21,r25
	ctx.r21.s64 = ctx.r25.s64 - ctx.r21.s64;
	// fadds f9,f8,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 + ctx.f9.f64));
	// subf r25,r14,r25
	ctx.r25.s64 = ctx.r25.s64 - ctx.r14.s64;
	// lfs f0,2076(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2076);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r24,r24,2,21,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0x7FC;
	// lfs f13,2080(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2080);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r25,r25,2,21,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0x7FC;
	// lfs f11,2084(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2084);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r21,r21,2,21,29
	ctx.r21.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0x7FC;
	// lfs f2,0(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,0(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lwz r14,84(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// std r5,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r5.u64);
	// lfsx f8,r24,r11
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + ctx.r11.u32);
	ctx.f8.f64 = double(temp.f32);
	// lwz r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// fmuls f0,f8,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfsx f8,r25,r11
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r11.u32);
	ctx.f8.f64 = double(temp.f32);
	// lfsx f30,r21,r11
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r11.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f11,f11,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// fmuls f13,f13,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f30.f64));
	// stfs f0,2088(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2088, temp.u32);
	// stfs f11,2096(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2096, temp.u32);
	// stfs f13,2092(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2092, temp.u32);
	// stfsx f1,r15,r11
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r15.u32 + ctx.r11.u32, temp.u32);
	// lwz r25,2048(r11)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// clrlwi r25,r25,23
	ctx.r25.u64 = ctx.r25.u32 & 0x1FF;
	// stw r25,2048(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2048, ctx.r25.u32);
	// lfs f8,524(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 524);
	ctx.f8.f64 = double(temp.f32);
	// lwz r25,512(r8)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r8.u32 + 512);
	// lfs f1,0(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lwz r24,516(r8)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r8.u32 + 516);
	// rlwinm r24,r24,2,0,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r24,r8
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + ctx.r8.u32);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,528(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 528);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f11,f0,f8,f9
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f8.f64), float(ctx.f9.f64)));
	// stfsx f11,r25,r8
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r25.u32 + ctx.r8.u32, temp.u32);
	// lwz r24,512(r8)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r8.u32 + 512);
	// lwz r25,516(r8)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r8.u32 + 516);
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// addi r24,r24,1
	ctx.r24.s64 = ctx.r24.s64 + 1;
	// fmadds f9,f13,f11,f0
	ctx.f9.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f0.f64)));
	// clrlwi r25,r25,25
	ctx.r25.u64 = ctx.r25.u32 & 0x7F;
	// stfs f9,532(r8)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r8.u32 + 532, temp.u32);
	// clrlwi r24,r24,25
	ctx.r24.u64 = ctx.r24.u32 & 0x7F;
	// stw r25,516(r8)
	PPC_STORE_U32(ctx.r8.u32 + 516, ctx.r25.u32);
	// stw r24,512(r8)
	PPC_STORE_U32(ctx.r8.u32 + 512, ctx.r24.u32);
	// lfs f11,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lwz r25,8208(r10)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8208);
	// lfs f0,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r24,8216(r10)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8216);
	// lfs f8,8224(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8224);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,8220(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8220);
	ctx.f9.f64 = double(temp.f32);
	// lwz r21,8192(r10)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r15,8212(r10)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8212);
	// subf r15,r15,r21
	ctx.r15.s64 = ctx.r21.s64 - ctx.r15.s64;
	// subf r25,r25,r21
	ctx.r25.s64 = ctx.r21.s64 - ctx.r25.s64;
	// fadds f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// rlwinm r15,r15,2,19,29
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0x1FFC;
	// lfs f13,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// subf r24,r24,r21
	ctx.r24.s64 = ctx.r21.s64 - ctx.r24.s64;
	// lfs f30,8228(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8228);
	ctx.f30.f64 = double(temp.f32);
	// rlwinm r25,r25,2,19,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0x1FFC;
	// rlwinm r24,r24,2,19,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0x1FFC;
	// lfsx f11,r15,r10
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + ctx.r10.u32);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f8,f11,f8
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// lfsx f11,r25,r10
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r10.u32);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f9,f11,f9
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// lfsx f29,r24,r10
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + ctx.r10.u32);
	ctx.f29.f64 = double(temp.f32);
	// rlwinm r25,r21,2,0,29
	ctx.r25.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f8,8236(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8236, temp.u32);
	// stfs f9,8232(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8232, temp.u32);
	// fmuls f8,f30,f29
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f29.f64));
	// stfs f8,8240(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8240, temp.u32);
	// stfsx f1,r25,r10
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r25.u32 + ctx.r10.u32, temp.u32);
	// lwz r25,8192(r10)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// std r11,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r11.u64);
	// clrlwi r25,r25,21
	ctx.r25.u64 = ctx.r25.u32 & 0x7FF;
	// lwz r24,92(r1)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r21,96(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r25,8192(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8192, ctx.r25.u32);
	// lfs f11,1036(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1036);
	ctx.f11.f64 = double(temp.f32);
	// lwz r25,1024(r7)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r7.u32 + 1024);
	// lfs f9,1040(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1040);
	ctx.f9.f64 = double(temp.f32);
	// lwz r11,1028(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 1028);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f8,r11,r7
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r7.u32);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f0,f8,f11,f0
	ctx.f0.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f11.f64), float(ctx.f0.f64)));
	// lfs f30,0(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// stfsx f0,r25,r7
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r25.u32 + ctx.r7.u32, temp.u32);
	// lwz r25,1028(r7)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r7.u32 + 1028);
	// addi r24,r25,1
	ctx.r24.s64 = ctx.r25.s64 + 1;
	// lwz r25,1024(r7)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r7.u32 + 1024);
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// fmadds f11,f9,f0,f8
	ctx.f11.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f0.f64), float(ctx.f8.f64)));
	// clrlwi r24,r24,24
	ctx.r24.u64 = ctx.r24.u32 & 0xFF;
	// stfs f11,1044(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 1044, temp.u32);
	// clrlwi r25,r25,24
	ctx.r25.u64 = ctx.r25.u32 & 0xFF;
	// lwz r14,104(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r24,1028(r7)
	PPC_STORE_U32(ctx.r7.u32 + 1028, ctx.r24.u32);
	// stw r25,1024(r7)
	PPC_STORE_U32(ctx.r7.u32 + 1024, ctx.r25.u32);
	// lfs f29,0(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	ctx.f29.f64 = double(temp.f32);
	// lfs f8,2064(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 2064);
	ctx.f8.f64 = double(temp.f32);
	// lwz r15,100(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r5,108(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lfs f9,2060(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 2060);
	ctx.f9.f64 = double(temp.f32);
	// lwz r25,2048(r6)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r6.u32 + 2048);
	// lwz r24,2052(r6)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r6.u32 + 2052);
	// rlwinm r24,r24,2,0,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r24,r6
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + ctx.r6.u32);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// fmadds f11,f9,f0,f30
	ctx.f11.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f0.f64), float(ctx.f30.f64)));
	// stfsx f11,r25,r6
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r25.u32 + ctx.r6.u32, temp.u32);
	// lwz r25,2052(r6)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r6.u32 + 2052);
	// fmadds f9,f11,f8,f0
	ctx.f9.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f8.f64), float(ctx.f0.f64)));
	// lwz r24,2048(r6)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r6.u32 + 2048);
	// addi r24,r24,1
	ctx.r24.s64 = ctx.r24.s64 + 1;
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// stfs f9,2068(r6)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r6.u32 + 2068, temp.u32);
	// clrlwi r24,r24,23
	ctx.r24.u64 = ctx.r24.u32 & 0x1FF;
	// clrlwi r25,r25,23
	ctx.r25.u64 = ctx.r25.u32 & 0x1FF;
	// stw r24,2048(r6)
	PPC_STORE_U32(ctx.r6.u32 + 2048, ctx.r24.u32);
	// stw r25,2052(r6)
	PPC_STORE_U32(ctx.r6.u32 + 2052, ctx.r25.u32);
	// lwz r21,4100(r9)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4100);
	// lwz r15,0(r15)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	// stw r15,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r15.u32);
	// lfs f8,4108(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4108);
	ctx.f8.f64 = double(temp.f32);
	// lwz r24,0(r14)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	// stw r24,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r24.u32);
	// lfs f9,4112(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4112);
	ctx.f9.f64 = double(temp.f32);
	// lfs f28,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f28.f64 = double(temp.f32);
	// lwz r24,4096(r9)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4096);
	// fmuls f1,f28,f1
	ctx.f1.f64 = double(float(ctx.f28.f64 * ctx.f1.f64));
	// lfs f28,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f30,f28,f30
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// subf r21,r21,r24
	ctx.r21.s64 = ctx.r24.s64 - ctx.r21.s64;
	// lwz r25,4104(r9)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4104);
	// subf r25,r25,r24
	ctx.r25.s64 = ctx.r24.s64 - ctx.r25.s64;
	// rlwinm r21,r21,2,20,29
	ctx.r21.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFC;
	// fadds f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 + ctx.f1.f64));
	// rlwinm r25,r25,2,20,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFC;
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lwz r15,128(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// rlwinm r24,r24,2,0,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f28,r21,r9
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r9.u32);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f28,f8
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// lwz r21,124(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lfsx f30,r25,r9
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r9.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmr f8,f12
	ctx.f8.f64 = ctx.f12.f64;
	// fmuls f9,f30,f9
	ctx.f9.f64 = double(float(ctx.f30.f64 * ctx.f9.f64));
	// stfs f9,4120(r9)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4120, temp.u32);
	// stfs f28,4116(r9)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4116, temp.u32);
	// fadds f2,f1,f2
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f2.f64));
	// stfsx f29,r24,r9
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r24.u32 + ctx.r9.u32, temp.u32);
	// ld r5,184(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 184);
	// ld r11,192(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// lwz r25,4096(r9)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4096);
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// clrlwi r25,r25,22
	ctx.r25.u64 = ctx.r25.u32 & 0x3FF;
	// stw r25,4096(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4096, ctx.r25.u32);
	// lwz r25,0(r15)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// lwz r25,0(r21)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	// stw r25,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r25.u32);
	// lfs f1,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f9,f2,f1
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// bne cr6,0x822e03ec
	if (!ctx.cr6.eq) goto loc_822E03EC;
	// addis r4,r31,1
	ctx.r4.s64 = ctx.r31.s64 + 65536;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// addi r4,r4,20352
	ctx.r4.s64 = ctx.r4.s64 + 20352;
	// lis r26,1
	ctx.r26.s64 = 65536;
	// ori r5,r5,20340
	ctx.r5.u64 = ctx.r5.u64 | 20340;
	// ori r26,r26,22420
	ctx.r26.u64 = ctx.r26.u64 | 22420;
	// addis r23,r31,1
	ctx.r23.s64 = ctx.r31.s64 + 65536;
	// lwz r25,2052(r4)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2052);
	// lfs f8,2060(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2060);
	ctx.f8.f64 = double(temp.f32);
	// lwz r24,2048(r4)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// lfs f6,2064(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2064);
	ctx.f6.f64 = double(temp.f32);
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f2,r31,r5
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r5.u32);
	ctx.f2.f64 = double(temp.f32);
	// rlwinm r24,r24,2,0,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f1,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f1.f64 = double(temp.f32);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// fadds f1,f1,f13
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f13.f64));
	// addis r26,r31,1
	ctx.r26.s64 = ctx.r31.s64 + 65536;
	// addi r23,r23,22452
	ctx.r23.s64 = ctx.r23.s64 + 22452;
	// lfsx f30,r25,r4
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r4.u32);
	ctx.f30.f64 = double(temp.f32);
	// ori r25,r5,22448
	ctx.r25.u64 = ctx.r5.u64 | 22448;
	// fmadds f8,f8,f30,f2
	ctx.f8.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f30.f64), float(ctx.f2.f64)));
	// stfsx f8,r24,r4
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r24.u32 + ctx.r4.u32, temp.u32);
	// addi r26,r26,22432
	ctx.r26.s64 = ctx.r26.s64 + 22432;
	// lis r22,1
	ctx.r22.s64 = 65536;
	// addis r21,r31,2
	ctx.r21.s64 = ctx.r31.s64 + 131072;
	// lis r20,1
	ctx.r20.s64 = 65536;
	// addi r21,r21,-26608
	ctx.r21.s64 = ctx.r21.s64 + -26608;
	// ori r22,r22,38916
	ctx.r22.u64 = ctx.r22.u64 | 38916;
	// ori r20,r20,38944
	ctx.r20.u64 = ctx.r20.u64 | 38944;
	// fmadds f6,f6,f8,f30
	ctx.f6.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f8.f64), float(ctx.f30.f64)));
	// stfs f6,2068(r4)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r4.u32 + 2068, temp.u32);
	// lwz r5,2048(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// addi r24,r5,1
	ctx.r24.s64 = ctx.r5.s64 + 1;
	// lwz r5,2052(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2052);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r5,r5,23
	ctx.r5.u64 = ctx.r5.u32 & 0x1FF;
	// clrlwi r24,r24,23
	ctx.r24.u64 = ctx.r24.u32 & 0x1FF;
	// stw r5,2052(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2052, ctx.r5.u32);
	// stw r24,2048(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2048, ctx.r24.u32);
	// lis r24,1
	ctx.r24.s64 = 65536;
	// addis r4,r31,1
	ctx.r4.s64 = ctx.r31.s64 + 65536;
	// ori r5,r24,22476
	ctx.r5.u64 = ctx.r24.u64 | 22476;
	// addi r4,r4,22480
	ctx.r4.s64 = ctx.r4.s64 + 22480;
	// lis r24,1
	ctx.r24.s64 = 65536;
	// ori r24,r24,22500
	ctx.r24.u64 = ctx.r24.u64 | 22500;
	// lfs f6,0(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lwz r19,8(r26)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// lfsx f8,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f8.f64 = double(temp.f32);
	// lfs f2,12(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// rlwinm r25,r19,2,0,29
	ctx.r25.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r25,r26
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r26.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f2,f30,f2,f1
	ctx.f2.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f2.f64), float(ctx.f1.f64)));
	// stfs f1,0(r26)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r26.u32 + 0, temp.u32);
	// stfs f6,4(r26)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4, temp.u32);
	// stfs f2,16(r26)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16, temp.u32);
	// lfs f2,16(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 16);
	ctx.f2.f64 = double(temp.f32);
	// lfs f6,0(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lfs f1,20(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 20);
	ctx.f1.f64 = double(temp.f32);
	// lwz r26,8(r23)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8);
	// rlwinm r26,r26,2,0,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r26,r23
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r23.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f8,f2,f30,f8
	ctx.f8.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f30.f64), float(ctx.f8.f64)));
	// fmuls f2,f1,f30
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// lfs f1,12(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 12);
	ctx.f1.f64 = double(temp.f32);
	// lfsx f30,r31,r5
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r5.u32);
	ctx.f30.f64 = double(temp.f32);
	// stfs f8,0(r23)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r23.u32 + 0, temp.u32);
	// stfs f6,4(r23)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r23.u32 + 4, temp.u32);
	// fmadds f8,f1,f8,f2
	ctx.f8.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f8.f64), float(ctx.f2.f64)));
	// stfs f8,24(r23)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r23.u32 + 24, temp.u32);
	// lwz r5,8(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lfs f2,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f2.f64 = double(temp.f32);
	// lfs f8,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f8.f64 = double(temp.f32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f6,r5,r4
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f1,f2,f6,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f2.f64), float(ctx.f6.f64), float(ctx.f30.f64)));
	// lfsx f6,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f6,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// lfs f2,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// stfs f2,4(r4)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// stfs f1,0(r4)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmuls f2,f8,f1
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f1.f64));
	// stfs f2,20(r4)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// lfsx f1,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f1.f64 = double(temp.f32);
	// lwz r4,8(r21)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r21.u32 + 8);
	// lfs f8,0(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addis r4,r31,1
	ctx.r4.s64 = ctx.r31.s64 + 65536;
	// lfsx f29,r5,r21
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r21.u32);
	ctx.f29.f64 = double(temp.f32);
	// lfsx f2,r31,r20
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r20.u32);
	ctx.f2.f64 = double(temp.f32);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// addi r4,r4,22512
	ctx.r4.s64 = ctx.r4.s64 + 22512;
	// lfs f30,12(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// stfs f1,0(r21)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r21.u32 + 0, temp.u32);
	// fmuls f1,f30,f29
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f29.f64));
	// stfs f1,16(r21)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r21.u32 + 16, temp.u32);
	// ori r23,r5,41028
	ctx.r23.u64 = ctx.r5.u64 | 41028;
	// stfs f8,4(r21)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r21.u32 + 4, temp.u32);
	// lwz r14,16388(r4)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16388);
	// lfs f1,16396(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16396);
	ctx.f1.f64 = double(temp.f32);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// lis r26,1
	ctx.r26.s64 = 65536;
	// lwz r16,16384(r4)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16384);
	// ori r20,r5,43136
	ctx.r20.u64 = ctx.r5.u64 | 43136;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// ori r26,r26,38920
	ctx.r26.u64 = ctx.r26.u64 | 38920;
	// ori r18,r5,43188
	ctx.r18.u64 = ctx.r5.u64 | 43188;
	// rlwinm r15,r16,2,0,29
	ctx.r15.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,16392(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16392);
	// subf r5,r5,r16
	ctx.r5.s64 = ctx.r16.s64 - ctx.r5.s64;
	// subf r16,r14,r16
	ctx.r16.s64 = ctx.r16.s64 - ctx.r14.s64;
	// lfsx f8,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f8.f64 = double(temp.f32);
	// rlwinm r5,r5,2,18,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0x3FFC;
	// lfs f30,16400(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16400);
	ctx.f30.f64 = double(temp.f32);
	// rlwinm r26,r16,2,18,29
	ctx.r26.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0x3FFC;
	// addis r25,r31,2
	ctx.r25.s64 = ctx.r31.s64 + 131072;
	// lis r24,1
	ctx.r24.s64 = 65536;
	// addi r25,r25,-24496
	ctx.r25.s64 = ctx.r25.s64 + -24496;
	// lfsx f29,r5,r4
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f29,f30
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f30.f64));
	// lfsx f28,r26,r4
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r4.u32);
	ctx.f28.f64 = double(temp.f32);
	// ori r24,r24,43108
	ctx.r24.u64 = ctx.r24.u64 | 43108;
	// fmuls f1,f28,f1
	ctx.f1.f64 = double(float(ctx.f28.f64 * ctx.f1.f64));
	// stfs f1,16404(r4)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16404, temp.u32);
	// stfs f30,16408(r4)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16408, temp.u32);
	// addis r22,r31,2
	ctx.r22.s64 = ctx.r31.s64 + 131072;
	// stfsx f6,r15,r4
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r15.u32 + ctx.r4.u32, temp.u32);
	// lwz r5,16384(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16384);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r5,r5,20
	ctx.r5.u64 = ctx.r5.u32 & 0xFFF;
	// addi r22,r22,-22416
	ctx.r22.s64 = ctx.r22.s64 + -22416;
	// stw r5,16384(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16384, ctx.r5.u32);
	// lfsx f6,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f6.f64 = double(temp.f32);
	// lwz r4,2052(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2052);
	// lfs f1,2060(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 2060);
	ctx.f1.f64 = double(temp.f32);
	// lfs f29,2064(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 2064);
	ctx.f29.f64 = double(temp.f32);
	// lis r21,1
	ctx.r21.s64 = 65536;
	// addis r19,r31,2
	ctx.r19.s64 = ctx.r31.s64 + 131072;
	// addis r17,r31,2
	ctx.r17.s64 = ctx.r31.s64 + 131072;
	// ori r21,r21,43164
	ctx.r21.u64 = ctx.r21.u64 | 43164;
	// addi r19,r19,-22396
	ctx.r19.s64 = ctx.r19.s64 + -22396;
	// addi r17,r17,-22368
	ctx.r17.s64 = ctx.r17.s64 + -22368;
	// lwz r5,2048(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2048);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f30.f64 = double(temp.f32);
	// fadds f30,f30,f13
	ctx.f30.f64 = double(float(ctx.f30.f64 + ctx.f13.f64));
	// lfsx f28,r4,r25
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r25.u32);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f6,f1,f28,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f28.f64), float(ctx.f6.f64)));
	// stfsx f6,r5,r25
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r25.u32, temp.u32);
	// lwz r5,2048(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2048);
	// addi r4,r5,1
	ctx.r4.s64 = ctx.r5.s64 + 1;
	// fmadds f1,f29,f6,f28
	ctx.f1.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f6.f64), float(ctx.f28.f64)));
	// lwz r5,2052(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2052);
	// clrlwi r4,r4,23
	ctx.r4.u64 = ctx.r4.u32 & 0x1FF;
	// stfs f1,2068(r25)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2068, temp.u32);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r4,2048(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2048, ctx.r4.u32);
	// clrlwi r4,r5,23
	ctx.r4.u64 = ctx.r5.u32 & 0x1FF;
	// stw r4,2052(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2052, ctx.r4.u32);
	// lfs f6,12(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 12);
	ctx.f6.f64 = double(temp.f32);
	// lwz r5,8(r22)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8);
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f1,r31,r20
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r20.u32);
	ctx.f1.f64 = double(temp.f32);
	// lfs f29,0(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	ctx.f29.f64 = double(temp.f32);
	// lis r26,1
	ctx.r26.s64 = 65536;
	// addis r25,r31,2
	ctx.r25.s64 = ctx.r31.s64 + 131072;
	// lis r23,2
	ctx.r23.s64 = 131072;
	// addi r25,r25,-5920
	ctx.r25.s64 = ctx.r25.s64 + -5920;
	// lfsx f28,r4,r22
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r22.u32);
	ctx.f28.f64 = double(temp.f32);
	// addis r4,r31,2
	ctx.r4.s64 = ctx.r31.s64 + 131072;
	// fmadds f6,f28,f6,f30
	ctx.f6.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f6.f64), float(ctx.f30.f64)));
	// stfs f6,16(r22)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r22.u32 + 16, temp.u32);
	// stfs f30,0(r22)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r22.u32 + 0, temp.u32);
	// addi r4,r4,-22336
	ctx.r4.s64 = ctx.r4.s64 + -22336;
	// stfs f29,4(r22)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r22.u32 + 4, temp.u32);
	// addis r24,r31,2
	ctx.r24.s64 = ctx.r31.s64 + 131072;
	// ori r23,r23,2324
	ctx.r23.u64 = ctx.r23.u64 | 2324;
	// addi r24,r24,2336
	ctx.r24.s64 = ctx.r24.s64 + 2336;
	// lis r22,2
	ctx.r22.s64 = 131072;
	// addis r20,r31,2
	ctx.r20.s64 = ctx.r31.s64 + 131072;
	// ori r22,r22,10552
	ctx.r22.u64 = ctx.r22.u64 | 10552;
	// addi r20,r20,10560
	ctx.r20.s64 = ctx.r20.s64 + 10560;
	// lfs f27,20(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 20);
	ctx.f27.f64 = double(temp.f32);
	// lfs f30,12(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// lwz r5,8(r19)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8);
	// lfs f6,16(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// lfs f29,0(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	ctx.f29.f64 = double(temp.f32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f28,r5,r19
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r19.u32);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f1,f6,f28,f1
	ctx.f1.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f28.f64), float(ctx.f1.f64)));
	// fmuls f6,f27,f28
	ctx.f6.f64 = double(float(ctx.f27.f64 * ctx.f28.f64));
	// lfsx f28,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f28.f64 = double(temp.f32);
	// stfs f29,4(r19)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r19.u32 + 4, temp.u32);
	// ori r5,r26,59604
	ctx.r5.u64 = ctx.r26.u64 | 59604;
	// fmadds f6,f30,f1,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f1.f64), float(ctx.f6.f64)));
	// stfs f6,24(r19)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r19.u32 + 24, temp.u32);
	// stfs f1,0(r19)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r19.u32 + 0, temp.u32);
	// lwz r19,8(r17)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8);
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f1,16(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// lis r26,1
	ctx.r26.s64 = 65536;
	// lfsx f27,r19,r17
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r17.u32);
	ctx.f27.f64 = double(temp.f32);
	// lis r21,2
	ctx.r21.s64 = 131072;
	// lfs f6,12(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 12);
	ctx.f6.f64 = double(temp.f32);
	// ori r26,r26,59632
	ctx.r26.u64 = ctx.r26.u64 | 59632;
	// lfsx f29,r31,r18
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r18.u32);
	ctx.f29.f64 = double(temp.f32);
	// ori r21,r21,18772
	ctx.r21.u64 = ctx.r21.u64 | 18772;
	// lfs f30,0(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f29,f29,f4
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f4.f64));
	// stfs f30,4(r17)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r17.u32 + 4, temp.u32);
	// fmadds f1,f1,f27,f28
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f27.f64), float(ctx.f28.f64)));
	// stfs f1,0(r17)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r17.u32 + 0, temp.u32);
	// fmuls f6,f6,f1
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f1.f64));
	// stfs f6,20(r17)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r17.u32 + 20, temp.u32);
	// lfs f1,16396(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16396);
	ctx.f1.f64 = double(temp.f32);
	// lfsx f6,r31,r5
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r5.u32);
	ctx.f6.f64 = double(temp.f32);
	// lwz r5,16388(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16388);
	// lwz r19,16384(r4)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16384);
	// rlwinm r18,r19,2,0,29
	ctx.r18.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r5,r19
	ctx.r5.s64 = ctx.r19.s64 - ctx.r5.s64;
	// rlwinm r5,r5,2,18,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0x3FFC;
	// lfsx f30,r5,r4
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// stfs f1,16404(r4)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16404, temp.u32);
	// stfsx f29,r18,r4
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r18.u32 + ctx.r4.u32, temp.u32);
	// lwz r5,16384(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16384);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r5,r5,20
	ctx.r5.u64 = ctx.r5.u32 & 0xFFF;
	// stw r5,16384(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16384, ctx.r5.u32);
	// lwz r4,8(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f1,r5,r25
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r25.u32);
	ctx.f1.f64 = double(temp.f32);
	// lfs f30,12(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// lfsx f29,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f29.f64 = double(temp.f32);
	// lfs f30,0(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// stfs f1,16(r25)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r25.u32 + 16, temp.u32);
	// stfs f30,4(r25)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r25.u32 + 4, temp.u32);
	// stfs f6,0(r25)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r25.u32 + 0, temp.u32);
	// lwz r4,8196(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8196);
	// lfsx f6,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f6.f64 = double(temp.f32);
	// fadds f1,f6,f0
	ctx.f1.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,8192(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8192);
	// lfs f30,8204(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 8204);
	ctx.f30.f64 = double(temp.f32);
	// addis r26,r31,2
	ctx.r26.s64 = ctx.r31.s64 + 131072;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f6,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f6.f64 = double(temp.f32);
	// lfs f28,8212(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 8212);
	ctx.f28.f64 = double(temp.f32);
	// addi r26,r26,18784
	ctx.r26.s64 = ctx.r26.s64 + 18784;
	// lfs f27,8208(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 8208);
	ctx.f27.f64 = double(temp.f32);
	// addis r25,r31,2
	ctx.r25.s64 = ctx.r31.s64 + 131072;
	// lfsx f26,r4,r24
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r24.u32);
	ctx.f26.f64 = double(temp.f32);
	// lis r4,2
	ctx.r4.s64 = 131072;
	// fmadds f30,f30,f26,f1
	ctx.f30.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f26.f64), float(ctx.f1.f64)));
	// addi r25,r25,22912
	ctx.r25.s64 = ctx.r25.s64 + 22912;
	// stfsx f30,r5,r24
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r24.u32, temp.u32);
	// lis r5,2
	ctx.r5.s64 = 131072;
	// fmuls f30,f28,f26
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// ori r22,r4,39316
	ctx.r22.u64 = ctx.r4.u64 | 39316;
	// ori r23,r5,22900
	ctx.r23.u64 = ctx.r5.u64 | 22900;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// addis r18,r31,3
	ctx.r18.s64 = ctx.r31.s64 + 196608;
	// ori r19,r5,39348
	ctx.r19.u64 = ctx.r5.u64 | 39348;
	// lwz r5,8196(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8196);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwz r4,8192(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8192);
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// fmadds f1,f27,f1,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f1.f64), float(ctx.f30.f64)));
	// stfs f1,8216(r24)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r24.u32 + 8216, temp.u32);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stw r5,8196(r24)
	PPC_STORE_U32(ctx.r24.u32 + 8196, ctx.r5.u32);
	// addi r18,r18,-26208
	ctx.r18.s64 = ctx.r18.s64 + -26208;
	// clrlwi r4,r4,21
	ctx.r4.u64 = ctx.r4.u32 & 0x7FF;
	// stw r4,8192(r24)
	PPC_STORE_U32(ctx.r24.u32 + 8192, ctx.r4.u32);
	// lfsx f30,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f30.f64 = double(temp.f32);
	// lwz r4,8196(r20)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r20.u32 + 8196);
	// lfs f1,8204(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 8204);
	ctx.f1.f64 = double(temp.f32);
	// lwz r5,8192(r20)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r20.u32 + 8192);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f28,8208(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 8208);
	ctx.f28.f64 = double(temp.f32);
	// lfsx f27,r4,r20
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r20.u32);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f6,f1,f27,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f27.f64), float(ctx.f6.f64)));
	// stfsx f6,r5,r20
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r20.u32, temp.u32);
	// lwz r5,8196(r20)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r20.u32 + 8196);
	// lwz r4,8192(r20)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r20.u32 + 8192);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// fmadds f1,f28,f6,f27
	ctx.f1.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f6.f64), float(ctx.f27.f64)));
	// clrlwi r4,r4,21
	ctx.r4.u64 = ctx.r4.u32 & 0x7FF;
	// stfs f1,8212(r20)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r20.u32 + 8212, temp.u32);
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// stw r4,8192(r20)
	PPC_STORE_U32(ctx.r20.u32 + 8192, ctx.r4.u32);
	// stw r5,8196(r20)
	PPC_STORE_U32(ctx.r20.u32 + 8196, ctx.r5.u32);
	// lfsx f1,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f1.f64 = double(temp.f32);
	// fadds f1,f1,f13
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f13.f64));
	// lwz r4,4100(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4100);
	// lfs f6,4108(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 4108);
	ctx.f6.f64 = double(temp.f32);
	// lwz r5,4096(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4096);
	// lfs f28,4112(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 4112);
	ctx.f28.f64 = double(temp.f32);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f27,r4,r26
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r26.u32);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f6,f6,f27,f30
	ctx.f6.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f27.f64), float(ctx.f30.f64)));
	// fadds f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// stfsx f6,r5,r26
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r26.u32, temp.u32);
	// fmadds f6,f28,f6,f27
	ctx.f6.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f6.f64), float(ctx.f27.f64)));
	// stfs f6,4116(r26)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4116, temp.u32);
	// lwz r5,4100(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4100);
	// lwz r4,4096(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4096);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r4,r4,22
	ctx.r4.u64 = ctx.r4.u32 & 0x3FF;
	// clrlwi r5,r5,22
	ctx.r5.u64 = ctx.r5.u32 & 0x3FF;
	// stw r4,4096(r26)
	PPC_STORE_U32(ctx.r26.u32 + 4096, ctx.r4.u32);
	// stw r5,4100(r26)
	PPC_STORE_U32(ctx.r26.u32 + 4100, ctx.r5.u32);
	// lfs f6,16396(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 16396);
	ctx.f6.f64 = double(temp.f32);
	// lwz r4,16388(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16388);
	// lfsx f30,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f30.f64 = double(temp.f32);
	// lwz r5,16384(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16384);
	// rlwinm r26,r5,2,0,29
	ctx.r26.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r4,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r4.s64;
	// rlwinm r5,r5,2,18,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0x3FFC;
	// lis r24,2
	ctx.r24.s64 = 131072;
	// lis r23,3
	ctx.r23.s64 = 196608;
	// ori r24,r24,64020
	ctx.r24.u64 = ctx.r24.u64 | 64020;
	// addis r22,r31,3
	ctx.r22.s64 = ctx.r31.s64 + 196608;
	// lfsx f28,r5,r25
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r25.u32);
	ctx.f28.f64 = double(temp.f32);
	// lis r21,3
	ctx.r21.s64 = 196608;
	// fmuls f6,f28,f6
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// stfs f6,16404(r25)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r25.u32 + 16404, temp.u32);
	// stfsx f1,r26,r25
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r26.u32 + ctx.r25.u32, temp.u32);
	// addis r26,r31,3
	ctx.r26.s64 = ctx.r31.s64 + 196608;
	// addis r20,r31,3
	ctx.r20.s64 = ctx.r31.s64 + 196608;
	// addi r26,r26,-17952
	ctx.r26.s64 = ctx.r26.s64 + -17952;
	// addi r22,r22,-1504
	ctx.r22.s64 = ctx.r22.s64 + -1504;
	// ori r23,r23,2612
	ctx.r23.u64 = ctx.r23.u64 | 2612;
	// addi r20,r20,19040
	ctx.r20.s64 = ctx.r20.s64 + 19040;
	// ori r21,r21,19028
	ctx.r21.u64 = ctx.r21.u64 | 19028;
	// lwz r5,16384(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16384);
	// addi r4,r5,1
	ctx.r4.s64 = ctx.r5.s64 + 1;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// clrlwi r4,r4,20
	ctx.r4.u64 = ctx.r4.u32 & 0xFFF;
	// ori r5,r5,47572
	ctx.r5.u64 = ctx.r5.u64 | 47572;
	// stw r4,16384(r25)
	PPC_STORE_U32(ctx.r25.u32 + 16384, ctx.r4.u32);
	// lfs f28,12(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 12);
	ctx.f28.f64 = double(temp.f32);
	// lis r4,2
	ctx.r4.s64 = 131072;
	// addis r25,r31,3
	ctx.r25.s64 = ctx.r31.s64 + 196608;
	// ori r4,r4,55800
	ctx.r4.u64 = ctx.r4.u64 | 55800;
	// addi r25,r25,-9728
	ctx.r25.s64 = ctx.r25.s64 + -9728;
	// lfs f1,0(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lwz r17,8(r18)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r18.u32 + 8);
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f6,16(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// lfsx f26,r17,r18
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + ctx.r18.u32);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f6,f6,f26
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// lfsx f27,r31,r19
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r19.u32);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f6,f28,f30,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f30.f64), float(ctx.f6.f64)));
	// stfs f1,4(r18)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r18.u32 + 4, temp.u32);
	// stfs f6,20(r18)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r18.u32 + 20, temp.u32);
	// stfs f30,0(r18)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r18.u32 + 0, temp.u32);
	// lfsx f1,r31,r5
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r5.u32);
	ctx.f1.f64 = double(temp.f32);
	// fadds f6,f1,f0
	ctx.f6.f64 = double(float(ctx.f1.f64 + ctx.f0.f64));
	// lwz r19,8192(r26)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// lfs f28,8212(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8212);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,8208(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8208);
	ctx.f26.f64 = double(temp.f32);
	// lwz r5,8196(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f1,8204(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8204);
	ctx.f1.f64 = double(temp.f32);
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f25,r5,r26
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r26.u32);
	ctx.f25.f64 = double(temp.f32);
	// lfsx f30,r31,r4
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r4.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f1,f1,f25,f6
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f25.f64), float(ctx.f6.f64)));
	// stfsx f1,r19,r26
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r19.u32 + ctx.r26.u32, temp.u32);
	// fmuls f28,f28,f25
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f25.f64));
	// lwz r5,8196(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwz r4,8192(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// fmadds f6,f26,f6,f28
	ctx.f6.f64 = double(std::fma(float(ctx.f26.f64), float(ctx.f6.f64), float(ctx.f28.f64)));
	// clrlwi r4,r4,21
	ctx.r4.u64 = ctx.r4.u32 & 0x7FF;
	// stfs f6,8216(r26)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r26.u32 + 8216, temp.u32);
	// stw r5,8196(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8196, ctx.r5.u32);
	// stw r4,8192(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8192, ctx.r4.u32);
	// lwz r4,8192(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// lfs f1,8204(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8204);
	ctx.f1.f64 = double(temp.f32);
	// lwz r5,8196(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f26,r5,r25
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r25.u32);
	ctx.f26.f64 = double(temp.f32);
	// lfsx f6,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f1,f1,f26,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f26.f64), float(ctx.f30.f64)));
	// lfs f28,8208(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8208);
	ctx.f28.f64 = double(temp.f32);
	// stfsx f1,r4,r25
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r25.u32, temp.u32);
	// fmadds f1,f28,f1,f26
	ctx.f1.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f1.f64), float(ctx.f26.f64)));
	// stfs f1,8212(r25)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8212, temp.u32);
	// lwz r5,8196(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// lwz r4,8192(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// clrlwi r4,r4,21
	ctx.r4.u64 = ctx.r4.u32 & 0x7FF;
	// stw r4,8192(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8192, ctx.r4.u32);
	// addis r26,r31,3
	ctx.r26.s64 = ctx.r31.s64 + 196608;
	// stw r5,8196(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8196, ctx.r5.u32);
	// add r24,r31,r28
	ctx.r24.u64 = ctx.r31.u64 + ctx.r28.u64;
	// addi r26,r26,2624
	ctx.r26.s64 = ctx.r26.s64 + 2624;
	// add r15,r31,r29
	ctx.r15.u64 = ctx.r31.u64 + ctx.r29.u64;
	// lwz r5,4100(r22)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r22.u32 + 4100);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f28,r5,r22
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r22.u32);
	ctx.f28.f64 = double(temp.f32);
	// lwz r4,4096(r22)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r22.u32 + 4096);
	// lfs f30,4108(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 4108);
	ctx.f30.f64 = double(temp.f32);
	// lis r5,3
	ctx.r5.s64 = 196608;
	// fmadds f6,f30,f28,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f28.f64), float(ctx.f6.f64)));
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f1,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f1.f64 = double(temp.f32);
	// ori r25,r5,19060
	ctx.r25.u64 = ctx.r5.u64 | 19060;
	// lfs f30,4112(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 4112);
	ctx.f30.f64 = double(temp.f32);
	// fadds f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// stfsx f6,r4,r22
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r22.u32, temp.u32);
	// fmadds f6,f30,f6,f28
	ctx.f6.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f6.f64), float(ctx.f28.f64)));
	// stfs f6,4116(r22)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r22.u32 + 4116, temp.u32);
	// lwz r5,4100(r22)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r22.u32 + 4100);
	// addi r4,r5,1
	ctx.r4.s64 = ctx.r5.s64 + 1;
	// lwz r5,4096(r22)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r22.u32 + 4096);
	// clrlwi r4,r4,22
	ctx.r4.u64 = ctx.r4.u32 & 0x3FF;
	// fadds f1,f1,f13
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f13.f64));
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r4,4100(r22)
	PPC_STORE_U32(ctx.r22.u32 + 4100, ctx.r4.u32);
	// fadds f13,f29,f2
	ctx.f13.f64 = double(float(ctx.f29.f64 + ctx.f2.f64));
	// clrlwi r5,r5,22
	ctx.r5.u64 = ctx.r5.u32 & 0x3FF;
	// stw r5,4096(r22)
	PPC_STORE_U32(ctx.r22.u32 + 4096, ctx.r5.u32);
	// lfsx f6,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f6.f64 = double(temp.f32);
	// fadds f28,f6,f27
	ctx.f28.f64 = double(float(ctx.f6.f64 + ctx.f27.f64));
	// lfsx f30,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f30.f64 = double(temp.f32);
	// fsubs f27,f6,f27
	ctx.f27.f64 = static_cast<float>(ctx.f6.f64 - ctx.f27.f64);
	// lfs f25,12(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 12);
	ctx.f25.f64 = double(temp.f32);
	// lwz r4,8(r20)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r20.u32 + 8);
	// fmuls f6,f25,f30
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f30.f64));
	// lfs f25,16(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 16);
	ctx.f25.f64 = double(temp.f32);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f26,0(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	ctx.f26.f64 = double(temp.f32);
	// lfsx f24,r5,r20
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r20.u32);
	ctx.f24.f64 = double(temp.f32);
	// stfs f26,4(r20)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r20.u32 + 4, temp.u32);
	// stfs f30,0(r20)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r20.u32 + 0, temp.u32);
	// fmadds f6,f25,f24,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f25.f64), float(ctx.f24.f64), float(ctx.f6.f64)));
	// stfs f6,20(r20)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r20.u32 + 20, temp.u32);
	// lwz r23,16392(r26)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16392);
	// lfs f26,16400(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 16400);
	ctx.f26.f64 = double(temp.f32);
	// lwz r25,16388(r26)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16388);
	// lfs f30,16396(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 16396);
	ctx.f30.f64 = double(temp.f32);
	// lwz r4,16384(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16384);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r23,r23,r4
	ctx.r23.s64 = ctx.r4.s64 - ctx.r23.s64;
	// lfsx f6,r31,r27
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r27.u32);
	ctx.f6.f64 = double(temp.f32);
	// subf r4,r25,r4
	ctx.r4.s64 = ctx.r4.s64 - ctx.r25.s64;
	// fadds f6,f6,f8
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f8.f64));
	// rlwinm r25,r23,2,18,29
	ctx.r25.u64 = rotl64(ctx.r23.u32 | (ctx.r23.u64 << 32), 2) & 0x3FFC;
	// rlwinm r4,r4,2,18,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0x3FFC;
	// lfsx f25,r25,r26
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r26.u32);
	ctx.f25.f64 = double(temp.f32);
	// lfsx f24,r4,r26
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r26.u32);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f26,f25,f26
	ctx.f26.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// fmuls f30,f24,f30
	ctx.f30.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// stfs f30,16404(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16404, temp.u32);
	// stfs f26,16408(r26)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16408, temp.u32);
	// stfsx f1,r5,r26
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r26.u32, temp.u32);
	// lwz r5,16384(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 16384);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// clrlwi r4,r5,20
	ctx.r4.u64 = ctx.r5.u32 & 0xFFF;
	// stw r4,16384(r26)
	PPC_STORE_U32(ctx.r26.u32 + 16384, ctx.r4.u32);
	// lfs f30,2064(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 2064);
	ctx.f30.f64 = double(temp.f32);
	// lwz r4,2048(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2048);
	// lwz r5,2052(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2052);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f26,r5,r24
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r24.u32);
	ctx.f26.f64 = double(temp.f32);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f1,2060(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 2060);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f1,f26,f28
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f26.f64), float(ctx.f28.f64)));
	// fadds f1,f1,f11
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f11.f64));
	// stfsx f1,r4,r24
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r24.u32, temp.u32);
	// fmadds f1,f30,f1,f26
	ctx.f1.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f1.f64), float(ctx.f26.f64)));
	// stfs f1,2068(r24)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r24.u32 + 2068, temp.u32);
	// lwz r5,2052(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2052);
	// fsubs f2,f29,f2
	ctx.f2.f64 = static_cast<float>(ctx.f29.f64 - ctx.f2.f64);
	// lwz r4,2048(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2048);
	// add r25,r31,r30
	ctx.r25.u64 = ctx.r31.u64 + ctx.r30.u64;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwz r22,80(r1)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lwz r16,136(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// clrlwi r5,r5,23
	ctx.r5.u64 = ctx.r5.u32 & 0x1FF;
	// lwz r17,140(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// clrlwi r4,r4,23
	ctx.r4.u64 = ctx.r4.u32 & 0x1FF;
	// lwz r18,144(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r5,2052(r24)
	PPC_STORE_U32(ctx.r24.u32 + 2052, ctx.r5.u32);
	// stw r4,2048(r24)
	PPC_STORE_U32(ctx.r24.u32 + 2048, ctx.r4.u32);
	// add r24,r31,r3
	ctx.r24.u64 = ctx.r31.u64 + ctx.r3.u64;
	// lwz r19,148(r1)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r20,152(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r23,156(r1)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// lwz r26,452(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// lwz r5,2052(r15)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r15.u32 + 2052);
	// lfs f1,2060(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 2060);
	ctx.f1.f64 = double(temp.f32);
	// lwz r4,2048(r15)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r15.u32 + 2048);
	// lfs f30,2064(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 2064);
	ctx.f30.f64 = double(temp.f32);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f29,r5,r15
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r15.u32);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f1,f1,f29,f27
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f29.f64), float(ctx.f27.f64)));
	// fadds f11,f1,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 + ctx.f11.f64));
	// stfsx f11,r4,r15
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r15.u32, temp.u32);
	// lwz r5,2048(r15)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r15.u32 + 2048);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lwz r4,2052(r15)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r15.u32 + 2052);
	// clrlwi r5,r5,23
	ctx.r5.u64 = ctx.r5.u32 & 0x1FF;
	// fmadds f1,f30,f11,f29
	ctx.f1.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f11.f64), float(ctx.f29.f64)));
	// stfs f1,2068(r15)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r15.u32 + 2068, temp.u32);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stw r5,2048(r15)
	PPC_STORE_U32(ctx.r15.u32 + 2048, ctx.r5.u32);
	// clrlwi r4,r4,23
	ctx.r4.u64 = ctx.r4.u32 & 0x1FF;
	// stw r4,2052(r15)
	PPC_STORE_U32(ctx.r15.u32 + 2052, ctx.r4.u32);
	// lwz r4,8196(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// lfs f11,8204(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8204);
	ctx.f11.f64 = double(temp.f32);
	// lfs f1,8208(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8208);
	ctx.f1.f64 = double(temp.f32);
	// lwz r5,8192(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r4,r25
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r25.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f13,f11,f30,f13
	ctx.f13.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f30.f64), float(ctx.f13.f64)));
	// fadds f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfsx f11,r5,r25
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r25.u32, temp.u32);
	// lwz r4,8196(r25)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lwz r5,8192(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// clrlwi r4,r4,21
	ctx.r4.u64 = ctx.r4.u32 & 0x7FF;
	// fmadds f1,f1,f11,f30
	ctx.f1.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f11.f64), float(ctx.f30.f64)));
	// stfs f1,8212(r25)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8212, temp.u32);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r4,8196(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8196, ctx.r4.u32);
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// stw r5,8192(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8192, ctx.r5.u32);
	// lwz r4,8196(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8196);
	// lfs f13,8204(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 8204);
	ctx.f13.f64 = double(temp.f32);
	// lfs f1,8208(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 8208);
	ctx.f1.f64 = double(temp.f32);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,8192(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8192);
	// lfsx f11,r5,r24
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r24.u32);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// fmadds f2,f13,f11,f2
	ctx.f2.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f2.f64)));
	// fadds f0,f2,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 + ctx.f0.f64));
	// stfsx f0,r5,r24
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r24.u32, temp.u32);
	// lwz r5,8196(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8196);
	// addi r25,r5,1
	ctx.r25.s64 = ctx.r5.s64 + 1;
	// lwz r4,8192(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8192);
	// clrlwi r25,r25,21
	ctx.r25.u64 = ctx.r25.u32 & 0x7FF;
	// fmadds f13,f1,f0,f11
	ctx.f13.f64 = double(std::fma(float(ctx.f1.f64), float(ctx.f0.f64), float(ctx.f11.f64)));
	// lwz r5,160(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// addi r21,r4,1
	ctx.r21.s64 = ctx.r4.s64 + 1;
	// lwz r4,164(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// stfs f13,8212(r24)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r24.u32 + 8212, temp.u32);
	// stw r25,8196(r24)
	PPC_STORE_U32(ctx.r24.u32 + 8196, ctx.r25.u32);
	// clrlwi r21,r21,21
	ctx.r21.u64 = ctx.r21.u32 & 0x7FF;
	// stw r21,8192(r24)
	PPC_STORE_U32(ctx.r24.u32 + 8192, ctx.r21.u32);
loc_822E03EC:
	// lwz r25,1024(r23)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r23.u32 + 1024);
	// fadds f1,f9,f6
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f9.f64 + ctx.f6.f64));
	// lwz r21,1044(r23)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r23.u32 + 1044);
	// lfs f0,1056(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 1056);
	ctx.f0.f64 = double(temp.f32);
	// lwz r24,1040(r23)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r23.u32 + 1040);
	// fsubs f2,f7,f0
	ctx.f2.f64 = static_cast<float>(ctx.f7.f64 - ctx.f0.f64);
	// subf r15,r21,r25
	ctx.r15.s64 = ctx.r25.s64 - ctx.r21.s64;
	// lwz r21,444(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// subf r14,r24,r25
	ctx.r14.s64 = ctx.r25.s64 - ctx.r24.s64;
	// std r11,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r11.u64);
	// rlwinm r15,r15,2,22,29
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0x3FC;
	// lwz r11,168(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// rlwinm r14,r14,2,22,29
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 2) & 0x3FC;
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// lfsu f13,4(r21)
	ea = 4 + ctx.r21.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r21.u32 = ea;
	ctx.f13.f64 = double(temp.f32);
	// lfsx f9,r15,r23
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + ctx.r23.u32);
	ctx.f9.f64 = double(temp.f32);
	// stw r21,444(r1)
	PPC_STORE_U32(ctx.r1.u32 + 444, ctx.r21.u32);
	// fmuls f30,f9,f0
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfsx f29,r14,r23
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + ctx.r23.u32);
	ctx.f29.f64 = double(temp.f32);
	// lfs f11,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// ld r11,192(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// fmuls f11,f11,f5
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f5.f64));
	// fmuls f9,f1,f5
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// fmadds f2,f29,f2,f30
	ctx.f2.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f2.f64), float(ctx.f30.f64)));
	// stfs f2,1060(r23)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r23.u32 + 1060, temp.u32);
	// ble cr6,0x822e045c
	if (!ctx.cr6.gt) goto loc_822E045C;
	// fsubs f0,f0,f3
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f3.f64);
	// b 0x822e047c
	goto loc_822E047C;
loc_822E045C:
	// lwz r21,1048(r23)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r23.u32 + 1048);
	// cmplw cr6,r21,r24
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, ctx.r24.u32, ctx.xer);
	// beq cr6,0x822e0470
	if (ctx.cr6.eq) goto loc_822E0470;
	// fmr f0,f7
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f7.f64;
	// b 0x822e0474
	goto loc_822E0474;
loc_822E0470:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f12.f64;
loc_822E0474:
	// stw r24,1044(r23)
	PPC_STORE_U32(ctx.r23.u32 + 1044, ctx.r24.u32);
	// stw r21,1040(r23)
	PPC_STORE_U32(ctx.r23.u32 + 1040, ctx.r21.u32);
loc_822E047C:
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,1056(r23)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r23.u32 + 1056, temp.u32);
	// lwz r24,172(r1)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// fsubs f0,f7,f10
	ctx.f0.f64 = static_cast<float>(ctx.f7.f64 - ctx.f10.f64);
	// lwz r21,176(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// fmuls f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// addi r22,r22,4
	ctx.r22.s64 = ctx.r22.s64 + 4;
	// stfsx f8,r25,r23
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r25.u32 + ctx.r23.u32, temp.u32);
	// stw r22,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r22.u32);
	// lwz r25,1024(r23)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r23.u32 + 1024);
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// clrlwi r25,r25,24
	ctx.r25.u64 = ctx.r25.u32 & 0xFF;
	// stw r25,1024(r23)
	PPC_STORE_U32(ctx.r23.u32 + 1024, ctx.r25.u32);
	// lfs f1,2064(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 2064);
	ctx.f1.f64 = double(temp.f32);
	// lwz r25,2052(r5)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2052);
	// lwz r15,2048(r5)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2048);
	// lfs f2,0(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f8,2060(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 2060);
	ctx.f8.f64 = double(temp.f32);
	// rlwinm r24,r15,2,0,29
	ctx.r24.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r25,r5
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r5.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f9,f30,f8,f9
	ctx.f9.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f8.f64), float(ctx.f9.f64)));
	// stfsx f9,r24,r5
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r24.u32 + ctx.r5.u32, temp.u32);
	// lwz r25,2052(r5)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2052);
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// lwz r24,2048(r5)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r5.u32 + 2048);
	// addi r24,r24,1
	ctx.r24.s64 = ctx.r24.s64 + 1;
	// clrlwi r25,r25,23
	ctx.r25.u64 = ctx.r25.u32 & 0x1FF;
	// fmadds f8,f9,f1,f30
	ctx.f8.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f1.f64), float(ctx.f30.f64)));
	// clrlwi r24,r24,23
	ctx.r24.u64 = ctx.r24.u32 & 0x1FF;
	// stfs f8,2068(r5)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r5.u32 + 2068, temp.u32);
	// stw r25,2052(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2052, ctx.r25.u32);
	// stw r24,2048(r5)
	PPC_STORE_U32(ctx.r5.u32 + 2048, ctx.r24.u32);
	// lfs f1,0(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lwz r25,2052(r4)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2052);
	// fmuls f9,f1,f10
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// lwz r24,2048(r4)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f8,2060(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2060);
	ctx.f8.f64 = double(temp.f32);
	// lfs f1,2064(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2064);
	ctx.f1.f64 = double(temp.f32);
	// rlwinm r24,r24,2,0,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// fadds f10,f10,f31
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f31.f64));
	// fmadds f30,f13,f0,f9
	ctx.f30.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f0.f64), float(ctx.f9.f64)));
	// stfs f30,0(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 0, temp.u32);
	// fmadds f0,f13,f0,f9
	ctx.f0.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f0.f64), float(ctx.f9.f64)));
	// lfsx f13,r25,r4
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r4.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f9,f13,f8,f2
	ctx.f9.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f8.f64), float(ctx.f2.f64)));
	// stfsx f9,r24,r4
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r24.u32 + ctx.r4.u32, temp.u32);
	// stfsu f0,4(r26)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r26.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r26.u32 = ea;
	// lwz r24,2048(r4)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2048);
	// stfsu f12,4(r26)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r26.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r26.u32 = ea;
	// lwz r25,2052(r4)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r4.u32 + 2052);
	// stfsu f12,4(r26)
	temp.f32 = float(ctx.f12.f64);
	ea = 4 + ctx.r26.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r26.u32 = ea;
	// addi r25,r25,1
	ctx.r25.s64 = ctx.r25.s64 + 1;
	// stfsu f11,4(r26)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r26.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r26.u32 = ea;
	// addi r24,r24,1
	ctx.r24.s64 = ctx.r24.s64 + 1;
	// stfsu f11,4(r26)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r26.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r26.u32 = ea;
	// clrlwi r25,r25,23
	ctx.r25.u64 = ctx.r25.u32 & 0x1FF;
	// clrlwi r24,r24,23
	ctx.r24.u64 = ctx.r24.u32 & 0x1FF;
	// fmadds f8,f9,f1,f13
	ctx.f8.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f1.f64), float(ctx.f13.f64)));
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// stfs f8,2068(r4)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r4.u32 + 2068, temp.u32);
	// stw r25,2052(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2052, ctx.r25.u32);
	// stw r26,452(r1)
	PPC_STORE_U32(ctx.r1.u32 + 452, ctx.r26.u32);
	// stw r24,2048(r4)
	PPC_STORE_U32(ctx.r4.u32 + 2048, ctx.r24.u32);
	// bdnz 0x822df840
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822DF840;
loc_822E0584:
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa74
	ctx.lr = 0x822E0590;
	__savefpr_24(ctx, base);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822E0594"))) PPC_WEAK_FUNC(sub_822E0594);
PPC_FUNC_IMPL(__imp__sub_822E0594) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822E0598"))) PPC_WEAK_FUNC(sub_822E0598);
PPC_FUNC_IMPL(__imp__sub_822E0598) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x822E05A0;
	__restfpr_14(ctx, base);
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa1c
	ctx.lr = 0x822E05A8;
	sub_8233FA1C(ctx, base);
	// stwu r1,-528(r1)
	ea = -528 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// stw r5,564(r1)
	PPC_STORE_U32(ctx.r1.u32 + 564, ctx.r5.u32);
	// mr r29,r6
	ctx.r29.u64 = ctx.r6.u64;
	// stw r6,572(r1)
	PPC_STORE_U32(ctx.r1.u32 + 572, ctx.r6.u32);
	// addis r3,r3,3
	ctx.r3.s64 = ctx.r3.s64 + 196608;
	// stw r4,556(r1)
	PPC_STORE_U32(ctx.r1.u32 + 556, ctx.r4.u32);
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// stw r7,580(r1)
	PPC_STORE_U32(ctx.r1.u32 + 580, ctx.r7.u32);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = ctx.f1.f64;
	// addi r3,r3,23248
	ctx.r3.s64 = ctx.r3.s64 + 23248;
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// bl 0x822dbc80
	ctx.lr = 0x822E05E0;
	sub_822DBC80(ctx, base);
	// addis r3,r31,5
	ctx.r3.s64 = ctx.r31.s64 + 327680;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// addi r3,r3,23296
	ctx.r3.s64 = ctx.r3.s64 + 23296;
	// bl 0x822db588
	ctx.lr = 0x822E05F4;
	sub_822DB588(ctx, base);
	// lis r11,3
	ctx.r11.s64 = 196608;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// ori r9,r11,23232
	ctx.r9.u64 = ctx.r11.u64 | 23232;
	// lfs f7,-28948(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f7.f64 = double(temp.f32);
	// lwzx r8,r31,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r9.u32);
	// fcmpu cr6,f31,f7
	ctx.cr6.compare(ctx.f31.f64, ctx.f7.f64);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// ble cr6,0x822e061c
	if (!ctx.cr6.gt) goto loc_822E061C;
	// fmr f12,f7
	ctx.f12.f64 = ctx.f7.f64;
loc_822E061C:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// fmr f6,f7
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = ctx.f7.f64;
	// beq cr6,0x822e1908
	if (ctx.cr6.eq) goto loc_822E1908;
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// lwz r11,580(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	// addis r9,r31,1
	ctx.r9.s64 = ctx.r31.s64 + 65536;
	// addis r8,r31,1
	ctx.r8.s64 = ctx.r31.s64 + 65536;
	// addis r7,r31,1
	ctx.r7.s64 = ctx.r31.s64 + 65536;
	// addis r6,r31,1
	ctx.r6.s64 = ctx.r31.s64 + 65536;
	// addi r10,r10,2188
	ctx.r10.s64 = ctx.r10.s64 + 2188;
	// addi r9,r9,2192
	ctx.r9.s64 = ctx.r9.s64 + 2192;
	// addi r8,r8,2740
	ctx.r8.s64 = ctx.r8.s64 + 2740;
	// stw r10,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r10.u32);
	// addi r7,r7,10984
	ctx.r7.s64 = ctx.r7.s64 + 10984;
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// addi r6,r6,10988
	ctx.r6.s64 = ctx.r6.s64 + 10988;
	// stw r8,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r8.u32);
	// addis r5,r31,1
	ctx.r5.s64 = ctx.r31.s64 + 65536;
	// stw r7,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r7.u32);
	// addis r4,r31,1
	ctx.r4.s64 = ctx.r31.s64 + 65536;
	// stw r6,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r6.u32);
	// addi r5,r5,10992
	ctx.r5.s64 = ctx.r5.s64 + 10992;
	// addi r4,r4,12052
	ctx.r4.s64 = ctx.r4.s64 + 12052;
	// addis r3,r31,1
	ctx.r3.s64 = ctx.r31.s64 + 65536;
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// stw r4,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r4.u32);
	// addis r9,r31,1
	ctx.r9.s64 = ctx.r31.s64 + 65536;
	// addis r8,r31,1
	ctx.r8.s64 = ctx.r31.s64 + 65536;
	// addis r7,r31,1
	ctx.r7.s64 = ctx.r31.s64 + 65536;
	// addis r6,r31,5
	ctx.r6.s64 = ctx.r31.s64 + 327680;
	// addi r3,r3,14132
	ctx.r3.s64 = ctx.r3.s64 + 14132;
	// addi r10,r10,80
	ctx.r10.s64 = ctx.r10.s64 + 80;
	// addi r9,r9,84
	ctx.r9.s64 = ctx.r9.s64 + 84;
	// stw r3,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r3.u32);
	// addi r8,r8,18260
	ctx.r8.s64 = ctx.r8.s64 + 18260;
	// stw r10,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r10.u32);
	// addi r7,r7,18264
	ctx.r7.s64 = ctx.r7.s64 + 18264;
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// addi r6,r6,25440
	ctx.r6.s64 = ctx.r6.s64 + 25440;
	// stw r8,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r8.u32);
	// addis r5,r31,5
	ctx.r5.s64 = ctx.r31.s64 + 327680;
	// stw r7,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r7.u32);
	// addis r4,r31,6
	ctx.r4.s64 = ctx.r31.s64 + 393216;
	// stw r6,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r6.u32);
	// addi r5,r5,25988
	ctx.r5.s64 = ctx.r5.s64 + 25988;
	// addi r4,r4,-31304
	ctx.r4.s64 = ctx.r4.s64 + -31304;
	// addis r19,r31,1
	ctx.r19.s64 = ctx.r31.s64 + 65536;
	// stw r5,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r5.u32);
	// addis r28,r31,6
	ctx.r28.s64 = ctx.r31.s64 + 393216;
	// stw r4,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r4.u32);
	// addis r18,r31,5
	ctx.r18.s64 = ctx.r31.s64 + 327680;
	// addis r17,r31,5
	ctx.r17.s64 = ctx.r31.s64 + 327680;
	// addis r3,r31,6
	ctx.r3.s64 = ctx.r31.s64 + 393216;
	// addis r10,r31,6
	ctx.r10.s64 = ctx.r31.s64 + 393216;
	// addis r9,r31,6
	ctx.r9.s64 = ctx.r31.s64 + 393216;
	// addis r8,r31,6
	ctx.r8.s64 = ctx.r31.s64 + 393216;
	// addis r7,r31,5
	ctx.r7.s64 = ctx.r31.s64 + 327680;
	// addis r6,r31,5
	ctx.r6.s64 = ctx.r31.s64 + 327680;
	// addi r19,r19,2184
	ctx.r19.s64 = ctx.r19.s64 + 2184;
	// addi r28,r28,-30224
	ctx.r28.s64 = ctx.r28.s64 + -30224;
	// addi r18,r18,25432
	ctx.r18.s64 = ctx.r18.s64 + 25432;
	// stw r19,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r19.u32);
	// addi r17,r17,25436
	ctx.r17.s64 = ctx.r17.s64 + 25436;
	// stw r28,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r28.u32);
	// addi r3,r3,-31300
	ctx.r3.s64 = ctx.r3.s64 + -31300;
	// stw r18,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r18.u32);
	// addi r10,r10,-31296
	ctx.r10.s64 = ctx.r10.s64 + -31296;
	// stw r17,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r17.u32);
	// addi r9,r9,-30236
	ctx.r9.s64 = ctx.r9.s64 + -30236;
	// stw r3,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, ctx.r3.u32);
	// addi r8,r8,-28156
	ctx.r8.s64 = ctx.r8.s64 + -28156;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// addi r7,r7,23336
	ctx.r7.s64 = ctx.r7.s64 + 23336;
	// stw r9,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r9.u32);
	// addi r6,r6,23340
	ctx.r6.s64 = ctx.r6.s64 + 23340;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// addis r5,r31,6
	ctx.r5.s64 = ctx.r31.s64 + 393216;
	// stw r7,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r7.u32);
	// addis r4,r31,6
	ctx.r4.s64 = ctx.r31.s64 + 393216;
	// stw r6,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r6.u32);
	// addi r5,r5,-24028
	ctx.r5.s64 = ctx.r5.s64 + -24028;
	// addi r4,r4,-24024
	ctx.r4.s64 = ctx.r4.s64 + -24024;
	// addis r26,r31,3
	ctx.r26.s64 = ctx.r31.s64 + 196608;
	// addis r22,r31,3
	ctx.r22.s64 = ctx.r31.s64 + 196608;
	// lwz r27,572(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	// addis r21,r31,3
	ctx.r21.s64 = ctx.r31.s64 + 196608;
	// stw r5,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r5.u32);
	// addi r22,r22,21140
	ctx.r22.s64 = ctx.r22.s64 + 21140;
	// stw r4,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r4.u32);
	// addis r16,r31,6
	ctx.r16.s64 = ctx.r31.s64 + 393216;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// stw r22,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r22.u32);
	// addis r22,r31,6
	ctx.r22.s64 = ctx.r31.s64 + 393216;
	// addi r21,r21,23220
	ctx.r21.s64 = ctx.r21.s64 + 23220;
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r27.u32);
	// addi r16,r16,-21948
	ctx.r16.s64 = ctx.r16.s64 + -21948;
	// addi r22,r22,-19868
	ctx.r22.s64 = ctx.r22.s64 + -19868;
	// stw r21,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r21.u32);
	// addis r3,r31,3
	ctx.r3.s64 = ctx.r31.s64 + 196608;
	// stw r16,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r16.u32);
	// stw r22,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, ctx.r22.u32);
	// addis r25,r31,3
	ctx.r25.s64 = ctx.r31.s64 + 196608;
	// addi r3,r3,23240
	ctx.r3.s64 = ctx.r3.s64 + 23240;
	// addis r24,r31,6
	ctx.r24.s64 = ctx.r31.s64 + 393216;
	// addis r23,r31,6
	ctx.r23.s64 = ctx.r31.s64 + 393216;
	// stw r3,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r3.u32);
	// addis r20,r31,3
	ctx.r20.s64 = ctx.r31.s64 + 196608;
	// lis r22,-32256
	ctx.r22.s64 = -2113929216;
	// lis r21,-32256
	ctx.r21.s64 = -2113929216;
	// lis r16,-32255
	ctx.r16.s64 = -2113863680;
	// addi r26,r26,19072
	ctx.r26.s64 = ctx.r26.s64 + 19072;
	// addi r25,r25,21152
	ctx.r25.s64 = ctx.r25.s64 + 21152;
	// addi r24,r24,-24016
	ctx.r24.s64 = ctx.r24.s64 + -24016;
	// lfs f1,5256(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 5256);
	ctx.f1.f64 = double(temp.f32);
	// addi r23,r23,-21936
	ctx.r23.s64 = ctx.r23.s64 + -21936;
	// lfs f2,5260(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 5260);
	ctx.f2.f64 = double(temp.f32);
	// addi r20,r20,23236
	ctx.r20.s64 = ctx.r20.s64 + 23236;
	// lfs f3,-1560(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + -1560);
	ctx.f3.f64 = double(temp.f32);
	// addis r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 65536;
	// stw r26,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r26.u32);
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// stw r25,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r25.u32);
	// addis r7,r31,1
	ctx.r7.s64 = ctx.r31.s64 + 65536;
	// stw r24,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r24.u32);
	// addis r5,r31,1
	ctx.r5.s64 = ctx.r31.s64 + 65536;
	// stw r23,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r23.u32);
	// addis r4,r31,1
	ctx.r4.s64 = ctx.r31.s64 + 65536;
	// stw r20,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r20.u32);
	// addis r3,r31,1
	ctx.r3.s64 = ctx.r31.s64 + 65536;
	// addis r9,r31,5
	ctx.r9.s64 = ctx.r31.s64 + 327680;
	// addis r8,r31,5
	ctx.r8.s64 = ctx.r31.s64 + 327680;
	// addis r6,r31,6
	ctx.r6.s64 = ctx.r31.s64 + 393216;
	// addis r30,r31,5
	ctx.r30.s64 = ctx.r31.s64 + 327680;
	// addis r29,r31,6
	ctx.r29.s64 = ctx.r31.s64 + 393216;
	// addi r11,r11,96
	ctx.r11.s64 = ctx.r11.s64 + 96;
	// addi r10,r10,2752
	ctx.r10.s64 = ctx.r10.s64 + 2752;
	// addi r7,r7,14144
	ctx.r7.s64 = ctx.r7.s64 + 14144;
	// addi r5,r5,2208
	ctx.r5.s64 = ctx.r5.s64 + 2208;
	// addi r4,r4,11008
	ctx.r4.s64 = ctx.r4.s64 + 11008;
	// addi r3,r3,12064
	ctx.r3.s64 = ctx.r3.s64 + 12064;
	// addi r9,r9,23344
	ctx.r9.s64 = ctx.r9.s64 + 23344;
	// addi r8,r8,26000
	ctx.r8.s64 = ctx.r8.s64 + 26000;
	// addi r6,r6,-28144
	ctx.r6.s64 = ctx.r6.s64 + -28144;
	// addi r30,r30,25456
	ctx.r30.s64 = ctx.r30.s64 + 25456;
	// addi r29,r29,-31280
	ctx.r29.s64 = ctx.r29.s64 + -31280;
	// b 0x822e086c
	goto loc_822E086C;
loc_822E0868:
	// lwz r17,192(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
loc_822E086C:
	// lwz r22,2048(r11)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// lfs f10,0(r19)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lwz r21,2064(r11)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2064);
	// lfs f0,2076(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2076);
	ctx.f0.f64 = double(temp.f32);
	// lwz r16,2068(r11)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2068);
	// rlwinm r15,r22,2,0,29
	ctx.r15.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// std r31,280(r1)
	PPC_STORE_U64(ctx.r1.u32 + 280, ctx.r31.u64);
	// subf r21,r21,r22
	ctx.r21.s64 = ctx.r22.s64 - ctx.r21.s64;
	// lwz r14,2072(r11)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2072);
	// subf r16,r16,r22
	ctx.r16.s64 = ctx.r22.s64 - ctx.r16.s64;
	// lwz r31,176(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// rlwinm r21,r21,2,21,29
	ctx.r21.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0x7FC;
	// subf r22,r14,r22
	ctx.r22.s64 = ctx.r22.s64 - ctx.r14.s64;
	// lwz r14,96(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// rlwinm r16,r16,2,21,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0x7FC;
	// lfs f13,2080(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2080);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r22,r22,2,21,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0x7FC;
	// lfs f11,2084(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2084);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,0(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lwz r31,104(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// fadds f4,f8,f10
	ctx.f4.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// lfsx f8,r21,r11
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r11.u32);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f0,f8,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfsx f30,r16,r11
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r11.u32);
	ctx.f30.f64 = double(temp.f32);
	// lfsx f8,r22,r11
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r11.u32);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f13,f13,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f30.f64));
	// lfs f5,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f11,f11,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// stfs f11,2096(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2096, temp.u32);
	// lwz r14,184(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// stfs f0,2088(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2088, temp.u32);
	// lwz r21,248(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// stfs f13,2092(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2092, temp.u32);
	// stw r21,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r21.u32);
	// stfsx f9,r15,r11
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r15.u32 + ctx.r11.u32, temp.u32);
	// lwz r22,2048(r11)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// lwz r16,112(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// clrlwi r22,r22,23
	ctx.r22.u64 = ctx.r22.u32 & 0x1FF;
	// lfs f10,4(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// stw r22,2048(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2048, ctx.r22.u32);
	// lfs f9,524(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 524);
	ctx.f9.f64 = double(temp.f32);
	// lwz r22,512(r5)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r5.u32 + 512);
	// lfs f8,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lwz r15,516(r5)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r5.u32 + 516);
	// rlwinm r15,r15,2,0,29
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r22,r22,2,0,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r15,r5
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + ctx.r5.u32);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f11,f0,f9,f4
	ctx.f11.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f9.f64), float(ctx.f4.f64)));
	// lfs f13,528(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 528);
	ctx.f13.f64 = double(temp.f32);
	// stfsx f11,r22,r5
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r22.u32 + ctx.r5.u32, temp.u32);
	// lwz r21,512(r5)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r5.u32 + 512);
	// lwz r22,516(r5)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r5.u32 + 516);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// addi r21,r21,1
	ctx.r21.s64 = ctx.r21.s64 + 1;
	// lwz r15,84(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// clrlwi r22,r22,25
	ctx.r22.u64 = ctx.r22.u32 & 0x7F;
	// fmadds f9,f13,f11,f0
	ctx.f9.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f0.f64)));
	// clrlwi r21,r21,25
	ctx.r21.u64 = ctx.r21.u32 & 0x7F;
	// stfs f9,532(r5)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r5.u32 + 532, temp.u32);
	// stw r22,516(r5)
	PPC_STORE_U32(ctx.r5.u32 + 516, ctx.r22.u32);
	// stw r21,512(r5)
	PPC_STORE_U32(ctx.r5.u32 + 512, ctx.r21.u32);
	// lfs f4,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// lwz r22,8212(r10)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8212);
	// lfs f0,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r21,8216(r10)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8216);
	// lfs f11,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lwz r15,8192(r10)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lfs f13,8220(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8220);
	ctx.f13.f64 = double(temp.f32);
	// lwz r14,8208(r10)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8208);
	// subf r14,r14,r15
	ctx.r14.s64 = ctx.r15.s64 - ctx.r14.s64;
	// rlwinm r16,r14,2,19,29
	ctx.r16.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 2) & 0x1FFC;
	// fadds f4,f0,f4
	ctx.f4.f64 = double(float(ctx.f0.f64 + ctx.f4.f64));
	// subf r22,r22,r15
	ctx.r22.s64 = ctx.r15.s64 - ctx.r22.s64;
	// lfs f9,8224(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8224);
	ctx.f9.f64 = double(temp.f32);
	// subf r21,r21,r15
	ctx.r21.s64 = ctx.r15.s64 - ctx.r21.s64;
	// rlwinm r22,r22,2,19,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0x1FFC;
	// rlwinm r21,r21,2,19,29
	ctx.r21.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0x1FFC;
	// lfsx f0,r16,r10
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r10.u32);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r16,r15,2,0,29
	ctx.r16.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r22,r10
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r10.u32);
	ctx.f30.f64 = double(temp.f32);
	// lwz r22,200(r1)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// fmuls f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfsx f13,r21,r10
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f29,8228(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8228);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f9,f30,f9
	ctx.f9.f64 = double(float(ctx.f30.f64 * ctx.f9.f64));
	// stfs f0,8232(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8232, temp.u32);
	// fmuls f0,f13,f29
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f29.f64));
	// stfs f9,8236(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8236, temp.u32);
	// std r11,264(r1)
	PPC_STORE_U64(ctx.r1.u32 + 264, ctx.r11.u64);
	// stfs f0,8240(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8240, temp.u32);
	// stw r22,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r22.u32);
	// stfsx f8,r16,r10
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r16.u32 + ctx.r10.u32, temp.u32);
	// lwz r22,8192(r10)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// lwz r15,120(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// clrlwi r22,r22,21
	ctx.r22.u64 = ctx.r22.u32 & 0x7FF;
	// lwz r21,136(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// stw r21,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r21.u32);
	// stw r22,8192(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8192, ctx.r22.u32);
	// lfs f13,1036(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1036);
	ctx.f13.f64 = double(temp.f32);
	// lwz r22,1024(r4)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r4.u32 + 1024);
	// lfs f30,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// lwz r11,1028(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 1028);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r22,r22,2,0,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r11,r4
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r4.u32);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,1040(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1040);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f13,f0,f13,f4
	ctx.f13.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f13.f64), float(ctx.f4.f64)));
	// stfsx f13,r22,r4
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r22.u32 + ctx.r4.u32, temp.u32);
	// lwz r21,1024(r4)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r4.u32 + 1024);
	// lwz r22,1028(r4)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r4.u32 + 1028);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// addi r21,r21,1
	ctx.r21.s64 = ctx.r21.s64 + 1;
	// fmadds f9,f13,f9,f0
	ctx.f9.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f9.f64), float(ctx.f0.f64)));
	// clrlwi r22,r22,24
	ctx.r22.u64 = ctx.r22.u32 & 0xFF;
	// stfs f9,1044(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 1044, temp.u32);
	// clrlwi r21,r21,24
	ctx.r21.u64 = ctx.r21.u32 & 0xFF;
	// lwz r14,232(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// stw r22,1028(r4)
	PPC_STORE_U32(ctx.r4.u32 + 1028, ctx.r22.u32);
	// stw r21,1024(r4)
	PPC_STORE_U32(ctx.r4.u32 + 1024, ctx.r21.u32);
	// lfs f4,2060(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 2060);
	ctx.f4.f64 = double(temp.f32);
	// lwz r22,2048(r3)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2048);
	// lfs f0,2064(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 2064);
	ctx.f0.f64 = double(temp.f32);
	// lwz r21,2052(r3)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2052);
	// rlwinm r21,r21,2,0,29
	ctx.r21.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r21,r3
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r3.u32);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r22,r22,2,0,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f29,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f9,f4,f13,f30
	ctx.f9.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f13.f64), float(ctx.f30.f64)));
	// stfsx f9,r22,r3
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r22.u32 + ctx.r3.u32, temp.u32);
	// lwz r14,144(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// fmadds f4,f9,f0,f13
	ctx.f4.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f0.f64), float(ctx.f13.f64)));
	// stfs f4,2068(r3)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r3.u32 + 2068, temp.u32);
	// lwz r16,272(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// lwz r31,128(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r15,84(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r22,2052(r3)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2052);
	// addi r21,r22,1
	ctx.r21.s64 = ctx.r22.s64 + 1;
	// lwz r22,2048(r3)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2048);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// clrlwi r21,r21,23
	ctx.r21.u64 = ctx.r21.u32 & 0x1FF;
	// clrlwi r22,r22,23
	ctx.r22.u64 = ctx.r22.u32 & 0x1FF;
	// stw r21,2052(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2052, ctx.r21.u32);
	// stw r22,2048(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2048, ctx.r22.u32);
	// lfs f0,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r22,4096(r7)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4096);
	// lfs f4,4108(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4108);
	ctx.f4.f64 = double(temp.f32);
	// lwz r21,4100(r7)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4100);
	// lwz r14,0(r14)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	// lwz r16,0(r31)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r16,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r16.u32);
	// lfs f9,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lwz r15,4104(r7)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4104);
	// stw r14,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r14.u32);
	// subf r21,r21,r22
	ctx.r21.s64 = ctx.r22.s64 - ctx.r21.s64;
	// rlwinm r16,r22,2,0,29
	ctx.r16.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f13,4112(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4112);
	ctx.f13.f64 = double(temp.f32);
	// lfs f28,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f28.f64 = double(temp.f32);
	// subf r22,r15,r22
	ctx.r22.s64 = ctx.r22.s64 - ctx.r15.s64;
	// lfs f27,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f27.f64 = double(temp.f32);
	// rlwinm r21,r21,2,20,29
	ctx.r21.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFC;
	// fmuls f30,f28,f30
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// rlwinm r22,r22,2,20,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFC;
	// fmuls f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 * ctx.f8.f64));
	// lwz r15,160(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// lwz r14,216(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// lfsx f26,r21,r7
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r7.u32);
	ctx.f26.f64 = double(temp.f32);
	// lwz r21,240(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	// lfsx f28,r22,r7
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r7.u32);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f4,f26,f4
	ctx.f4.f64 = double(float(ctx.f26.f64 * ctx.f4.f64));
	// stfs f4,4116(r7)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4116, temp.u32);
	// fmuls f4,f28,f13
	ctx.f4.f64 = double(float(ctx.f28.f64 * ctx.f13.f64));
	// stfs f4,4120(r7)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4120, temp.u32);
	// stfsx f29,r16,r7
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r16.u32 + ctx.r7.u32, temp.u32);
	// lwz r22,4096(r7)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4096);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// clrlwi r22,r22,22
	ctx.r22.u64 = ctx.r22.u32 & 0x3FF;
	// fadds f13,f30,f8
	ctx.f13.f64 = double(float(ctx.f30.f64 + ctx.f8.f64));
	// stw r22,4096(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4096, ctx.r22.u32);
	// lfs f29,0(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	ctx.f29.f64 = double(temp.f32);
	// lwz r16,2072(r9)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2072);
	// lfs f28,0(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	ctx.f28.f64 = double(temp.f32);
	// lwz r22,2064(r9)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2064);
	// lfs f4,2080(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2080);
	ctx.f4.f64 = double(temp.f32);
	// lwz r11,2068(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2068);
	// fadds f13,f13,f5
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f5.f64));
	// lwz r17,2048(r9)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2048);
	// lfs f5,0(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f30,2084(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2084);
	ctx.f30.f64 = double(temp.f32);
	// lwz r31,0(r20)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	// subf r11,r11,r17
	ctx.r11.s64 = ctx.r17.s64 - ctx.r11.s64;
	// lfs f8,2076(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2076);
	ctx.f8.f64 = double(temp.f32);
	// subf r16,r16,r17
	ctx.r16.s64 = ctx.r17.s64 - ctx.r16.s64;
	// subf r22,r22,r17
	ctx.r22.s64 = ctx.r17.s64 - ctx.r22.s64;
	// fadds f29,f29,f5
	ctx.f29.f64 = double(float(ctx.f29.f64 + ctx.f5.f64));
	// rlwinm r21,r16,2,21,29
	ctx.r21.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0x7FC;
	// stw r31,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r31.u32);
	// rlwinm r16,r11,2,21,29
	ctx.r16.u64 = rotl64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0x7FC;
	// rlwinm r22,r22,2,21,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0x7FC;
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f5,r21,r9
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r9.u32);
	ctx.f5.f64 = double(temp.f32);
	// lfsx f27,r16,r9
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r9.u32);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f5,f30,f5
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f5.f64));
	// lfsx f30,r22,r9
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r9.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f8,f30,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f8.f64));
	// fmuls f4,f27,f4
	ctx.f4.f64 = double(float(ctx.f27.f64 * ctx.f4.f64));
	// stfs f8,2088(r9)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2088, temp.u32);
	// stfs f5,2096(r9)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2096, temp.u32);
	// stfs f4,2092(r9)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2092, temp.u32);
	// stfsx f10,r17,r9
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r9.u32, temp.u32);
	// lwz r22,2048(r9)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2048);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// lfs f10,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f10.f64 = double(temp.f32);
	// clrlwi r22,r22,23
	ctx.r22.u64 = ctx.r22.u32 & 0x1FF;
	// fmuls f5,f13,f10
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// stw r22,2048(r9)
	PPC_STORE_U32(ctx.r9.u32 + 2048, ctx.r22.u32);
	// lfs f13,528(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 528);
	ctx.f13.f64 = double(temp.f32);
	// lwz r22,516(r30)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r30.u32 + 516);
	// rlwinm r22,r22,2,0,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f8,524(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 524);
	ctx.f8.f64 = double(temp.f32);
	// lwz r21,512(r30)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r30.u32 + 512);
	// rlwinm r21,r21,2,0,29
	ctx.r21.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f10,r22,r30
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r30.u32);
	ctx.f10.f64 = double(temp.f32);
	// lfs f4,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f8,f10,f8,f29
	ctx.f8.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f8.f64), float(ctx.f29.f64)));
	// stfsx f8,r21,r30
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r21.u32 + ctx.r30.u32, temp.u32);
	// fmadds f13,f13,f8,f10
	ctx.f13.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f8.f64), float(ctx.f10.f64)));
	// stfs f13,532(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r30.u32 + 532, temp.u32);
	// lwz r22,516(r30)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r30.u32 + 516);
	// addi r21,r22,1
	ctx.r21.s64 = ctx.r22.s64 + 1;
	// lwz r22,512(r30)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r30.u32 + 512);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// clrlwi r22,r22,25
	ctx.r22.u64 = ctx.r22.u32 & 0x7F;
	// clrlwi r21,r21,25
	ctx.r21.u64 = ctx.r21.u32 & 0x7F;
	// stw r21,516(r30)
	PPC_STORE_U32(ctx.r30.u32 + 516, ctx.r21.u32);
	// stw r22,512(r30)
	PPC_STORE_U32(ctx.r30.u32 + 512, ctx.r22.u32);
	// lwz r22,8192(r8)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8192);
	// rlwinm r17,r22,2,0,29
	ctx.r17.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f8,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lwz r21,8208(r8)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8208);
	// lwz r16,8212(r8)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8212);
	// subf r21,r21,r22
	ctx.r21.s64 = ctx.r22.s64 - ctx.r21.s64;
	// lwz r15,8216(r8)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8216);
	// lwz r14,256(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// subf r16,r16,r22
	ctx.r16.s64 = ctx.r22.s64 - ctx.r16.s64;
	// subf r22,r15,r22
	ctx.r22.s64 = ctx.r22.s64 - ctx.r15.s64;
	// lwz r15,88(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rlwinm r21,r21,2,19,29
	ctx.r21.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0x1FFC;
	// lfs f13,8220(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8220);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r22,r22,2,19,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0x1FFC;
	// lfs f30,8224(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8224);
	ctx.f30.f64 = double(temp.f32);
	// rlwinm r16,r16,2,19,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0x1FFC;
	// lfs f29,8228(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8228);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f27.f64 = double(temp.f32);
	// lwz r11,224(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// fadds f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 + ctx.f8.f64));
	// lfs f10,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfsx f27,r21,r8
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r8.u32);
	ctx.f27.f64 = double(temp.f32);
	// lwz r15,92(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// fmuls f13,f27,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// lfsx f27,r22,r8
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r8.u32);
	ctx.f27.f64 = double(temp.f32);
	// lfsx f26,r16,r8
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r8.u32);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f27,f29
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// fmuls f30,f26,f30
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// stfs f13,8232(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8232, temp.u32);
	// stfs f30,8236(r8)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8236, temp.u32);
	// lwz r14,100(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// stfs f29,8240(r8)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8240, temp.u32);
	// stfsx f4,r17,r8
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r8.u32, temp.u32);
	// lwz r22,8192(r8)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8192);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// clrlwi r22,r22,21
	ctx.r22.u64 = ctx.r22.u32 & 0x7FF;
	// stw r22,8192(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8192, ctx.r22.u32);
	// lfs f30,1036(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 1036);
	ctx.f30.f64 = double(temp.f32);
	// lwz r22,1024(r29)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1024);
	// lfs f29,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f29.f64 = double(temp.f32);
	// lwz r21,1028(r29)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1028);
	// rlwinm r21,r21,2,0,29
	ctx.r21.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r22,r22,2,0,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f27,r21,r29
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r29.u32);
	ctx.f27.f64 = double(temp.f32);
	// lfs f13,1040(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 1040);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f8,f27,f30,f8
	ctx.f8.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f30.f64), float(ctx.f8.f64)));
	// stfsx f8,r22,r29
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r22.u32 + ctx.r29.u32, temp.u32);
	// fmadds f13,f8,f13,f27
	ctx.f13.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f13.f64), float(ctx.f27.f64)));
	// stfs f13,1044(r29)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r29.u32 + 1044, temp.u32);
	// lwz r22,1028(r29)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1028);
	// lwz r21,1024(r29)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1024);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// addi r21,r21,1
	ctx.r21.s64 = ctx.r21.s64 + 1;
	// clrlwi r22,r22,24
	ctx.r22.u64 = ctx.r22.u32 & 0xFF;
	// clrlwi r21,r21,24
	ctx.r21.u64 = ctx.r21.u32 & 0xFF;
	// stw r22,1028(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1028, ctx.r22.u32);
	// stw r21,1024(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1024, ctx.r21.u32);
	// lfs f30,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// lwz r21,2052(r28)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2052);
	// rlwinm r21,r21,2,0,29
	ctx.r21.u64 = rotl64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r22,2048(r28)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2048);
	// rlwinm r22,r22,2,0,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f27,r21,r28
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r28.u32);
	ctx.f27.f64 = double(temp.f32);
	// lwz r15,108(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lfs f8,2060(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 2060);
	ctx.f8.f64 = double(temp.f32);
	// lfs f13,2064(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 2064);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f8,f8,f27,f29
	ctx.f8.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f27.f64), float(ctx.f29.f64)));
	// stfsx f8,r22,r28
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r22.u32 + ctx.r28.u32, temp.u32);
	// lwz r22,2052(r28)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2052);
	// addi r21,r22,1
	ctx.r21.s64 = ctx.r22.s64 + 1;
	// lwz r22,2048(r28)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2048);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// fmadds f13,f8,f13,f27
	ctx.f13.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f13.f64), float(ctx.f27.f64)));
	// clrlwi r21,r21,23
	ctx.r21.u64 = ctx.r21.u32 & 0x1FF;
	// stfs f13,2068(r28)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// clrlwi r22,r22,23
	ctx.r22.u64 = ctx.r22.u32 & 0x1FF;
	// stw r21,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r21.u32);
	// stw r22,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r22.u32);
	// lfs f26,4112(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4112);
	ctx.f26.f64 = double(temp.f32);
	// lwz r16,4104(r6)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4104);
	// lwz r17,4096(r6)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4096);
	// lwz r22,4100(r6)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4100);
	// lfs f27,4108(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4108);
	ctx.f27.f64 = double(temp.f32);
	// lwz r21,0(r14)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	// subf r22,r22,r17
	ctx.r22.s64 = ctx.r17.s64 - ctx.r22.s64;
	// subf r16,r16,r17
	ctx.r16.s64 = ctx.r17.s64 - ctx.r16.s64;
	// stw r21,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r21.u32);
	// rlwinm r22,r22,2,20,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFC;
	// rlwinm r21,r16,2,20,29
	ctx.r21.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0xFFC;
	// lwz r11,124(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r14,116(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lfs f25,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f25.f64 = double(temp.f32);
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f29,f25,f29
	ctx.f29.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// lfsx f24,r22,r6
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r6.u32);
	ctx.f24.f64 = double(temp.f32);
	// lwz r16,0(r15)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	// lfsx f25,r21,r6
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + ctx.r6.u32);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f27,f24,f27
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// lfs f8,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f26,f25,f26
	ctx.f26.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// lfs f13,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lwz r21,140(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// stfs f27,4116(r6)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4116, temp.u32);
	// ld r31,280(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 280);
	// stfs f26,4120(r6)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4120, temp.u32);
	// stw r16,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r16.u32);
	// stfsx f30,r17,r6
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r6.u32, temp.u32);
	// lwz r22,4096(r6)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4096);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// ld r11,264(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 264);
	// clrlwi r22,r22,22
	ctx.r22.u64 = ctx.r22.u32 & 0x3FF;
	// lfs f30,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f4,f30,f4
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// stw r22,4096(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4096, ctx.r22.u32);
	// fadds f4,f29,f4
	ctx.f4.f64 = double(float(ctx.f29.f64 + ctx.f4.f64));
	// fadds f4,f4,f28
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f28.f64));
	// lwz r22,0(r20)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	// lwz r21,0(r21)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	// cmpwi cr6,r21,0
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 0, ctx.xer);
	// stw r22,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r22.u32);
	// lfs f30,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f4,f4,f30
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// bne cr6,0x822e1754
	if (!ctx.cr6.eq) goto loc_822E1754;
	// addis r27,r31,1
	ctx.r27.s64 = ctx.r31.s64 + 65536;
	// lis r28,1
	ctx.r28.s64 = 65536;
	// addi r27,r27,20352
	ctx.r27.s64 = ctx.r27.s64 + 20352;
	// lis r26,1
	ctx.r26.s64 = 65536;
	// ori r28,r28,20340
	ctx.r28.u64 = ctx.r28.u64 | 20340;
	// ori r26,r26,22420
	ctx.r26.u64 = ctx.r26.u64 | 22420;
	// addis r23,r31,1
	ctx.r23.s64 = ctx.r31.s64 + 65536;
	// lwz r25,2052(r27)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r27.u32 + 2052);
	// lfs f7,2060(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 2060);
	ctx.f7.f64 = double(temp.f32);
	// lwz r24,2048(r27)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r27.u32 + 2048);
	// lfs f6,2064(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 2064);
	ctx.f6.f64 = double(temp.f32);
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r31,r28
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r28.u32);
	ctx.f30.f64 = double(temp.f32);
	// rlwinm r24,r24,2,0,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f29,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f29.f64 = double(temp.f32);
	// lis r28,1
	ctx.r28.s64 = 65536;
	// fadds f29,f29,f11
	ctx.f29.f64 = double(float(ctx.f29.f64 + ctx.f11.f64));
	// addis r26,r31,1
	ctx.r26.s64 = ctx.r31.s64 + 65536;
	// addi r23,r23,22452
	ctx.r23.s64 = ctx.r23.s64 + 22452;
	// lfsx f28,r25,r27
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r27.u32);
	ctx.f28.f64 = double(temp.f32);
	// ori r25,r28,22448
	ctx.r25.u64 = ctx.r28.u64 | 22448;
	// fmadds f7,f7,f28,f30
	ctx.f7.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f28.f64), float(ctx.f30.f64)));
	// stfsx f7,r24,r27
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r24.u32 + ctx.r27.u32, temp.u32);
	// lis r28,1
	ctx.r28.s64 = 65536;
	// addi r26,r26,22432
	ctx.r26.s64 = ctx.r26.s64 + 22432;
	// ori r22,r28,22476
	ctx.r22.u64 = ctx.r28.u64 | 22476;
	// lis r28,1
	ctx.r28.s64 = 65536;
	// addis r24,r31,1
	ctx.r24.s64 = ctx.r31.s64 + 65536;
	// ori r20,r28,22500
	ctx.r20.u64 = ctx.r28.u64 | 22500;
	// lis r28,1
	ctx.r28.s64 = 65536;
	// addi r24,r24,22480
	ctx.r24.s64 = ctx.r24.s64 + 22480;
	// ori r18,r28,38916
	ctx.r18.u64 = ctx.r28.u64 | 38916;
	// fmadds f6,f6,f7,f28
	ctx.f6.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f7.f64), float(ctx.f28.f64)));
	// stfs f6,2068(r27)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r27.u32 + 2068, temp.u32);
	// lis r21,1
	ctx.r21.s64 = 65536;
	// addis r19,r31,2
	ctx.r19.s64 = ctx.r31.s64 + 131072;
	// ori r21,r21,38944
	ctx.r21.u64 = ctx.r21.u64 | 38944;
	// addi r19,r19,-26608
	ctx.r19.s64 = ctx.r19.s64 + -26608;
	// lwz r28,2052(r27)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + 2052);
	// addi r17,r28,1
	ctx.r17.s64 = ctx.r28.s64 + 1;
	// lwz r28,2048(r27)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + 2048);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r17,r17,23
	ctx.r17.u64 = ctx.r17.u32 & 0x1FF;
	// clrlwi r28,r28,23
	ctx.r28.u64 = ctx.r28.u32 & 0x1FF;
	// stw r17,2052(r27)
	PPC_STORE_U32(ctx.r27.u32 + 2052, ctx.r17.u32);
	// stw r28,2048(r27)
	PPC_STORE_U32(ctx.r27.u32 + 2048, ctx.r28.u32);
	// lwz r28,8(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// lfsx f6,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f6.f64 = double(temp.f32);
	// lfs f30,0(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f7,12(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// lfsx f28,r28,r26
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r26.u32);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f7,f28,f7,f29
	ctx.f7.f64 = double(std::fma(float(ctx.f28.f64), float(ctx.f7.f64), float(ctx.f29.f64)));
	// stfs f29,0(r26)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r26.u32 + 0, temp.u32);
	// stfs f7,16(r26)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16, temp.u32);
	// stfs f30,4(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4, temp.u32);
	// lfs f30,20(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 20);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,12(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 12);
	ctx.f29.f64 = double(temp.f32);
	// lfs f7,16(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 16);
	ctx.f7.f64 = double(temp.f32);
	// lfsx f28,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f28.f64 = double(temp.f32);
	// lwz r28,8(r23)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f26,r28,r23
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r23.u32);
	ctx.f26.f64 = double(temp.f32);
	// lfs f27,0(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 0);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f6,f7,f26,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f26.f64), float(ctx.f6.f64)));
	// fmuls f30,f30,f26
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f26.f64));
	// stfs f27,4(r23)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r23.u32 + 4, temp.u32);
	// stfs f6,0(r23)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r23.u32 + 0, temp.u32);
	// fmadds f7,f29,f6,f30
	ctx.f7.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f6.f64), float(ctx.f30.f64)));
	// stfs f7,24(r23)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r23.u32 + 24, temp.u32);
	// lfsx f6,r31,r20
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r20.u32);
	ctx.f6.f64 = double(temp.f32);
	// lfs f29,16(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 16);
	ctx.f29.f64 = double(temp.f32);
	// lfs f7,12(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// lwz r28,8(r24)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f30,0(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// lfsx f27,r28,r24
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r24.u32);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f29,f29,f27,f28
	ctx.f29.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f27.f64), float(ctx.f28.f64)));
	// stfs f29,0(r24)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r24.u32 + 0, temp.u32);
	// fmuls f7,f7,f29
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f29.f64));
	// stfs f7,20(r24)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r24.u32 + 20, temp.u32);
	// stfs f30,4(r24)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r24.u32 + 4, temp.u32);
	// lwz r28,8(r19)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8);
	// lfs f7,12(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f28,0(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	ctx.f28.f64 = double(temp.f32);
	// addis r27,r31,1
	ctx.r27.s64 = ctx.r31.s64 + 65536;
	// lfsx f27,r28,r19
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r19.u32);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f7,f7,f27
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// lfsx f30,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f30.f64 = double(temp.f32);
	// lis r28,1
	ctx.r28.s64 = 65536;
	// lfsx f29,r31,r18
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r18.u32);
	ctx.f29.f64 = double(temp.f32);
	// addi r27,r27,22512
	ctx.r27.s64 = ctx.r27.s64 + 22512;
	// stfs f29,0(r19)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r19.u32 + 0, temp.u32);
	// ori r23,r28,41028
	ctx.r23.u64 = ctx.r28.u64 | 41028;
	// stfs f7,16(r19)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r19.u32 + 16, temp.u32);
	// lis r28,1
	ctx.r28.s64 = 65536;
	// stfs f28,4(r19)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r19.u32 + 4, temp.u32);
	// lis r26,1
	ctx.r26.s64 = 65536;
	// ori r20,r28,43136
	ctx.r20.u64 = ctx.r28.u64 | 43136;
	// fmuls f6,f6,f3
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// lis r28,1
	ctx.r28.s64 = 65536;
	// ori r26,r26,38920
	ctx.r26.u64 = ctx.r26.u64 | 38920;
	// ori r18,r28,43188
	ctx.r18.u64 = ctx.r28.u64 | 43188;
	// addis r25,r31,2
	ctx.r25.s64 = ctx.r31.s64 + 131072;
	// lis r24,1
	ctx.r24.s64 = 65536;
	// addi r25,r25,-24496
	ctx.r25.s64 = ctx.r25.s64 + -24496;
	// ori r24,r24,43108
	ctx.r24.u64 = ctx.r24.u64 | 43108;
	// addis r22,r31,2
	ctx.r22.s64 = ctx.r31.s64 + 131072;
	// lis r21,1
	ctx.r21.s64 = 65536;
	// addi r22,r22,-22416
	ctx.r22.s64 = ctx.r22.s64 + -22416;
	// addis r19,r31,2
	ctx.r19.s64 = ctx.r31.s64 + 131072;
	// addis r17,r31,2
	ctx.r17.s64 = ctx.r31.s64 + 131072;
	// ori r21,r21,43164
	ctx.r21.u64 = ctx.r21.u64 | 43164;
	// addi r19,r19,-22396
	ctx.r19.s64 = ctx.r19.s64 + -22396;
	// addi r17,r17,-22368
	ctx.r17.s64 = ctx.r17.s64 + -22368;
	// lfsx f28,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f28.f64 = double(temp.f32);
	// lwz r28,16384(r27)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16384);
	// lfs f29,16396(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 16396);
	ctx.f29.f64 = double(temp.f32);
	// lwz r15,16388(r27)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16388);
	// lfs f7,16400(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 16400);
	ctx.f7.f64 = double(temp.f32);
	// lwz r14,16392(r27)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16392);
	// rlwinm r16,r28,2,0,29
	ctx.r16.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r15,r15,r28
	ctx.r15.s64 = ctx.r28.s64 - ctx.r15.s64;
	// subf r28,r14,r28
	ctx.r28.s64 = ctx.r28.s64 - ctx.r14.s64;
	// rlwinm r26,r15,2,18,29
	ctx.r26.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0x3FFC;
	// rlwinm r28,r28,2,18,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0x3FFC;
	// lfsx f27,r26,r27
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r27.u32);
	ctx.f27.f64 = double(temp.f32);
	// lfsx f26,r28,r27
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r27.u32);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f7,f26,f7
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// stfs f7,16408(r27)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16408, temp.u32);
	// fmuls f29,f27,f29
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// stfs f29,16404(r27)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16404, temp.u32);
	// stfsx f6,r16,r27
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r16.u32 + ctx.r27.u32, temp.u32);
	// lwz r28,16384(r27)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16384);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r28,r28,20
	ctx.r28.u64 = ctx.r28.u32 & 0xFFF;
	// stw r28,16384(r27)
	PPC_STORE_U32(ctx.r27.u32 + 16384, ctx.r28.u32);
	// lfs f6,2064(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 2064);
	ctx.f6.f64 = double(temp.f32);
	// lwz r28,2052(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2052);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r27,2048(r25)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2048);
	// lfsx f7,r28,r25
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r25.u32);
	ctx.f7.f64 = double(temp.f32);
	// lfs f29,2060(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 2060);
	ctx.f29.f64 = double(temp.f32);
	// rlwinm r28,r27,2,0,29
	ctx.r28.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f27,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f29,f29,f7,f27
	ctx.f29.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f7.f64), float(ctx.f27.f64)));
	// lfsx f27,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f27.f64 = double(temp.f32);
	// stfsx f29,r28,r25
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r28.u32 + ctx.r25.u32, temp.u32);
	// fmadds f6,f6,f29,f7
	ctx.f6.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f29.f64), float(ctx.f7.f64)));
	// stfs f6,2068(r25)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2068, temp.u32);
	// fadds f7,f27,f10
	ctx.f7.f64 = double(float(ctx.f27.f64 + ctx.f10.f64));
	// lwz r27,2048(r25)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2048);
	// lwz r28,2052(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2052);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r28,r28,23
	ctx.r28.u64 = ctx.r28.u32 & 0x1FF;
	// clrlwi r27,r27,23
	ctx.r27.u64 = ctx.r27.u32 & 0x1FF;
	// stw r28,2052(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2052, ctx.r28.u32);
	// stw r27,2048(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2048, ctx.r27.u32);
	// lfs f6,0(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lwz r28,8(r22)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f27,r31,r20
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r20.u32);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,12(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 12);
	ctx.f29.f64 = double(temp.f32);
	// addis r27,r31,2
	ctx.r27.s64 = ctx.r31.s64 + 131072;
	// lis r26,1
	ctx.r26.s64 = 65536;
	// addi r27,r27,-22336
	ctx.r27.s64 = ctx.r27.s64 + -22336;
	// lis r25,1
	ctx.r25.s64 = 65536;
	// lfsx f26,r28,r22
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r22.u32);
	ctx.f26.f64 = double(temp.f32);
	// lis r24,1
	ctx.r24.s64 = 65536;
	// stfs f6,4(r22)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r22.u32 + 4, temp.u32);
	// fmadds f6,f26,f29,f7
	ctx.f6.f64 = double(std::fma(float(ctx.f26.f64), float(ctx.f29.f64), float(ctx.f7.f64)));
	// stfs f6,16(r22)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r22.u32 + 16, temp.u32);
	// ori r24,r24,59632
	ctx.r24.u64 = ctx.r24.u64 | 59632;
	// stfs f7,0(r22)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r22.u32 + 0, temp.u32);
	// lwz r28,8(r19)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8);
	// lfs f7,16(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 16);
	ctx.f7.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f6,20(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// lis r23,2
	ctx.r23.s64 = 131072;
	// lfs f29,12(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 12);
	ctx.f29.f64 = double(temp.f32);
	// addis r22,r31,2
	ctx.r22.s64 = ctx.r31.s64 + 131072;
	// lfsx f26,r28,r19
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r19.u32);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f7,f7,f26,f27
	ctx.f7.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f26.f64), float(ctx.f27.f64)));
	// fmuls f6,f6,f26
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// lfsx f27,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,0(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	ctx.f26.f64 = double(temp.f32);
	// ori r28,r26,59604
	ctx.r28.u64 = ctx.r26.u64 | 59604;
	// stfs f7,0(r19)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r19.u32 + 0, temp.u32);
	// ori r26,r25,59608
	ctx.r26.u64 = ctx.r25.u64 | 59608;
	// stfs f26,4(r19)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r19.u32 + 4, temp.u32);
	// addis r25,r31,2
	ctx.r25.s64 = ctx.r31.s64 + 131072;
	// fmadds f7,f29,f7,f6
	ctx.f7.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f7.f64), float(ctx.f6.f64)));
	// stfs f7,24(r19)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r19.u32 + 24, temp.u32);
	// lfs f6,16(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// addi r25,r25,-5920
	ctx.r25.s64 = ctx.r25.s64 + -5920;
	// lfs f26,0(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	ctx.f26.f64 = double(temp.f32);
	// lis r21,2
	ctx.r21.s64 = 131072;
	// lfsx f29,r31,r18
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r18.u32);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f29,f3
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// lwz r20,8(r17)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8);
	// lfs f7,12(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// addi r22,r22,2336
	ctx.r22.s64 = ctx.r22.s64 + 2336;
	// rlwinm r20,r20,2,0,29
	ctx.r20.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// ori r23,r23,2324
	ctx.r23.u64 = ctx.r23.u64 | 2324;
	// ori r21,r21,10552
	ctx.r21.u64 = ctx.r21.u64 | 10552;
	// lfsx f25,r20,r17
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + ctx.r17.u32);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f6,f6,f25,f27
	ctx.f6.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f25.f64), float(ctx.f27.f64)));
	// fmuls f7,f7,f6
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f6.f64));
	// stfs f6,0(r17)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r17.u32 + 0, temp.u32);
	// stfs f7,20(r17)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r17.u32 + 20, temp.u32);
	// stfs f26,4(r17)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r17.u32 + 4, temp.u32);
	// lfs f7,16400(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 16400);
	ctx.f7.f64 = double(temp.f32);
	// lwz r18,16384(r27)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16384);
	// lwz r19,16392(r27)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16392);
	// lfsx f26,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f26.f64 = double(temp.f32);
	// lwz r20,16388(r27)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16388);
	// subf r20,r20,r18
	ctx.r20.s64 = ctx.r18.s64 - ctx.r20.s64;
	// subf r19,r19,r18
	ctx.r19.s64 = ctx.r18.s64 - ctx.r19.s64;
	// lfsx f27,r31,r28
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r28.u32);
	ctx.f27.f64 = double(temp.f32);
	// rlwinm r28,r20,2,18,29
	ctx.r28.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0x3FFC;
	// lfs f6,16396(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 16396);
	ctx.f6.f64 = double(temp.f32);
	// rlwinm r26,r19,2,18,29
	ctx.r26.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0x3FFC;
	// rlwinm r20,r18,2,0,29
	ctx.r20.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f25,r28,r27
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r27.u32);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f6,f25,f6
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// lfsx f24,r26,r27
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r27.u32);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f7,f24,f7
	ctx.f7.f64 = double(float(ctx.f24.f64 * ctx.f7.f64));
	// stfs f6,16404(r27)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16404, temp.u32);
	// stfs f7,16408(r27)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16408, temp.u32);
	// stfsx f29,r20,r27
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r20.u32 + ctx.r27.u32, temp.u32);
	// lwz r28,16384(r27)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16384);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r28,r28,20
	ctx.r28.u64 = ctx.r28.u32 & 0xFFF;
	// stw r28,16384(r27)
	PPC_STORE_U32(ctx.r27.u32 + 16384, ctx.r28.u32);
	// lfs f6,12(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 12);
	ctx.f6.f64 = double(temp.f32);
	// lwz r28,8(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f29,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f29.f64 = double(temp.f32);
	// lfsx f7,r28,r25
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r25.u32);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f6,f7
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// lfs f7,0(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,4(r25)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r25.u32 + 4, temp.u32);
	// addis r26,r31,2
	ctx.r26.s64 = ctx.r31.s64 + 131072;
	// stfs f6,16(r25)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r25.u32 + 16, temp.u32);
	// addis r24,r31,2
	ctx.r24.s64 = ctx.r31.s64 + 131072;
	// stfs f27,0(r25)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r25.u32 + 0, temp.u32);
	// addi r26,r26,10560
	ctx.r26.s64 = ctx.r26.s64 + 10560;
	// lis r25,2
	ctx.r25.s64 = 131072;
	// addi r24,r24,18784
	ctx.r24.s64 = ctx.r24.s64 + 18784;
	// ori r25,r25,18772
	ctx.r25.u64 = ctx.r25.u64 | 18772;
	// addis r20,r31,2
	ctx.r20.s64 = ctx.r31.s64 + 131072;
	// addi r20,r20,22912
	ctx.r20.s64 = ctx.r20.s64 + 22912;
	// lwz r28,8196(r22)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8196);
	// lfsx f7,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f7.f64 = double(temp.f32);
	// lwz r27,8192(r22)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8192);
	// fadds f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// lfs f24,8212(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 8212);
	ctx.f24.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f27,8204(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 8204);
	ctx.f27.f64 = double(temp.f32);
	// lfsx f25,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f25.f64 = double(temp.f32);
	// lis r23,2
	ctx.r23.s64 = 131072;
	// lfs f6,8208(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 8208);
	ctx.f6.f64 = double(temp.f32);
	// ori r23,r23,39316
	ctx.r23.u64 = ctx.r23.u64 | 39316;
	// lfsx f23,r28,r22
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r22.u32);
	ctx.f23.f64 = double(temp.f32);
	// lis r28,2
	ctx.r28.s64 = 131072;
	// fmadds f27,f27,f23,f7
	ctx.f27.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f23.f64), float(ctx.f7.f64)));
	// stfsx f27,r27,r22
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r22.u32, temp.u32);
	// ori r21,r28,22900
	ctx.r21.u64 = ctx.r28.u64 | 22900;
	// fmuls f24,f24,f23
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f23.f64));
	// lis r28,2
	ctx.r28.s64 = 131072;
	// lwz r27,8192(r22)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8192);
	// ori r19,r28,39320
	ctx.r19.u64 = ctx.r28.u64 | 39320;
	// lwz r28,8196(r22)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8196);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r27,r27,21
	ctx.r27.u64 = ctx.r27.u32 & 0x7FF;
	// clrlwi r28,r28,21
	ctx.r28.u64 = ctx.r28.u32 & 0x7FF;
	// stw r27,8192(r22)
	PPC_STORE_U32(ctx.r22.u32 + 8192, ctx.r27.u32);
	// stw r28,8196(r22)
	PPC_STORE_U32(ctx.r22.u32 + 8196, ctx.r28.u32);
	// fmadds f6,f6,f7,f24
	ctx.f6.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f7.f64), float(ctx.f24.f64)));
	// stfs f6,8216(r22)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r22.u32 + 8216, temp.u32);
	// lfsx f6,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f6.f64 = double(temp.f32);
	// lwz r28,8196(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f24,r28,r26
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r26.u32);
	ctx.f24.f64 = double(temp.f32);
	// lwz r27,8192(r26)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f7,8204(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8204);
	ctx.f7.f64 = double(temp.f32);
	// lfs f27,8208(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8208);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f7,f7,f24,f25
	ctx.f7.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f24.f64), float(ctx.f25.f64)));
	// stfsx f7,r27,r26
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r26.u32, temp.u32);
	// lwz r28,8196(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// lwz r27,8192(r26)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// clrlwi r28,r28,21
	ctx.r28.u64 = ctx.r28.u32 & 0x7FF;
	// fmadds f7,f27,f7,f24
	ctx.f7.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f7.f64), float(ctx.f24.f64)));
	// stfs f7,8212(r26)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r26.u32 + 8212, temp.u32);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// stw r28,8196(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8196, ctx.r28.u32);
	// clrlwi r27,r27,21
	ctx.r27.u64 = ctx.r27.u32 & 0x7FF;
	// stw r27,8192(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8192, ctx.r27.u32);
	// lfs f25,4112(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 4112);
	ctx.f25.f64 = double(temp.f32);
	// lfs f7,4108(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 4108);
	ctx.f7.f64 = double(temp.f32);
	// lwz r28,4100(r24)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4100);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r27,4096(r24)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4096);
	// lfsx f24,r28,r24
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r24.u32);
	ctx.f24.f64 = double(temp.f32);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// fmadds f6,f7,f24,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f24.f64), float(ctx.f6.f64)));
	// lfsx f27,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f27.f64 = double(temp.f32);
	// fadds f7,f6,f0
	ctx.f7.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// stfsx f7,r27,r24
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r24.u32, temp.u32);
	// fmadds f6,f25,f7,f24
	ctx.f6.f64 = double(std::fma(float(ctx.f25.f64), float(ctx.f7.f64), float(ctx.f24.f64)));
	// stfs f6,4116(r24)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r24.u32 + 4116, temp.u32);
	// lwz r28,4100(r24)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4100);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// lwz r27,4096(r24)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4096);
	// clrlwi r28,r28,22
	ctx.r28.u64 = ctx.r28.u32 & 0x3FF;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// stw r28,4100(r24)
	PPC_STORE_U32(ctx.r24.u32 + 4100, ctx.r28.u32);
	// clrlwi r27,r27,22
	ctx.r27.u64 = ctx.r27.u32 & 0x3FF;
	// stw r27,4096(r24)
	PPC_STORE_U32(ctx.r24.u32 + 4096, ctx.r27.u32);
	// fadds f11,f27,f11
	ctx.f11.f64 = double(float(ctx.f27.f64 + ctx.f11.f64));
	// lwz r28,16384(r20)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r20.u32 + 16384);
	// rlwinm r25,r28,2,0,29
	ctx.r25.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r27,16388(r20)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r20.u32 + 16388);
	// lfs f25,16396(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 16396);
	ctx.f25.f64 = double(temp.f32);
	// lwz r26,16392(r20)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r20.u32 + 16392);
	// lfs f24,16400(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 16400);
	ctx.f24.f64 = double(temp.f32);
	// subf r27,r27,r28
	ctx.r27.s64 = ctx.r28.s64 - ctx.r27.s64;
	// lfsx f7,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f7.f64 = double(temp.f32);
	// subf r28,r26,r28
	ctx.r28.s64 = ctx.r28.s64 - ctx.r26.s64;
	// lfsx f6,r31,r19
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r19.u32);
	ctx.f6.f64 = double(temp.f32);
	// rlwinm r27,r27,2,18,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0x3FFC;
	// rlwinm r28,r28,2,18,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0x3FFC;
	// addis r26,r31,3
	ctx.r26.s64 = ctx.r31.s64 + 196608;
	// lis r24,2
	ctx.r24.s64 = 131072;
	// addi r26,r26,-26208
	ctx.r26.s64 = ctx.r26.s64 + -26208;
	// lfsx f27,r27,r20
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + ctx.r20.u32);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f27,f27,f25
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f25.f64));
	// lfsx f23,r28,r20
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r20.u32);
	ctx.f23.f64 = double(temp.f32);
	// lis r28,2
	ctx.r28.s64 = 131072;
	// fmuls f25,f23,f24
	ctx.f25.f64 = double(float(ctx.f23.f64 * ctx.f24.f64));
	// stfs f27,16404(r20)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r20.u32 + 16404, temp.u32);
	// stfs f25,16408(r20)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r20.u32 + 16408, temp.u32);
	// ori r22,r28,55800
	ctx.r22.u64 = ctx.r28.u64 | 55800;
	// stfsx f11,r25,r20
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r25.u32 + ctx.r20.u32, temp.u32);
	// lis r28,2
	ctx.r28.s64 = 131072;
	// ori r27,r24,39348
	ctx.r27.u64 = ctx.r24.u64 | 39348;
	// ori r18,r28,64020
	ctx.r18.u64 = ctx.r28.u64 | 64020;
	// lis r23,2
	ctx.r23.s64 = 131072;
	// addis r25,r31,3
	ctx.r25.s64 = ctx.r31.s64 + 196608;
	// ori r24,r23,47572
	ctx.r24.u64 = ctx.r23.u64 | 47572;
	// addi r25,r25,-17952
	ctx.r25.s64 = ctx.r25.s64 + -17952;
	// addis r23,r31,3
	ctx.r23.s64 = ctx.r31.s64 + 196608;
	// lis r21,3
	ctx.r21.s64 = 196608;
	// lis r19,3
	ctx.r19.s64 = 196608;
	// addis r17,r31,3
	ctx.r17.s64 = ctx.r31.s64 + 196608;
	// addis r16,r31,3
	ctx.r16.s64 = ctx.r31.s64 + 196608;
	// addi r23,r23,-9728
	ctx.r23.s64 = ctx.r23.s64 + -9728;
	// ori r21,r21,2612
	ctx.r21.u64 = ctx.r21.u64 | 2612;
	// addi r17,r17,-1504
	ctx.r17.s64 = ctx.r17.s64 + -1504;
	// ori r19,r19,19028
	ctx.r19.u64 = ctx.r19.u64 | 19028;
	// addi r16,r16,19040
	ctx.r16.s64 = ctx.r16.s64 + 19040;
	// lwz r28,16384(r20)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r20.u32 + 16384);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r28,r28,20
	ctx.r28.u64 = ctx.r28.u32 & 0xFFF;
	// stw r28,16384(r20)
	PPC_STORE_U32(ctx.r20.u32 + 16384, ctx.r28.u32);
	// lfsx f25,r31,r27
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r27.u32);
	ctx.f25.f64 = double(temp.f32);
	// lwz r28,8(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// lfs f11,16(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 16);
	ctx.f11.f64 = double(temp.f32);
	// lfs f24,0(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	ctx.f24.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f27,12(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 12);
	ctx.f27.f64 = double(temp.f32);
	// lfsx f23,r28,r26
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r26.u32);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f11,f11,f23
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f23.f64));
	// stfs f7,0(r26)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r26.u32 + 0, temp.u32);
	// fmadds f7,f27,f7,f11
	ctx.f7.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f7.f64), float(ctx.f11.f64)));
	// stfs f7,20(r26)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r26.u32 + 20, temp.u32);
	// stfs f24,4(r26)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4, temp.u32);
	// lfsx f11,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f11.f64 = double(temp.f32);
	// lwz r28,8196(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// fadds f7,f11,f13
	ctx.f7.f64 = double(float(ctx.f11.f64 + ctx.f13.f64));
	// lwz r27,8192(r25)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f11,8204(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8204);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f27,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f27.f64 = double(temp.f32);
	// lfs f24,8208(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8208);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,8212(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8212);
	ctx.f23.f64 = double(temp.f32);
	// lfsx f22,r28,r25
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r25.u32);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f23,f23,f22
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// fmadds f11,f11,f22,f7
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f22.f64), float(ctx.f7.f64)));
	// stfsx f11,r27,r25
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r25.u32, temp.u32);
	// fmadds f7,f24,f7,f23
	ctx.f7.f64 = double(std::fma(float(ctx.f24.f64), float(ctx.f7.f64), float(ctx.f23.f64)));
	// stfs f7,8216(r25)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8216, temp.u32);
	// lwz r28,8196(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// lwz r27,8192(r25)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r28,r28,21
	ctx.r28.u64 = ctx.r28.u32 & 0x7FF;
	// clrlwi r27,r27,21
	ctx.r27.u64 = ctx.r27.u32 & 0x7FF;
	// stw r28,8196(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8196, ctx.r28.u32);
	// lis r26,3
	ctx.r26.s64 = 196608;
	// stw r27,8192(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8192, ctx.r27.u32);
	// lfsx f24,r31,r18
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r18.u32);
	ctx.f24.f64 = double(temp.f32);
	// ori r26,r26,19060
	ctx.r26.u64 = ctx.r26.u64 | 19060;
	// fadds f23,f29,f30
	ctx.f23.f64 = double(float(ctx.f29.f64 + ctx.f30.f64));
	// addis r25,r31,3
	ctx.r25.s64 = ctx.r31.s64 + 196608;
	// addis r22,r31,1
	ctx.r22.s64 = ctx.r31.s64 + 65536;
	// addi r25,r25,2624
	ctx.r25.s64 = ctx.r25.s64 + 2624;
	// addis r20,r31,2
	ctx.r20.s64 = ctx.r31.s64 + 131072;
	// addi r22,r22,18272
	ctx.r22.s64 = ctx.r22.s64 + 18272;
	// addi r20,r20,-26576
	ctx.r20.s64 = ctx.r20.s64 + -26576;
	// lwz r28,8196(r23)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8196);
	// lfs f11,8204(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 8204);
	ctx.f11.f64 = double(temp.f32);
	// lwz r27,8192(r23)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8192);
	// lfs f7,8208(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 8208);
	ctx.f7.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f22,r28,r23
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r23.u32);
	ctx.f22.f64 = double(temp.f32);
	// lis r28,3
	ctx.r28.s64 = 196608;
	// fmadds f11,f11,f22,f27
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f22.f64), float(ctx.f27.f64)));
	// stfsx f11,r27,r23
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r23.u32, temp.u32);
	// lwz r27,8192(r23)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8192);
	// ori r24,r28,19032
	ctx.r24.u64 = ctx.r28.u64 | 19032;
	// fmadds f7,f7,f11,f22
	ctx.f7.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f11.f64), float(ctx.f22.f64)));
	// stfs f7,8212(r23)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r23.u32 + 8212, temp.u32);
	// lwz r28,8196(r23)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8196);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r28,r28,21
	ctx.r28.u64 = ctx.r28.u32 & 0x7FF;
	// clrlwi r27,r27,21
	ctx.r27.u64 = ctx.r27.u32 & 0x7FF;
	// stw r27,8192(r23)
	PPC_STORE_U32(ctx.r23.u32 + 8192, ctx.r27.u32);
	// stw r28,8196(r23)
	PPC_STORE_U32(ctx.r23.u32 + 8196, ctx.r28.u32);
	// lwz r28,4100(r17)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r17.u32 + 4100);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f7,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f7.f64 = double(temp.f32);
	// lwz r27,4096(r17)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r17.u32 + 4096);
	// fadds f10,f7,f10
	ctx.f10.f64 = double(float(ctx.f7.f64 + ctx.f10.f64));
	// lfsx f7,r28,r17
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r17.u32);
	ctx.f7.f64 = double(temp.f32);
	// lfs f11,4108(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 4108);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// fmadds f11,f11,f7,f24
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f7.f64), float(ctx.f24.f64)));
	// lfs f27,4112(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 4112);
	ctx.f27.f64 = double(temp.f32);
	// fadds f11,f11,f13
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f13.f64));
	// stfsx f11,r27,r17
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r17.u32, temp.u32);
	// lwz r28,4100(r17)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r17.u32 + 4100);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// lwz r27,4096(r17)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r17.u32 + 4096);
	// clrlwi r28,r28,22
	ctx.r28.u64 = ctx.r28.u32 & 0x3FF;
	// fmadds f7,f27,f11,f7
	ctx.f7.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f11.f64), float(ctx.f7.f64)));
	// stfs f7,4116(r17)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r17.u32 + 4116, temp.u32);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// stw r28,4100(r17)
	PPC_STORE_U32(ctx.r17.u32 + 4100, ctx.r28.u32);
	// clrlwi r27,r27,22
	ctx.r27.u64 = ctx.r27.u32 & 0x3FF;
	// stw r27,4096(r17)
	PPC_STORE_U32(ctx.r17.u32 + 4096, ctx.r27.u32);
	// lfsx f11,r31,r19
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r19.u32);
	ctx.f11.f64 = double(temp.f32);
	// lwz r28,8(r16)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r16.u32 + 8);
	// lfs f7,12(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f7,f7,f11
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f11.f64));
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f24,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f24.f64 = double(temp.f32);
	// fadds f21,f24,f25
	ctx.f21.f64 = double(float(ctx.f24.f64 + ctx.f25.f64));
	// lfs f27,16(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 16);
	ctx.f27.f64 = double(temp.f32);
	// fsubs f25,f24,f25
	ctx.f25.f64 = static_cast<float>(ctx.f24.f64 - ctx.f25.f64);
	// lfs f22,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f22.f64 = double(temp.f32);
	// lfsx f24,r28,r16
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r16.u32);
	ctx.f24.f64 = double(temp.f32);
	// stfs f11,0(r16)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r16.u32 + 0, temp.u32);
	// stfs f22,4(r16)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r16.u32 + 4, temp.u32);
	// fmadds f11,f27,f24,f7
	ctx.f11.f64 = double(std::fma(float(ctx.f27.f64), float(ctx.f24.f64), float(ctx.f7.f64)));
	// stfs f11,20(r16)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r16.u32 + 20, temp.u32);
	// lfs f11,16400(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 16400);
	ctx.f11.f64 = double(temp.f32);
	// lwz r26,16392(r25)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16392);
	// lfsx f7,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f7.f64 = double(temp.f32);
	// lwz r28,16384(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16384);
	// lfs f27,16396(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 16396);
	ctx.f27.f64 = double(temp.f32);
	// lwz r27,16388(r25)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16388);
	// subf r27,r27,r28
	ctx.r27.s64 = ctx.r28.s64 - ctx.r27.s64;
	// subf r26,r26,r28
	ctx.r26.s64 = ctx.r28.s64 - ctx.r26.s64;
	// fadds f7,f7,f28
	ctx.f7.f64 = double(float(ctx.f7.f64 + ctx.f28.f64));
	// rlwinm r27,r27,2,18,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0x3FFC;
	// rlwinm r26,r26,2,18,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0x3FFC;
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f28,r26,r25
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r25.u32);
	ctx.f28.f64 = double(temp.f32);
	// lfsx f24,r27,r25
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + ctx.r25.u32);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f11,f28,f11
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// stfs f11,16408(r25)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r25.u32 + 16408, temp.u32);
	// fmuls f28,f24,f27
	ctx.f28.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// stfs f28,16404(r25)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r25.u32 + 16404, temp.u32);
	// addis r26,r31,2
	ctx.r26.s64 = ctx.r31.s64 + 131072;
	// addis r17,r31,3
	ctx.r17.s64 = ctx.r31.s64 + 196608;
	// fadds f6,f6,f26
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f26.f64));
	// stfsx f10,r28,r25
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r28.u32 + ctx.r25.u32, temp.u32);
	// fsubs f10,f29,f30
	ctx.f10.f64 = static_cast<float>(ctx.f29.f64 - ctx.f30.f64);
	// addi r26,r26,-5888
	ctx.r26.s64 = ctx.r26.s64 + -5888;
	// addi r17,r17,-26176
	ctx.r17.s64 = ctx.r17.s64 + -26176;
	// lwz r28,16384(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16384);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r28,r28,20
	ctx.r28.u64 = ctx.r28.u32 & 0xFFF;
	// stw r28,16384(r25)
	PPC_STORE_U32(ctx.r25.u32 + 16384, ctx.r28.u32);
	// lwz r27,2048(r22)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2048);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r28,2052(r22)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2052);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f11,2060(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 2060);
	ctx.f11.f64 = double(temp.f32);
	// lfsx f29,r28,r22
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r22.u32);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f11,f11,f29,f21
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f29.f64), float(ctx.f21.f64)));
	// lfs f30,2064(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 2064);
	ctx.f30.f64 = double(temp.f32);
	// fadds f9,f11,f9
	ctx.f9.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// stfsx f9,r27,r22
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r22.u32, temp.u32);
	// fmadds f11,f30,f9,f29
	ctx.f11.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f9.f64), float(ctx.f29.f64)));
	// stfs f11,2068(r22)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r22.u32 + 2068, temp.u32);
	// lwz r27,2048(r22)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2048);
	// lwz r28,2052(r22)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2052);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r28,r28,23
	ctx.r28.u64 = ctx.r28.u32 & 0x1FF;
	// clrlwi r27,r27,23
	ctx.r27.u64 = ctx.r27.u32 & 0x1FF;
	// stw r28,2052(r22)
	PPC_STORE_U32(ctx.r22.u32 + 2052, ctx.r28.u32);
	// stw r27,2048(r22)
	PPC_STORE_U32(ctx.r22.u32 + 2048, ctx.r27.u32);
	// lfs f11,2060(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 2060);
	ctx.f11.f64 = double(temp.f32);
	// lwz r28,2052(r20)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2052);
	// lwz r27,2048(r20)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2048);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f9,2064(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 2064);
	ctx.f9.f64 = double(temp.f32);
	// lfsx f30,r28,r20
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r20.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f11,f11,f30,f25
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f30.f64), float(ctx.f25.f64)));
	// fadds f8,f11,f8
	ctx.f8.f64 = double(float(ctx.f11.f64 + ctx.f8.f64));
	// stfsx f8,r27,r20
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r20.u32, temp.u32);
	// fmadds f11,f9,f8,f30
	ctx.f11.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f8.f64), float(ctx.f30.f64)));
	// stfs f11,2068(r20)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r20.u32 + 2068, temp.u32);
	// lwz r28,2052(r20)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2052);
	// lwz r27,2048(r20)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2048);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r28,r28,23
	ctx.r28.u64 = ctx.r28.u32 & 0x1FF;
	// clrlwi r27,r27,23
	ctx.r27.u64 = ctx.r27.u32 & 0x1FF;
	// stw r28,2052(r20)
	PPC_STORE_U32(ctx.r20.u32 + 2052, ctx.r28.u32);
	// stw r27,2048(r20)
	PPC_STORE_U32(ctx.r20.u32 + 2048, ctx.r27.u32);
	// lwz r28,8196(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f9,8204(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8204);
	ctx.f9.f64 = double(temp.f32);
	// lwz r27,8192(r26)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// lfsx f11,r28,r26
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r26.u32);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f9,f9,f11,f23
	ctx.f9.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f11.f64), float(ctx.f23.f64)));
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f8,8208(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8208);
	ctx.f8.f64 = double(temp.f32);
	// fadds f13,f9,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f13.f64));
	// stfsx f13,r27,r26
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r26.u32, temp.u32);
	// fmadds f11,f8,f13,f11
	ctx.f11.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f13.f64), float(ctx.f11.f64)));
	// stfs f11,8212(r26)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r26.u32 + 8212, temp.u32);
	// lwz r27,8192(r26)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// lwz r28,8196(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r27,r27,21
	ctx.r27.u64 = ctx.r27.u32 & 0x7FF;
	// clrlwi r28,r28,21
	ctx.r28.u64 = ctx.r28.u32 & 0x7FF;
	// stw r27,8192(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8192, ctx.r27.u32);
	// stw r28,8196(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8196, ctx.r28.u32);
	// lfs f9,8204(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 8204);
	ctx.f9.f64 = double(temp.f32);
	// lwz r28,8192(r17)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8192);
	// lwz r27,8196(r17)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8196);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f8,8208(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 8208);
	ctx.f8.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r18,164(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// lwz r20,172(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// lwz r19,180(r1)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r23,188(r1)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lfsx f13,r27,r17
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + ctx.r17.u32);
	ctx.f13.f64 = double(temp.f32);
	// lwz r24,196(r1)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// fmadds f11,f9,f13,f10
	ctx.f11.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f13.f64), float(ctx.f10.f64)));
	// lwz r25,204(r1)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// lwz r26,212(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// fadds f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// stfsx f10,r28,r17
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r28.u32 + ctx.r17.u32, temp.u32);
	// fmadds f9,f8,f10,f13
	ctx.f9.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f10.f64), float(ctx.f13.f64)));
	// stfs f9,8212(r17)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r17.u32 + 8212, temp.u32);
	// lwz r28,8196(r17)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8196);
	// lwz r27,8192(r17)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8192);
	// addi r16,r28,1
	ctx.r16.s64 = ctx.r28.s64 + 1;
	// lwz r28,220(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// addi r15,r27,1
	ctx.r15.s64 = ctx.r27.s64 + 1;
	// lwz r27,80(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r16,r16,21
	ctx.r16.u64 = ctx.r16.u32 & 0x7FF;
	// clrlwi r15,r15,21
	ctx.r15.u64 = ctx.r15.u32 & 0x7FF;
	// stw r16,8196(r17)
	PPC_STORE_U32(ctx.r17.u32 + 8196, ctx.r16.u32);
	// stw r15,8192(r17)
	PPC_STORE_U32(ctx.r17.u32 + 8192, ctx.r15.u32);
loc_822E1754:
	// lwz r17,2052(r26)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2052);
	// fadds f11,f5,f7
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f5.f64 + ctx.f7.f64));
	// lfs f13,2060(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 2060);
	ctx.f13.f64 = double(temp.f32);
	// fadds f8,f4,f6
	ctx.f8.f64 = double(float(ctx.f4.f64 + ctx.f6.f64));
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r16,2048(r26)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2048);
	// lwz r15,228(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// lfs f10,2064(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 2064);
	ctx.f10.f64 = double(temp.f32);
	// rlwinm r16,r16,2,0,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r22,556(r1)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	// lwz r14,236(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// std r11,264(r1)
	PPC_STORE_U64(ctx.r1.u32 + 264, ctx.r11.u64);
	// lfsx f5,r17,r26
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + ctx.r26.u32);
	ctx.f5.f64 = double(temp.f32);
	// lwz r11,244(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// fmuls f4,f5,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// lfs f13,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f9,0(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lwz r15,252(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lfsu f0,4(r22)
	ea = 4 + ctx.r22.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r22.u32 = ea;
	ctx.f0.f64 = double(temp.f32);
	// lwz r21,564(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	// addi r22,r22,4
	ctx.r22.s64 = ctx.r22.s64 + 4;
	// stw r22,556(r1)
	PPC_STORE_U32(ctx.r1.u32 + 556, ctx.r22.u32);
	// fmadds f11,f11,f2,f4
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f2.f64), float(ctx.f4.f64)));
	// stfsx f11,r16,r26
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r16.u32 + ctx.r26.u32, temp.u32);
	// fmadds f10,f11,f10,f5
	ctx.f10.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f10.f64), float(ctx.f5.f64)));
	// stfs f10,2068(r26)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2068, temp.u32);
	// lwz r22,2052(r26)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2052);
	// lwz r17,2048(r26)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2048);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// addi r17,r17,1
	ctx.r17.s64 = ctx.r17.s64 + 1;
	// clrlwi r22,r22,23
	ctx.r22.u64 = ctx.r22.u32 & 0x1FF;
	// clrlwi r17,r17,23
	ctx.r17.u64 = ctx.r17.u32 & 0x1FF;
	// stw r22,2052(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2052, ctx.r22.u32);
	// stw r17,2048(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2048, ctx.r17.u32);
	// lwz r22,2052(r25)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2052);
	// lfs f5,2060(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 2060);
	ctx.f5.f64 = double(temp.f32);
	// lwz r17,2048(r25)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2048);
	// lfs f4,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r22,r22,2,0,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f11,2064(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 2064);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f10,r22,r25
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r25.u32);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f5,f10,f5,f13
	ctx.f5.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f5.f64), float(ctx.f13.f64)));
	// stfsx f5,r17,r25
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r25.u32, temp.u32);
	// fmadds f13,f5,f11,f10
	ctx.f13.f64 = double(std::fma(float(ctx.f5.f64), float(ctx.f11.f64), float(ctx.f10.f64)));
	// stfs f13,2068(r25)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2068, temp.u32);
	// lwz r17,2048(r25)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2048);
	// lwz r22,2052(r25)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2052);
	// addi r17,r17,1
	ctx.r17.s64 = ctx.r17.s64 + 1;
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// clrlwi r17,r17,23
	ctx.r17.u64 = ctx.r17.u32 & 0x1FF;
	// clrlwi r22,r22,23
	ctx.r22.u64 = ctx.r22.u32 & 0x1FF;
	// stw r17,2048(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2048, ctx.r17.u32);
	// stw r22,2052(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2052, ctx.r22.u32);
	// lwz r22,2052(r24)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2052);
	// lfs f11,2060(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 2060);
	ctx.f11.f64 = double(temp.f32);
	// lwz r17,2048(r24)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2048);
	// lfs f10,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// rlwinm r22,r22,2,0,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f5,2064(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 2064);
	ctx.f5.f64 = double(temp.f32);
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r22,r24
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r24.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f11,f13,f11
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmadds f8,f8,f2,f11
	ctx.f8.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f2.f64), float(ctx.f11.f64)));
	// stfsx f8,r17,r24
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r24.u32, temp.u32);
	// fmadds f5,f8,f5,f13
	ctx.f5.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f5.f64), float(ctx.f13.f64)));
	// stfs f5,2068(r24)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r24.u32 + 2068, temp.u32);
	// lwz r22,2052(r24)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2052);
	// lwz r17,2048(r24)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2048);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// addi r17,r17,1
	ctx.r17.s64 = ctx.r17.s64 + 1;
	// clrlwi r22,r22,23
	ctx.r22.u64 = ctx.r22.u32 & 0x1FF;
	// clrlwi r17,r17,23
	ctx.r17.u64 = ctx.r17.u32 & 0x1FF;
	// stw r22,2052(r24)
	PPC_STORE_U32(ctx.r24.u32 + 2052, ctx.r22.u32);
	// stw r17,2048(r24)
	PPC_STORE_U32(ctx.r24.u32 + 2048, ctx.r17.u32);
	// lwz r22,2052(r23)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r23.u32 + 2052);
	// lfs f13,2060(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 2060);
	ctx.f13.f64 = double(temp.f32);
	// lwz r17,2048(r23)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r23.u32 + 2048);
	// lfs f11,2064(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 2064);
	ctx.f11.f64 = double(temp.f32);
	// lfs f8,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// rlwinm r22,r22,2,0,29
	ctx.r22.u64 = rotl64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f5,r22,r23
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + ctx.r23.u32);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f13,f5,f13,f10
	ctx.f13.f64 = double(std::fma(float(ctx.f5.f64), float(ctx.f13.f64), float(ctx.f10.f64)));
	// addi r27,r27,8
	ctx.r27.s64 = ctx.r27.s64 + 8;
	// fsubs f10,f1,f12
	ctx.f10.f64 = static_cast<float>(ctx.f1.f64 - ctx.f12.f64);
	// ld r11,264(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 264);
	// fmuls f4,f4,f12
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r27.u32);
	// fmuls f8,f8,f12
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// stfsx f13,r17,r23
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r23.u32, temp.u32);
	// fadds f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f31.f64));
	// fmadds f5,f13,f11,f5
	ctx.f5.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f11.f64), float(ctx.f5.f64)));
	// stfs f5,2068(r23)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r23.u32 + 2068, temp.u32);
	// fmadds f4,f10,f9,f4
	ctx.f4.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f9.f64), float(ctx.f4.f64)));
	// stfs f4,0(r21)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r21.u32 + 0, temp.u32);
	// fmadds f0,f10,f0,f8
	ctx.f0.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f0.f64), float(ctx.f8.f64)));
	// stfsu f0,4(r21)
	temp.f32 = float(ctx.f0.f64);
	ea = 4 + ctx.r21.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r21.u32 = ea;
	// addi r22,r21,4
	ctx.r22.s64 = ctx.r21.s64 + 4;
	// stw r22,564(r1)
	PPC_STORE_U32(ctx.r1.u32 + 564, ctx.r22.u32);
	// lwz r22,2052(r23)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r23.u32 + 2052);
	// addi r21,r22,1
	ctx.r21.s64 = ctx.r22.s64 + 1;
	// lwz r22,2048(r23)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r23.u32 + 2048);
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// clrlwi r21,r21,23
	ctx.r21.u64 = ctx.r21.u32 & 0x1FF;
	// clrlwi r22,r22,23
	ctx.r22.u64 = ctx.r22.u32 & 0x1FF;
	// stw r21,2052(r23)
	PPC_STORE_U32(ctx.r23.u32 + 2052, ctx.r21.u32);
	// stw r22,2048(r23)
	PPC_STORE_U32(ctx.r23.u32 + 2048, ctx.r22.u32);
	// bdnz 0x822e0868
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822E0868;
loc_822E1908:
	// addi r1,r1,528
	ctx.r1.s64 = ctx.r1.s64 + 528;
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa68
	ctx.lr = 0x822E1914;
	__savefpr_21(ctx, base);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822E1918"))) PPC_WEAK_FUNC(sub_822E1918);
PPC_FUNC_IMPL(__imp__sub_822E1918) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e430
	ctx.lr = 0x822E1920;
	__restfpr_14(ctx, base);
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa10
	ctx.lr = 0x822E1928;
	sub_8233FA10(ctx, base);
	// stwu r1,-592(r1)
	ea = -592 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// stw r5,628(r1)
	PPC_STORE_U32(ctx.r1.u32 + 628, ctx.r5.u32);
	// mr r29,r6
	ctx.r29.u64 = ctx.r6.u64;
	// stw r6,636(r1)
	PPC_STORE_U32(ctx.r1.u32 + 636, ctx.r6.u32);
	// addis r3,r3,3
	ctx.r3.s64 = ctx.r3.s64 + 196608;
	// stw r4,620(r1)
	PPC_STORE_U32(ctx.r1.u32 + 620, ctx.r4.u32);
	// mr r27,r5
	ctx.r27.u64 = ctx.r5.u64;
	// stw r7,644(r1)
	PPC_STORE_U32(ctx.r1.u32 + 644, ctx.r7.u32);
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// fmr f27,f1
	ctx.fpscr.disableFlushMode();
	ctx.f27.f64 = ctx.f1.f64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// addi r3,r3,23248
	ctx.r3.s64 = ctx.r3.s64 + 23248;
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// bl 0x822dbc80
	ctx.lr = 0x822E1964;
	sub_822DBC80(ctx, base);
	// addis r3,r31,5
	ctx.r3.s64 = ctx.r31.s64 + 327680;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// addi r3,r3,23296
	ctx.r3.s64 = ctx.r3.s64 + 23296;
	// bl 0x822db588
	ctx.lr = 0x822E1978;
	sub_822DB588(ctx, base);
	// lis r11,3
	ctx.r11.s64 = 196608;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// ori r9,r11,23232
	ctx.r9.u64 = ctx.r11.u64 | 23232;
	// lfs f11,-28948(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28948);
	ctx.f11.f64 = double(temp.f32);
	// lwzx r8,r31,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r9.u32);
	// fcmpu cr6,f27,f11
	ctx.cr6.compare(ctx.f27.f64, ctx.f11.f64);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// lfs f13,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// ble cr6,0x822e19a0
	if (!ctx.cr6.gt) goto loc_822E19A0;
	// fmr f13,f11
	ctx.f13.f64 = ctx.f11.f64;
loc_822E19A0:
	// fmr f30,f11
	ctx.fpscr.disableFlushMode();
	ctx.f30.f64 = ctx.f11.f64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// fmr f29,f11
	ctx.f29.f64 = ctx.f11.f64;
	// beq cr6,0x822e2ecc
	if (ctx.cr6.eq) goto loc_822E2ECC;
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// lwz r11,644(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	// addis r9,r31,1
	ctx.r9.s64 = ctx.r31.s64 + 65536;
	// addi r10,r10,2184
	ctx.r10.s64 = ctx.r10.s64 + 2184;
	// addi r9,r9,2188
	ctx.r9.s64 = ctx.r9.s64 + 2188;
	// addis r8,r31,1
	ctx.r8.s64 = ctx.r31.s64 + 65536;
	// stw r10,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r10.u32);
	// addis r7,r31,1
	ctx.r7.s64 = ctx.r31.s64 + 65536;
	// stw r9,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r9.u32);
	// addis r6,r31,1
	ctx.r6.s64 = ctx.r31.s64 + 65536;
	// addis r5,r31,1
	ctx.r5.s64 = ctx.r31.s64 + 65536;
	// addis r4,r31,1
	ctx.r4.s64 = ctx.r31.s64 + 65536;
	// addis r3,r31,1
	ctx.r3.s64 = ctx.r31.s64 + 65536;
	// addi r8,r8,2192
	ctx.r8.s64 = ctx.r8.s64 + 2192;
	// addi r7,r7,2740
	ctx.r7.s64 = ctx.r7.s64 + 2740;
	// addi r6,r6,10984
	ctx.r6.s64 = ctx.r6.s64 + 10984;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// addi r5,r5,10988
	ctx.r5.s64 = ctx.r5.s64 + 10988;
	// stw r7,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, ctx.r7.u32);
	// addi r4,r4,10992
	ctx.r4.s64 = ctx.r4.s64 + 10992;
	// stw r6,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r6.u32);
	// addi r3,r3,12052
	ctx.r3.s64 = ctx.r3.s64 + 12052;
	// stw r5,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r5.u32);
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// stw r4,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r4.u32);
	// addis r9,r31,1
	ctx.r9.s64 = ctx.r31.s64 + 65536;
	// stw r3,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, ctx.r3.u32);
	// addi r10,r10,14132
	ctx.r10.s64 = ctx.r10.s64 + 14132;
	// addi r9,r9,80
	ctx.r9.s64 = ctx.r9.s64 + 80;
	// addis r8,r31,1
	ctx.r8.s64 = ctx.r31.s64 + 65536;
	// stw r10,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r10.u32);
	// addis r7,r31,1
	ctx.r7.s64 = ctx.r31.s64 + 65536;
	// stw r9,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r9.u32);
	// addis r6,r31,1
	ctx.r6.s64 = ctx.r31.s64 + 65536;
	// addis r5,r31,5
	ctx.r5.s64 = ctx.r31.s64 + 327680;
	// addis r4,r31,5
	ctx.r4.s64 = ctx.r31.s64 + 327680;
	// addis r3,r31,5
	ctx.r3.s64 = ctx.r31.s64 + 327680;
	// addi r8,r8,84
	ctx.r8.s64 = ctx.r8.s64 + 84;
	// addi r7,r7,18260
	ctx.r7.s64 = ctx.r7.s64 + 18260;
	// addi r6,r6,18264
	ctx.r6.s64 = ctx.r6.s64 + 18264;
	// stw r8,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r8.u32);
	// addi r5,r5,25432
	ctx.r5.s64 = ctx.r5.s64 + 25432;
	// stw r7,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, ctx.r7.u32);
	// addi r4,r4,25436
	ctx.r4.s64 = ctx.r4.s64 + 25436;
	// stw r6,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r6.u32);
	// addi r3,r3,25440
	ctx.r3.s64 = ctx.r3.s64 + 25440;
	// stw r5,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r5.u32);
	// addis r10,r31,5
	ctx.r10.s64 = ctx.r31.s64 + 327680;
	// stw r4,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r4.u32);
	// addis r9,r31,6
	ctx.r9.s64 = ctx.r31.s64 + 393216;
	// stw r3,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r3.u32);
	// addi r10,r10,25988
	ctx.r10.s64 = ctx.r10.s64 + 25988;
	// addi r9,r9,-31304
	ctx.r9.s64 = ctx.r9.s64 + -31304;
	// addis r8,r31,6
	ctx.r8.s64 = ctx.r31.s64 + 393216;
	// stw r10,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r10.u32);
	// addis r28,r31,6
	ctx.r28.s64 = ctx.r31.s64 + 393216;
	// stw r9,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r9.u32);
	// addis r7,r31,6
	ctx.r7.s64 = ctx.r31.s64 + 393216;
	// addis r6,r31,6
	ctx.r6.s64 = ctx.r31.s64 + 393216;
	// addis r5,r31,6
	ctx.r5.s64 = ctx.r31.s64 + 393216;
	// addis r4,r31,5
	ctx.r4.s64 = ctx.r31.s64 + 327680;
	// addis r3,r31,5
	ctx.r3.s64 = ctx.r31.s64 + 327680;
	// addi r8,r8,-31300
	ctx.r8.s64 = ctx.r8.s64 + -31300;
	// addi r28,r28,-30224
	ctx.r28.s64 = ctx.r28.s64 + -30224;
	// addi r7,r7,-31296
	ctx.r7.s64 = ctx.r7.s64 + -31296;
	// stw r8,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r8.u32);
	// addi r6,r6,-30236
	ctx.r6.s64 = ctx.r6.s64 + -30236;
	// stw r28,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r28.u32);
	// addi r5,r5,-28156
	ctx.r5.s64 = ctx.r5.s64 + -28156;
	// stw r7,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r7.u32);
	// addi r4,r4,23336
	ctx.r4.s64 = ctx.r4.s64 + 23336;
	// stw r6,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r6.u32);
	// addi r3,r3,23340
	ctx.r3.s64 = ctx.r3.s64 + 23340;
	// stw r5,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r5.u32);
	// addis r10,r31,6
	ctx.r10.s64 = ctx.r31.s64 + 393216;
	// stw r4,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r4.u32);
	// addis r9,r31,6
	ctx.r9.s64 = ctx.r31.s64 + 393216;
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// addi r10,r10,-24028
	ctx.r10.s64 = ctx.r10.s64 + -24028;
	// addi r9,r9,-24024
	ctx.r9.s64 = ctx.r9.s64 + -24024;
	// addis r8,r31,6
	ctx.r8.s64 = ctx.r31.s64 + 393216;
	// addis r23,r31,6
	ctx.r23.s64 = ctx.r31.s64 + 393216;
	// lwz r19,636(r1)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	// addis r22,r31,3
	ctx.r22.s64 = ctx.r31.s64 + 196608;
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
	// addi r23,r23,-17776
	ctx.r23.s64 = ctx.r23.s64 + -17776;
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// addi r22,r22,23236
	ctx.r22.s64 = ctx.r22.s64 + 23236;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// addi r8,r8,-15696
	ctx.r8.s64 = ctx.r8.s64 + -15696;
	// stw r23,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r23.u32);
	// stw r22,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r22.u32);
	// addis r21,r31,3
	ctx.r21.s64 = ctx.r31.s64 + 196608;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// addis r23,r31,6
	ctx.r23.s64 = ctx.r31.s64 + 393216;
	// addis r22,r31,6
	ctx.r22.s64 = ctx.r31.s64 + 393216;
	// stw r19,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r19.u32);
	// addi r21,r21,23240
	ctx.r21.s64 = ctx.r21.s64 + 23240;
	// addi r23,r23,-13612
	ctx.r23.s64 = ctx.r23.s64 + -13612;
	// addi r22,r22,-13608
	ctx.r22.s64 = ctx.r22.s64 + -13608;
	// stw r21,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r21.u32);
	// stw r23,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r23.u32);
	// addis r21,r31,3
	ctx.r21.s64 = ctx.r31.s64 + 196608;
	// stw r22,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r22.u32);
	// addis r23,r31,3
	ctx.r23.s64 = ctx.r31.s64 + 196608;
	// addis r22,r31,6
	ctx.r22.s64 = ctx.r31.s64 + 393216;
	// addi r21,r21,21140
	ctx.r21.s64 = ctx.r21.s64 + 21140;
	// addi r23,r23,23220
	ctx.r23.s64 = ctx.r23.s64 + 23220;
	// addi r22,r22,-21948
	ctx.r22.s64 = ctx.r22.s64 + -21948;
	// stw r21,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r21.u32);
	// addis r7,r31,6
	ctx.r7.s64 = ctx.r31.s64 + 393216;
	// stw r23,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r23.u32);
	// addis r6,r31,6
	ctx.r6.s64 = ctx.r31.s64 + 393216;
	// stw r22,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r22.u32);
	// addi r7,r7,-21936
	ctx.r7.s64 = ctx.r7.s64 + -21936;
	// addi r6,r6,-19856
	ctx.r6.s64 = ctx.r6.s64 + -19856;
	// addis r26,r31,3
	ctx.r26.s64 = ctx.r31.s64 + 196608;
	// stw r7,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r7.u32);
	// addis r25,r31,3
	ctx.r25.s64 = ctx.r31.s64 + 196608;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// addis r24,r31,6
	ctx.r24.s64 = ctx.r31.s64 + 393216;
	// addis r21,r31,6
	ctx.r21.s64 = ctx.r31.s64 + 393216;
	// addis r23,r31,6
	ctx.r23.s64 = ctx.r31.s64 + 393216;
	// addis r22,r31,6
	ctx.r22.s64 = ctx.r31.s64 + 393216;
	// lwz r20,92(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// addi r26,r26,19072
	ctx.r26.s64 = ctx.r26.s64 + 19072;
	// addi r25,r25,21152
	ctx.r25.s64 = ctx.r25.s64 + 21152;
	// addi r24,r24,-24016
	ctx.r24.s64 = ctx.r24.s64 + -24016;
	// stw r26,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r26.u32);
	// addis r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 65536;
	// stw r25,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r25.u32);
	// addis r10,r31,1
	ctx.r10.s64 = ctx.r31.s64 + 65536;
	// stw r24,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r24.u32);
	// addis r7,r31,1
	ctx.r7.s64 = ctx.r31.s64 + 65536;
	// addis r5,r31,1
	ctx.r5.s64 = ctx.r31.s64 + 65536;
	// addis r4,r31,1
	ctx.r4.s64 = ctx.r31.s64 + 65536;
	// addis r3,r31,1
	ctx.r3.s64 = ctx.r31.s64 + 65536;
	// addis r9,r31,5
	ctx.r9.s64 = ctx.r31.s64 + 327680;
	// addis r8,r31,5
	ctx.r8.s64 = ctx.r31.s64 + 327680;
	// addis r6,r31,6
	ctx.r6.s64 = ctx.r31.s64 + 393216;
	// addis r30,r31,5
	ctx.r30.s64 = ctx.r31.s64 + 327680;
	// addis r29,r31,6
	ctx.r29.s64 = ctx.r31.s64 + 393216;
	// addi r21,r21,-19868
	ctx.r21.s64 = ctx.r21.s64 + -19868;
	// addi r23,r23,-17788
	ctx.r23.s64 = ctx.r23.s64 + -17788;
	// addi r22,r22,-15708
	ctx.r22.s64 = ctx.r22.s64 + -15708;
	// stw r21,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r21.u32);
	// addi r11,r11,96
	ctx.r11.s64 = ctx.r11.s64 + 96;
	// stw r23,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, ctx.r23.u32);
	// addi r10,r10,2752
	ctx.r10.s64 = ctx.r10.s64 + 2752;
	// stw r22,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, ctx.r22.u32);
	// addi r7,r7,14144
	ctx.r7.s64 = ctx.r7.s64 + 14144;
	// addi r5,r5,2208
	ctx.r5.s64 = ctx.r5.s64 + 2208;
	// addi r4,r4,11008
	ctx.r4.s64 = ctx.r4.s64 + 11008;
	// addi r3,r3,12064
	ctx.r3.s64 = ctx.r3.s64 + 12064;
	// addi r9,r9,23344
	ctx.r9.s64 = ctx.r9.s64 + 23344;
	// addi r8,r8,26000
	ctx.r8.s64 = ctx.r8.s64 + 26000;
	// addi r6,r6,-28144
	ctx.r6.s64 = ctx.r6.s64 + -28144;
	// addi r30,r30,25456
	ctx.r30.s64 = ctx.r30.s64 + 25456;
	// addi r29,r29,-31280
	ctx.r29.s64 = ctx.r29.s64 + -31280;
	// lis r18,-32255
	ctx.r18.s64 = -2113863680;
	// lis r17,-32256
	ctx.r17.s64 = -2113929216;
	// lis r16,-32256
	ctx.r16.s64 = -2113929216;
	// lis r15,-32256
	ctx.r15.s64 = -2113929216;
	// lwz r23,88(r1)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lis r14,-32255
	ctx.r14.s64 = -2113863680;
	// lwz r22,84(r1)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r21,100(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lfs f26,1800(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 1800);
	ctx.f26.f64 = double(temp.f32);
	// lfs f1,5256(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 5256);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,5268(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 5268);
	ctx.f31.f64 = double(temp.f32);
	// lfs f5,5260(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 5260);
	ctx.f5.f64 = double(temp.f32);
	// lfs f28,-1560(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + -1560);
	ctx.f28.f64 = double(temp.f32);
loc_822E1C60:
	// lwz r18,2048(r11)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// lfs f0,2076(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2076);
	ctx.f0.f64 = double(temp.f32);
	// lwz r17,2064(r11)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2064);
	// lfs f12,2080(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2080);
	ctx.f12.f64 = double(temp.f32);
	// lwz r16,2068(r11)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2068);
	// rlwinm r15,r18,2,0,29
	ctx.r15.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r14,2072(r11)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2072);
	// subf r17,r17,r18
	ctx.r17.s64 = ctx.r18.s64 - ctx.r17.s64;
	// subf r16,r16,r18
	ctx.r16.s64 = ctx.r18.s64 - ctx.r16.s64;
	// std r31,304(r1)
	PPC_STORE_U64(ctx.r1.u32 + 304, ctx.r31.u64);
	// subf r18,r14,r18
	ctx.r18.s64 = ctx.r18.s64 - ctx.r14.s64;
	// lwz r31,244(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r14,204(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// rlwinm r17,r17,2,21,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0x7FC;
	// rlwinm r16,r16,2,21,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0x7FC;
	// std r27,312(r1)
	PPC_STORE_U64(ctx.r1.u32 + 312, ctx.r27.u64);
	// rlwinm r18,r18,2,21,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0x7FC;
	// lwz r27,116(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lfs f10,2084(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2084);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lwz r31,132(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lfs f7,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// lwz r14,252(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lfsx f2,r17,r11
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + ctx.r11.u32);
	ctx.f2.f64 = double(temp.f32);
	// fadds f3,f7,f8
	ctx.f3.f64 = double(float(ctx.f7.f64 + ctx.f8.f64));
	// lfsx f8,r16,r11
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r11.u32);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f2,f0
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfsx f2,r18,r11
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r11.u32);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f0,f12,f8
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f6,0(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f12,f10,f2
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// stfs f7,2088(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2088, temp.u32);
	// lwz r17,124(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stfs f0,2092(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2092, temp.u32);
	// lwz r16,212(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// stfs f12,2096(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2096, temp.u32);
	// lfs f9,0(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// stfsx f9,r15,r11
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r15.u32 + ctx.r11.u32, temp.u32);
	// lfs f4,4(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// lwz r19,2048(r11)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2048);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// clrlwi r19,r19,23
	ctx.r19.u64 = ctx.r19.u32 & 0x1FF;
	// stw r19,2048(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2048, ctx.r19.u32);
	// lfs f10,524(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 524);
	ctx.f10.f64 = double(temp.f32);
	// lwz r19,512(r5)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r5.u32 + 512);
	// lfs f9,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lwz r18,516(r5)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r5.u32 + 516);
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f8,r18,r5
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r5.u32);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,528(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 528);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f3,f8,f10,f3
	ctx.f3.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f10.f64), float(ctx.f3.f64)));
	// stfsx f3,r19,r5
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r19.u32 + ctx.r5.u32, temp.u32);
	// lwz r18,512(r5)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r5.u32 + 512);
	// lwz r19,516(r5)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r5.u32 + 516);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// addi r18,r18,1
	ctx.r18.s64 = ctx.r18.s64 + 1;
	// fmadds f2,f7,f3,f8
	ctx.f2.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f3.f64), float(ctx.f8.f64)));
	// clrlwi r19,r19,25
	ctx.r19.u64 = ctx.r19.u32 & 0x7F;
	// stfs f2,532(r5)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r5.u32 + 532, temp.u32);
	// clrlwi r18,r18,25
	ctx.r18.u64 = ctx.r18.u32 & 0x7F;
	// stw r19,516(r5)
	PPC_STORE_U32(ctx.r5.u32 + 516, ctx.r19.u32);
	// stw r18,512(r5)
	PPC_STORE_U32(ctx.r5.u32 + 512, ctx.r18.u32);
	// lfs f0,0(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r19,8216(r10)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8216);
	// lfs f12,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lwz r18,8212(r10)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8212);
	// lfs f8,8220(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8220);
	ctx.f8.f64 = double(temp.f32);
	// lwz r17,8192(r10)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lfs f7,8224(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8224);
	ctx.f7.f64 = double(temp.f32);
	// lwz r16,8208(r10)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8208);
	// subf r16,r16,r17
	ctx.r16.s64 = ctx.r17.s64 - ctx.r16.s64;
	// rlwinm r16,r16,2,19,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0x1FFC;
	// fadds f3,f12,f0
	ctx.f3.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// subf r18,r18,r17
	ctx.r18.s64 = ctx.r17.s64 - ctx.r18.s64;
	// lfs f10,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// subf r19,r19,r17
	ctx.r19.s64 = ctx.r17.s64 - ctx.r19.s64;
	// lfs f0,8228(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8228);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r18,r18,2,19,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0x1FFC;
	// rlwinm r19,r19,2,19,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0x1FFC;
	// lfsx f2,r16,r10
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r10.u32);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f12,f2,f8
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
	// lfsx f8,r18,r10
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r10.u32);
	ctx.f8.f64 = double(temp.f32);
	// lfsx f2,r19,r10
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r10.u32);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f8,f8,f7
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// fmuls f7,f2,f0
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f7,8240(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8240, temp.u32);
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f12,8232(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8232, temp.u32);
	// stfs f8,8236(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8236, temp.u32);
	// lwz r16,288(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// lwz r15,140(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r14,220(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lwz r31,148(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stfsx f9,r17,r10
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r10.u32, temp.u32);
	// lwz r19,8192(r10)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// lwz r17,156(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// clrlwi r19,r19,21
	ctx.r19.u64 = ctx.r19.u32 & 0x7FF;
	// lwz r27,260(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// stw r19,8192(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8192, ctx.r19.u32);
	// lfs f2,1036(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1036);
	ctx.f2.f64 = double(temp.f32);
	// lwz r19,1024(r4)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 1024);
	// lfs f7,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// lwz r18,1028(r4)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r4.u32 + 1028);
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f12,r18,r4
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r4.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,1040(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1040);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f12,f2,f3
	ctx.f3.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f2.f64), float(ctx.f3.f64)));
	// stfsx f3,r19,r4
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r19.u32 + ctx.r4.u32, temp.u32);
	// fmadds f2,f3,f0,f12
	ctx.f2.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfs f2,1044(r4)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r4.u32 + 1044, temp.u32);
	// lwz r18,1024(r4)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r4.u32 + 1024);
	// lwz r19,1028(r4)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 1028);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// addi r18,r18,1
	ctx.r18.s64 = ctx.r18.s64 + 1;
	// clrlwi r19,r19,24
	ctx.r19.u64 = ctx.r19.u32 & 0xFF;
	// clrlwi r18,r18,24
	ctx.r18.u64 = ctx.r18.u32 & 0xFF;
	// stw r19,1028(r4)
	PPC_STORE_U32(ctx.r4.u32 + 1028, ctx.r19.u32);
	// stw r18,1024(r4)
	PPC_STORE_U32(ctx.r4.u32 + 1024, ctx.r18.u32);
	// lfs f0,2060(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 2060);
	ctx.f0.f64 = double(temp.f32);
	// lwz r18,2048(r3)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2048);
	// lfs f12,2064(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 2064);
	ctx.f12.f64 = double(temp.f32);
	// lwz r19,2052(r3)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2052);
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f8,r19,r3
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r3.u32);
	ctx.f8.f64 = double(temp.f32);
	// rlwinm r19,r18,2,0,29
	ctx.r19.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f2,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f3,f0,f8,f7
	ctx.f3.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f8.f64), float(ctx.f7.f64)));
	// stfsx f3,r19,r3
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r19.u32 + ctx.r3.u32, temp.u32);
	// fmadds f0,f3,f12,f8
	ctx.f0.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f12.f64), float(ctx.f8.f64)));
	// stfs f0,2068(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 2068, temp.u32);
	// lwz r19,2052(r3)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2052);
	// addi r18,r19,1
	ctx.r18.s64 = ctx.r19.s64 + 1;
	// lwz r19,2048(r3)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2048);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// clrlwi r18,r18,23
	ctx.r18.u64 = ctx.r18.u32 & 0x1FF;
	// clrlwi r19,r19,23
	ctx.r19.u64 = ctx.r19.u32 & 0x1FF;
	// stw r18,2052(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2052, ctx.r18.u32);
	// stw r19,2048(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2048, ctx.r19.u32);
	// lfs f0,0(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r19,4096(r7)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4096);
	// lfs f3,4108(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4108);
	ctx.f3.f64 = double(temp.f32);
	// lwz r18,4100(r7)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4100);
	// lwz r17,0(r14)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	// lwz r16,4104(r7)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4104);
	// lwz r15,0(r31)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r15,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r15.u32);
	// subf r18,r18,r19
	ctx.r18.s64 = ctx.r19.s64 - ctx.r18.s64;
	// stw r17,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r17.u32);
	// rlwinm r17,r19,2,0,29
	ctx.r17.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f12,4112(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4112);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r18,r18,2,20,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFC;
	// lfs f8,0(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// subf r19,r16,r19
	ctx.r19.s64 = ctx.r19.s64 - ctx.r16.s64;
	// lfs f25,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f25.f64 = double(temp.f32);
	// lwz r16,96(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// rlwinm r19,r19,2,20,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFC;
	// lfsx f24,r18,r7
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r7.u32);
	ctx.f24.f64 = double(temp.f32);
	// lwz r18,164(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// fmuls f3,f24,f3
	ctx.f3.f64 = double(float(ctx.f24.f64 * ctx.f3.f64));
	// fmuls f7,f25,f7
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f7.f64));
	// lfsx f25,r19,r7
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r7.u32);
	ctx.f25.f64 = double(temp.f32);
	// stfs f3,4116(r7)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4116, temp.u32);
	// fmuls f12,f25,f12
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f12.f64));
	// stfs f12,4120(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4120, temp.u32);
	// std r11,296(r1)
	PPC_STORE_U64(ctx.r1.u32 + 296, ctx.r11.u64);
	// stfsx f2,r17,r7
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r7.u32, temp.u32);
	// lwz r19,4096(r7)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4096);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// lfs f24,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f24.f64 = double(temp.f32);
	// clrlwi r19,r19,22
	ctx.r19.u64 = ctx.r19.u32 & 0x3FF;
	// fmuls f9,f24,f9
	ctx.f9.f64 = double(float(ctx.f24.f64 * ctx.f9.f64));
	// std r10,280(r1)
	PPC_STORE_U64(ctx.r1.u32 + 280, ctx.r10.u64);
	// stw r19,4096(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4096, ctx.r19.u32);
	// lfs f12,0(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lwz r11,2068(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2068);
	// lfs f3,2080(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2080);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,2084(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2084);
	ctx.f2.f64 = double(temp.f32);
	// lwz r19,2048(r9)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2048);
	// rlwinm r10,r19,2,0,29
	ctx.r10.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,292(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r17,2064(r9)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2064);
	// fadds f9,f7,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// lfs f7,2076(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2076);
	ctx.f7.f64 = double(temp.f32);
	// lwz r27,2072(r9)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2072);
	// lwz r14,236(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// lfs f25,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f25.f64 = double(temp.f32);
	// subf r27,r27,r19
	ctx.r27.s64 = ctx.r19.s64 - ctx.r27.s64;
	// fadds f12,f25,f12
	ctx.f12.f64 = double(float(ctx.f25.f64 + ctx.f12.f64));
	// lwz r16,0(r16)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	// rlwinm r18,r27,2,21,29
	ctx.r18.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0x7FC;
	// lwz r31,180(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lfs f24,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f24.f64 = double(temp.f32);
	// fadds f9,f9,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f6.f64));
	// lfsx f25,r18,r9
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r9.u32);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f2,f2,f25
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f25.f64));
	// subf r17,r17,r19
	ctx.r17.s64 = ctx.r19.s64 - ctx.r17.s64;
	// stw r16,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r16.u32);
	// subf r19,r11,r19
	ctx.r19.s64 = ctx.r19.s64 - ctx.r11.s64;
	// rlwinm r17,r17,2,21,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0x7FC;
	// rlwinm r19,r19,2,21,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0x7FC;
	// lfsx f6,r17,r9
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + ctx.r9.u32);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f6,f7
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// lfsx f6,r19,r9
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r9.u32);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f6,f3
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// stfs f6,2092(r9)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2092, temp.u32);
	// stfs f7,2088(r9)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2088, temp.u32);
	// stfs f2,2096(r9)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2096, temp.u32);
	// stfsx f4,r10,r9
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, temp.u32);
	// lfs f4,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f9,f4
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// lwz r19,2048(r9)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2048);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// clrlwi r19,r19,23
	ctx.r19.u64 = ctx.r19.u32 & 0x1FF;
	// stw r19,2048(r9)
	PPC_STORE_U32(ctx.r9.u32 + 2048, ctx.r19.u32);
	// lfs f2,524(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 524);
	ctx.f2.f64 = double(temp.f32);
	// lwz r18,516(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + 516);
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f7,r18,r30
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r30.u32);
	ctx.f7.f64 = double(temp.f32);
	// lwz r19,512(r30)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r30.u32 + 512);
	// lfs f9,528(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 528);
	ctx.f9.f64 = double(temp.f32);
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f25,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f6,f7,f2,f12
	ctx.f6.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f2.f64), float(ctx.f12.f64)));
	// stfsx f6,r19,r30
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r19.u32 + ctx.r30.u32, temp.u32);
	// lwz r19,516(r30)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r30.u32 + 516);
	// addi r18,r19,1
	ctx.r18.s64 = ctx.r19.s64 + 1;
	// lwz r19,512(r30)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r30.u32 + 512);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// fmadds f4,f9,f6,f7
	ctx.f4.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f6.f64), float(ctx.f7.f64)));
	// clrlwi r18,r18,25
	ctx.r18.u64 = ctx.r18.u32 & 0x7F;
	// stfs f4,532(r30)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r30.u32 + 532, temp.u32);
	// clrlwi r19,r19,25
	ctx.r19.u64 = ctx.r19.u32 & 0x7F;
	// stw r18,516(r30)
	PPC_STORE_U32(ctx.r30.u32 + 516, ctx.r18.u32);
	// stw r19,512(r30)
	PPC_STORE_U32(ctx.r30.u32 + 512, ctx.r19.u32);
	// lfs f2,8220(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8220);
	ctx.f2.f64 = double(temp.f32);
	// lwz r16,8216(r8)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8216);
	// lwz r19,8212(r8)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8212);
	// lwz r18,8192(r8)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8192);
	// rlwinm r17,r18,2,0,29
	ctx.r17.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r19,r19,r18
	ctx.r19.s64 = ctx.r18.s64 - ctx.r19.s64;
	// lwz r15,8208(r8)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8208);
	// rlwinm r19,r19,2,19,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0x1FFC;
	// lwz r14,188(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lfs f12,8224(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8224);
	ctx.f12.f64 = double(temp.f32);
	// subf r15,r15,r18
	ctx.r15.s64 = ctx.r18.s64 - ctx.r15.s64;
	// lwz r11,196(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// subf r18,r16,r18
	ctx.r18.s64 = ctx.r18.s64 - ctx.r16.s64;
	// lwz r16,104(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// rlwinm r15,r15,2,19,29
	ctx.r15.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0x1FFC;
	// lfs f7,8228(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8228);
	ctx.f7.f64 = double(temp.f32);
	// rlwinm r18,r18,2,19,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0x1FFC;
	// lfsx f23,r19,r8
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r8.u32);
	ctx.f23.f64 = double(temp.f32);
	// lfs f6,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f12,f23,f12
	ctx.f12.f64 = double(float(ctx.f23.f64 * ctx.f12.f64));
	// lfs f9,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lwz r14,108(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lfs f4,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// lwz r16,272(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// lfsx f21,r15,r8
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + ctx.r8.u32);
	ctx.f21.f64 = double(temp.f32);
	// fadds f6,f4,f6
	ctx.f6.f64 = double(float(ctx.f4.f64 + ctx.f6.f64));
	// lfsx f22,r18,r8
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r8.u32);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f2,f21,f2
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f2.f64));
	// fmuls f7,f22,f7
	ctx.f7.f64 = double(float(ctx.f22.f64 * ctx.f7.f64));
	// stfs f7,8240(r8)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8240, temp.u32);
	// stfs f2,8232(r8)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8232, temp.u32);
	// lwz r15,112(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stfs f12,8236(r8)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8236, temp.u32);
	// stfsx f25,r17,r8
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r8.u32, temp.u32);
	// lwz r19,8192(r8)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8192);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// clrlwi r19,r19,21
	ctx.r19.u64 = ctx.r19.u32 & 0x7FF;
	// stw r19,8192(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8192, ctx.r19.u32);
	// lfs f4,1036(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 1036);
	ctx.f4.f64 = double(temp.f32);
	// lwz r18,1028(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1028);
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r19,1024(r29)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1024);
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f12,r18,r29
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r29.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfs f23,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f7,f12,f4,f6
	ctx.f7.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f4.f64), float(ctx.f6.f64)));
	// lfs f2,1040(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 1040);
	ctx.f2.f64 = double(temp.f32);
	// stfsx f7,r19,r29
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r19.u32 + ctx.r29.u32, temp.u32);
	// lwz r19,1028(r29)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1028);
	// lwz r18,1024(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1024);
	// addi r18,r18,1
	ctx.r18.s64 = ctx.r18.s64 + 1;
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// fmadds f6,f7,f2,f12
	ctx.f6.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f2.f64), float(ctx.f12.f64)));
	// clrlwi r18,r18,24
	ctx.r18.u64 = ctx.r18.u32 & 0xFF;
	// stfs f6,1044(r29)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r29.u32 + 1044, temp.u32);
	// clrlwi r19,r19,24
	ctx.r19.u64 = ctx.r19.u32 & 0xFF;
	// stw r18,1024(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1024, ctx.r18.u32);
	// stw r19,1028(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1028, ctx.r19.u32);
	// lfs f12,2064(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 2064);
	ctx.f12.f64 = double(temp.f32);
	// lwz r18,2048(r28)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2048);
	// lwz r19,2052(r28)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2052);
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f4,2060(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 2060);
	ctx.f4.f64 = double(temp.f32);
	// lfsx f7,r19,r28
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r28.u32);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f4,f7,f23
	ctx.f6.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f7.f64), float(ctx.f23.f64)));
	// lfs f2,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f4,f6,f12,f7
	ctx.f4.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f12.f64), float(ctx.f7.f64)));
	// stfsx f6,r18,r28
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r18.u32 + ctx.r28.u32, temp.u32);
	// stfs f4,2068(r28)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// lwz r19,2052(r28)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2052);
	// addi r18,r19,1
	ctx.r18.s64 = ctx.r19.s64 + 1;
	// lwz r19,2048(r28)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2048);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// clrlwi r18,r18,23
	ctx.r18.u64 = ctx.r18.u32 & 0x1FF;
	// clrlwi r19,r19,23
	ctx.r19.u64 = ctx.r19.u32 & 0x1FF;
	// stw r18,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r18.u32);
	// stw r19,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r19.u32);
	// lfs f21,4112(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4112);
	ctx.f21.f64 = double(temp.f32);
	// lwz r18,4100(r6)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4100);
	// lfs f22,4108(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4108);
	ctx.f22.f64 = double(temp.f32);
	// lwz r17,4104(r6)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4104);
	// lwz r19,4096(r6)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4096);
	// rlwinm r16,r19,2,0,29
	ctx.r16.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r18,r18,r19
	ctx.r18.s64 = ctx.r19.s64 - ctx.r18.s64;
	// subf r19,r17,r19
	ctx.r19.s64 = ctx.r19.s64 - ctx.r17.s64;
	// lwz r17,120(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// rlwinm r18,r18,2,20,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFC;
	// lwz r11,136(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// rlwinm r19,r19,2,20,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFC;
	// lwz r14,128(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r15,0(r15)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	// fmr f7,f11
	ctx.f7.f64 = ctx.f11.f64;
	// lwz r17,0(r17)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	// fmr f6,f11
	ctx.f6.f64 = ctx.f11.f64;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lfsx f20,r18,r6
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r6.u32);
	ctx.f20.f64 = double(temp.f32);
	// ld r31,304(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 304);
	// lfsx f19,r19,r6
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r6.u32);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f22,f20,f22
	ctx.f22.f64 = double(float(ctx.f20.f64 * ctx.f22.f64));
	// lfs f4,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f21,f19,f21
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// lfs f12,0(r14)
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// stw r17,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r17.u32);
	// stfs f22,4116(r6)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4116, temp.u32);
	// stw r15,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r15.u32);
	// stfs f21,4120(r6)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4120, temp.u32);
	// ld r27,312(r1)
	ctx.r27.u64 = PPC_LOAD_U64(ctx.r1.u32 + 312);
	// stfsx f2,r16,r6
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r16.u32 + ctx.r6.u32, temp.u32);
	// ld r11,296(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 296);
	// lfs f2,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f2,f23
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f23.f64));
	// lwz r19,4096(r6)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4096);
	// lfs f22,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f25,f22,f25
	ctx.f25.f64 = double(float(ctx.f22.f64 * ctx.f25.f64));
	// addi r18,r19,1
	ctx.r18.s64 = ctx.r19.s64 + 1;
	// lwz r19,96(r1)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// clrlwi r18,r18,22
	ctx.r18.u64 = ctx.r18.u32 & 0x3FF;
	// stw r18,4096(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4096, ctx.r18.u32);
	// fadds f2,f2,f25
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f25.f64));
	// fadds f2,f2,f24
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f24.f64));
	// lwz r19,0(r19)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	// lwz r18,0(r10)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// ld r10,280(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 280);
	// cmpwi cr6,r18,0
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 0, ctx.xer);
	// stw r19,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r19.u32);
	// lfs f25,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f2,f2,f25
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f25.f64));
	// bne cr6,0x822e2b5c
	if (!ctx.cr6.eq) goto loc_822E2B5C;
	// addis r27,r31,1
	ctx.r27.s64 = ctx.r31.s64 + 65536;
	// lis r28,1
	ctx.r28.s64 = 65536;
	// addi r27,r27,20352
	ctx.r27.s64 = ctx.r27.s64 + 20352;
	// lis r26,1
	ctx.r26.s64 = 65536;
	// ori r28,r28,20340
	ctx.r28.u64 = ctx.r28.u64 | 20340;
	// ori r26,r26,22420
	ctx.r26.u64 = ctx.r26.u64 | 22420;
	// addis r23,r31,1
	ctx.r23.s64 = ctx.r31.s64 + 65536;
	// lwz r25,2052(r27)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r27.u32 + 2052);
	// lfs f7,2060(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 2060);
	ctx.f7.f64 = double(temp.f32);
	// lwz r24,2048(r27)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r27.u32 + 2048);
	// lfs f6,2064(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 2064);
	ctx.f6.f64 = double(temp.f32);
	// rlwinm r25,r25,2,0,29
	ctx.r25.u64 = rotl64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r31,r28
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r28.u32);
	ctx.f30.f64 = double(temp.f32);
	// rlwinm r24,r24,2,0,29
	ctx.r24.u64 = rotl64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f29,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f29.f64 = double(temp.f32);
	// lis r28,1
	ctx.r28.s64 = 65536;
	// fadds f29,f29,f10
	ctx.f29.f64 = double(float(ctx.f29.f64 + ctx.f10.f64));
	// addis r26,r31,1
	ctx.r26.s64 = ctx.r31.s64 + 65536;
	// addi r23,r23,22452
	ctx.r23.s64 = ctx.r23.s64 + 22452;
	// lfsx f25,r25,r27
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r27.u32);
	ctx.f25.f64 = double(temp.f32);
	// ori r25,r28,22448
	ctx.r25.u64 = ctx.r28.u64 | 22448;
	// fmadds f7,f7,f25,f30
	ctx.f7.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f25.f64), float(ctx.f30.f64)));
	// stfsx f7,r24,r27
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r24.u32 + ctx.r27.u32, temp.u32);
	// lis r28,1
	ctx.r28.s64 = 65536;
	// addi r26,r26,22432
	ctx.r26.s64 = ctx.r26.s64 + 22432;
	// ori r22,r28,22476
	ctx.r22.u64 = ctx.r28.u64 | 22476;
	// lis r28,1
	ctx.r28.s64 = 65536;
	// addis r24,r31,1
	ctx.r24.s64 = ctx.r31.s64 + 65536;
	// ori r20,r28,22500
	ctx.r20.u64 = ctx.r28.u64 | 22500;
	// lis r28,1
	ctx.r28.s64 = 65536;
	// addi r24,r24,22480
	ctx.r24.s64 = ctx.r24.s64 + 22480;
	// ori r18,r28,38916
	ctx.r18.u64 = ctx.r28.u64 | 38916;
	// lwz r28,2052(r27)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + 2052);
	// addi r17,r28,1
	ctx.r17.s64 = ctx.r28.s64 + 1;
	// fmadds f6,f6,f7,f25
	ctx.f6.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f7.f64), float(ctx.f25.f64)));
	// lwz r28,2048(r27)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + 2048);
	// clrlwi r17,r17,23
	ctx.r17.u64 = ctx.r17.u32 & 0x1FF;
	// stfs f6,2068(r27)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r27.u32 + 2068, temp.u32);
	// addis r21,r31,2
	ctx.r21.s64 = ctx.r31.s64 + 131072;
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// stw r17,2052(r27)
	PPC_STORE_U32(ctx.r27.u32 + 2052, ctx.r17.u32);
	// lis r19,1
	ctx.r19.s64 = 65536;
	// clrlwi r28,r28,23
	ctx.r28.u64 = ctx.r28.u32 & 0x1FF;
	// addi r21,r21,-26608
	ctx.r21.s64 = ctx.r21.s64 + -26608;
	// stw r28,2048(r27)
	PPC_STORE_U32(ctx.r27.u32 + 2048, ctx.r28.u32);
	// lfs f30,0(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// ori r19,r19,38944
	ctx.r19.u64 = ctx.r19.u64 | 38944;
	// lfsx f6,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f6.f64 = double(temp.f32);
	// lwz r28,8(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f25,r28,r26
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r26.u32);
	ctx.f25.f64 = double(temp.f32);
	// lfs f7,12(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f7,f25,f7,f29
	ctx.f7.f64 = double(std::fma(float(ctx.f25.f64), float(ctx.f7.f64), float(ctx.f29.f64)));
	// stfs f30,4(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4, temp.u32);
	// stfs f7,16(r26)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r26.u32 + 16, temp.u32);
	// stfs f29,0(r26)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r26.u32 + 0, temp.u32);
	// lfs f29,16(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 16);
	ctx.f29.f64 = double(temp.f32);
	// lwz r28,8(r23)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8);
	// lfs f7,12(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r28,r23
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r23.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f6,f29,f30,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f29.f64), float(ctx.f30.f64), float(ctx.f6.f64)));
	// lfs f25,20(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 20);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f30,f25,f30
	ctx.f30.f64 = double(float(ctx.f25.f64 * ctx.f30.f64));
	// lfsx f29,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f29.f64 = double(temp.f32);
	// lfs f25,0(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 0);
	ctx.f25.f64 = double(temp.f32);
	// stfs f6,0(r23)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r23.u32 + 0, temp.u32);
	// stfs f25,4(r23)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r23.u32 + 4, temp.u32);
	// fmadds f7,f7,f6,f30
	ctx.f7.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f6.f64), float(ctx.f30.f64)));
	// stfs f7,24(r23)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r23.u32 + 24, temp.u32);
	// lfs f25,0(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	ctx.f25.f64 = double(temp.f32);
	// lwz r28,8(r24)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f6,r31,r20
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r20.u32);
	ctx.f6.f64 = double(temp.f32);
	// lfs f7,12(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// lfs f30,16(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 16);
	ctx.f30.f64 = double(temp.f32);
	// lfsx f24,r28,r24
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r24.u32);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f30,f30,f24,f29
	ctx.f30.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f24.f64), float(ctx.f29.f64)));
	// stfs f25,4(r24)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r24.u32 + 4, temp.u32);
	// fmuls f6,f6,f28
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f28.f64));
	// fmuls f7,f7,f30
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f30.f64));
	// stfs f30,0(r24)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r24.u32 + 0, temp.u32);
	// addis r27,r31,1
	ctx.r27.s64 = ctx.r31.s64 + 65536;
	// stfs f7,20(r24)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r24.u32 + 20, temp.u32);
	// lwz r28,8(r21)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r21.u32 + 8);
	// lfs f7,12(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f25,0(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 0);
	ctx.f25.f64 = double(temp.f32);
	// addi r27,r27,22512
	ctx.r27.s64 = ctx.r27.s64 + 22512;
	// lfsx f24,r28,r21
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r21.u32);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f7,f7,f24
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f24.f64));
	// lfsx f29,r31,r19
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r19.u32);
	ctx.f29.f64 = double(temp.f32);
	// lis r28,1
	ctx.r28.s64 = 65536;
	// lfsx f30,r31,r18
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r18.u32);
	ctx.f30.f64 = double(temp.f32);
	// lis r26,1
	ctx.r26.s64 = 65536;
	// stfs f30,0(r21)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r21.u32 + 0, temp.u32);
	// ori r23,r28,41028
	ctx.r23.u64 = ctx.r28.u64 | 41028;
	// stfs f7,16(r21)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r21.u32 + 16, temp.u32);
	// lis r28,1
	ctx.r28.s64 = 65536;
	// stfs f25,4(r21)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r21.u32 + 4, temp.u32);
	// ori r26,r26,38920
	ctx.r26.u64 = ctx.r26.u64 | 38920;
	// ori r20,r28,43136
	ctx.r20.u64 = ctx.r28.u64 | 43136;
	// lis r28,1
	ctx.r28.s64 = 65536;
	// addis r25,r31,2
	ctx.r25.s64 = ctx.r31.s64 + 131072;
	// ori r18,r28,43188
	ctx.r18.u64 = ctx.r28.u64 | 43188;
	// addi r25,r25,-24496
	ctx.r25.s64 = ctx.r25.s64 + -24496;
	// lis r24,1
	ctx.r24.s64 = 65536;
	// addis r22,r31,2
	ctx.r22.s64 = ctx.r31.s64 + 131072;
	// ori r24,r24,43108
	ctx.r24.u64 = ctx.r24.u64 | 43108;
	// addi r22,r22,-22416
	ctx.r22.s64 = ctx.r22.s64 + -22416;
	// lis r21,1
	ctx.r21.s64 = 65536;
	// addis r19,r31,2
	ctx.r19.s64 = ctx.r31.s64 + 131072;
	// addis r17,r31,2
	ctx.r17.s64 = ctx.r31.s64 + 131072;
	// ori r21,r21,43164
	ctx.r21.u64 = ctx.r21.u64 | 43164;
	// addi r19,r19,-22396
	ctx.r19.s64 = ctx.r19.s64 + -22396;
	// addi r17,r17,-22368
	ctx.r17.s64 = ctx.r17.s64 + -22368;
	// lwz r15,16392(r27)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16392);
	// lwz r28,16388(r27)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16388);
	// lfsx f7,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f7.f64 = double(temp.f32);
	// lwz r16,16384(r27)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16384);
	// lfs f25,16396(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 16396);
	ctx.f25.f64 = double(temp.f32);
	// subf r28,r28,r16
	ctx.r28.s64 = ctx.r16.s64 - ctx.r28.s64;
	// lfs f30,16400(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 16400);
	ctx.f30.f64 = double(temp.f32);
	// rlwinm r28,r28,2,18,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0x3FFC;
	// subf r26,r15,r16
	ctx.r26.s64 = ctx.r16.s64 - ctx.r15.s64;
	// rlwinm r16,r16,2,0,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r26,r26,2,18,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0x3FFC;
	// lfsx f24,r28,r27
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r27.u32);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f24,f25
	ctx.f25.f64 = double(float(ctx.f24.f64 * ctx.f25.f64));
	// lfsx f24,r26,r27
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r27.u32);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f30,f24,f30
	ctx.f30.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// stfs f30,16408(r27)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16408, temp.u32);
	// stfs f25,16404(r27)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16404, temp.u32);
	// stfsx f6,r16,r27
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r16.u32 + ctx.r27.u32, temp.u32);
	// lwz r28,16384(r27)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16384);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r28,r28,20
	ctx.r28.u64 = ctx.r28.u32 & 0xFFF;
	// stw r28,16384(r27)
	PPC_STORE_U32(ctx.r27.u32 + 16384, ctx.r28.u32);
	// lwz r28,2052(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2052);
	// lfsx f30,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f30.f64 = double(temp.f32);
	// lwz r27,2048(r25)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2048);
	// lfs f25,2060(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 2060);
	ctx.f25.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f24,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f24.f64 = double(temp.f32);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// fadds f24,f24,f9
	ctx.f24.f64 = double(float(ctx.f24.f64 + ctx.f9.f64));
	// lfsx f6,r28,r25
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r25.u32);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f30,f25,f6,f30
	ctx.f30.f64 = double(std::fma(float(ctx.f25.f64), float(ctx.f6.f64), float(ctx.f30.f64)));
	// lfs f25,2064(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 2064);
	ctx.f25.f64 = double(temp.f32);
	// stfsx f30,r27,r25
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r25.u32, temp.u32);
	// lwz r27,2048(r25)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2048);
	// fmadds f6,f25,f30,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f25.f64), float(ctx.f30.f64), float(ctx.f6.f64)));
	// stfs f6,2068(r25)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2068, temp.u32);
	// lwz r28,2052(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2052);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r27,r27,23
	ctx.r27.u64 = ctx.r27.u32 & 0x1FF;
	// clrlwi r28,r28,23
	ctx.r28.u64 = ctx.r28.u32 & 0x1FF;
	// stw r27,2048(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2048, ctx.r27.u32);
	// stw r28,2052(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2052, ctx.r28.u32);
	// lfs f30,12(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// lfsx f6,r31,r20
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r20.u32);
	ctx.f6.f64 = double(temp.f32);
	// lwz r28,8(r22)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8);
	// lfs f25,0(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	ctx.f25.f64 = double(temp.f32);
	// addis r27,r31,2
	ctx.r27.s64 = ctx.r31.s64 + 131072;
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r27,r27,-22336
	ctx.r27.s64 = ctx.r27.s64 + -22336;
	// lis r26,1
	ctx.r26.s64 = 65536;
	// addis r25,r31,2
	ctx.r25.s64 = ctx.r31.s64 + 131072;
	// ori r26,r26,59608
	ctx.r26.u64 = ctx.r26.u64 | 59608;
	// lfsx f23,r28,r22
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r22.u32);
	ctx.f23.f64 = double(temp.f32);
	// lis r28,1
	ctx.r28.s64 = 65536;
	// fmadds f30,f23,f30,f24
	ctx.f30.f64 = double(std::fma(float(ctx.f23.f64), float(ctx.f30.f64), float(ctx.f24.f64)));
	// stfs f30,16(r22)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r22.u32 + 16, temp.u32);
	// stfs f25,4(r22)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r22.u32 + 4, temp.u32);
	// ori r28,r28,59604
	ctx.r28.u64 = ctx.r28.u64 | 59604;
	// stfs f24,0(r22)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r22.u32 + 0, temp.u32);
	// addi r25,r25,-5920
	ctx.r25.s64 = ctx.r25.s64 + -5920;
	// lfs f30,16(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 16);
	ctx.f30.f64 = double(temp.f32);
	// lis r24,1
	ctx.r24.s64 = 65536;
	// lis r23,2
	ctx.r23.s64 = 131072;
	// ori r24,r24,59632
	ctx.r24.u64 = ctx.r24.u64 | 59632;
	// addis r22,r31,2
	ctx.r22.s64 = ctx.r31.s64 + 131072;
	// ori r23,r23,2324
	ctx.r23.u64 = ctx.r23.u64 | 2324;
	// addi r22,r22,2336
	ctx.r22.s64 = ctx.r22.s64 + 2336;
	// lwz r20,8(r19)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8);
	// lfsx f24,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,0(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	ctx.f22.f64 = double(temp.f32);
	// lis r21,2
	ctx.r21.s64 = 131072;
	// rlwinm r20,r20,2,0,29
	ctx.r20.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f23,20(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 20);
	ctx.f23.f64 = double(temp.f32);
	// lfs f25,12(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 12);
	ctx.f25.f64 = double(temp.f32);
	// ori r21,r21,10552
	ctx.r21.u64 = ctx.r21.u64 | 10552;
	// lfsx f21,r20,r19
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + ctx.r19.u32);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f23,f23,f21
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// fmadds f6,f30,f21,f6
	ctx.f6.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f21.f64), float(ctx.f6.f64)));
	// stfs f6,0(r19)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r19.u32 + 0, temp.u32);
	// stfs f22,4(r19)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r19.u32 + 4, temp.u32);
	// fmadds f6,f25,f6,f23
	ctx.f6.f64 = double(std::fma(float(ctx.f25.f64), float(ctx.f6.f64), float(ctx.f23.f64)));
	// stfs f6,24(r19)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r19.u32 + 24, temp.u32);
	// lfs f23,0(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	ctx.f23.f64 = double(temp.f32);
	// lfs f30,12(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// lwz r20,8(r17)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8);
	// rlwinm r20,r20,2,0,29
	ctx.r20.u64 = rotl64(ctx.r20.u32 | (ctx.r20.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f22,r20,r17
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + ctx.r17.u32);
	ctx.f22.f64 = double(temp.f32);
	// lfs f6,16(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f6,f6,f22,f24
	ctx.f6.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f22.f64), float(ctx.f24.f64)));
	// lfsx f25,r31,r18
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r18.u32);
	ctx.f25.f64 = double(temp.f32);
	// stfs f23,4(r17)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r17.u32 + 4, temp.u32);
	// fmuls f25,f25,f28
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// fmuls f30,f30,f6
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f6.f64));
	// stfs f30,20(r17)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r17.u32 + 20, temp.u32);
	// stfs f6,0(r17)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r17.u32 + 0, temp.u32);
	// lfsx f6,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f6.f64 = double(temp.f32);
	// lwz r19,16392(r27)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16392);
	// lfs f24,16400(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 16400);
	ctx.f24.f64 = double(temp.f32);
	// lwz r18,16384(r27)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16384);
	// lfsx f23,r31,r28
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r28.u32);
	ctx.f23.f64 = double(temp.f32);
	// lwz r20,16388(r27)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16388);
	// lfs f30,16396(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 16396);
	ctx.f30.f64 = double(temp.f32);
	// subf r28,r20,r18
	ctx.r28.s64 = ctx.r18.s64 - ctx.r20.s64;
	// subf r19,r19,r18
	ctx.r19.s64 = ctx.r18.s64 - ctx.r19.s64;
	// rlwinm r28,r28,2,18,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0x3FFC;
	// rlwinm r26,r19,2,18,29
	ctx.r26.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0x3FFC;
	// rlwinm r17,r18,2,0,29
	ctx.r17.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f21,r28,r27
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r27.u32);
	ctx.f21.f64 = double(temp.f32);
	// lfsx f22,r26,r27
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r27.u32);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f30,f21,f30
	ctx.f30.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// fmuls f24,f22,f24
	ctx.f24.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// stfs f30,16404(r27)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16404, temp.u32);
	// stfs f24,16408(r27)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16408, temp.u32);
	// stfsx f25,r17,r27
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r27.u32, temp.u32);
	// lwz r28,16384(r27)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + 16384);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r28,r28,20
	ctx.r28.u64 = ctx.r28.u32 & 0xFFF;
	// stw r28,16384(r27)
	PPC_STORE_U32(ctx.r27.u32 + 16384, ctx.r28.u32);
	// lwz r28,8(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8);
	// lfs f30,12(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// lfs f24,0(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	ctx.f24.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f25,r28,r25
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r25.u32);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f30,f30,f25
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// lfsx f25,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f25.f64 = double(temp.f32);
	// stfs f24,4(r25)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r25.u32 + 4, temp.u32);
	// addis r26,r31,2
	ctx.r26.s64 = ctx.r31.s64 + 131072;
	// stfs f23,0(r25)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r25.u32 + 0, temp.u32);
	// addis r24,r31,2
	ctx.r24.s64 = ctx.r31.s64 + 131072;
	// stfs f30,16(r25)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r25.u32 + 16, temp.u32);
	// addi r26,r26,10560
	ctx.r26.s64 = ctx.r26.s64 + 10560;
	// lis r25,2
	ctx.r25.s64 = 131072;
	// addi r24,r24,18784
	ctx.r24.s64 = ctx.r24.s64 + 18784;
	// ori r25,r25,18772
	ctx.r25.u64 = ctx.r25.u64 | 18772;
	// addis r20,r31,2
	ctx.r20.s64 = ctx.r31.s64 + 131072;
	// addi r20,r20,22912
	ctx.r20.s64 = ctx.r20.s64 + 22912;
	// lwz r27,8192(r22)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8192);
	// lfs f24,8204(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 8204);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,8208(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 8208);
	ctx.f22.f64 = double(temp.f32);
	// lwz r28,8196(r22)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8196);
	// lfsx f30,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f30.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// fadds f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 + ctx.f0.f64));
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f21,8212(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 8212);
	ctx.f21.f64 = double(temp.f32);
	// lis r23,2
	ctx.r23.s64 = 131072;
	// lfsx f20,r28,r22
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r22.u32);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f21,f21,f20
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f20.f64));
	// lfsx f23,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f24,f24,f20,f30
	ctx.f24.f64 = double(std::fma(float(ctx.f24.f64), float(ctx.f20.f64), float(ctx.f30.f64)));
	// stfsx f24,r27,r22
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r22.u32, temp.u32);
	// fmadds f30,f22,f30,f21
	ctx.f30.f64 = double(std::fma(float(ctx.f22.f64), float(ctx.f30.f64), float(ctx.f21.f64)));
	// stfs f30,8216(r22)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r22.u32 + 8216, temp.u32);
	// lis r28,2
	ctx.r28.s64 = 131072;
	// ori r23,r23,39316
	ctx.r23.u64 = ctx.r23.u64 | 39316;
	// ori r21,r28,22900
	ctx.r21.u64 = ctx.r28.u64 | 22900;
	// lis r28,2
	ctx.r28.s64 = 131072;
	// ori r19,r28,39320
	ctx.r19.u64 = ctx.r28.u64 | 39320;
	// lwz r28,8196(r22)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8196);
	// lwz r27,8192(r22)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r22.u32 + 8192);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r28,r28,21
	ctx.r28.u64 = ctx.r28.u32 & 0x7FF;
	// clrlwi r27,r27,21
	ctx.r27.u64 = ctx.r27.u32 & 0x7FF;
	// stw r28,8196(r22)
	PPC_STORE_U32(ctx.r22.u32 + 8196, ctx.r28.u32);
	// stw r27,8192(r22)
	PPC_STORE_U32(ctx.r22.u32 + 8192, ctx.r27.u32);
	// lwz r27,8192(r26)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// lwz r28,8196(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// lfs f30,8204(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8204);
	ctx.f30.f64 = double(temp.f32);
	// lfsx f24,r31,r25
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r25.u32);
	ctx.f24.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f22,8208(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8208);
	ctx.f22.f64 = double(temp.f32);
	// lfsx f21,r28,r26
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r26.u32);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f30,f30,f21,f23
	ctx.f30.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f21.f64), float(ctx.f23.f64)));
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f30,r27,r26
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r26.u32, temp.u32);
	// fmadds f30,f22,f30,f21
	ctx.f30.f64 = double(std::fma(float(ctx.f22.f64), float(ctx.f30.f64), float(ctx.f21.f64)));
	// stfs f30,8212(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 8212, temp.u32);
	// lwz r27,8192(r26)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// lwz r28,8196(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r27,r27,21
	ctx.r27.u64 = ctx.r27.u32 & 0x7FF;
	// clrlwi r28,r28,21
	ctx.r28.u64 = ctx.r28.u32 & 0x7FF;
	// stw r27,8192(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8192, ctx.r27.u32);
	// stw r28,8196(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8196, ctx.r28.u32);
	// lfs f23,4108(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 4108);
	ctx.f23.f64 = double(temp.f32);
	// lwz r27,4096(r24)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4096);
	// lwz r28,4100(r24)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4100);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f21,r28,r24
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r24.u32);
	ctx.f21.f64 = double(temp.f32);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f30,4112(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 4112);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f24,f23,f21,f24
	ctx.f24.f64 = double(std::fma(float(ctx.f23.f64), float(ctx.f21.f64), float(ctx.f24.f64)));
	// lfsx f22,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f22.f64 = double(temp.f32);
	// fadds f24,f24,f0
	ctx.f24.f64 = double(float(ctx.f24.f64 + ctx.f0.f64));
	// stfsx f24,r27,r24
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r24.u32, temp.u32);
	// lwz r28,4100(r24)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4100);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// lwz r27,4096(r24)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4096);
	// fmadds f30,f30,f24,f21
	ctx.f30.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f24.f64), float(ctx.f21.f64)));
	// stfs f30,4116(r24)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r24.u32 + 4116, temp.u32);
	// clrlwi r28,r28,22
	ctx.r28.u64 = ctx.r28.u32 & 0x3FF;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r27,r27,22
	ctx.r27.u64 = ctx.r27.u32 & 0x3FF;
	// stw r27,4096(r24)
	PPC_STORE_U32(ctx.r24.u32 + 4096, ctx.r27.u32);
	// stw r28,4100(r24)
	PPC_STORE_U32(ctx.r24.u32 + 4100, ctx.r28.u32);
	// fadds f10,f22,f10
	ctx.f10.f64 = double(float(ctx.f22.f64 + ctx.f10.f64));
	// lwz r28,16384(r20)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r20.u32 + 16384);
	// rlwinm r25,r28,2,0,29
	ctx.r25.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r27,16388(r20)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r20.u32 + 16388);
	// lfs f23,16396(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 16396);
	ctx.f23.f64 = double(temp.f32);
	// lwz r26,16392(r20)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r20.u32 + 16392);
	// lfs f21,16400(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 16400);
	ctx.f21.f64 = double(temp.f32);
	// subf r27,r27,r28
	ctx.r27.s64 = ctx.r28.s64 - ctx.r27.s64;
	// lfsx f30,r31,r23
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r23.u32);
	ctx.f30.f64 = double(temp.f32);
	// subf r28,r26,r28
	ctx.r28.s64 = ctx.r28.s64 - ctx.r26.s64;
	// lfsx f24,r31,r19
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r19.u32);
	ctx.f24.f64 = double(temp.f32);
	// rlwinm r27,r27,2,18,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0x3FFC;
	// rlwinm r28,r28,2,18,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0x3FFC;
	// addis r26,r31,3
	ctx.r26.s64 = ctx.r31.s64 + 196608;
	// lis r24,2
	ctx.r24.s64 = 131072;
	// addi r26,r26,-26208
	ctx.r26.s64 = ctx.r26.s64 + -26208;
	// lfsx f22,r27,r20
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + ctx.r20.u32);
	ctx.f22.f64 = double(temp.f32);
	// ori r27,r24,39348
	ctx.r27.u64 = ctx.r24.u64 | 39348;
	// lfsx f20,r28,r20
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r20.u32);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f23,f22,f23
	ctx.f23.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// fmuls f22,f20,f21
	ctx.f22.f64 = double(float(ctx.f20.f64 * ctx.f21.f64));
	// stfs f23,16404(r20)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r20.u32 + 16404, temp.u32);
	// stfs f22,16408(r20)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r20.u32 + 16408, temp.u32);
	// lis r28,2
	ctx.r28.s64 = 131072;
	// stfsx f10,r25,r20
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r25.u32 + ctx.r20.u32, temp.u32);
	// lis r23,2
	ctx.r23.s64 = 131072;
	// ori r22,r28,55800
	ctx.r22.u64 = ctx.r28.u64 | 55800;
	// lis r28,2
	ctx.r28.s64 = 131072;
	// ori r24,r23,47572
	ctx.r24.u64 = ctx.r23.u64 | 47572;
	// ori r18,r28,64020
	ctx.r18.u64 = ctx.r28.u64 | 64020;
	// addis r25,r31,3
	ctx.r25.s64 = ctx.r31.s64 + 196608;
	// addis r23,r31,3
	ctx.r23.s64 = ctx.r31.s64 + 196608;
	// addi r25,r25,-17952
	ctx.r25.s64 = ctx.r25.s64 + -17952;
	// lis r21,3
	ctx.r21.s64 = 196608;
	// lis r19,3
	ctx.r19.s64 = 196608;
	// addis r17,r31,3
	ctx.r17.s64 = ctx.r31.s64 + 196608;
	// addis r16,r31,3
	ctx.r16.s64 = ctx.r31.s64 + 196608;
	// addi r23,r23,-9728
	ctx.r23.s64 = ctx.r23.s64 + -9728;
	// ori r21,r21,2612
	ctx.r21.u64 = ctx.r21.u64 | 2612;
	// addi r17,r17,-1504
	ctx.r17.s64 = ctx.r17.s64 + -1504;
	// ori r19,r19,19028
	ctx.r19.u64 = ctx.r19.u64 | 19028;
	// addi r16,r16,19040
	ctx.r16.s64 = ctx.r16.s64 + 19040;
	// lwz r28,16384(r20)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r20.u32 + 16384);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r28,r28,20
	ctx.r28.u64 = ctx.r28.u32 & 0xFFF;
	// stw r28,16384(r20)
	PPC_STORE_U32(ctx.r20.u32 + 16384, ctx.r28.u32);
	// lfs f23,16(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 16);
	ctx.f23.f64 = double(temp.f32);
	// lwz r28,8(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// lfsx f22,r31,r27
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r27.u32);
	ctx.f22.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f21,0(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	ctx.f21.f64 = double(temp.f32);
	// lfs f10,12(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f10,f10,f30
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f30.f64));
	// lfsx f20,r28,r26
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r26.u32);
	ctx.f20.f64 = double(temp.f32);
	// stfs f30,0(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 0, temp.u32);
	// fmadds f10,f23,f20,f10
	ctx.f10.f64 = double(std::fma(float(ctx.f23.f64), float(ctx.f20.f64), float(ctx.f10.f64)));
	// stfs f10,20(r26)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r26.u32 + 20, temp.u32);
	// stfs f21,4(r26)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4, temp.u32);
	// lfsx f10,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f10.f64 = double(temp.f32);
	// lfsx f23,r31,r22
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r22.u32);
	ctx.f23.f64 = double(temp.f32);
	// lwz r28,8196(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// lfs f30,8204(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8204);
	ctx.f30.f64 = double(temp.f32);
	// lwz r27,8192(r25)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// fadds f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f12.f64));
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f21,8208(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8208);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,8212(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 8212);
	ctx.f20.f64 = double(temp.f32);
	// lfsx f19,r28,r25
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r25.u32);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f20,f20,f19
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f19.f64));
	// fmadds f30,f30,f19,f10
	ctx.f30.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f19.f64), float(ctx.f10.f64)));
	// stfsx f30,r27,r25
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r25.u32, temp.u32);
	// fmadds f10,f21,f10,f20
	ctx.f10.f64 = double(std::fma(float(ctx.f21.f64), float(ctx.f10.f64), float(ctx.f20.f64)));
	// stfs f10,8216(r25)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8216, temp.u32);
	// lwz r28,8192(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8192);
	// addi r27,r28,1
	ctx.r27.s64 = ctx.r28.s64 + 1;
	// lwz r28,8196(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8196);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r27,r27,21
	ctx.r27.u64 = ctx.r27.u32 & 0x7FF;
	// clrlwi r28,r28,21
	ctx.r28.u64 = ctx.r28.u32 & 0x7FF;
	// stw r27,8192(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8192, ctx.r27.u32);
	// lis r26,3
	ctx.r26.s64 = 196608;
	// stw r28,8196(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8196, ctx.r28.u32);
	// addis r25,r31,3
	ctx.r25.s64 = ctx.r31.s64 + 196608;
	// ori r26,r26,19060
	ctx.r26.u64 = ctx.r26.u64 | 19060;
	// fadds f20,f25,f29
	ctx.f20.f64 = double(float(ctx.f25.f64 + ctx.f29.f64));
	// addi r25,r25,2624
	ctx.r25.s64 = ctx.r25.s64 + 2624;
	// addis r22,r31,1
	ctx.r22.s64 = ctx.r31.s64 + 65536;
	// addis r20,r31,2
	ctx.r20.s64 = ctx.r31.s64 + 131072;
	// addi r22,r22,18272
	ctx.r22.s64 = ctx.r22.s64 + 18272;
	// addi r20,r20,-26576
	ctx.r20.s64 = ctx.r20.s64 + -26576;
	// lwz r28,8196(r23)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8196);
	// lfs f10,8204(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 8204);
	ctx.f10.f64 = double(temp.f32);
	// lwz r27,8192(r23)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8192);
	// lfsx f21,r31,r18
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r18.u32);
	ctx.f21.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f30,8208(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 8208);
	ctx.f30.f64 = double(temp.f32);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f19,r28,r23
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r23.u32);
	ctx.f19.f64 = double(temp.f32);
	// lis r28,3
	ctx.r28.s64 = 196608;
	// fmadds f10,f10,f19,f23
	ctx.f10.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f19.f64), float(ctx.f23.f64)));
	// stfsx f10,r27,r23
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r23.u32, temp.u32);
	// ori r24,r28,19032
	ctx.r24.u64 = ctx.r28.u64 | 19032;
	// lwz r28,8196(r23)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8196);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r28,r28,21
	ctx.r28.u64 = ctx.r28.u32 & 0x7FF;
	// fmadds f10,f30,f10,f19
	ctx.f10.f64 = double(std::fma(float(ctx.f30.f64), float(ctx.f10.f64), float(ctx.f19.f64)));
	// stfs f10,8212(r23)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r23.u32 + 8212, temp.u32);
	// lwz r27,8192(r23)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8192);
	// stw r28,8196(r23)
	PPC_STORE_U32(ctx.r23.u32 + 8196, ctx.r28.u32);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r27,r27,21
	ctx.r27.u64 = ctx.r27.u32 & 0x7FF;
	// stw r27,8192(r23)
	PPC_STORE_U32(ctx.r23.u32 + 8192, ctx.r27.u32);
	// lwz r28,4100(r17)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r17.u32 + 4100);
	// lfsx f30,r31,r21
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r21.u32);
	ctx.f30.f64 = double(temp.f32);
	// fadds f9,f30,f9
	ctx.f9.f64 = double(float(ctx.f30.f64 + ctx.f9.f64));
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f10,4108(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 4108);
	ctx.f10.f64 = double(temp.f32);
	// lwz r27,4096(r17)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r17.u32 + 4096);
	// lfs f23,4112(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 4112);
	ctx.f23.f64 = double(temp.f32);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f30,r28,r17
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r17.u32);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f10,f10,f30,f21
	ctx.f10.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f30.f64), float(ctx.f21.f64)));
	// fadds f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f12.f64));
	// stfsx f10,r27,r17
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r17.u32, temp.u32);
	// fmadds f10,f23,f10,f30
	ctx.f10.f64 = double(std::fma(float(ctx.f23.f64), float(ctx.f10.f64), float(ctx.f30.f64)));
	// stfs f10,4116(r17)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r17.u32 + 4116, temp.u32);
	// lwz r27,4096(r17)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r17.u32 + 4096);
	// lwz r28,4100(r17)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r17.u32 + 4100);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r28,r28,22
	ctx.r28.u64 = ctx.r28.u32 & 0x3FF;
	// clrlwi r27,r27,22
	ctx.r27.u64 = ctx.r27.u32 & 0x3FF;
	// stw r28,4100(r17)
	PPC_STORE_U32(ctx.r17.u32 + 4100, ctx.r28.u32);
	// stw r27,4096(r17)
	PPC_STORE_U32(ctx.r17.u32 + 4096, ctx.r27.u32);
	// lfs f30,12(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// lwz r28,8(r16)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r16.u32 + 8);
	// lfsx f10,r31,r19
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r19.u32);
	ctx.f10.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f30,f30,f10
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// lfsx f21,r31,r26
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r26.u32);
	ctx.f21.f64 = double(temp.f32);
	// fadds f18,f21,f22
	ctx.f18.f64 = double(float(ctx.f21.f64 + ctx.f22.f64));
	// lfs f23,16(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 16);
	ctx.f23.f64 = double(temp.f32);
	// fsubs f22,f21,f22
	ctx.f22.f64 = static_cast<float>(ctx.f21.f64 - ctx.f22.f64);
	// lfsx f21,r28,r16
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r16.u32);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f19.f64 = double(temp.f32);
	// stfs f10,0(r16)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r16.u32 + 0, temp.u32);
	// stfs f19,4(r16)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r16.u32 + 4, temp.u32);
	// fmadds f10,f23,f21,f30
	ctx.f10.f64 = double(std::fma(float(ctx.f23.f64), float(ctx.f21.f64), float(ctx.f30.f64)));
	// stfs f10,20(r16)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r16.u32 + 20, temp.u32);
	// lwz r27,16388(r25)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16388);
	// lfsx f30,r31,r24
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r24.u32);
	ctx.f30.f64 = double(temp.f32);
	// lwz r26,16392(r25)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16392);
	// lfs f10,16400(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 16400);
	ctx.f10.f64 = double(temp.f32);
	// lwz r28,16384(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16384);
	// subf r27,r27,r28
	ctx.r27.s64 = ctx.r28.s64 - ctx.r27.s64;
	// subf r26,r26,r28
	ctx.r26.s64 = ctx.r28.s64 - ctx.r26.s64;
	// lfs f23,16396(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 16396);
	ctx.f23.f64 = double(temp.f32);
	// fadds f30,f30,f7
	ctx.f30.f64 = double(float(ctx.f30.f64 + ctx.f7.f64));
	// rlwinm r27,r27,2,18,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0x3FFC;
	// rlwinm r26,r26,2,18,29
	ctx.r26.u64 = rotl64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0x3FFC;
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f21,r27,r25
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + ctx.r25.u32);
	ctx.f21.f64 = double(temp.f32);
	// lfsx f19,r26,r25
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r25.u32);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f23,f21,f23
	ctx.f23.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// fmuls f10,f19,f10
	ctx.f10.f64 = double(float(ctx.f19.f64 * ctx.f10.f64));
	// stfs f10,16408(r25)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r25.u32 + 16408, temp.u32);
	// stfs f23,16404(r25)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r25.u32 + 16404, temp.u32);
	// addis r26,r31,2
	ctx.r26.s64 = ctx.r31.s64 + 131072;
	// addis r19,r31,3
	ctx.r19.s64 = ctx.r31.s64 + 196608;
	// stfsx f9,r28,r25
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r28.u32 + ctx.r25.u32, temp.u32);
	// fsubs f9,f25,f29
	ctx.f9.f64 = static_cast<float>(ctx.f25.f64 - ctx.f29.f64);
	// fadds f29,f24,f6
	ctx.f29.f64 = double(float(ctx.f24.f64 + ctx.f6.f64));
	// addi r26,r26,-5888
	ctx.r26.s64 = ctx.r26.s64 + -5888;
	// addi r19,r19,-26176
	ctx.r19.s64 = ctx.r19.s64 + -26176;
	// lwz r28,16384(r25)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r25.u32 + 16384);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// clrlwi r28,r28,20
	ctx.r28.u64 = ctx.r28.u32 & 0xFFF;
	// stw r28,16384(r25)
	PPC_STORE_U32(ctx.r25.u32 + 16384, ctx.r28.u32);
	// lwz r28,2052(r22)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2052);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f24,r28,r22
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r22.u32);
	ctx.f24.f64 = double(temp.f32);
	// lwz r27,2048(r22)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2048);
	// lfs f10,2060(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 2060);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f10,f10,f24,f18
	ctx.f10.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f24.f64), float(ctx.f18.f64)));
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f25,2064(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 2064);
	ctx.f25.f64 = double(temp.f32);
	// fadds f8,f10,f8
	ctx.f8.f64 = double(float(ctx.f10.f64 + ctx.f8.f64));
	// stfsx f8,r27,r22
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r22.u32, temp.u32);
	// fmadds f10,f25,f8,f24
	ctx.f10.f64 = double(std::fma(float(ctx.f25.f64), float(ctx.f8.f64), float(ctx.f24.f64)));
	// stfs f10,2068(r22)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r22.u32 + 2068, temp.u32);
	// lwz r28,2052(r22)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2052);
	// lwz r27,2048(r22)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2048);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r28,r28,23
	ctx.r28.u64 = ctx.r28.u32 & 0x1FF;
	// clrlwi r27,r27,23
	ctx.r27.u64 = ctx.r27.u32 & 0x1FF;
	// stw r28,2052(r22)
	PPC_STORE_U32(ctx.r22.u32 + 2052, ctx.r28.u32);
	// stw r27,2048(r22)
	PPC_STORE_U32(ctx.r22.u32 + 2048, ctx.r27.u32);
	// lfs f8,2060(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 2060);
	ctx.f8.f64 = double(temp.f32);
	// lwz r28,2052(r20)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2052);
	// lfs f10,2064(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 2064);
	ctx.f10.f64 = double(temp.f32);
	// lwz r27,2048(r20)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2048);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f25,r28,r20
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r20.u32);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f8,f8,f25,f22
	ctx.f8.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f25.f64), float(ctx.f22.f64)));
	// fadds f4,f8,f4
	ctx.f4.f64 = double(float(ctx.f8.f64 + ctx.f4.f64));
	// stfsx f4,r27,r20
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r20.u32, temp.u32);
	// fmadds f10,f10,f4,f25
	ctx.f10.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f4.f64), float(ctx.f25.f64)));
	// stfs f10,2068(r20)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r20.u32 + 2068, temp.u32);
	// lwz r28,2052(r20)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2052);
	// lwz r27,2048(r20)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2048);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r28,r28,23
	ctx.r28.u64 = ctx.r28.u32 & 0x1FF;
	// clrlwi r27,r27,23
	ctx.r27.u64 = ctx.r27.u32 & 0x1FF;
	// stw r28,2052(r20)
	PPC_STORE_U32(ctx.r20.u32 + 2052, ctx.r28.u32);
	// stw r27,2048(r20)
	PPC_STORE_U32(ctx.r20.u32 + 2048, ctx.r27.u32);
	// lwz r28,8196(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// lfs f8,8204(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8204);
	ctx.f8.f64 = double(temp.f32);
	// lwz r27,8192(r26)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// lfs f4,8208(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 8208);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f10,r28,r26
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r26.u32);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f8,f8,f10,f20
	ctx.f8.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f10.f64), float(ctx.f20.f64)));
	// fadds f12,f8,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// stfsx f12,r27,r26
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r26.u32, temp.u32);
	// fmadds f10,f4,f12,f10
	ctx.f10.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f12.f64), float(ctx.f10.f64)));
	// stfs f10,8212(r26)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r26.u32 + 8212, temp.u32);
	// lwz r28,8196(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8196);
	// lwz r27,8192(r26)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8192);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// clrlwi r28,r28,21
	ctx.r28.u64 = ctx.r28.u32 & 0x7FF;
	// clrlwi r27,r27,21
	ctx.r27.u64 = ctx.r27.u32 & 0x7FF;
	// stw r28,8196(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8196, ctx.r28.u32);
	// stw r27,8192(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8192, ctx.r27.u32);
	// lfs f8,8204(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 8204);
	ctx.f8.f64 = double(temp.f32);
	// lwz r28,8196(r19)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8196);
	// lwz r27,8192(r19)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8192);
	// rlwinm r28,r28,2,0,29
	ctx.r28.u64 = rotl64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f4,8208(r19)
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + 8208);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r27,r27,2,0,29
	ctx.r27.u64 = rotl64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r21,100(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r22,84(r1)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r23,88(r1)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r24,176(r1)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lfsx f12,r28,r19
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r19.u32);
	ctx.f12.f64 = double(temp.f32);
	// lwz r25,184(r1)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// fmadds f10,f8,f12,f9
	ctx.f10.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f12.f64), float(ctx.f9.f64)));
	// lwz r26,192(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r20,92(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// fadds f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// stfsx f9,r27,r19
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r27.u32 + ctx.r19.u32, temp.u32);
	// fmadds f8,f4,f9,f12
	ctx.f8.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f9.f64), float(ctx.f12.f64)));
	// stfs f8,8212(r19)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r19.u32 + 8212, temp.u32);
	// lwz r28,8196(r19)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8196);
	// lwz r27,8192(r19)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r19.u32 + 8192);
	// addi r17,r28,1
	ctx.r17.s64 = ctx.r28.s64 + 1;
	// lwz r28,200(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// addi r16,r27,1
	ctx.r16.s64 = ctx.r27.s64 + 1;
	// lwz r27,628(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	// clrlwi r17,r17,21
	ctx.r17.u64 = ctx.r17.u32 & 0x7FF;
	// clrlwi r16,r16,21
	ctx.r16.u64 = ctx.r16.u32 & 0x7FF;
	// stw r17,8196(r19)
	PPC_STORE_U32(ctx.r19.u32 + 8196, ctx.r17.u32);
	// stw r16,8192(r19)
	PPC_STORE_U32(ctx.r19.u32 + 8192, ctx.r16.u32);
loc_822E2B5C:
	// fadds f12,f3,f30
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f3.f64 + ctx.f30.f64));
	// lwz r19,2048(r20)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2048);
	// fadds f10,f2,f29
	ctx.f10.f64 = double(float(ctx.f2.f64 + ctx.f29.f64));
	// lwz r17,2068(r20)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2068);
	// std r10,296(r1)
	PPC_STORE_U64(ctx.r1.u32 + 296, ctx.r10.u64);
	// lfs f0,2080(r20)
	temp.u32 = PPC_LOAD_U32(ctx.r20.u32 + 2080);
	ctx.f0.f64 = double(temp.f32);
	// subf r17,r17,r19
	ctx.r17.s64 = ctx.r19.s64 - ctx.r17.s64;
	// lwz r16,2064(r20)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2064);
	// lwz r15,208(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// fsubs f2,f1,f0
	ctx.f2.f64 = static_cast<float>(ctx.f1.f64 - ctx.f0.f64);
	// addi r14,r17,-1
	ctx.r14.s64 = ctx.r17.s64 + -1;
	// lwz r18,620(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	// rlwinm r17,r17,2,21,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0x7FC;
	// std r11,280(r1)
	PPC_STORE_U64(ctx.r1.u32 + 280, ctx.r11.u64);
	// rlwinm r14,r14,2,21,29
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 2) & 0x7FC;
	// lwz r11,216(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// subf r16,r16,r19
	ctx.r16.s64 = ctx.r19.s64 - ctx.r16.s64;
	// fcmpu cr6,f0,f11
	ctx.cr6.compare(ctx.f0.f64, ctx.f11.f64);
	// lfs f4,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f12,f12,f5
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f5.f64));
	// addi r10,r16,-1
	ctx.r10.s64 = ctx.r16.s64 + -1;
	// fmuls f10,f10,f5
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// lfsx f3,r17,r20
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + ctx.r20.u32);
	ctx.f3.f64 = double(temp.f32);
	// lfsx f25,r14,r20
	temp.u32 = PPC_LOAD_U32(ctx.r14.u32 + ctx.r20.u32);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f24,f3,f0
	ctx.f24.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f25,f25,f0
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// rlwinm r16,r16,2,21,29
	ctx.r16.u64 = rotl64(ctx.r16.u32 | (ctx.r16.u64 << 32), 2) & 0x7FC;
	// rlwinm r15,r10,2,21,29
	ctx.r15.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x7FC;
	// lfs f9,0(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lfsu f8,4(r18)
	ea = 4 + ctx.r18.u32;
	temp.u32 = PPC_LOAD_U32(ea);
	ctx.r18.u32 = ea;
	ctx.f8.f64 = double(temp.f32);
	// fmuls f4,f4,f5
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// lfs f3,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// ld r11,280(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 280);
	// addi r18,r18,4
	ctx.r18.s64 = ctx.r18.s64 + 4;
	// fmuls f3,f3,f5
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// lfsx f23,r16,r20
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + ctx.r20.u32);
	ctx.f23.f64 = double(temp.f32);
	// lfsx f22,r15,r20
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + ctx.r20.u32);
	ctx.f22.f64 = double(temp.f32);
	// stw r18,620(r1)
	PPC_STORE_U32(ctx.r1.u32 + 620, ctx.r18.u32);
	// fadds f21,f10,f12
	ctx.f21.f64 = double(float(ctx.f10.f64 + ctx.f12.f64));
	// fmadds f24,f23,f2,f24
	ctx.f24.f64 = double(std::fma(float(ctx.f23.f64), float(ctx.f2.f64), float(ctx.f24.f64)));
	// ld r10,296(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 296);
	// fmadds f2,f22,f2,f25
	ctx.f2.f64 = double(std::fma(float(ctx.f22.f64), float(ctx.f2.f64), float(ctx.f25.f64)));
	// stfs f2,2088(r20)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r20.u32 + 2088, temp.u32);
	// stfs f24,2084(r20)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r20.u32 + 2084, temp.u32);
	// fmuls f2,f21,f31
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f31.f64));
	// ble cr6,0x822e2c1c
	if (!ctx.cr6.gt) goto loc_822E2C1C;
	// fsubs f0,f0,f26
	ctx.f0.f64 = static_cast<float>(ctx.f0.f64 - ctx.f26.f64);
	// b 0x822e2c40
	goto loc_822E2C40;
loc_822E2C1C:
	// lwz r18,2072(r20)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2072);
	// lwz r17,2064(r20)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2064);
	// cmplw cr6,r18,r17
	ctx.cr6.compare<uint32_t>(ctx.r18.u32, ctx.r17.u32, ctx.xer);
	// beq cr6,0x822e2c34
	if (ctx.cr6.eq) goto loc_822E2C34;
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f1.f64;
	// b 0x822e2c38
	goto loc_822E2C38;
loc_822E2C34:
	// fmr f0,f11
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f11.f64;
loc_822E2C38:
	// stw r17,2068(r20)
	PPC_STORE_U32(ctx.r20.u32 + 2068, ctx.r17.u32);
	// stw r18,2064(r20)
	PPC_STORE_U32(ctx.r20.u32 + 2064, ctx.r18.u32);
loc_822E2C40:
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,2080(r20)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r20.u32 + 2080, temp.u32);
	// lwz r18,224(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// lwz r17,232(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// lwz r16,240(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	// lwz r15,248(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// stfsx f7,r19,r20
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r19.u32 + ctx.r20.u32, temp.u32);
	// lwz r19,2048(r20)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2048);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// clrlwi r19,r19,23
	ctx.r19.u64 = ctx.r19.u32 & 0x1FF;
	// rlwinm r14,r19,2,0,29
	ctx.r14.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r19,2048(r20)
	PPC_STORE_U32(ctx.r20.u32 + 2048, ctx.r19.u32);
	// stfsx f6,r14,r20
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r14.u32 + ctx.r20.u32, temp.u32);
	// lwz r19,2048(r20)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r20.u32 + 2048);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// clrlwi r19,r19,23
	ctx.r19.u64 = ctx.r19.u32 & 0x1FF;
	// stw r19,2048(r20)
	PPC_STORE_U32(ctx.r20.u32 + 2048, ctx.r19.u32);
	// lwz r19,2052(r26)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2052);
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r14,2048(r26)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2048);
	// lfsx f25,r19,r26
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r26.u32);
	ctx.f25.f64 = double(temp.f32);
	// lfs f0,2060(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 2060);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r14,r14,2,0,29
	ctx.r14.u64 = rotl64(ctx.r14.u32 | (ctx.r14.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f7,0(r18)
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,2064(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 2064);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f0,f25,f0,f12
	ctx.f0.f64 = double(std::fma(float(ctx.f25.f64), float(ctx.f0.f64), float(ctx.f12.f64)));
	// stfsx f0,r14,r26
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r14.u32 + ctx.r26.u32, temp.u32);
	// fmadds f12,f0,f6,f25
	ctx.f12.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f6.f64), float(ctx.f25.f64)));
	// stfs f12,2068(r26)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2068, temp.u32);
	// lwz r19,2052(r26)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2052);
	// lwz r18,2048(r26)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2048);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// addi r18,r18,1
	ctx.r18.s64 = ctx.r18.s64 + 1;
	// clrlwi r19,r19,23
	ctx.r19.u64 = ctx.r19.u32 & 0x1FF;
	// clrlwi r18,r18,23
	ctx.r18.u64 = ctx.r18.u32 & 0x1FF;
	// stw r19,2052(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2052, ctx.r19.u32);
	// stw r18,2048(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2048, ctx.r18.u32);
	// lwz r19,2052(r25)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2052);
	// lfs f6,2060(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 2060);
	ctx.f6.f64 = double(temp.f32);
	// lwz r18,2048(r25)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2048);
	// lfs f0,0(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f12,2064(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 2064);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f25,r19,r25
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r25.u32);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f7,f25,f6,f7
	ctx.f7.f64 = double(std::fma(float(ctx.f25.f64), float(ctx.f6.f64), float(ctx.f7.f64)));
	// stfsx f7,r18,r25
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r18.u32 + ctx.r25.u32, temp.u32);
	// lwz r19,2052(r25)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2052);
	// fmadds f6,f12,f7,f25
	ctx.f6.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f7.f64), float(ctx.f25.f64)));
	// lwz r18,2048(r25)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2048);
	// stfs f6,2068(r25)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2068, temp.u32);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// addi r18,r18,1
	ctx.r18.s64 = ctx.r18.s64 + 1;
	// clrlwi r19,r19,23
	ctx.r19.u64 = ctx.r19.u32 & 0x1FF;
	// clrlwi r18,r18,23
	ctx.r18.u64 = ctx.r18.u32 & 0x1FF;
	// stw r19,2052(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2052, ctx.r19.u32);
	// stw r18,2048(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2048, ctx.r18.u32);
	// lfs f12,2060(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 2060);
	ctx.f12.f64 = double(temp.f32);
	// lwz r19,2052(r24)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2052);
	// lwz r18,2048(r24)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2048);
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f25,r19,r24
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r24.u32);
	ctx.f25.f64 = double(temp.f32);
	// lfs f7,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f12,f12,f25,f10
	ctx.f12.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f25.f64), float(ctx.f10.f64)));
	// lfs f6,2064(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 2064);
	ctx.f6.f64 = double(temp.f32);
	// stfsx f12,r18,r24
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r18.u32 + ctx.r24.u32, temp.u32);
	// lwz r19,2052(r24)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2052);
	// lwz r18,2048(r24)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r24.u32 + 2048);
	// addi r18,r18,1
	ctx.r18.s64 = ctx.r18.s64 + 1;
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// fmadds f10,f6,f12,f25
	ctx.f10.f64 = double(std::fma(float(ctx.f6.f64), float(ctx.f12.f64), float(ctx.f25.f64)));
	// clrlwi r18,r18,23
	ctx.r18.u64 = ctx.r18.u32 & 0x1FF;
	// stfs f10,2068(r24)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r24.u32 + 2068, temp.u32);
	// clrlwi r19,r19,23
	ctx.r19.u64 = ctx.r19.u32 & 0x1FF;
	// stw r18,2048(r24)
	PPC_STORE_U32(ctx.r24.u32 + 2048, ctx.r18.u32);
	// stw r19,2052(r24)
	PPC_STORE_U32(ctx.r24.u32 + 2052, ctx.r19.u32);
	// lfs f6,2064(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 2064);
	ctx.f6.f64 = double(temp.f32);
	// lwz r19,2052(r23)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r23.u32 + 2052);
	// lfs f12,0(r15)
	temp.u32 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r19,r19,2,0,29
	ctx.r19.u64 = rotl64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r18,2048(r23)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r23.u32 + 2048);
	// lfsx f25,r19,r23
	temp.u32 = PPC_LOAD_U32(ctx.r19.u32 + ctx.r23.u32);
	ctx.f25.f64 = double(temp.f32);
	// lfs f10,2060(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 2060);
	ctx.f10.f64 = double(temp.f32);
	// fadds f24,f8,f9
	ctx.f24.f64 = double(float(ctx.f8.f64 + ctx.f9.f64));
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// fmadds f7,f10,f25,f7
	ctx.f7.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f25.f64), float(ctx.f7.f64)));
	// fsubs f10,f1,f13
	ctx.f10.f64 = static_cast<float>(ctx.f1.f64 - ctx.f13.f64);
	// lwz r17,256(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r16,264(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// fmuls f4,f4,f13
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// fmuls f3,f3,f13
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// lwz r19,80(r1)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stfsx f7,r18,r23
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r18.u32 + ctx.r23.u32, temp.u32);
	// addi r19,r19,8
	ctx.r19.s64 = ctx.r19.s64 + 8;
	// stw r19,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r19.u32);
	// fmadds f7,f7,f6,f25
	ctx.f7.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f6.f64), float(ctx.f25.f64)));
	// stfs f7,2068(r23)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r23.u32 + 2068, temp.u32);
	// fmuls f6,f10,f9
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f9.f64));
	// fmuls f9,f10,f8
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// fmuls f8,f24,f10
	ctx.f8.f64 = double(float(ctx.f24.f64 * ctx.f10.f64));
	// fmadds f7,f0,f13,f6
	ctx.f7.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f13.f64), float(ctx.f6.f64)));
	// stfs f7,0(r27)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// fmadds f6,f12,f13,f9
	ctx.f6.f64 = double(std::fma(float(ctx.f12.f64), float(ctx.f13.f64), float(ctx.f9.f64)));
	// stfsu f6,4(r27)
	temp.f32 = float(ctx.f6.f64);
	ea = 4 + ctx.r27.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r27.u32 = ea;
	// lwz r18,2048(r23)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r23.u32 + 2048);
	// addi r15,r18,1
	ctx.r15.s64 = ctx.r18.s64 + 1;
	// lwz r18,2052(r23)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r23.u32 + 2052);
	// addi r18,r18,1
	ctx.r18.s64 = ctx.r18.s64 + 1;
	// clrlwi r15,r15,23
	ctx.r15.u64 = ctx.r15.u32 & 0x1FF;
	// clrlwi r18,r18,23
	ctx.r18.u64 = ctx.r18.u32 & 0x1FF;
	// stw r15,2048(r23)
	PPC_STORE_U32(ctx.r23.u32 + 2048, ctx.r15.u32);
	// stw r18,2052(r23)
	PPC_STORE_U32(ctx.r23.u32 + 2052, ctx.r18.u32);
	// lwz r18,2052(r22)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2052);
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r15,2048(r22)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2048);
	// lfs f12,0(r17)
	temp.u32 = PPC_LOAD_U32(ctx.r17.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfsx f9,r18,r22
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r22.u32);
	ctx.f9.f64 = double(temp.f32);
	// rlwinm r17,r15,2,0,29
	ctx.r17.u64 = rotl64(ctx.r15.u32 | (ctx.r15.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f0,2060(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 2060);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,2064(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 2064);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f7,f0,f9,f2
	ctx.f7.f64 = double(std::fma(float(ctx.f0.f64), float(ctx.f9.f64), float(ctx.f2.f64)));
	// stfsx f7,r17,r22
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r22.u32, temp.u32);
	// fmadds f6,f10,f7,f9
	ctx.f6.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f7.f64), float(ctx.f9.f64)));
	// stfs f6,2068(r22)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r22.u32 + 2068, temp.u32);
	// lwz r17,2048(r22)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2048);
	// lwz r18,2052(r22)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r22.u32 + 2052);
	// addi r17,r17,1
	ctx.r17.s64 = ctx.r17.s64 + 1;
	// addi r18,r18,1
	ctx.r18.s64 = ctx.r18.s64 + 1;
	// clrlwi r17,r17,23
	ctx.r17.u64 = ctx.r17.u32 & 0x1FF;
	// clrlwi r18,r18,23
	ctx.r18.u64 = ctx.r18.u32 & 0x1FF;
	// stw r17,2048(r22)
	PPC_STORE_U32(ctx.r22.u32 + 2048, ctx.r17.u32);
	// stw r18,2052(r22)
	PPC_STORE_U32(ctx.r22.u32 + 2052, ctx.r18.u32);
	// lwz r18,2052(r21)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r21.u32 + 2052);
	// lfs f2,2060(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 2060);
	ctx.f2.f64 = double(temp.f32);
	// lwz r17,2048(r21)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r21.u32 + 2048);
	// lfs f0,0(r16)
	temp.u32 = PPC_LOAD_U32(ctx.r16.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r18,r18,2,0,29
	ctx.r18.u64 = rotl64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f10,2064(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 2064);
	ctx.f10.f64 = double(temp.f32);
	// rlwinm r17,r17,2,0,29
	ctx.r17.u64 = rotl64(ctx.r17.u32 | (ctx.r17.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f9,f0,f13
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fadds f13,f13,f27
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f27.f64));
	// lfsx f7,r18,r21
	temp.u32 = PPC_LOAD_U32(ctx.r18.u32 + ctx.r21.u32);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f7,f2,f12
	ctx.f6.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f2.f64), float(ctx.f12.f64)));
	// stfsx f6,r17,r21
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r17.u32 + ctx.r21.u32, temp.u32);
	// fmadds f2,f8,f31,f9
	ctx.f2.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f31.f64), float(ctx.f9.f64)));
	// stfsu f2,4(r27)
	temp.f32 = float(ctx.f2.f64);
	ea = 4 + ctx.r27.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r27.u32 = ea;
	// stfsu f11,4(r27)
	temp.f32 = float(ctx.f11.f64);
	ea = 4 + ctx.r27.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r27.u32 = ea;
	// stfsu f4,4(r27)
	temp.f32 = float(ctx.f4.f64);
	ea = 4 + ctx.r27.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r27.u32 = ea;
	// stfsu f3,4(r27)
	temp.f32 = float(ctx.f3.f64);
	ea = 4 + ctx.r27.u32;
	PPC_STORE_U32(ea, temp.u32);
	ctx.r27.u32 = ea;
	// addi r27,r27,4
	ctx.r27.s64 = ctx.r27.s64 + 4;
	// fmadds f0,f10,f6,f7
	ctx.f0.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f6.f64), float(ctx.f7.f64)));
	// stfs f0,2068(r21)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r21.u32 + 2068, temp.u32);
	// stw r27,628(r1)
	PPC_STORE_U32(ctx.r1.u32 + 628, ctx.r27.u32);
	// lwz r18,2052(r21)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r21.u32 + 2052);
	// lwz r17,2048(r21)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r21.u32 + 2048);
	// addi r18,r18,1
	ctx.r18.s64 = ctx.r18.s64 + 1;
	// addi r17,r17,1
	ctx.r17.s64 = ctx.r17.s64 + 1;
	// clrlwi r18,r18,23
	ctx.r18.u64 = ctx.r18.u32 & 0x1FF;
	// clrlwi r17,r17,23
	ctx.r17.u64 = ctx.r17.u32 & 0x1FF;
	// stw r18,2052(r21)
	PPC_STORE_U32(ctx.r21.u32 + 2052, ctx.r18.u32);
	// stw r17,2048(r21)
	PPC_STORE_U32(ctx.r21.u32 + 2048, ctx.r17.u32);
	// bdnz 0x822e1c60
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822E1C60;
loc_822E2ECC:
	// addi r1,r1,592
	ctx.r1.s64 = ctx.r1.s64 + 592;
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8233fa5c
	ctx.lr = 0x822E2ED8;
	__savefpr_18(ctx, base);
	// b 0x8233e480
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822E2EDC"))) PPC_WEAK_FUNC(sub_822E2EDC);
PPC_FUNC_IMPL(__imp__sub_822E2EDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822E2EE0"))) PPC_WEAK_FUNC(sub_822E2EE0);
PPC_FUNC_IMPL(__imp__sub_822E2EE0) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r5,1
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 1, ctx.xer);
	// bne cr6,0x822e2eec
	if (!ctx.cr6.eq) goto loc_822E2EEC;
	// b 0x822ddf58
	sub_822DDF58(ctx, base);
	return;
loc_822E2EEC:
	// cmplwi cr6,r5,2
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 2, ctx.xer);
	// bne cr6,0x822e2ef8
	if (!ctx.cr6.eq) goto loc_822E2EF8;
	// b 0x822de508
	sub_822DE508(ctx, base);
	return;
loc_822E2EF8:
	// cmplwi cr6,r4,1
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 1, ctx.xer);
	// bne cr6,0x822e2f04
	if (!ctx.cr6.eq) goto loc_822E2F04;
	// b 0x822de4b0
	sub_822DE4B0(ctx, base);
	return;
loc_822E2F04:
	// b 0x822de6e8
	sub_822DE6E8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822E2F08"))) PPC_WEAK_FUNC(sub_822E2F08);
PPC_FUNC_IMPL(__imp__sub_822E2F08) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x822e2f2c
	if (!ctx.cr6.eq) goto loc_822E2F2C;
	// b 0x822de7a0
	sub_822DE7A0(ctx, base);
	return;
loc_822E2F2C:
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bne cr6,0x822e2f38
	if (!ctx.cr6.eq) goto loc_822E2F38;
	// b 0x822e0598
	sub_822E0598(ctx, base);
	return;
loc_822E2F38:
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// bne cr6,0x822e2f44
	if (!ctx.cr6.eq) goto loc_822E2F44;
	// b 0x822df638
	sub_822DF638(ctx, base);
	return;
loc_822E2F44:
	// b 0x822e1918
	sub_822E1918(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822E2F48"))) PPC_WEAK_FUNC(sub_822E2F48);
PPC_FUNC_IMPL(__imp__sub_822E2F48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e45c
	ctx.lr = 0x822E2F50;
	__restfpr_25(ctx, base);
	// addi r12,r1,-64
	ctx.r12.s64 = ctx.r1.s64 + -64;
	// bl 0x8233fa2c
	ctx.lr = 0x822E2F58;
	sub_8233FA2C(ctx, base);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// li r4,40
	ctx.r4.s64 = 40;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lfs f30,1976(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1976);
	ctx.f30.f64 = double(temp.f32);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// bl 0x822ddde8
	ctx.lr = 0x822E2F74;
	sub_822DDDE8(ctx, base);
	// addis r3,r29,1
	ctx.r3.s64 = ctx.r29.s64 + 65536;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// addi r3,r3,48
	ctx.r3.s64 = ctx.r3.s64 + 48;
	// lfs f0,1972(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1972);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,-28948(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// stfs f30,0(r3)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// stfs f31,8(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// stfs f31,12(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f31,16(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// stfs f31,20(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// stfs f31,24(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// stfs f31,28(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// bl 0x822db440
	ctx.lr = 0x822E2FB0;
	sub_822DB440(ctx, base);
	// lis r28,-32255
	ctx.r28.s64 = -2113863680;
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lis r7,1
	ctx.r7.s64 = 65536;
	// addis r26,r29,1
	ctx.r26.s64 = ctx.r29.s64 + 65536;
	// addi r27,r28,-1408
	ctx.r27.s64 = ctx.r28.s64 + -1408;
	// ori r6,r8,80
	ctx.r6.u64 = ctx.r8.u64 | 80;
	// ori r11,r7,84
	ctx.r11.u64 = ctx.r7.u64 | 84;
	// addi r26,r26,96
	ctx.r26.s64 = ctx.r26.s64 + 96;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f0,44(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 44);
	ctx.f0.f64 = double(temp.f32);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lfs f13,48(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// stfsx f0,r29,r6
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r29.u32 + ctx.r6.u32, temp.u32);
	// stfsx f13,r29,r11
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r29.u32 + ctx.r11.u32, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E2FF0;
	sub_8233EAF0(ctx, base);
	// addis r25,r29,1
	ctx.r25.s64 = ctx.r29.s64 + 65536;
	// lfs f0,4(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f13,8(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// addi r25,r25,2208
	ctx.r25.s64 = ctx.r25.s64 + 2208;
	// stfs f0,2080(r26)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2080, temp.u32);
	// li r9,509
	ctx.r9.s64 = 509;
	// lfs f30,-1408(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + -1408);
	ctx.f30.f64 = double(temp.f32);
	// li r8,508
	ctx.r8.s64 = 508;
	// lfs f12,12(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// li r31,0
	ctx.r31.s64 = 0;
	// stfs f30,2076(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2076, temp.u32);
	// li r30,1
	ctx.r30.s64 = 1;
	// lfs f0,1968(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1968);
	ctx.f0.f64 = double(temp.f32);
	// li r7,83
	ctx.r7.s64 = 83;
	// stfs f13,2084(r26)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2084, temp.u32);
	// li r5,512
	ctx.r5.s64 = 512;
	// stfs f31,2088(r26)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2088, temp.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stfs f31,2092(r26)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2092, temp.u32);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stfs f31,2096(r26)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2096, temp.u32);
	// stw r31,2048(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2048, ctx.r31.u32);
	// stw r9,2064(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2064, ctx.r9.u32);
	// stw r8,2068(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2068, ctx.r8.u32);
	// stw r30,2072(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2072, ctx.r30.u32);
	// stfs f12,524(r25)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r25.u32 + 524, temp.u32);
	// stfs f0,528(r25)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r25.u32 + 528, temp.u32);
	// stw r7,520(r25)
	PPC_STORE_U32(ctx.r25.u32 + 520, ctx.r7.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3068;
	sub_8233EAF0(ctx, base);
	// lwz r6,520(r25)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r25.u32 + 520);
	// addis r28,r29,1
	ctx.r28.s64 = ctx.r29.s64 + 65536;
	// stfs f31,532(r25)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 532, temp.u32);
	// neg r11,r6
	ctx.r11.s64 = -ctx.r6.s64;
	// stw r31,512(r25)
	PPC_STORE_U32(ctx.r25.u32 + 512, ctx.r31.u32);
	// addi r28,r28,2752
	ctx.r28.s64 = ctx.r28.s64 + 2752;
	// clrlwi r10,r11,25
	ctx.r10.u64 = ctx.r11.u32 & 0x7F;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r10,516(r25)
	PPC_STORE_U32(ctx.r25.u32 + 516, ctx.r10.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822E3098;
	sub_8233EAF0(ctx, base);
	// lfs f0,20(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// addis r25,r29,1
	ctx.r25.s64 = ctx.r29.s64 + 65536;
	// stfs f0,8224(r28)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8224, temp.u32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f0,-1640(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1640);
	ctx.f0.f64 = double(temp.f32);
	// addi r25,r25,11008
	ctx.r25.s64 = ctx.r25.s64 + 11008;
	// stfs f30,8220(r28)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8220, temp.u32);
	// li r7,2039
	ctx.r7.s64 = 2039;
	// stfs f31,8232(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8232, temp.u32);
	// li r6,2038
	ctx.r6.s64 = 2038;
	// stfs f31,8236(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8236, temp.u32);
	// li r4,600
	ctx.r4.s64 = 600;
	// stfs f0,8228(r28)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8228, temp.u32);
	// lis r26,-32256
	ctx.r26.s64 = -2113929216;
	// stfs f31,8240(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8240, temp.u32);
	// li r3,211
	ctx.r3.s64 = 211;
	// stw r4,8216(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8216, ctx.r4.u32);
	// stw r31,8192(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8192, ctx.r31.u32);
	// li r5,1024
	ctx.r5.s64 = 1024;
	// stw r7,8208(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8208, ctx.r7.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r6,8212(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8212, ctx.r6.u32);
	// lfs f29,5276(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5276);
	ctx.f29.f64 = double(temp.f32);
	// stw r3,1032(r25)
	PPC_STORE_U32(ctx.r25.u32 + 1032, ctx.r3.u32);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// lfs f28,5268(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 5268);
	ctx.f28.f64 = double(temp.f32);
	// stfs f28,1036(r25)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r25.u32 + 1036, temp.u32);
	// stfs f29,1040(r25)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r25.u32 + 1040, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3110;
	sub_8233EAF0(ctx, base);
	// lwz r10,1032(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 1032);
	// addis r28,r29,1
	ctx.r28.s64 = ctx.r29.s64 + 65536;
	// li r11,311
	ctx.r11.s64 = 311;
	// stfs f31,1044(r25)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 1044, temp.u32);
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// stw r31,1024(r25)
	PPC_STORE_U32(ctx.r25.u32 + 1024, ctx.r31.u32);
	// addi r28,r28,12064
	ctx.r28.s64 = ctx.r28.s64 + 12064;
	// clrlwi r8,r9,24
	ctx.r8.u64 = ctx.r9.u32 & 0xFF;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r8,1028(r25)
	PPC_STORE_U32(ctx.r25.u32 + 1028, ctx.r8.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f28,2060(r28)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2060, temp.u32);
	// stw r11,2056(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2056, ctx.r11.u32);
	// stfs f29,2064(r28)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2064, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3150;
	sub_8233EAF0(ctx, base);
	// lwz r6,2056(r28)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// addis r25,r29,1
	ctx.r25.s64 = ctx.r29.s64 + 65536;
	// li r7,1020
	ctx.r7.s64 = 1020;
	// lfs f0,36(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// neg r3,r6
	ctx.r3.s64 = -ctx.r6.s64;
	// lfs f13,40(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// addi r25,r25,14144
	ctx.r25.s64 = ctx.r25.s64 + 14144;
	// stfs f31,2068(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// clrlwi r11,r3,23
	ctx.r11.u64 = ctx.r3.u32 & 0x1FF;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// li r5,4096
	ctx.r5.s64 = 4096;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r11.u32);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stfs f0,4108(r25)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r25.u32 + 4108, temp.u32);
	// stw r7,4100(r25)
	PPC_STORE_U32(ctx.r25.u32 + 4100, ctx.r7.u32);
	// stfs f13,4112(r25)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r25.u32 + 4112, temp.u32);
	// stw r30,4104(r25)
	PPC_STORE_U32(ctx.r25.u32 + 4104, ctx.r30.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E319C;
	sub_8233EAF0(ctx, base);
	// addis r28,r29,1
	ctx.r28.s64 = ctx.r29.s64 + 65536;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f0,76(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 76);
	ctx.f0.f64 = double(temp.f32);
	// addi r28,r28,18272
	ctx.r28.s64 = ctx.r28.s64 + 18272;
	// stfs f31,4116(r25)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 4116, temp.u32);
	// li r9,409
	ctx.r9.s64 = 409;
	// stfs f31,4120(r25)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 4120, temp.u32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stw r31,4096(r25)
	PPC_STORE_U32(ctx.r25.u32 + 4096, ctx.r31.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lfs f30,1964(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1964);
	ctx.f30.f64 = double(temp.f32);
	// stfs f0,2060(r28)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2060, temp.u32);
	// stw r9,2056(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2056, ctx.r9.u32);
	// stfs f30,2064(r28)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2064, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E31DC;
	sub_8233EAF0(ctx, base);
	// addis r25,r29,1
	ctx.r25.s64 = ctx.r29.s64 + 65536;
	// addi r25,r25,20352
	ctx.r25.s64 = ctx.r25.s64 + 20352;
	// lwz r7,2056(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// li r8,257
	ctx.r8.s64 = 257;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stfs f31,2068(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// neg r6,r7
	ctx.r6.s64 = -ctx.r7.s64;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// clrlwi r11,r6,23
	ctx.r11.u64 = ctx.r6.u32 & 0x1FF;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stw r11,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r11.u32);
	// stfs f28,2060(r25)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2060, temp.u32);
	// stfs f29,2064(r25)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2064, temp.u32);
	// stw r8,2056(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2056, ctx.r8.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E321C;
	sub_8233EAF0(ctx, base);
	// lwz r5,2056(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2056);
	// addis r10,r29,1
	ctx.r10.s64 = ctx.r29.s64 + 65536;
	// stfs f31,2068(r25)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2068, temp.u32);
	// addis r9,r29,1
	ctx.r9.s64 = ctx.r29.s64 + 65536;
	// stw r31,2048(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2048, ctx.r31.u32);
	// neg r4,r5
	ctx.r4.s64 = -ctx.r5.s64;
	// lfs f0,52(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 52);
	ctx.f0.f64 = double(temp.f32);
	// addis r8,r29,1
	ctx.r8.s64 = ctx.r29.s64 + 65536;
	// lfs f13,56(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 56);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,22432
	ctx.r10.s64 = ctx.r10.s64 + 22432;
	// lfs f12,60(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 60);
	ctx.f12.f64 = double(temp.f32);
	// addi r9,r9,22452
	ctx.r9.s64 = ctx.r9.s64 + 22452;
	// lfs f11,64(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 64);
	ctx.f11.f64 = double(temp.f32);
	// addi r8,r8,22480
	ctx.r8.s64 = ctx.r8.s64 + 22480;
	// lfs f10,68(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 68);
	ctx.f10.f64 = double(temp.f32);
	// clrlwi r3,r4,23
	ctx.r3.u64 = ctx.r4.u32 & 0x1FF;
	// lfs f9,72(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 72);
	ctx.f9.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// stw r3,2052(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2052, ctx.r3.u32);
	// stfs f0,12(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfs f31,0(r10)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stw r30,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r30.u32);
	// stfs f31,4(r10)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// stfs f31,16(r10)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// stw r30,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r30.u32);
	// stfs f31,0(r9)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f31,4(r9)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// stfs f13,12(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
	// stfs f12,16(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 16, temp.u32);
	// stfs f11,20(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 20, temp.u32);
	// stfs f31,24(r9)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + 24, temp.u32);
	// stw r30,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r30.u32);
	// stfs f31,0(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f31,4(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// stfs f10,12(r8)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r8.u32 + 12, temp.u32);
	// stfs f9,16(r8)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r8.u32 + 16, temp.u32);
	// stfs f31,20(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 20, temp.u32);
	// lfd f28,9016(r7)
	ctx.f28.u64 = PPC_LOAD_U64(ctx.r7.u32 + 9016);
	// lfd f27,-17064(r6)
	ctx.f27.u64 = PPC_LOAD_U64(ctx.r6.u32 + -17064);
	// fmr f2,f28
	ctx.f2.f64 = ctx.f28.f64;
	// fmr f1,f27
	ctx.f1.f64 = ctx.f27.f64;
	// bl 0x8233c318
	ctx.lr = 0x822E32C8;
	sub_8233C318(ctx, base);
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// addis r28,r29,1
	ctx.r28.s64 = ctx.r29.s64 + 65536;
	// lfs f0,84(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r28,r28,22512
	ctx.r28.s64 = ctx.r28.s64 + 22512;
	// li r11,3432
	ctx.r11.s64 = 3432;
	// li r5,16384
	ctx.r5.s64 = 16384;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lfs f26,5256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5256);
	ctx.f26.f64 = double(temp.f32);
	// stfs f26,16396(r28)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16396, temp.u32);
	// stw r11,16388(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16388, ctx.r11.u32);
	// stw r11,16392(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16392, ctx.r11.u32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f12,16400(r28)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16400, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3308;
	sub_8233EAF0(ctx, base);
	// addis r9,r29,2
	ctx.r9.s64 = ctx.r29.s64 + 131072;
	// addis r25,r29,2
	ctx.r25.s64 = ctx.r29.s64 + 131072;
	// lfs f0,20(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// stfs f31,16404(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16404, temp.u32);
	// addi r9,r9,-26608
	ctx.r9.s64 = ctx.r9.s64 + -26608;
	// stfs f31,16408(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16408, temp.u32);
	// stw r31,16384(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16384, ctx.r31.u32);
	// addi r25,r25,-26576
	ctx.r25.s64 = ctx.r25.s64 + -26576;
	// li r8,383
	ctx.r8.s64 = 383;
	// lfs f13,76(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 76);
	ctx.f13.f64 = double(temp.f32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stfs f31,0(r9)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stfs f31,4(r9)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stfs f0,12(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
	// stfs f31,16(r9)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + 16, temp.u32);
	// stw r30,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r30.u32);
	// stfs f13,2060(r25)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2060, temp.u32);
	// stw r8,2056(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2056, ctx.r8.u32);
	// stfs f30,2064(r25)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2064, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3360;
	sub_8233EAF0(ctx, base);
	// lwz r6,2056(r25)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r25.u32 + 2056);
	// addis r28,r29,2
	ctx.r28.s64 = ctx.r29.s64 + 131072;
	// li r7,233
	ctx.r7.s64 = 233;
	// lfs f0,5268(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 5268);
	ctx.f0.f64 = double(temp.f32);
	// neg r3,r6
	ctx.r3.s64 = -ctx.r6.s64;
	// stfs f31,2068(r25)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 2068, temp.u32);
	// addi r28,r28,-24496
	ctx.r28.s64 = ctx.r28.s64 + -24496;
	// stw r31,2048(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2048, ctx.r31.u32);
	// clrlwi r11,r3,23
	ctx.r11.u64 = ctx.r3.u32 & 0x1FF;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,2052(r25)
	PPC_STORE_U32(ctx.r25.u32 + 2052, ctx.r11.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f0,2060(r28)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2060, temp.u32);
	// stw r7,2056(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2056, ctx.r7.u32);
	// stfs f29,2064(r28)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2064, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E33A4;
	sub_8233EAF0(ctx, base);
	// lwz r7,2056(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// addis r10,r29,2
	ctx.r10.s64 = ctx.r29.s64 + 131072;
	// addis r9,r29,2
	ctx.r9.s64 = ctx.r29.s64 + 131072;
	// stfs f31,2068(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// neg r6,r7
	ctx.r6.s64 = -ctx.r7.s64;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// addi r10,r10,-22416
	ctx.r10.s64 = ctx.r10.s64 + -22416;
	// lfs f0,52(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 52);
	ctx.f0.f64 = double(temp.f32);
	// addis r8,r29,2
	ctx.r8.s64 = ctx.r29.s64 + 131072;
	// lfs f13,60(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 60);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r9,-22396
	ctx.r9.s64 = ctx.r9.s64 + -22396;
	// lfs f12,64(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 64);
	ctx.f12.f64 = double(temp.f32);
	// clrlwi r5,r6,23
	ctx.r5.u64 = ctx.r6.u32 & 0x1FF;
	// fmr f2,f28
	ctx.f2.f64 = ctx.f28.f64;
	// addi r8,r8,-22368
	ctx.r8.s64 = ctx.r8.s64 + -22368;
	// fmr f1,f27
	ctx.f1.f64 = ctx.f27.f64;
	// stw r5,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r5.u32);
	// stfs f0,12(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfs f31,0(r10)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stw r30,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r30.u32);
	// stfs f31,4(r10)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// stfs f31,16(r10)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// stw r30,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r30.u32);
	// lfs f0,56(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 56);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,12(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
	// stfs f13,16(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 16, temp.u32);
	// stfs f31,0(r9)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f31,4(r9)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// stfs f12,20(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 20, temp.u32);
	// stfs f31,24(r9)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + 24, temp.u32);
	// stw r30,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r30.u32);
	// lfs f0,68(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 68);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,72(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 72);
	ctx.f13.f64 = double(temp.f32);
	// stfs f31,0(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f31,4(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// stfs f0,12(r8)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 12, temp.u32);
	// stfs f13,16(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 16, temp.u32);
	// stfs f31,20(r8)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r8.u32 + 20, temp.u32);
	// bl 0x8233c318
	ctx.lr = 0x822E3440;
	sub_8233C318(ctx, base);
	// frsp f11,f1
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// addis r28,r29,2
	ctx.r28.s64 = ctx.r29.s64 + 131072;
	// lfs f0,84(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// li r11,3820
	ctx.r11.s64 = 3820;
	// addi r28,r28,-22336
	ctx.r28.s64 = ctx.r28.s64 + -22336;
	// li r5,16384
	ctx.r5.s64 = 16384;
	// li r4,0
	ctx.r4.s64 = 0;
	// stfs f26,16396(r28)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16396, temp.u32);
	// stw r11,16388(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16388, ctx.r11.u32);
	// stw r11,16392(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16392, ctx.r11.u32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f10,16400(r28)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16400, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3478;
	sub_8233EAF0(ctx, base);
	// addis r11,r29,2
	ctx.r11.s64 = ctx.r29.s64 + 131072;
	// addis r25,r29,2
	ctx.r25.s64 = ctx.r29.s64 + 131072;
	// stfs f31,16404(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16404, temp.u32);
	// addi r11,r11,-5920
	ctx.r11.s64 = ctx.r11.s64 + -5920;
	// stfs f31,16408(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16408, temp.u32);
	// addi r25,r25,-5888
	ctx.r25.s64 = ctx.r25.s64 + -5888;
	// stw r31,16384(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16384, ctx.r31.u32);
	// li r10,1511
	ctx.r10.s64 = 1511;
	// lfs f0,20(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// li r5,8192
	ctx.r5.s64 = 8192;
	// lfs f25,76(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 76);
	ctx.f25.f64 = double(temp.f32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stfs f31,0(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfs f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r30,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r30.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f31,16(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stw r10,8200(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8200, ctx.r10.u32);
	// stfs f25,8204(r25)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8204, temp.u32);
	// stfs f30,8208(r25)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8208, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E34D0;
	sub_8233EAF0(ctx, base);
	// lwz r8,8200(r25)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8200);
	// addis r28,r29,2
	ctx.r28.s64 = ctx.r29.s64 + 131072;
	// li r9,1061
	ctx.r9.s64 = 1061;
	// stfs f31,8212(r25)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8212, temp.u32);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// stw r31,8192(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8192, ctx.r31.u32);
	// addi r28,r28,2336
	ctx.r28.s64 = ctx.r28.s64 + 2336;
	// clrlwi r6,r7,21
	ctx.r6.u64 = ctx.r7.u32 & 0x7FF;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r6,8196(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8196, ctx.r6.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f25,8204(r28)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8204, temp.u32);
	// stw r9,8200(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8200, ctx.r9.u32);
	// stfs f25,8208(r28)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8208, temp.u32);
	// stfs f30,8212(r28)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8212, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3514;
	sub_8233EAF0(ctx, base);
	// lwz r10,8200(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8200);
	// addis r25,r29,2
	ctx.r25.s64 = ctx.r29.s64 + 131072;
	// li r11,853
	ctx.r11.s64 = 853;
	// stfs f31,8216(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8216, temp.u32);
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// stw r31,8192(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8192, ctx.r31.u32);
	// addi r25,r25,10560
	ctx.r25.s64 = ctx.r25.s64 + 10560;
	// clrlwi r8,r9,21
	ctx.r8.u64 = ctx.r9.u32 & 0x7FF;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r8,8196(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8196, ctx.r8.u32);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stfs f25,8204(r25)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8204, temp.u32);
	// stw r11,8200(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8200, ctx.r11.u32);
	// stfs f30,8208(r25)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8208, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3554;
	sub_8233EAF0(ctx, base);
	// lwz r6,8200(r25)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8200);
	// addis r28,r29,2
	ctx.r28.s64 = ctx.r29.s64 + 131072;
	// li r7,541
	ctx.r7.s64 = 541;
	// lfs f0,5268(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 5268);
	ctx.f0.f64 = double(temp.f32);
	// neg r3,r6
	ctx.r3.s64 = -ctx.r6.s64;
	// stfs f31,8212(r25)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8212, temp.u32);
	// addi r28,r28,18784
	ctx.r28.s64 = ctx.r28.s64 + 18784;
	// stw r31,8192(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8192, ctx.r31.u32);
	// clrlwi r11,r3,21
	ctx.r11.u64 = ctx.r3.u32 & 0x7FF;
	// li r5,4096
	ctx.r5.s64 = 4096;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,8196(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8196, ctx.r11.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f0,4108(r28)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r28.u32 + 4108, temp.u32);
	// stw r7,4104(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4104, ctx.r7.u32);
	// stfs f29,4112(r28)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r28.u32 + 4112, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3598;
	sub_8233EAF0(ctx, base);
	// lwz r10,4104(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4104);
	// stfs f31,4116(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 4116, temp.u32);
	// stw r31,4096(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4096, ctx.r31.u32);
	// fmr f2,f28
	ctx.f2.f64 = ctx.f28.f64;
	// fmr f1,f27
	ctx.f1.f64 = ctx.f27.f64;
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// clrlwi r8,r9,22
	ctx.r8.u64 = ctx.r9.u32 & 0x3FF;
	// stw r8,4100(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4100, ctx.r8.u32);
	// bl 0x8233c318
	ctx.lr = 0x822E35BC;
	sub_8233C318(ctx, base);
	// frsp f9,f1
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f1.f64));
	// addis r28,r29,2
	ctx.r28.s64 = ctx.r29.s64 + 131072;
	// lfs f0,84(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// li r11,1510
	ctx.r11.s64 = 1510;
	// addi r28,r28,22912
	ctx.r28.s64 = ctx.r28.s64 + 22912;
	// li r5,16384
	ctx.r5.s64 = 16384;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f26,16396(r28)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16396, temp.u32);
	// stw r11,16388(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16388, ctx.r11.u32);
	// stw r11,16392(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16392, ctx.r11.u32);
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f8,16400(r28)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16400, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E35F4;
	sub_8233EAF0(ctx, base);
	// addis r6,r29,3
	ctx.r6.s64 = ctx.r29.s64 + 196608;
	// addis r25,r29,3
	ctx.r25.s64 = ctx.r29.s64 + 196608;
	// stfs f31,16404(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16404, temp.u32);
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// stfs f31,16408(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16408, temp.u32);
	// addi r6,r6,-26208
	ctx.r6.s64 = ctx.r6.s64 + -26208;
	// stw r31,16384(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16384, ctx.r31.u32);
	// addi r25,r25,-26176
	ctx.r25.s64 = ctx.r25.s64 + -26176;
	// li r11,1657
	ctx.r11.s64 = 1657;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// lfs f0,-1580(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -1580);
	ctx.f0.f64 = double(temp.f32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stfs f31,0(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// stfs f31,4(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// stw r30,8(r6)
	PPC_STORE_U32(ctx.r6.u32 + 8, ctx.r30.u32);
	// stfs f0,12(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 12, temp.u32);
	// stfs f0,16(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 16, temp.u32);
	// stfs f31,20(r6)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r6.u32 + 20, temp.u32);
	// stw r11,8200(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8200, ctx.r11.u32);
	// stfs f25,8204(r25)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8204, temp.u32);
	// stfs f30,8208(r25)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8208, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3650;
	sub_8233EAF0(ctx, base);
	// lwz r9,8200(r25)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8200);
	// addis r28,r29,3
	ctx.r28.s64 = ctx.r29.s64 + 196608;
	// li r10,1103
	ctx.r10.s64 = 1103;
	// stfs f31,8212(r25)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8212, temp.u32);
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// stw r31,8192(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8192, ctx.r31.u32);
	// addi r28,r28,-17952
	ctx.r28.s64 = ctx.r28.s64 + -17952;
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r7,8196(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8196, ctx.r7.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f25,8204(r28)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8204, temp.u32);
	// stw r10,8200(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8200, ctx.r10.u32);
	// stfs f25,8208(r28)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8208, temp.u32);
	// stfs f30,8212(r28)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8212, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3694;
	sub_8233EAF0(ctx, base);
	// lwz r3,8200(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8200);
	// addis r25,r29,3
	ctx.r25.s64 = ctx.r29.s64 + 196608;
	// li r6,887
	ctx.r6.s64 = 887;
	// stfs f31,8216(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8216, temp.u32);
	// neg r11,r3
	ctx.r11.s64 = -ctx.r3.s64;
	// stw r31,8192(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8192, ctx.r31.u32);
	// addi r25,r25,-9728
	ctx.r25.s64 = ctx.r25.s64 + -9728;
	// clrlwi r10,r11,21
	ctx.r10.u64 = ctx.r11.u32 & 0x7FF;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r10,8196(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8196, ctx.r10.u32);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stfs f25,8204(r25)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8204, temp.u32);
	// stw r6,8200(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8200, ctx.r6.u32);
	// stfs f30,8208(r25)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8208, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E36D4;
	sub_8233EAF0(ctx, base);
	// addis r28,r29,3
	ctx.r28.s64 = ctx.r29.s64 + 196608;
	// lfs f0,5268(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 5268);
	ctx.f0.f64 = double(temp.f32);
	// stfs f31,8212(r25)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 8212, temp.u32);
	// li r9,491
	ctx.r9.s64 = 491;
	// addi r28,r28,-1504
	ctx.r28.s64 = ctx.r28.s64 + -1504;
	// stw r31,8192(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8192, ctx.r31.u32);
	// li r5,4096
	ctx.r5.s64 = 4096;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r8,8200(r25)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8200);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// clrlwi r6,r7,21
	ctx.r6.u64 = ctx.r7.u32 & 0x7FF;
	// stw r6,8196(r25)
	PPC_STORE_U32(ctx.r25.u32 + 8196, ctx.r6.u32);
	// stfs f0,4108(r28)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r28.u32 + 4108, temp.u32);
	// stfs f29,4112(r28)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r28.u32 + 4112, temp.u32);
	// stw r9,4104(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4104, ctx.r9.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3718;
	sub_8233EAF0(ctx, base);
	// lwz r5,4104(r28)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4104);
	// stfs f31,4116(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 4116, temp.u32);
	// stw r31,4096(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4096, ctx.r31.u32);
	// neg r4,r5
	ctx.r4.s64 = -ctx.r5.s64;
	// fmr f2,f28
	ctx.f2.f64 = ctx.f28.f64;
	// fmr f1,f27
	ctx.f1.f64 = ctx.f27.f64;
	// clrlwi r3,r4,22
	ctx.r3.u64 = ctx.r4.u32 & 0x3FF;
	// stw r3,4100(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4100, ctx.r3.u32);
	// bl 0x8233c318
	ctx.lr = 0x822E373C;
	sub_8233C318(ctx, base);
	// frsp f7,f1
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f1.f64));
	// addis r28,r29,3
	ctx.r28.s64 = ctx.r29.s64 + 196608;
	// lfs f0,84(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// li r11,1438
	ctx.r11.s64 = 1438;
	// addi r28,r28,2624
	ctx.r28.s64 = ctx.r28.s64 + 2624;
	// li r5,16384
	ctx.r5.s64 = 16384;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f26,16396(r28)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16396, temp.u32);
	// stw r11,16388(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16388, ctx.r11.u32);
	// stw r11,16392(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16392, ctx.r11.u32);
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f6,16400(r28)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16400, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3774;
	sub_8233EAF0(ctx, base);
	// addis r11,r29,3
	ctx.r11.s64 = ctx.r29.s64 + 196608;
	// addis r26,r29,3
	ctx.r26.s64 = ctx.r29.s64 + 196608;
	// stfs f31,16404(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16404, temp.u32);
	// addi r11,r11,19040
	ctx.r11.s64 = ctx.r11.s64 + 19040;
	// stfs f31,16408(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 16408, temp.u32);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// stw r31,16384(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16384, ctx.r31.u32);
	// addi r26,r26,19072
	ctx.r26.s64 = ctx.r26.s64 + 19072;
	// lfs f0,120(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// li r9,131
	ctx.r9.s64 = 131;
	// lfs f30,124(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 124);
	ctx.f30.f64 = double(temp.f32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stfs f31,0(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lfs f29,1960(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1960);
	ctx.f29.f64 = double(temp.f32);
	// stfs f31,4(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r30,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r30.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f31,20(r11)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stw r9,2056(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2056, ctx.r9.u32);
	// stfs f30,2060(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2060, temp.u32);
	// stfs f29,2064(r26)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2064, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E37D8;
	sub_8233EAF0(ctx, base);
	// lwz r7,2056(r26)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2056);
	// addis r30,r29,3
	ctx.r30.s64 = ctx.r29.s64 + 196608;
	// li r8,113
	ctx.r8.s64 = 113;
	// stfs f31,2068(r26)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2068, temp.u32);
	// neg r6,r7
	ctx.r6.s64 = -ctx.r7.s64;
	// stw r31,2048(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2048, ctx.r31.u32);
	// addi r30,r30,21152
	ctx.r30.s64 = ctx.r30.s64 + 21152;
	// clrlwi r3,r6,23
	ctx.r3.u64 = ctx.r6.u32 & 0x1FF;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stw r3,2052(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2052, ctx.r3.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stfs f30,2060(r30)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r30.u32 + 2060, temp.u32);
	// stw r8,2056(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2056, ctx.r8.u32);
	// stfs f29,2064(r30)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r30.u32 + 2064, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3818;
	sub_8233EAF0(ctx, base);
	// lwz r9,2056(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 2056);
	// lis r11,3
	ctx.r11.s64 = 196608;
	// stfs f31,2068(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r30.u32 + 2068, temp.u32);
	// stw r31,2048(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2048, ctx.r31.u32);
	// ori r10,r11,23232
	ctx.r10.u64 = ctx.r11.u64 | 23232;
	// fmr f2,f28
	ctx.f2.f64 = ctx.f28.f64;
	// fmr f1,f27
	ctx.f1.f64 = ctx.f27.f64;
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// clrlwi r7,r8,23
	ctx.r7.u64 = ctx.r8.u32 & 0x1FF;
	// stw r7,2052(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2052, ctx.r7.u32);
	// stfsx f26,r29,r10
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r29.u32 + ctx.r10.u32, temp.u32);
	// bl 0x8233c318
	ctx.lr = 0x822E3848;
	sub_8233C318(ctx, base);
	// lis r5,3
	ctx.r5.s64 = 196608;
	// lis r6,3
	ctx.r6.s64 = 196608;
	// frsp f5,f1
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = double(float(ctx.f1.f64));
	// ori r3,r5,23240
	ctx.r3.u64 = ctx.r5.u64 | 23240;
	// ori r4,r6,23236
	ctx.r4.u64 = ctx.r6.u64 | 23236;
	// stwx r31,r29,r3
	PPC_STORE_U32(ctx.r29.u32 + ctx.r3.u32, ctx.r31.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stfsx f5,r29,r4
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r29.u32 + ctx.r4.u32, temp.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// addi r12,r1,-64
	ctx.r12.s64 = ctx.r1.s64 + -64;
	// bl 0x8233fa78
	ctx.lr = 0x822E3874;
	__savefpr_25(ctx, base);
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822E3878"))) PPC_WEAK_FUNC(sub_822E3878);
PPC_FUNC_IMPL(__imp__sub_822E3878) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x822e2f48
	ctx.lr = 0x822E3894;
	sub_822E2F48(ctx, base);
	// addis r30,r31,3
	ctx.r30.s64 = ctx.r31.s64 + 196608;
	// li r5,1024
	ctx.r5.s64 = 1024;
	// addi r30,r30,23248
	ctx.r30.s64 = ctx.r30.s64 + 23248;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8233eaf0
	ctx.lr = 0x822E38AC;
	sub_8233EAF0(ctx, base);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// li r11,20
	ctx.r11.s64 = 20;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,1040(r30)
	PPC_STORE_U32(ctx.r30.u32 + 1040, ctx.r11.u32);
	// lfs f13,1976(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1976);
	ctx.f13.f64 = double(temp.f32);
	// stw r8,1024(r30)
	PPC_STORE_U32(ctx.r30.u32 + 1024, ctx.r8.u32);
	// lfs f0,-28948(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28948);
	ctx.f0.f64 = double(temp.f32);
	// stw r11,1044(r30)
	PPC_STORE_U32(ctx.r30.u32 + 1044, ctx.r11.u32);
	// stfs f13,1052(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r30.u32 + 1052, temp.u32);
	// stw r11,1048(r30)
	PPC_STORE_U32(ctx.r30.u32 + 1048, ctx.r11.u32);
	// stfs f0,1056(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 1056, temp.u32);
	// stfs f0,1060(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 1060, temp.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822E38FC"))) PPC_WEAK_FUNC(sub_822E38FC);
PPC_FUNC_IMPL(__imp__sub_822E38FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822E3900"))) PPC_WEAK_FUNC(sub_822E3900);
PPC_FUNC_IMPL(__imp__sub_822E3900) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e45c
	ctx.lr = 0x822E3908;
	__restfpr_25(ctx, base);
	// stfd f29,-88(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.f29.u64);
	// stfd f30,-80(r1)
	PPC_STORE_U64(ctx.r1.u32 + -80, ctx.f30.u64);
	// stfd f31,-72(r1)
	PPC_STORE_U64(ctx.r1.u32 + -72, ctx.f31.u64);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// bl 0x822e2f48
	ctx.lr = 0x822E3920;
	sub_822E2F48(ctx, base);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// addis r3,r29,3
	ctx.r3.s64 = ctx.r29.s64 + 196608;
	// li r4,40
	ctx.r4.s64 = 40;
	// addi r3,r3,23248
	ctx.r3.s64 = ctx.r3.s64 + 23248;
	// lfs f30,1976(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1976);
	ctx.f30.f64 = double(temp.f32);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// bl 0x822dde98
	ctx.lr = 0x822E393C;
	sub_822DDE98(ctx, base);
	// addis r3,r29,5
	ctx.r3.s64 = ctx.r29.s64 + 327680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// addi r3,r3,23296
	ctx.r3.s64 = ctx.r3.s64 + 23296;
	// lfs f0,1972(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1972);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,-28948(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// stfs f30,0(r3)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// stfs f31,8(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// stfs f31,12(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f31,16(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// stfs f31,20(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// stfs f31,24(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// stfs f31,28(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// stfs f31,32(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 32, temp.u32);
	// stfs f31,36(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// bl 0x822db768
	ctx.lr = 0x822E3980;
	sub_822DB768(ctx, base);
	// lis r28,-32255
	ctx.r28.s64 = -2113863680;
	// lis r8,5
	ctx.r8.s64 = 327680;
	// lis r7,5
	ctx.r7.s64 = 327680;
	// addis r26,r29,5
	ctx.r26.s64 = ctx.r29.s64 + 327680;
	// addi r27,r28,-1408
	ctx.r27.s64 = ctx.r28.s64 + -1408;
	// ori r6,r8,23336
	ctx.r6.u64 = ctx.r8.u64 | 23336;
	// ori r11,r7,23340
	ctx.r11.u64 = ctx.r7.u64 | 23340;
	// addi r26,r26,23344
	ctx.r26.s64 = ctx.r26.s64 + 23344;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f0,44(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 44);
	ctx.f0.f64 = double(temp.f32);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lfs f13,48(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// stfsx f0,r29,r6
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r29.u32 + ctx.r6.u32, temp.u32);
	// stfsx f13,r29,r11
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r29.u32 + ctx.r11.u32, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E39C0;
	sub_8233EAF0(ctx, base);
	// addis r25,r29,5
	ctx.r25.s64 = ctx.r29.s64 + 327680;
	// lfs f0,4(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f13,8(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// addi r25,r25,25456
	ctx.r25.s64 = ctx.r25.s64 + 25456;
	// stfs f0,2080(r26)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2080, temp.u32);
	// li r9,509
	ctx.r9.s64 = 509;
	// lfs f30,-1408(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + -1408);
	ctx.f30.f64 = double(temp.f32);
	// li r8,508
	ctx.r8.s64 = 508;
	// lfs f12,12(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// li r31,0
	ctx.r31.s64 = 0;
	// stfs f30,2076(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2076, temp.u32);
	// li r30,1
	ctx.r30.s64 = 1;
	// lfs f0,1968(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1968);
	ctx.f0.f64 = double(temp.f32);
	// li r7,97
	ctx.r7.s64 = 97;
	// stfs f13,2084(r26)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2084, temp.u32);
	// li r5,512
	ctx.r5.s64 = 512;
	// stfs f31,2088(r26)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2088, temp.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stfs f31,2092(r26)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2092, temp.u32);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stfs f31,2096(r26)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2096, temp.u32);
	// stw r31,2048(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2048, ctx.r31.u32);
	// stw r9,2064(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2064, ctx.r9.u32);
	// stw r8,2068(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2068, ctx.r8.u32);
	// stw r30,2072(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2072, ctx.r30.u32);
	// stfs f12,524(r25)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r25.u32 + 524, temp.u32);
	// stfs f0,528(r25)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r25.u32 + 528, temp.u32);
	// stw r7,520(r25)
	PPC_STORE_U32(ctx.r25.u32 + 520, ctx.r7.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3A38;
	sub_8233EAF0(ctx, base);
	// addis r28,r29,5
	ctx.r28.s64 = ctx.r29.s64 + 327680;
	// lwz r6,520(r25)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r25.u32 + 520);
	// stfs f31,532(r25)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r25.u32 + 532, temp.u32);
	// addi r28,r28,26000
	ctx.r28.s64 = ctx.r28.s64 + 26000;
	// stw r31,512(r25)
	PPC_STORE_U32(ctx.r25.u32 + 512, ctx.r31.u32);
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// neg r11,r6
	ctx.r11.s64 = -ctx.r6.s64;
	// clrlwi r10,r11,25
	ctx.r10.u64 = ctx.r11.u32 & 0x7F;
	// stw r10,516(r25)
	PPC_STORE_U32(ctx.r25.u32 + 516, ctx.r10.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3A68;
	sub_8233EAF0(ctx, base);
	// addis r26,r29,6
	ctx.r26.s64 = ctx.r29.s64 + 393216;
	// lfs f0,20(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// stfs f0,8224(r28)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8224, temp.u32);
	// addi r26,r26,-31280
	ctx.r26.s64 = ctx.r26.s64 + -31280;
	// stfs f30,8220(r28)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8220, temp.u32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// stfs f31,8232(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8232, temp.u32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// stfs f31,8236(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8236, temp.u32);
	// li r6,1297
	ctx.r6.s64 = 1297;
	// stfs f31,8240(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8240, temp.u32);
	// li r5,1296
	ctx.r5.s64 = 1296;
	// lfs f0,-1640(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -1640);
	ctx.f0.f64 = double(temp.f32);
	// li r4,600
	ctx.r4.s64 = 600;
	// stfs f0,8228(r28)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r28.u32 + 8228, temp.u32);
	// li r3,223
	ctx.r3.s64 = 223;
	// stw r5,8212(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8212, ctx.r5.u32);
	// stw r4,8216(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8216, ctx.r4.u32);
	// li r5,1024
	ctx.r5.s64 = 1024;
	// stw r31,8192(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8192, ctx.r31.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r6,8208(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8208, ctx.r6.u32);
	// lfs f30,5268(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5268);
	ctx.f30.f64 = double(temp.f32);
	// stw r3,1032(r26)
	PPC_STORE_U32(ctx.r26.u32 + 1032, ctx.r3.u32);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lfs f29,5276(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 5276);
	ctx.f29.f64 = double(temp.f32);
	// stfs f30,1036(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 1036, temp.u32);
	// stfs f29,1040(r26)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r26.u32 + 1040, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3AE0;
	sub_8233EAF0(ctx, base);
	// lwz r10,1032(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1032);
	// addis r28,r29,6
	ctx.r28.s64 = ctx.r29.s64 + 393216;
	// li r11,293
	ctx.r11.s64 = 293;
	// stfs f31,1044(r26)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 1044, temp.u32);
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// stw r31,1024(r26)
	PPC_STORE_U32(ctx.r26.u32 + 1024, ctx.r31.u32);
	// addi r28,r28,-30224
	ctx.r28.s64 = ctx.r28.s64 + -30224;
	// clrlwi r8,r9,24
	ctx.r8.u64 = ctx.r9.u32 & 0xFF;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r8,1028(r26)
	PPC_STORE_U32(ctx.r26.u32 + 1028, ctx.r8.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f30,2060(r28)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2060, temp.u32);
	// stw r11,2056(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2056, ctx.r11.u32);
	// stfs f29,2064(r28)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2064, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3B20;
	sub_8233EAF0(ctx, base);
	// lwz r6,2056(r28)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// addis r26,r29,6
	ctx.r26.s64 = ctx.r29.s64 + 393216;
	// li r7,1020
	ctx.r7.s64 = 1020;
	// lfs f0,36(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// neg r3,r6
	ctx.r3.s64 = -ctx.r6.s64;
	// lfs f13,40(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// addi r26,r26,-28144
	ctx.r26.s64 = ctx.r26.s64 + -28144;
	// stfs f31,2068(r28)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// clrlwi r11,r3,23
	ctx.r11.u64 = ctx.r3.u32 & 0x1FF;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// li r5,4096
	ctx.r5.s64 = 4096;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r11.u32);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// stfs f0,4108(r26)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4108, temp.u32);
	// stw r7,4100(r26)
	PPC_STORE_U32(ctx.r26.u32 + 4100, ctx.r7.u32);
	// stfs f13,4112(r26)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4112, temp.u32);
	// stw r30,4104(r26)
	PPC_STORE_U32(ctx.r26.u32 + 4104, ctx.r30.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3B6C;
	sub_8233EAF0(ctx, base);
	// addis r30,r29,6
	ctx.r30.s64 = ctx.r29.s64 + 393216;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f30,124(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 124);
	ctx.f30.f64 = double(temp.f32);
	// addi r30,r30,-24016
	ctx.r30.s64 = ctx.r30.s64 + -24016;
	// stfs f31,4116(r26)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4116, temp.u32);
	// stfs f31,4120(r26)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 4120, temp.u32);
	// stw r31,4096(r26)
	PPC_STORE_U32(ctx.r26.u32 + 4096, ctx.r31.u32);
	// li r9,107
	ctx.r9.s64 = 107;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// lfs f29,1960(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1960);
	ctx.f29.f64 = double(temp.f32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stfs f30,2060(r30)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r30.u32 + 2060, temp.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stfs f29,2064(r30)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r30.u32 + 2064, temp.u32);
	// stw r9,2056(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2056, ctx.r9.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3BAC;
	sub_8233EAF0(ctx, base);
	// lwz r8,2056(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 2056);
	// addis r28,r29,6
	ctx.r28.s64 = ctx.r29.s64 + 393216;
	// stfs f31,2068(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r30.u32 + 2068, temp.u32);
	// neg r6,r8
	ctx.r6.s64 = -ctx.r8.s64;
	// stw r31,2048(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2048, ctx.r31.u32);
	// addi r28,r28,-21936
	ctx.r28.s64 = ctx.r28.s64 + -21936;
	// clrlwi r3,r6,23
	ctx.r3.u64 = ctx.r6.u32 & 0x1FF;
	// li r7,127
	ctx.r7.s64 = 127;
	// stw r3,2052(r30)
	PPC_STORE_U32(ctx.r30.u32 + 2052, ctx.r3.u32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f30,2060(r28)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2060, temp.u32);
	// stfs f29,2064(r28)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2064, temp.u32);
	// stw r7,2056(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2056, ctx.r7.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3BEC;
	sub_8233EAF0(ctx, base);
	// lwz r11,2056(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// stfs f31,2068(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// clrlwi r9,r10,23
	ctx.r9.u64 = ctx.r10.u32 & 0x1FF;
	// stw r9,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r9.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lfd f29,-88(r1)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// lfd f30,-80(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// lfd f31,-72(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// b 0x8233e4ac
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822E3C1C"))) PPC_WEAK_FUNC(sub_822E3C1C);
PPC_FUNC_IMPL(__imp__sub_822E3C1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822E3C20"))) PPC_WEAK_FUNC(sub_822E3C20);
PPC_FUNC_IMPL(__imp__sub_822E3C20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e468
	ctx.lr = 0x822E3C28;
	__restfpr_28(ctx, base);
	// stfd f29,-64(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.f29.u64);
	// stfd f30,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f30.u64);
	// stfd f31,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// bl 0x822e3900
	ctx.lr = 0x822E3C40;
	sub_822E3900(ctx, base);
	// addis r29,r30,6
	ctx.r29.s64 = ctx.r30.s64 + 393216;
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// addi r29,r29,-19856
	ctx.r29.s64 = ctx.r29.s64 + -19856;
	// li r9,103
	ctx.r9.s64 = 103;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f30,-1284(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -1284);
	ctx.f30.f64 = double(temp.f32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lfs f29,1960(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1960);
	ctx.f29.f64 = double(temp.f32);
	// stfs f30,2060(r29)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2060, temp.u32);
	// stw r9,2056(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2056, ctx.r9.u32);
	// stfs f29,2064(r29)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2064, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3C78;
	sub_8233EAF0(ctx, base);
	// lwz r6,2056(r29)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r29.u32 + 2056);
	// addis r28,r30,6
	ctx.r28.s64 = ctx.r30.s64 + 393216;
	// lis r8,-32249
	ctx.r8.s64 = -2113470464;
	// neg r3,r6
	ctx.r3.s64 = -ctx.r6.s64;
	// addi r28,r28,-17776
	ctx.r28.s64 = ctx.r28.s64 + -17776;
	// clrlwi r11,r3,23
	ctx.r11.u64 = ctx.r3.u32 & 0x1FF;
	// li r31,0
	ctx.r31.s64 = 0;
	// li r7,97
	ctx.r7.s64 = 97;
	// lfs f31,-28948(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -28948);
	ctx.f31.f64 = double(temp.f32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stfs f31,2068(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2068, temp.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r31,2048(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2048, ctx.r31.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stw r11,2052(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2052, ctx.r11.u32);
	// stfs f30,2060(r28)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2060, temp.u32);
	// stw r7,2056(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2056, ctx.r7.u32);
	// stfs f29,2064(r28)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2064, temp.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3CC4;
	sub_8233EAF0(ctx, base);
	// lwz r10,2056(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2056);
	// addis r29,r30,6
	ctx.r29.s64 = ctx.r30.s64 + 393216;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stfs f31,2068(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r28.u32 + 2068, temp.u32);
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// stw r31,2048(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2048, ctx.r31.u32);
	// addi r29,r29,-15696
	ctx.r29.s64 = ctx.r29.s64 + -15696;
	// clrlwi r8,r9,23
	ctx.r8.u64 = ctx.r9.u32 & 0x1FF;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stw r8,2052(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2052, ctx.r8.u32);
	// bl 0x8233eaf0
	ctx.lr = 0x822E3CF4;
	sub_8233EAF0(ctx, base);
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// li r11,40
	ctx.r11.s64 = 40;
	// stfs f31,2080(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2080, temp.u32);
	// stfs f31,2084(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2084, temp.u32);
	// stw r31,2048(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2048, ctx.r31.u32);
	// stfs f31,2088(r29)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2088, temp.u32);
	// stw r11,2064(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2064, ctx.r11.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stw r11,2068(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2068, ctx.r11.u32);
	// lfs f0,1976(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 1976);
	ctx.f0.f64 = double(temp.f32);
	// stw r11,2072(r29)
	PPC_STORE_U32(ctx.r29.u32 + 2072, ctx.r11.u32);
	// stfs f0,2076(r29)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r29.u32 + 2076, temp.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f29,-64(r1)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// lfd f30,-56(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x8233e4b8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822E3D38"))) PPC_WEAK_FUNC(sub_822E3D38);
PPC_FUNC_IMPL(__imp__sub_822E3D38) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r5,1
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 1, ctx.xer);
	// bne cr6,0x822e3d4c
	if (!ctx.cr6.eq) goto loc_822E3D4C;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// b 0x822e2f48
	sub_822E2F48(ctx, base);
	return;
loc_822E3D4C:
	// cmplwi cr6,r5,2
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 2, ctx.xer);
	// bne cr6,0x822e3d60
	if (!ctx.cr6.eq) goto loc_822E3D60;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// b 0x822e3900
	sub_822E3900(ctx, base);
	return;
loc_822E3D60:
	// cmplwi cr6,r4,1
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 1, ctx.xer);
	// bne cr6,0x822e3d74
	if (!ctx.cr6.eq) goto loc_822E3D74;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// b 0x822e3878
	sub_822E3878(ctx, base);
	return;
loc_822E3D74:
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// b 0x822e3c20
	sub_822E3C20(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822E3D80"))) PPC_WEAK_FUNC(sub_822E3D80);
PPC_FUNC_IMPL(__imp__sub_822E3D80) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822E3D84"))) PPC_WEAK_FUNC(sub_822E3D84);
PPC_FUNC_IMPL(__imp__sub_822E3D84) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822E3D88"))) PPC_WEAK_FUNC(sub_822E3D88);
PPC_FUNC_IMPL(__imp__sub_822E3D88) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822E3D90;
	__restfpr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r27,0
	ctx.r27.s64 = 0;
	// lwz r5,528(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 528);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r7,r3,32
	ctx.r7.s64 = ctx.r3.s64 + 32;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
loc_822E3DAC:
	// lwz r9,0(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// b 0x822e3dfc
	goto loc_822E3DFC;
loc_822E3DB4:
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x822e3dcc
	if (ctx.cr6.eq) goto loc_822E3DCC;
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// b 0x822e3dd0
	goto loc_822E3DD0;
loc_822E3DCC:
	// mr r8,r27
	ctx.r8.u64 = ctx.r27.u64;
loc_822E3DD0:
	// lwz r11,556(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 556);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822e3df8
	if (ctx.cr6.eq) goto loc_822E3DF8;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_822E3DE4:
	// lwz r10,56(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 56);
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r11,r11,12
	ctx.r11.s64 = ctx.r11.s64 + 12;
	// stw r27,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r27.u32);
	// bdnz 0x822e3de4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822E3DE4;
loc_822E3DF8:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
loc_822E3DFC:
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x822e3db4
	if (!ctx.cr6.eq) goto loc_822E3DB4;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x822e3e1c
	if (ctx.cr6.eq) goto loc_822E3E1C;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r7,r7,40
	ctx.r7.s64 = ctx.r7.s64 + 40;
	// cmplwi cr6,r6,13
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 13, ctx.xer);
	// blt cr6,0x822e3dac
	if (ctx.cr6.lt) goto loc_822E3DAC;
loc_822E3E1C:
	// lwz r30,540(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 540);
	// lis r11,-32198
	ctx.r11.s64 = -2110128128;
	// lwz r10,536(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 536);
	// lwz r28,552(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 552);
	// addi r29,r11,-6376
	ctx.r29.s64 = ctx.r11.s64 + -6376;
	// cmplw cr6,r30,r10
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x822e3e7c
	if (ctx.cr6.eq) goto loc_822E3E7C;
	// lwz r3,532(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 532);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822e3e4c
	if (ctx.cr6.eq) goto loc_822E3E4C;
	// bl 0x822ffa28
	ctx.lr = 0x822E3E48;
	sub_822FFA28(ctx, base);
	// stw r27,532(r31)
	PPC_STORE_U32(ctx.r31.u32 + 532, ctx.r27.u32);
loc_822E3E4C:
	// lwz r11,556(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 556);
	// lis r5,8343
	ctx.r5.s64 = 546766848;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// li r6,32
	ctx.r6.s64 = 32;
	// mullw r4,r11,r30
	ctx.r4.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r30.s32);
	// ori r5,r5,6
	ctx.r5.u64 = ctx.r5.u64 | 6;
	// bl 0x822b1f58
	ctx.lr = 0x822E3E68;
	sub_822B1F58(ctx, base);
	// stw r3,532(r31)
	PPC_STORE_U32(ctx.r31.u32 + 532, ctx.r3.u32);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq 0x822e3ed4
	if (ctx.cr0.eq) goto loc_822E3ED4;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// stw r30,536(r31)
	PPC_STORE_U32(ctx.r31.u32 + 536, ctx.r30.u32);
loc_822E3E7C:
	// lwz r11,548(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 548);
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x822e3ecc
	if (!ctx.cr6.gt) goto loc_822E3ECC;
	// lwz r3,544(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 544);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822e3e9c
	if (ctx.cr6.eq) goto loc_822E3E9C;
	// bl 0x822ffa28
	ctx.lr = 0x822E3E98;
	sub_822FFA28(ctx, base);
	// stw r27,544(r31)
	PPC_STORE_U32(ctx.r31.u32 + 544, ctx.r27.u32);
loc_822E3E9C:
	// lwz r11,556(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 556);
	// lis r5,8343
	ctx.r5.s64 = 546766848;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// li r6,32
	ctx.r6.s64 = 32;
	// mullw r4,r11,r28
	ctx.r4.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r28.s32);
	// ori r5,r5,6
	ctx.r5.u64 = ctx.r5.u64 | 6;
	// bl 0x822b1f58
	ctx.lr = 0x822E3EB8;
	sub_822B1F58(ctx, base);
	// stw r3,544(r31)
	PPC_STORE_U32(ctx.r31.u32 + 544, ctx.r3.u32);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq 0x822e3ed4
	if (ctx.cr0.eq) goto loc_822E3ED4;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// stw r28,548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 548, ctx.r28.u32);
loc_822E3ECC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
loc_822E3ED4:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x822e3ecc
	goto loc_822E3ECC;
}

__attribute__((alias("__imp__sub_822E3EE0"))) PPC_WEAK_FUNC(sub_822E3EE0);
PPC_FUNC_IMPL(__imp__sub_822E3EE0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8233e464
	ctx.lr = 0x822E3EE8;
	__restfpr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r29,528(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 528);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// li r27,0
	ctx.r27.s64 = 0;
	// addi r28,r3,32
	ctx.r28.s64 = ctx.r3.s64 + 32;
loc_822E3EFC:
	// lwz r31,0(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// b 0x822e3f2c
	goto loc_822E3F2C;
loc_822E3F04:
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x822e3f1c
	if (ctx.cr6.eq) goto loc_822E3F1C;
	// lwz r31,4(r31)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// b 0x822e3f20
	goto loc_822E3F20;
loc_822E3F1C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_822E3F20:
	// lwz r4,556(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 556);
	// bl 0x822fcb70
	ctx.lr = 0x822E3F28;
	sub_822FCB70(ctx, base);
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
loc_822E3F2C:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x822e3f04
	if (!ctx.cr6.eq) goto loc_822E3F04;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x822e3f4c
	if (ctx.cr6.eq) goto loc_822E3F4C;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// addi r28,r28,40
	ctx.r28.s64 = ctx.r28.s64 + 40;
	// cmplwi cr6,r27,13
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 13, ctx.xer);
	// blt cr6,0x822e3efc
	if (ctx.cr6.lt) goto loc_822E3EFC;
loc_822E3F4C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8233e4b4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822E3F54"))) PPC_WEAK_FUNC(sub_822E3F54);
PPC_FUNC_IMPL(__imp__sub_822E3F54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

